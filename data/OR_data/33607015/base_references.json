{
    "base_references": [
        {
            "title": "Visual impairment corrected via cataract surgery and 5-year survival in a prospective cohort.",
            "abstract": "Purpose:\n        \n      \n      To compare mortality risk between cataract surgical patients with corrected and persistent visual impairment.\n    \n\n\n          Design:\n        \n      \n      Cohort study.\n    \n\n\n          Methods:\n        \n      \n      A total of 1864 consecutive patients, aged ≥64 years, undergoing phacoemulsification surgery at Westmead Hospital were followed annually for 5 years postoperatively. Visual impairment status in the surgical eye was categorized as none (presenting visual acuity [VA], ≥20/40), mild (VA <20/40-20/60), or moderate-severe (VA <20/60). All-cause mortality was obtained from the Australian National Death Index.\n    \n\n\n          Results:\n        \n      \n      Of 901 patients with moderate-severe visual impairment before surgery, 60.4% (n = 544), 15.5% (n = 140), and 24.1% (n = 217) had no, mild, or moderate-severe visual impairment in the surgical eye, respectively, 1 month postoperatively. Age-standardized 5-year mortality rates were nonsignificantly lower in patients with either mild (24.7%, 95% confidence interval [CI] 16.5%-32.9%) or no visual impairment (24.1%, 95% CI 19.9%-28.4%) post surgery compared to that in patients whose moderate-severe visual impairment persisted (30.6%, 95% CI 23.3%-37.9%). After adjusting for age, sex, smoking, body mass index, and individual comorbid conditions, such as hypertension, diabetes, angina, myocardial infarction, stroke, transient ischemic attack, and kidney disease, patients with no visual impairment 1 month postoperatively had a lower mortality risk (HR 0.73, 95% CI 0.52-1.01) compared to those with persistent moderate-severe visual impairment after surgery. This finding was significant (HR 0.71, 95% CI 0.51-0.99) after additional adjustment for number of medications taken (continuous variable) and number (≥3 vs <3) of comorbid conditions.\n    \n\n\n          Conclusion:\n        \n      \n      Correcting moderate-severe visual impairment in older patients with phacoemulsification surgery was associated with a lower mortality risk, compared to surgical patients whose visual impairment persisted postoperatively."
        },
        {
            "title": "Prognostic usefulness of contemporary stress echocardiography in patients with left bundle branch block and impact of contrast use in improving prediction of outcome.",
            "abstract": "Aims:\n        \n      \n      Patients with symptomatic left bundle branch block (LBBB) may have myocardial ischaemia due to both coronary artery disease and/or cardiomyopathy (microcirculatory abnormalities) and may have concomitant left ventricular (LV) dysfunction. We aimed to assess the feasibility and prognostic value of contemporary stress echocardiography (SE), which can uncover both pathophysiologies in LBBB patients in routine clinical practice, and also aimed to assess the additive value of contrast SE.\n    \n\n\n          Methods and results:\n        \n      \n      Accordingly, 190 consecutive patients (age 70.5 ± 11.3 years, LV ejection fraction = 50.1 ± 10%) with symptomatic LBBB who underwent SE over 6 years were assessed, of which 142 (75%) underwent contrast SE and 176 (92.6%) had diagnostic SE. Inducible ischaemia was present in 25 (14.2%) patients. During follow-up (35.4 ± 20.2 months) there were 32 deaths (18%) and 18 (10.2%) first cardiovascular (CV) events (acute myocardial infarction/mortality) in the 176 patients with diagnostic studies. Wall thickening score index at peak stress (WTSIpeak), which measures combined LV function and inducible ischaemia, was an independent predictor of mortality (HR = 3.78, 95% CI = 1.39-10.31, P = 0.01) and CV events (HR = 3.96, 95% CI = 1.1-14.3, P = 0.036). An abnormal SE (myocardial ischaemia and/or abnormal LV function) predicted an almost three-fold increase in all-cause mortality and CV events compared with normal SE. Amongst the confounders affecting assessment of wall thickening in LBBB and conventional prognostic variables, use of contrast was an independent predictor (P = 0.034) of WTSI1.16 (optimal predictor of mortality/CV outcome).\n    \n\n\n          Conclusion:\n        \n      \n      SE in patients with LBBB demonstrated high feasibility and the combination of LV systolic function and myocardial ischaemia provided important prognostic information. Contrast-enhanced SE improved the prediction of outcome."
        },
        {
            "title": "Serious renal dysfunction after percutaneous coronary interventions can be predicted.",
            "abstract": "Background:\n        \n      \n      A prediction rule for determining the post-percutaneous coronary intervention (PCI) risk of developing contrast-induced nephropathy (> or = 25% or > or = 0.5 mg/dL increase in creatinine) has been reported. However, little work has been done on predicting pre-PCI patient-specific risk for developing more serious renal dysfunction (SRD; new dialysis, > or = 2.0 mg/dL absolute increase in creatinine, or a > or = 50% increase in creatinine). We hypothesized that preprocedural patient characteristics could be used to predict the risk of post-PCI SRD.\n    \n\n\n          Methods:\n        \n      \n      Data were prospectively collected on a consecutive series of 11141 patients undergoing PCI without dialysis in northern New England from 2003 to 2005. Multivariate logistic regression model was used to identify the combination of patient characteristics most predictive of developing post-PCI SRD. The ability of the model to discriminate was quantified using a bootstrap validated C-Index (area under the receiver operating characteristic [ROC] curve). Its calibration was tested with a Hosmer-Lemeshow statistic. The model was validated on PCI procedures in 2006.\n    \n\n\n          Results:\n        \n      \n      Serious renal dysfunction occurred in 0.74% of patients (83/11141) with an associated inhospital mortality of 19.3% versus 0.9% in those without SRD. The model discriminated well between patients who did and did not develop SRD after PCI (ROC 0.87, 95% CI 0.82-0.91). Preprocedural creatinine (37%), congestive heart failure (24%), and diabetes (15%) accounted for 76% of the predictive ability of the model. The other factors contributed 24%: urgent and emergent priority (10%), preprocedural intra-aortic balloon pump use (8%), age > or = 80 years (5%), and female sex (1%). Validation of the model was successful with ROC: 0.84 (95% CI 0.80-0.89).\n    \n\n\n          Conclusions:\n        \n      \n      Although infrequent, the occurrence of SRD after PCI is associated with a very high inhospital mortality. We developed and validated a robust clinical prediction rule to determine which patients are at high risk for SRD. Use of this model may help physicians perform targeted interventions to reduce this risk."
        },
        {
            "title": "[Evaluation of severity in acute pancreatitis].",
            "abstract": "Acute pancreatitis has a variable etiology and natural history, and some patients have severe complications with a significant risk of death. The prediction of severe disease should be achieved by careful ongoing clinical assessment coupled with the use of a multiple factor scoring system and imaging studies. Over the past 30 years several scoring systems have been developed to predict the severity of acute pancreatitis. However, there are no complete scoring index with high sensitivity and specificity till now. The interest in new biological markers and predictive models for identifying severe acute pancreatitis testifies to the continued clinical importance of early severity prediction. Among them, IL-6, IL-10, procalcitonin, and trypsinogen activation peptide are most likely to be used in clinical practice as predictors of severity. Even if contrast-enhanced CT has been considered the gold standard for diagnosing pancreatic necrosis, early scanning for the prediction of severity is limited because the full extent of pancreatic necrosis may not develop within the first 48 hour of presentation."
        },
        {
            "title": "How to do safe sternal reentry and the risk factors of redo cardiac surgery: a 21-year review with zero major cardiac injury.",
            "abstract": "Objectives:\n        \n      \n      Resternotomy is a common part of cardiac surgical practice. Associated with resternotomy are the risks of cardiac injury and catastrophic hemorrhage and the subsequent elevated morbidity and mortality in the operating room or during the postoperative period. The technique of direct vision resternotomy is safe and has fewer, if any, serious cardiac injuries. The technique, the reduced need for groin cannulation and the overall low operative mortality and morbidity are the focus of this restrospective analysis.\n    \n\n\n          Methods:\n        \n      \n      The records of 495 patients undergoing 546 resternotomies over a 21-year period to January 2000 were reviewed. All consecutive reoperations by the one surgeon comprised patients over the age of 20 at first resternotomy: M:F 343:203, mean age 57 years (range 20 to 85, median age 60). The mean NYHA grade was 2.3 [with 67 patients (I), 273 (II), 159 (III), 43 (IV), and 4 (V classification)] with elective reoperation in 94.6%. Cardiac injury was graded into five groups and the incidence and reasons for groin cannulation estimated. The morbidity and mortality as a result of the reoperation and resternotomy were assessed.\n    \n\n\n          Results:\n        \n      \n      The hospital/30 day mortality was 2.9% (95% CI: 1.6%-4.4%) (16 deaths) over the 21 years. First (481), second (53), and third (12) resternotomies produced 307 uncomplicated technical reopenings, 203 slower but uncomplicated procedures, 9 minor superficial cardiac lacerations, and no moderate or severe cardiac injuries. Direct vision resternotomy is crystalized into the principle that only adhesions that are visualized from below are divided and only sternal bone that is freed of adhesions is sewn. Groin exposure was never performed prophylactically for resternotomy. Fourteen patients (2.6%) had such cannulation for aortic dissection/aneurysm (9 patients), excessive sternal adherence of cardiac structures (3 patients), presurgery cardiac arrest (1 patient), and high aortic cannulation desired and not possible (1 patient). The average postop blood loss was 594 mL (95% CI:558-631) in the first 12 hours. The need to return to the operating room for control of excessive bleeding was 2% (11 patients). Blood transfusion was given in 65% of the resternotomy procedures over the 21 years (mean 854 mL: 95% CI 765-945 mL) and 41% over the last 5 years.\n    \n\n\n          Conclusions:\n        \n      \n      The technique of direct vision resternotomy has been associated with zero moderate or major cardiac injury/catastrophic hemorrhage at reoperation. Few patients have required groin cannulation. In the postoperative period, there was acceptable blood loss, transfusion rates, reduced morbidity, and moderate low mortality for this potentially high risk group."
        },
        {
            "title": "Is there a direct association between age-related eye diseases and mortality? The Rotterdam Study.",
            "abstract": "Purpose:\n        \n      \n      To study mortality in subjects with age-related maculopathy (ARM), cataract, or open-angle glaucoma (OAG) in comparison with those without these disorders.\n    \n\n\n          Design:\n        \n      \n      Population-based prospective cohort study.\n    \n\n\n          Participants:\n        \n      \n      Subjects (n = 6339) aged 55 years and older from the population-based Rotterdam Study for whom complete information on eye disease status was present.\n    \n\n\n          Main outcome measures:\n        \n      \n      Vital status continuously monitored from 1990 until January 1, 2000.\n    \n\n\n          Methods:\n        \n      \n      The diagnosis of ARM was made according to the International Classification System. Cataract, determined on biomicroscopy, was defined as any sign of nuclear or (sub)cortical cataract, or both, in at least one eye with a visual acuity of 20/40 or less. Aphakia and pseudophakia in at least one eye were classified as operated cataract. Definite OAG was defined as a glaucomatous optic neuropathy combined with a glaucomatous visual field defect. Diagnoses were assessed at baseline. Mortality hazard ratios were computed using Cox proportional hazard regression analysis, adjusted for appropriate confounders (age, gender, smoking status, body mass index, cholesterol level, atherosclerosis, hypertension, history of cardiovascular disease, and diabetes mellitus).\n    \n\n\n          Results:\n        \n      \n      The adjusted mortality hazard ratio for subjects with AMD (n = 104) was 0.94 (95% confidence interval [CI], 0.52-1.68), with biomicroscopic cataract (n = 951) was 0.94 (95% CI, 0.74-1.21), with surgical cataract (n = 298) was 1.20 (95% CI, 0.86-1.68), and with definite OAG (n = 44) was 0.39 (95% CI, 0.10-1.55).\n    \n\n\n          Conclusions:\n        \n      \n      Both ARM and cataract are predictors of shorter survival because they have risk factors that also affect mortality. When adjusted for these factors, ARM, cataract, and OAG were themselves not significantly associated with mortality."
        },
        {
            "title": "[A longitudinal study on health status and factors relating to it in elderly residents of a community].",
            "abstract": "The purpose of the present study is to clarify the relationship of physical, sociological and psychological factors to health status, the decline in activities of daily living (ADL) and death, in community elderly residents, by analysis of participation or non-participation in a comprehensive health survey. Subjects were 737 elderly residents (306 males, 431 females) aged 70-75 years living at home in Fujishimamachi, a rural town of Yamagata prefecture. A baseline comprehensive health survey including door-to-door survey for non-respondents to the comprehensive survey, was performed in 1986, and repeated at follow-up five years later. The results obtained are as follows: 1) In both sexes, survival rate was highest in the respondents to the comprehensive health survey and lowest in non-respondents to either surveys. Causes of death, place of death, the bedridden period prior to death and ADL before bedridden status were different among those three groups. 2) Factors predisposing to ADL decline (all subjects had competent normal ADL at the baseline survey) were low self-rated health, low score of the Index of Competence developed by Tokyo Metropolitan Institute of Gerontology (the TMIG Index of Competence), inactivity in daily life especially in household affairs and social activities. The predicting factors of death were almost similar to those of ADL decline mentioned above. The relationship of the factors to health status was stronger in those who had to be surveyed door-to-door compared to respondents to the comprehensive health survey, and in women than in men. 3) Blood pressure, ECG findings, retinal findings, hand grip, memory as measured by the Benton Visual Retention Test showed no relationship to either ADL decline or mortality. 4) The present study shows that subjective health and functional capacity are useful to predict the outcome of community elderly residents. Further, life styles and health behavior have a significant effect on outcome prediction."
        },
        {
            "title": "Acute Kidney Injury and Outcome After Heart Transplantation: Large Differences in Performance of Scoring Systems.",
            "abstract": "Background:\n        \n      \n      Kidney function is an important aspect for patient outcome after heart transplantation (HTX). Acute kidney injury (AKI) is defined by changes in serum creatinine (SCr) and diuresis with risk/injury/failure/loss/end stage (RIFLE), acute kidney injury network (AKIN), or kidney disease: improving global outcomes (KDIGO) scores.\n    \n\n\n          Methods:\n        \n      \n      We investigated the effect of perioperative AKI on 1-year mortality after HTX over a period of 10 years at a single-center university hospital. Multivariable Cox proportional-hazards regression analyzed the association between 1-year mortality and potential risk factors. Receiver operating curves for 1-year mortality were calculated to determine sensitivity and specificity of scores.\n    \n\n\n          Results:\n        \n      \n      Sixty of 346 patients (17%) died within the first year. Acute kidney injury was a predictor of mortality only in the high-risk AKI groups of all scores: Hazard ratios (95% confidence interval) for RIFLE F: 7.164 (3.307-15.523); KDIGO/AKIN stage 3: 3.492 (2.006-6.081). Within each score, we identified patient groups, which had no elevated risk for an adverse outcome despite their allocation to the milder forms of AKI. In multivariable regression analysis, primary graft dysfunction was the predominant perioperative risk factor for 1-year mortality.\n    \n\n\n          Conclusions:\n        \n      \n      In contrast to other patient cohorts, mild forms of perioperative AKI are of subordinate influence on patient outcome in HTX."
        },
        {
            "title": "Orbital irradiation for Graves' ophthalmopathy: Is it safe? A long-term follow-up study.",
            "abstract": "Purpose:\n        \n      \n      We evaluated the frequency of long-term complications of orbital irradiation (radiation-induced tumors, cataract, and retinopathy) in comparison with glucocorticoids.\n    \n\n\n          Design:\n        \n      \n      We conducted a follow-up study in a cohort of 245 Graves' ophthalmopathy patients who had been treated with retrobulbar irradiation (20 Gy in 2 weeks) and/or oral glucocorticoids between 1982 and 1993 in our institution. Irradiated patients were compared with nonirradiated patients.\n    \n\n\n          Methods:\n        \n      \n      Data on mortality and cause of death were obtained. Living patients were invited to participate in a follow-up study. Possible retinopathy was assessed in a masked fashion and defined as the presence of > or =1 hemorrhages and/or microaneurysms on red-free retina photographs. If >5 lesions were present, patients were categorized as suffering from definite retinopathy. Cataract was assessed using the Lens Opacity Classification System II score.\n    \n\n\n          Main outcome measures:\n        \n      \n      Mortality, prevalence of retinopathy, prevalence of cataract, and type of cataract.\n    \n\n\n          Results:\n        \n      \n      Thirty-seven of the 245 patients had died, none of them from an intracranial tumor. Mortality was similar in the irradiated (27/159 [17%]) and nonirradiated patients (10/86 [12%]; P = 0.264). One hundred fifty-seven of the 208 living patients (75%) consented to participate in a follow-up ophthalmologic investigation; the mean follow-up time (+/- standard deviation) was 11+/-3 years. Possible retinopathy was present in 15% of patients, 22 of the irradiated and 1 of the nonirradiated patients (P = 0.002). In 5 patients (all had been irradiated), definite retinopathy (i.e., >5 retinal lesions) was present. Of these, 3 had diabetes mellitus, and 1 had hypertension. Diabetes was associated with both possible (P = 0.029) and definite (P = 0.005) retinopathy, with a relative risk of 21 (95% confidence interval, 3-179). The prevalence and severity of cataract were similar in the radiotherapy group (29%) and the glucocorticoid group (34%); it should be noted that 88 of 104 of the irradiated patients were also treated with oral glucocorticoids.\n    \n\n\n          Conclusion:\n        \n      \n      The data suggest that orbital irradiation for Graves' ophthalmopathy is a safe treatment modality, except possibly for diabetic patients."
        },
        {
            "title": "Disability, psychosocial factors and mortality among the elderly in a rural French population.",
            "abstract": "The purpose of this work is to identify risk markers of mortality in a cohort of 645 people aged 60 and over. The study was carried out in rural areas in south west France. Data were collected by questionnaire in 1982. Mortality was determined 4 years later; 111 deaths were registered. The analysis of age-adjusted odds ratios (OR) showed strong relationships between mortality and disability (OR = 7.75), compared health (OR = 3.94), self-rated health (OR = 2.47), home comfort (OR = 0.52), physical activity (OR = 0.32), sociability (OR = 0.43) and two subjective well-being items: the feeling of uselessness (OR = 3.51), and the lack of projects for the future (OR = 2.35). By contrast, no significant association was observed with reported morbidity and social support. Two multivariate analyses were performed: the first on longevity using Cox's regression model, the second on mortality using a linear discriminant analysis. The results of these analyses were translated into a simple set of 8 independent risk markers for the identification of a \"high risk group\" of mortality within 4 years. The sensitivity of this mortality risk indicator was 73% and its specificity 77%."
        },
        {
            "title": "Nationwide epidemiological survey of autoimmune pancreatitis in Japan in 2016.",
            "abstract": "Background:\n        \n      \n      To further clarify the clinico-epidemiological features of autoimmune pancreatitis (AIP) in Japan, we conducted the fourth nationwide epidemiological survey.\n    \n\n\n          Methods:\n        \n      \n      This study consisted of two stage surveys; the number of AIP patients was estimated by the first survey and their clinical features were assessed by the second survey. We surveyed the AIP patients who had visited hospitals in 2016.\n    \n\n\n          Results:\n        \n      \n      The estimated number of AIP patients in 2016 was 13,436, with an overall prevalence rate of 10.1 per 100,000 persons. The estimated number of newly diagnosed patients was 3984, with an annual incidence rate of 3.1 per 100,000 persons. Compared to the 2011 survey, both numbers more than doubled. We obtained detailed clinical information of 1474 AIP patients. The male-to-female sex ratio was 2.94, the mean age was 68.1, and mean age at diagnosis was 64.8. At diagnosis, 63% patients were symptomatic and nearly half of them presented jaundice. Pancreatic cysts were found in 9% of the patients and calcifications in 6%. Histopathological examination was performed in 64%, mainly by endoscopic ultrasonography-guided fine needle aspiration. Extra-pancreatic lesions were detected in 60% of the patients. Eighty-four % patients received the initial steroid therapy, and 85% received maintenance steroid therapy. Kaplan-Meier analysis revealed that the relapsed survival was 14% at 3 years, 25% at 5 years, 40% at 10 years, and 50% at 15 years. Mortality was favorable, but pancreatic cancer accounted for death in one quarter of fatal cases.\n    \n\n\n          Conclusion:\n        \n      \n      We clarified the current status of AIP in Japan."
        },
        {
            "title": "Is the risk of contrast-induced nephropathy a real contraindication to perform intravenous contrast enhanced Computed Tomography for non-traumatic acute abdomen in Emergency Surgery Department?",
            "abstract": "Background:\n        \n      \n      Contrast enhanced Computed Tomography (CCT) is the most used imaging test to investigate acute abdominal clinical conditions, because of its high sensitivity and specificity. It is mandatory to make a correct and prompt diagnosis when life threatening abdominal diseases as mesenteric ischemia are suspected. Contrast medium administration was linked to acute renal failure, therefore radiologist often prefer to perform CCT without contrast in patients needing to undergo the exam with increased serum creatinine. The aim of the review was to focus on the incidence of contrast induced nephropathy in patients presenting non-traumatic acute abdominal clinical conditions, who underwent CCT with intravenous contrast agent administration in emergency setting.\n    \n\n\n          Materials and methods:\n        \n      \n      The systematic review protocol was guided by the Preferred Reporting Items for Systematic Reviews and Meta-analyses Protocol (PRISMA-P). Quality of the evidence will be evaluated using the Grading of Recommendations Assessment, Development and Evaluation (GRADE) methodology.\n    \n\n\n          Results:\n        \n      \n      The strongest currently available evidence on the incidence of post-contrast acute kidney injury (AKI) following intravenous contrast agent administration consists in a meta-analysis of observational studies. Data extracted from meta-analyses demonstrate that, compared with non-contrast CT, CCT was not significantly associated with AKI. Moreover, the risk of AKI (RR=0.79; 95% confidence interval [CI]: 0.62, 1.02; P=.07), death (RR=0.95; 95% CI: 0.55, 1.67; P=.87), and dialysis (RR=0.88; 95% CI: 0.23, 3.43; P=.85) is similar, compared with the risk of AKI in the non-contrast medium group. Furthermore, intravenous low-osmolality iodinated contrast material is a nephrotoxic risk factor, but not in patients with a stable SCr level less than 1.5 mg/dL, therefore many factors other than contrast material could affect PC-AKI rates.\n    \n\n\n          Discussion and conclusions:\n        \n      \n      The benefits of diagnostic information gained from contrast enhanced TC in assessing AA are fundamental in some clinical scenarios. The risk of contrast induced nephropathy (CIN) is negligible in patients with normal renal function but the incidence appears to rise to as high as 25% in patients with pre-existing renal impairment or in the presence of risk factors such as diabetes, advanced age, vascular disease and use of certain concurrent medications. The incidence of CIN/AKI after intravenous contrast administration is very low in general population. Radiologists and referring physicians should be familiar with the risk factors for renal disease, CIN and preventing measures."
        },
        {
            "title": "Is there a need for the Fournier's gangrene severity index? Comparison of scoring systems for outcome prediction in patients with Fournier's gangrene.",
            "abstract": "Study Type - Prognosis (prospective cohort) Level of Evidence 2a. What's known on the subject? and What does the study add? Fournier's gangrene (FG) is a rare but life-threatening disease challenging the treating medical staff. Despite the fact that antibiotic therapy combined with surgery and intensive care surveillance are performed as standard treatment, mortality rates remain high. There have been efforts to develop a reliable tool to predict severity of the disease, not only to identify patients at highest risk of major complications or death but also to provide a target for medical teams and researchers aiming to improve outcome and to gather information for counselling patients. Laor et al. published the FG severity index (FGSI) in 1995 presenting a complex prediction score solely for patients with FG. Fifteen years later, Yilmazlar et al. suggested a new and supposedly more powerful scoring system, the Uludag FGSI (UFGSI), adding an age score and an extent of disease score to the FGSI. In the present study population we applied two scoring systems for outcome prediction that are solitarily applicable in patients with FG (FGSI, UFGSI), as well as two general scoring systems such as the established age-adjusted Charlson Comorbidity Index (ACCI) and the recently introduced surgical Apgar Score (sAPGAR) to compare them and to test whether one system might be superior to the other. In addition, we identified potential prognostic factors in the study population. By contrast to many earlier studies, we performed a combined prospective and retrospective analysis and provided a 30-day follow up. In the cohort of the present study, older patients with comorbidities as well as a need for mechanical ventilation and blood transfusion are at higher risk of lethal outcome. All scores are useful to predict mortality. Despite including more variables, the UFGSI does not seem to be more powerful than the FGSI. In daily routine we suggest applying ACCI and sAPGAR, as they are more easily calculated, generally applicable and well validated.\n    \n\n\n          Objective:\n        \n      \n      • To compare four published scoring systems for outcome prediction (Fournier's gangrene severity index [FGSI], Uludag FGSI [UFGSI], age-adjusted Charlson Comorbidity Index [ACCI] and surgical Apgar Score [sAPGAR]) and evaluate risk factors in patients with Fournier's gangrene (FG).\n    \n\n\n          Patients and methods:\n        \n      \n      • In all, 44 patients were analysed. The scores were applied. • A Mann-Whitney U-test, Fisher's exact test, receiver operator characteristic (ROC) analysis and Pearson correlation analysis were performed.\n    \n\n\n          Results:\n        \n      \n      • The results of the present study show a significant association among FGSI (P= 0.002), UFGSI (P= 0.002), ACCI (P= 0.004), sAPGAR (P= 0.018) and death. • The differences between the area under the receiver operating characteristic curve of the scores were not significant. • Non-survivors were older (P= 0.046), had a greater incidence of acute renal failure (P < 0.001) and coagulopathy (P= 0.041), were treated more often with mechanical ventilation (P= 0.001) and received more packed red blood cells (RBCs; P= 0.001).\n    \n\n\n          Conclusion:\n        \n      \n      • Older patients with comorbidities and need for mechanical ventilation and RBCs are at higher risk for death. • In the present cohort, scores calculated easily at the bedside, such as ACCI and sAPGAR, seemed to be as good at predicting outcome in patients with FG as FGSI and UFGSI."
        },
        {
            "title": "Vision impairment predicts 5 year mortality.",
            "abstract": "Aim:\n        \n      \n      To describe predictors of mortality in the 5 year follow up of the Melbourne Visual Impairment Project (VIP) cohort.\n    \n\n\n          Methods:\n        \n      \n      The Melbourne VIP was a population based study of the distribution and determinants of age related eye disease in a cluster random sample of Melbourne residents aged 40 years and older. Baseline examinations were conducted between 1992 and 1994. In 1997, 5 year follow up examinations of the original cohort commenced. Causes of death were obtained from the National Death Index for all reported deaths.\n    \n\n\n          Results:\n        \n      \n      Of the original 3271 participants, 231 (7.1%) were reported to have died in the intervening 5 years. Of the remaining 3040 participants eligible to return for follow up examinations, 2594 (85% of eligible) did participate, 51 (2%) had moved interstate or overseas, 83 (3%) could not be traced, and 312 (10%) refused to participate. Best corrected visual acuity <6/12 (OR=2.34) was associated with a significantly increased risk of mortality, as were increasing age (OR=1.09), male sex (OR=1.62), increased duration of cigarette smoking (OR=2.06 for smoking >30 years), increased duration of hypertension (OR=1.51 for duration >10 years), and arthritis (OR=1.42).\n    \n\n\n          Conclusions:\n        \n      \n      Even mild visual impairment increases the risk of death more than twofold. Further research is needed to determine why decreased visual acuity is associated with increased risk of mortality."
        },
        {
            "title": "INCREASING SLEEP DURATION IS ASSOCIATED WITH GEOGRAPHIC ATROPHY AND AGE-RELATED MACULAR DEGENERATION.",
            "abstract": "Purpose:\n        \n      \n      Sleeping too much or too little has been associated with adverse health outcomes including total mortality, cardiovascular disease, Type 2 diabetes, and hypertension. This study explored the relationship between sleep patterns and age-related macular degeneration (AMD).\n    \n\n\n          Methods:\n        \n      \n      One thousand and three consecutive patients in a retina practice were prospectively surveyed regarding sleep histories. Each patient then had a masked ophthalmic examination and was graded on the modified Wisconsin Age-Related Maculopathy System. The relationship between AMD grade and sleep hours was analyzed in a logistic regression model. Multivariable analysis was performed after adjustment for age, gender, and smoking history.\n    \n\n\n          Results:\n        \n      \n      In multivariable analysis, controlling for age, gender, and smoking history, sleep hours are not associated with neovascular AMD (P = 0.97) but are associated with geographic atrophy (P = 0.02). Sleeping >8 hours is associated with geographic atrophy (age-adjusted odds ratio, 7.09; 95% confidence interval, 1.59-31.6) compared with patients without AMD.\n    \n\n\n          Conclusion:\n        \n      \n      Longer sleep duration is associated with geographic atrophy secondary to AMD. These altered sleep patterns may be another morbidity of AMD, but further study is necessary."
        },
        {
            "title": "A prospective cohort study of retinal arteriolar narrowing and mortality.",
            "abstract": "The authors examined the relation of narrowed retinal arteriolar diameters, a marker of hypertensive damage, to mortality in a population-based cohort of 4,926 persons aged 43-84 years living in Wisconsin. A computer-assisted method was used to measure retinal vessel diameters from digitized retinal photographs taken at the baseline examination (1988-1990). These measurements were summarized as the retinal arteriole-to-venule ratio (AVR), with a smaller AVR indicating narrower arterioles compared with venules. Its relation to 10-year mortality was analyzed by using Cox proportional hazards models, adjusting for age, gender, blood pressure, diabetes, and other risk factors. No relation was found between smaller AVR and increased mortality. In relation to the largest AVR quartile, the adjusted relative risks of all-cause mortality were 0.93 for the smallest AVR quartile, 0.71 for the second AVR quartile, and 0.80 for the third AVR quartile. Results were largely similar in analyses of cause-specific mortality (vascular disease and non-vascular-disease mortality) and in subgroups stratified by age, gender, and diabetes and hypertension status. These data contrast with recent studies showing a relation between narrowed retinal arterioles and increased cardiovascular risk, suggesting that further research is needed to understand the systemic associations of retinal microvascular changes."
        },
        {
            "title": "The prognostic value of sensory impairment in older persons.",
            "abstract": "Objectives:\n        \n      \n      To determine the relationships between visual and hearing impairment and subsequent functional dependence and mortality among community-dwelling older persons.\n    \n\n\n          Design:\n        \n      \n      A Prospective, cohort study.\n    \n\n\n          Setting:\n        \n      \n      Community-based.\n    \n\n\n          Participants:\n        \n      \n      A total of 5444 men and women aged 55 to 74 years at baseline.\n    \n\n\n          Measurements:\n        \n      \n      Self-reported and measured visual impairment, self-reported and measured hearing impairment, self-reported and measured combined sensory impairment, 10-year mortality, and dependency in activities of daily living (ADL), instrumental ADL (IADL), and Rosow-Breslau (RB) function.\n    \n\n\n          Results:\n        \n      \n      In multiply-adjusted models, adjusting for length of follow-up, socio-demographic characteristics, and chronic conditions, only measured visual impairment was predictive of mortality. Measured visual impairment was also predictive of 10-year ADL and IADL dependence; measured hearing impairment was predictive of RB dependence. Self-reported visual impairment predicted functional impairment on all scales at 10 years, although self-reported hearing impairment predicted only subsequent RB dependence. Measured combined impairment was associated with the highest risk of 10-year functional dependence.\n    \n\n\n          Conclusions:\n        \n      \n      Sensory impairment is predictive of subsequent functional impairment in older persons."
        },
        {
            "title": "Predictors of mortality among nursing home residents with a diagnosis of Parkinson's disease.",
            "abstract": "Background:\n        \n      \n      Little is known about predictors of mortality among Parkinson patients living in long term care.\n    \n\n\n          Material/methods:\n        \n      \n      We conducted a 3-year follow-up study on 15,237 PD residents aged 65 years and older using the Systematic Assessment in Geriatric drug use via Epidemiology (SAGE) database. The SAGE database consists of the Minimum Data Set (MDS) data collected on over 400,000 nursing home residents in 5 U. S. states, including demographic characteristics, dementia severity, comorbidity and other clinical variables. Information on death was derived through linkage to Medicare files. Baseline characteristics were used to predict survival using univariate and multivariate Cox proportional hazard models.\n    \n\n\n          Results:\n        \n      \n      The overall 3-year mortality rate was 50%. Advanced age (relative rate (RR) 2.22; 95% confidence interval (CI) 1.99-2.47, for patients 85+ years), male gender (RR 1.73; 95% CI 1.60-1.87), severe functional (RR 1.81; 95% CI 1.53-2.13) and cognitive (RR 1.54; 95% CI 1.38-1.72) impairment, the presence of vision problems (RR 1.25; 95% CI 1.20-1.57), pressure ulcers (RR 1.25; 95% 1.14-1.37), and a diagnosis of congestive heart failure (RR 1.49; 95% CI 1.35-1.65), diabetes mellitus (RR 1.22; 95% 1.11-1.35) and pneumonia (RR 1.39; 95% CI 1.09-1.77) were independent predictors of death. The specific presence of aspiration pneumonia had the highest mortality risk ratio among all comorbidities (RR 1.58; CI 0.97-2.56). African-Americans and other minority groups were less likely to die relative to white PD residents.\n    \n\n\n          Conclusions:\n        \n      \n      Age, sex, functional and cognitive impairment and the diagnosis of pneumonia or congestive heart failure were the strongest predictors of death. Minority groups have a reduced risk of death relative to white PD nursing home residents."
        },
        {
            "title": "Motor Phenotype in Neurodegenerative Disorders: Gait and Balance Platform Study Design Protocol for the Ontario Neurodegenerative Research Initiative (ONDRI).",
            "abstract": "Background:\n        \n      \n      The association of cognitive and motor impairments in Alzheimer's disease and other neurodegenerative diseases is thought to be related to damage in the common brain networks shared by cognitive and cortical motor control processes. These common brain networks play a pivotal role in selecting movements and postural synergies that meet an individual's needs. Pathology in this \"highest level\" of motor control produces abnormalities of gait and posture referred to as highest-level gait disorders. Impairments in cognition and mobility, including falls, are present in almost all neurodegenerative diseases, suggesting common mechanisms that still need to be unraveled.\n    \n\n\n          Objective:\n        \n      \n      To identify motor-cognitive profiles across neurodegenerative diseases in a large cohort of patients.\n    \n\n\n          Methods:\n        \n      \n      Cohort study that includes up to 500 participants, followed every year for three years, across five neurodegenerative disease groups: Alzheimer's disease/mild cognitive impairment, frontotemporal degeneration, vascular cognitive impairment, amyotrophic lateral sclerosis, and Parkinson's disease. Gait and balance will be assessed using accelerometers and electronic walkways, evaluated at different levels of cognitive and sensory complexity, using the dual-task paradigm.\n    \n\n\n          Results:\n        \n      \n      Comparison of cognitive and motor performances across neurodegenerative groups will allow the identification of motor-cognitive phenotypes through the standardized evaluation of gait and balance characteristics.\n    \n\n\n          Conclusions:\n        \n      \n      As part of the Ontario Neurodegenerative Research Initiative (ONDRI), the gait and balance platform aims to identify motor-cognitive profiles across neurodegenerative diseases. Gait assessment, particularly while dual-tasking, will help dissect the cognitive and motor contribution in mobility and cognitive decline, progression to dementia syndromes, and future adverse outcomes including falls and mortality."
        },
        {
            "title": "Adherence to the American Diabetes Association retinal screening guidelines for population with diabetes in the United States.",
            "abstract": "Purpose:\n        \n      \n      (1) To assess long-term adherence to American Diabetes Association guideline-recommended retinal screening among population with diabetes in the United States. (2) To determine factors associated with long-term adherence to routine eye screening exams.\n    \n\n\n          Methods:\n        \n      \n      A retrospective cohort study was conducted in adult patients with diabetes identified from January 2009 to December 2010. Patients were followed until disenrollment, death, or study end date (December 2013). A patient was defined as adherent when having at least one exam in each 12-month period if there was evidence of retinopathy, or at least one exam in each 24-month period if there was no evidence of retinopathy. Multivariate logistic regressions were used to investigate patient demographics and other baseline characteristics associated with adherence to guidelines.\n    \n\n\n          Results:\n        \n      \n      A total of 204,073 patients were identified; the mean age (SD) was 61 (13) years and 48% were female. Overall, 71.1% were adherent to the retinal screening guidelines during a median of 4.8 years of follow-up including 27.7% who received an eye exam every year. Patient socioeconomic status (younger age, black race, lower income/education), less comorbidity, insulin use, higher specialist copayment plans, and proxies for poor patient behavior (lower adherence to the oral hypoglycemic agents, less diabetes education, hemoglobin A1C >9%) were associated with nonadherence to routine eye screening exams.\n    \n\n\n          Conclusion:\n        \n      \n      During nearly 5 years of follow-up, 28.9% of patients with diabetes were nonadherent to the retinal screening guidelines. Future research should focus on the development of interventions to address modifiable factors associated with nonadherence."
        },
        {
            "title": "[Dynamic high-contrast computed tomography in the diagnosis of pancreatic necrosis].",
            "abstract": "This work intended to find out whether dynamic high-contrast computed axial tomography (CAT scan) in effective and more useful than conventional computed tomography for the diagnosis of pancreatic necrosis in patients with severe acute pancreatitis.\n    \n\n\n          Background:\n        \n      \n      Although many methods have been used to predict the severity and extent of pancreatic necrosis, few studies have assessed computed tomography.\n    \n\n\n          Methods:\n        \n      \n      Longitudinal, prospective and comparative study was performed on consecutive patients with severe acute pancreatitis in which dynamic high-contrast computed tomography and conventional computed tomography were carried out.\n    \n\n\n          Results:\n        \n      \n      In pancreatic necrosis, pancreatic densities were lower. Necrosis by itself was also associated with higher severity scores, but not with a higher mortality rate. However, mortality rates were influenced by the extent of pancreatic necrosis. Sensitivity of 100%, specificity of 68%, positive predictive and negative predictive values of 100 and 42% were achieved.\n    \n\n\n          Statistics:\n        \n      \n      Densities of TCNL and TCAC were analyzed using student's t test of independent samples. The correlation of the aorta/pancreas index and the presence of necrosis was analyzed using a linear correlation test.\n    \n\n\n          Conclusions:\n        \n      \n      The authors conclude that dynamic high-contrast computed tomography is more effective than conventional computed tomography in diagnosing pancreatic necrosis. Moreover, the diagnosis of pancreatic necrosis and its extent makes it possible to formulate a prognosis as to severity of the disease."
        },
        {
            "title": "Interaction of atrial fibrillation and antithrombotics on outcome in intracerebral hemorrhage.",
            "abstract": "Objective:\n        \n      \n      To analyze the clinical differences between patients with primary intracerebral hemorrhage (ICH) with and without atrial fibrillation (AF) and assess whether the effect of the antithrombotic pretreatment on outcome is modified by the presence of AF.\n    \n\n\n          Methods:\n        \n      \n      In this prospective observational study, researchers from 2 university hospitals included 1,106 consecutive patients with ICH. Clinical characteristics were described and stratified by presence of AF. In-hospital and 3-month mortality and 3-month disability were analyzed, considering antithrombotic pretreatment (none, antiplatelets, or oral anticoagulants) and AF (yes/no).\n    \n\n\n          Results:\n        \n      \n      AF was present in 21.9% of primary ICH cases. Patients with AF-ICH were older, with more vascular risk factors, more antithrombotic pretreatment, higher clinical severity, higher hematoma volume, and higher in-hospital and 3-month mortality. Do-not-resuscitate orders were applied more frequently in AF-ICH cases. After 2 different adjustment models, mortality remained significantly higher in patients with AF-ICH. However, after introducing previous antithrombotic treatment in the model, the adjusted odds ratio for 3-month mortality was 1.45 (95% confidence interval 0.74-2.85, p = 0.284) for patients with AF-ICH compared with non-AF cases. AF modified the effect of antithrombotic pretreatment on in-hospital (p int = 0.077) and 3-month mortality (p int = 0.008). Among patients without AF, antithrombotic pretreatment increased mortality; no effect was observed in patients with AF-ICH.\n    \n\n\n          Conclusions:\n        \n      \n      Patients with AF and ICH had increased mortality; however, AF had no independent effect on mortality after adjustment for antithrombotic pretreatment. Conversely, antithrombotic pretreatment had a deleterious effect on outcome in patients with ICH without AF, but no detectable effect in patients with AF with ICH."
        },
        {
            "title": "Microsurgical Treatment of Tuberculum Sellae Meningiomas with Visual Impairments: A Chinese Experience of 56 Cases.",
            "abstract": "Aim:\n        \n      \n      Tuberculum sellae meningiomas (TSMs) are suprasellar lesions that commonly extend to the medial side of the optic nerve, resulting in visual impairments in the affected eye as the initial and most common symptoms. The primary goal of surgical treatment for TSM is the preservation or improvement of visual function. The aim of the present study was to assess the clinical outcomes of TSMs treated with microsurgery at our center.\n    \n\n\n          Material and methods:\n        \n      \n      A retrospective analysis was performed on 56 patients with TSMs associated with visual impairments who visited our neurosurgery center between January 2008 and January 2012. These patients underwent microsurgery using the unilateral subfrontal approach (n=22), the lateral frontal base approach (n=28), or the frontotemporal approach (n=6). Outcomes were complete tumor removal, improvements of visual impairments and survival to the surgery. After surgery, patients were classified according to Simpson classification: Grade I in 22 cases, grade II in 28 cases, and grade IV in 6 cases.\n    \n\n\n          Results:\n        \n      \n      Postoperatively, visual impairments were improved in 47 cases, unchanged in 7 cases, and worsened in 2 cases. There was no patient's death.\n    \n\n\n          Conclusion:\n        \n      \n      TSMs are operated in China with favorable outcomes."
        },
        {
            "title": "The diagnostic sensitivity of dynamic contrast-enhanced magnetic resonance imaging and breast-specific gamma imaging in women with calcified and non-calcified DCIS.",
            "abstract": "Background:\n        \n      \n      Early detection of breast cancer reduces mortality. Therefore, diagnosis of ductal carcinoma in situ (DCIS) is important.\n    \n\n\n          Purpose:\n        \n      \n      To compare the sensitivities of dynamic contrast-enhanced magnetic resonance imaging (DCE-MRI) and breast-specific gamma imaging (BSGI) in pathologically proven calcified and non-calcified DCIS.\n    \n\n\n          Material and methods:\n        \n      \n      Thirty-five patients with pathologically diagnosed DCIS from 1 June 2009 through 31 December 2011, underwent a protocol involving both breast MRI and BSGI. Each image was assessed by a separate dedicated breast radiologist. All lesions were divided into two groups; with or without microcalcifications on mammograms. In cases without microcalcifications, we recorded the mass, asymmetry, or negative findings on mammography. On MRI, the enhancement pattern was categorized as mass or non-mass-like enhancement. On BSGI, the uptake pattern was analyzed. The histopathological features of the lesions were obtained. Statistical analysis of the sensitivity of each modality was performed using McNemar's test.\n    \n\n\n          Results:\n        \n      \n      Thirty-five women with a mean age of 48 years (range, 26-69 years) were enrolled in the study. The total sensitivities of MRI and BSGI in the 35 cases were 91.4% (32 of 35 DCIS) and 68.6% (24 of 35 DCIS), respectively. Eighteen cases with DCIS displayed microcalcifications on mammography, while 17 cases did not. Of these 17 cases without microcalcifications on mammography, 88.2% (15 of 17 DCIS) were detected by MRI and 52.9% (9 of 17 DCIS) by BSGI. Of 18 cases with microcalcifications on mammography, 94.4% (17 of 18 DCIS) were detected by MRI and 83.3% (15 of 19 DCIS) by BSGI.\n    \n\n\n          Conclusion:\n        \n      \n      MRI showed a higher sensitivity for the detection of calcified and non-calcified DCIS and is more helpful than BSGI in cases without microcalcifications on mammography."
        },
        {
            "title": "MEDICAL AND NEUROLOGICAL COMPLICATIONS AMONG STROKE PATIENTS ADMITTED FOR INPATIENT CARE IN ADDIS ABABA, ETHIOPIA.",
            "abstract": "Background and purpose:\n        \n      \n      Medical and neurologic complications of acute stroke adversely impact patient outcome and in some cases can be preventable. There is scarcity of data in the African medical setup and none to date in our country to our knowledge. The current study aims to describe types and frequencies of neuro-medical complications occurring in hospitalized patients after an acute stroke and to identify risk factors for development of these complications and the role of these factors on mortality.\n    \n\n\n          Methods:\n        \n      \n      A total of 71 patients with acute stroke (excluding Sub-arachnoid Hemorrhage) who were admitted to three hospitals in Addis Ababa from June 2008 to March 2009 were included in the study. These patients were prospectively followed until their discharge or death to look for the nature and frequency of neuro-medical complications. Basic demographic data, stroke related medical information, pre-existing medical conditions, admission laboratory and imaging findings were recorded. All events were documented for each patient using pre-defined medical complication using a data collection format. Descriptive and analytic statistical tests were performed to measure associations between risk and outcome factors.\n    \n\n\n          Results:\n        \n      \n      Stroke-complications were detected in 71.8% (51/71) of the study participants and the most frequent complication was aspiration pneumonia which occurred in 33.8% (24/71). Miscellaneous complications such as sepsis, hypokalemia exposure keratitis were detected in 25% (17/71) of stroke patients. Complications were more common in patients with severe neurologic deficit as measured by Glasgow coma scale (GSC) and old age. GCS < 12 and age > 40 years were both significantly associated with developments of complications after stroke (p < 0.05). A total of 17 (23%) patients died during their in-patient stay. GCS < 12 was significantly associated with mortality related to stroke in the admitted patients (p = 0.0002) while there was no association between old age and mortality.\n    \n\n\n          Conclusions:\n        \n      \n      Complications after stroke are common and are major factors contributing to mortality. Being aware of the types of common complications and associated risk factors helps the clinical team involved in the care of stroke patients to make preparations and plans for the best possible care and to take preventive measures that will save a lot of lives with best possible use of meager resources available such as educating the population to avoid oral feeding for patients with altered mental state and physicians to evaluate gag reflex bedside swallowing test and proper positioning of patients to avoid aspiration pneumonia. GSC measurement at admission is an important predictor of complications and death following stroke."
        },
        {
            "title": "Late results and prognosis after carotid artery surgery.",
            "abstract": "Carotid reconstruction was performed on 75 patients (84 operations) mainly because of transient ischemic attacks (TIA) and/or amaurosis fugax (67%) or TIA with incomplete recovery (20%). The operative mortality was 2.4%. The incidence of permanent postoperative neurologic deficit was 6.0%. The postoperative observation time was 1 year to 13 years 5 months (mean 66 months). At follow-up 87% of the survivors were symptom-free. Three new strokes, one of them not related to the operated side, occurred follow-up, and 26 more patients died. The relative cumulative 5-year survival was 87.3%. Of the total 28 deaths, 16 were due to myocardial infarction. Mortality was significantly heightened and cumulative 5-year survival reduced when preoperative ECG had indicated coronary heart disease. The high incidence of deaths from myocardial disease during long-term follow-up was directly related to preoperative presence of coronary heart disease."
        },
        {
            "title": "Association between red blood cell distribution width and macrovascular and microvascular complications in diabetes.",
            "abstract": "Aims/hypothesis:\n        \n      \n      Red blood cell distribution width (RDW) has been reported to be a risk marker of morbidity and mortality for cardiovascular disease in various study populations. However, no studies have investigated the relationship between RDW and diabetes complications. We therefore evaluated RDW as a marker of macrovascular and microvascular complications in a nationally representative sample of the adult diabetes population in the USA.\n    \n\n\n          Methods:\n        \n      \n      A cross-sectional study was performed using the nationwide 1988 to 1994 data set from the Third National Health and Nutrition Examination Survey. The association between RDW quartiles and macrovascular and microvascular complications was evaluated in 2,497 non-pregnant adults aged 20 years and older and affected by diabetes. Logistic regression modelling was used to adjust for potential confounding.\n    \n\n\n          Results:\n        \n      \n      Compared with the lowest RDW quartile, higher RDW values (3rd and 4th quartiles) were associated with increased adjusted odds of any vascular complication (OR 4th quartile 2.06 [95% CI 1.11, 3.83]), myocardial infarction (OR 4th quartile 2.45 [95% CI 1.13, 5.28]), heart failure (OR 4th quartile 4.40 [95% CI 1.99, 9.72]), stroke (OR 4th quartile 2.56 [95% CI 1.21, 5.42]) and nephropathy (OR 4th quartile 2.33 [95% CI 1.42, 3.82]). The odds of developing diabetic retinopathy were not significantly increased across RDW quartiles.\n    \n\n\n          Conclusions/interpretation:\n        \n      \n      Higher RDW values are associated with increased odds of developing cardiovascular disease and nephropathy in a nationally representative sample of USA adults with diabetes. RDW may be an important clinical marker of vascular complications in diabetes and one that is independent of traditional risk factors and disease duration."
        },
        {
            "title": "Shape and Enhancement Characteristics of Pancreatic Neuroendocrine Tumor on Preoperative Contrast-enhanced Computed Tomography May be Prognostic Indicators.",
            "abstract": "Background:\n        \n      \n      Prognostic indicators of the malignant potential of pancreatic neuroendocrine tumors (PNET) are limited. We assessed tumor shape and enhancement pattern on contrast-enhanced computed tomography as predictors of malignant potential.\n    \n\n\n          Methods:\n        \n      \n      Sixty cases of PNET patients undergoing curative surgery from 2001 to 2014 were enrolled onto our retrospective study. Preoperative enhanced CTs were assessed, and criteria defined for regularly shaped and enhancing tumors (group 1), and irregularly shaped and/or enhancing tumors (group 2). The relation of tumor shape and enhancement pattern to outcome was assessed.\n    \n\n\n          Results:\n        \n      \n      Interobserver agreement was substantial (kappa = 0.74). Group 2 (n = 24) was significantly correlated with synchronous liver metastasis (23 vs. 0 %), lymph node metastasis (36 vs. 3 %), pathologic capsular invasion (68 vs. 8 %), larger tumor size (30 vs. 12 mm), tumor, node, metastasis classification system (TNM) stage III/IV disease (46 vs. 3 %), and histologic grade 2/3 (41 vs. 0 %). Multivariate analysis revealed that tumor grade 2/3 and group 2 criteria correlated with tumor relapse (hazard ratio 6.5 and 13.6, P = 0.0071 and 0.039, respectively), and that only group 2 criteria were independently correlated with poor overall survival (hazard ratio 5.56e + 9, P = 0.0041).\n    \n\n\n          Conclusions:\n        \n      \n      Irregular tumor shape/enhancement on preoperative computed tomography is a negative prognostic factor after curative surgery for PNET."
        },
        {
            "title": "Predictability of frailty index and its components on mortality in older adults in China.",
            "abstract": "Background:\n        \n      \n      Frailty represents an increased vulnerability to external stressors due to decreased physiological reserve and dysfunction in multiple bodily systems. The relationship between frailty and mortality has been well-documented in the literature. However, less is known about the predictive powers of frailty index and its components on mortality when they are simultaneously present. This study aimed to examine the predictive powers of frailty index and its multiple components on mortality in a nationally representative sample of older adults in China.\n    \n\n\n          Methods:\n        \n      \n      We used a sample of 13,731 older adults from the 2008/2009 and 2011/2012 waves of the Chinese Longitudinal Healthy Longevity Survey (CLHLS). Frailty was measured using the cumulative deficit approach, and was constructed from 38 health variables (39 deficits). We selected 8 major sets of components: activities of daily living (ADL) (6 deficits), instrumental ADL (IADL) (8 deficits), functional limitations (5 deficits), overall cognitive functioning (1 deficit), chronic disease conditions (11 deficits), self-reported health (2 deficits), hearing and vision impairment (2 deficits), and psychological distress (1 deficit). Survival analysis was used to examine the roles of the frailty and its components in mortality.\n    \n\n\n          Results:\n        \n      \n      Results showed that almost all the components of the frailty index (except chronic diseases) were significant predictors of mortality when examined individually. Among the components, ADL and IADL disabilities remained significant when considering all the components simultaneously. When the frailty and its components were simultaneously analyzed, the frailty remained a robust predictor of mortality across the age and sex groups, while most components lost their significance except ADL, IADL, and cognitive function components in some cases.\n    \n\n\n          Conclusions:\n        \n      \n      Frailty measured by cumulative deficits has a stronger predictive power on mortality than its all individual components. ​ADL and IADL disability play a greater role in mortality than other components when considering all the components of frailty."
        },
        {
            "title": "Association of severity of conjunctival and corneal calcification with all-cause 1-year mortality in maintenance haemodialysis patients.",
            "abstract": "Background:\n        \n      \n      Conjunctival and corneal calcification (CCC) is the most common form of metastatic calcification in patients with chronic renal failure. The aim of this study is to investigate if severity of CCC correlates with vascular calcification and mortality in maintenance haemodialysis (MHD) patients.\n    \n\n\n          Methods:\n        \n      \n      One hundred and nine MHD patients were recruited. CCC was evaluated by external eye photographs, and was graded and scored according to modified Porter and Crombie classification system described by Tokuyama et al. Chest X-ray examination was used to evaluate aortic arch calcification. Geographic, haematological, biochemical and dialysis-related data were obtained. The patients were analysed for traditional and non-traditional risk factors for cardiovascular disease stratified by severity of CCC. All patients were followed up for 1 year to investigate the risks for mortality.\n    \n\n\n          Results:\n        \n      \n      Forty-three, 35 and 31 patients had mild (scores ≤ 4), moderate and severe (scores ≥ 9) CCC at baseline, respectively. With trend estimation, patients with severe CCC had a significantly higher percentage of severe aortic arch calcification. Multiple linear regression analysis showed that hypertension, haemodialysis duration and corrected calcium level were associated with scores of CCC in MHD patients. Moreover, age, corrected calcium-phosphate level, and moderate and severe CCC were associated with grades of aortic arch calcification. At 1-year follow-up, 11 of 109 (10.1%) patients had died. Multivariate Cox proportional hazards model showed that age, corrected calcium and severe CCC were significant risk factors for all-cause 1-year mortality in MHD patients. Each increment of one score of CCC is associated with a 26.4% increased risk for all-cause mortality.\n    \n\n\n          Conclusions:\n        \n      \n      Severity of CCC, which is easily obtained at bedside, acts as an independent predictor for all-cause 1-year mortality in MHD patients."
        },
        {
            "title": "Allergic rhinoconjunctivitis: burden of disease.",
            "abstract": "Even though there is no mortality associated with allergic rhinoconjunctivitis (AR), there is significant morbidity in sufferers of this condition. The exact number of patients with AR is difficult to ascertain, with studies showing ranges from 9 to 42% of the population. Recently, the Allergies in America survey found that 14.2% of the adult U.S. population has been diagnosed with AR. It is well established that AR has a profound influence on the patient's quality of life. Not only do people with AR complain of rhinorrhea, nasal congestion, sneezing, itching, and associated eye problems disturbing, but they also have impaired emotional wellbeing and social functioning. Costs are a major burden in AR studies showing at least $6 billion/year. Although most attention related to costs in AR have been evaluating direct costs due to physician consultation and medical treatment, it is now clear that indirect costs are a major aspect of total costs in AR, especially for American businesses. Indirect costs include absenteeism from work or school because of illness and decreased productivity when at work or presenteeism. AR should be treated seriously by the medical community. Proper treatment of AR patients should not only greatly improve their quality of life, but also bring down health care costs, especially indirect ones, associated with this condition."
        },
        {
            "title": "Heritability of intraocular pressure in older female twins.",
            "abstract": "Purpose:\n        \n      \n      To examine the heritability of intraocular pressure (IOP) among older women not diagnosed as having glaucoma.\n    \n\n\n          Design:\n        \n      \n      Cross-sectional twin study.\n    \n\n\n          Participants:\n        \n      \n      94 monozygotic (MZ) and 96 dizygotic (DZ) female twin pairs aged 63-76 years and not diagnosed as having glaucoma.\n    \n\n\n          Methods:\n        \n      \n      Intraocular pressure was measured using a noncontact tonometer. The contributions of genetic and environmental factors to individual differences in IOP were estimated by applying an independent pathway model to twin data.\n    \n\n\n          Main outcome measures:\n        \n      \n      Contribution of genetic and environmental effects to the variation in IOP among MZ and DZ twins.\n    \n\n\n          Results:\n        \n      \n      Mean IOP of the study population was 14.1 mmHg (+/- standard deviation 3.1) with no differences observed neither between the MZ and the DZ individuals, nor between the left and the right eyes. The pair-wise correlations for IOP of the right eye were .61 in MZ and .25 in DZ and for the left eye .63 and .42. The phenotypic correlation between the left and the right eye IOP was high (r = 0.81), suggesting that they were indices of a single trait. Quantitative genetic modeling revealed that for both eyes 64% (95% confidence interval [CI], 53-71) of the variance in IOP was explained by additive genetic effects and 18% (95% CI, 11-27) by nonshared environmental factors in common. In addition, 18% (95% CI, 15-23) of the variance in IOP was explained by nonshared environmental factors specific to each eye.\n    \n\n\n          Conclusions:\n        \n      \n      Additive genetic influences explained most of the individual differences in IOP among older women not diagnosed as having glaucoma. Because elevated IOP is an important risk factor for glaucoma, genetic factors underlying IOP may have a significant role in determining the risk for glaucoma, a complex progressive disease leading to death of ganglion cells."
        },
        {
            "title": "The impact of a vision screening law on older driver fatality rates.",
            "abstract": "Objective:\n        \n      \n      To evaluate the impact of the Florida visual acuity licensing standard for drivers 80 years and older on fatal motor vehicle collision (MVC) involvement.\n    \n\n\n          Methods:\n        \n      \n      Motor vehicle collision fatality rates for all Florida residents and for drivers 80 years and older were compared before and after the visual acuity licensing standard was implemented in January 2004.\n    \n\n\n          Results:\n        \n      \n      From 2001 to 2006, there was a nonsignificant (P = .06) increase in MVC fatality rates in Florida; in contrast, fatality rates among drivers 80 years and older demonstrated a significant downward linear trend (P = .01). When comparing prelaw (2001-2003) and postlaw (2004-2006) periods, the fatality rate among all-aged occupants increased by 6% (rate ratio, 1.06; 95% confidence interval, 0.99-1.14); conversely, fatalities among drivers 80 years and older decreased significantly by 17% (rate ratio, 0.83; 95% confidence interval, 0.72-0.98).\n    \n\n\n          Conclusions:\n        \n      \n      Despite little evidence for an association between visual acuity and MVC involvement, the results of this study suggest that a vision screening law targeting Floridians 80 years and older resulted in a reduction in the MVC fatality rate among such drivers. The exact mechanism responsible for this association is unclear and future research should attempt to identify what might explain this relationship."
        },
        {
            "title": "Health related outcomes among people with type 2 diabetes by country of birth: Result from the 45 and Up Study.",
            "abstract": "Background:\n        \n      \n      Ethnic variation in the occurrence of type 2 diabetes, complications, mortality, and health behaviours has been reported. The current research examined patterns of health-related outcomes by country of birth in people with diabetes aged 45years and over in New South Wales, Australia.\n    \n\n\n          Methods:\n        \n      \n      This study was based on the baseline data of 266,848 participants aged 45years and over from \"The Sax Institute's 45 and Up Study\" (2006-2009), NSW; Australia's most populous state. Health-related factors including self-rated overall health, Quality of Life (QoL), eyesight, subjective memory complaint, hearing loss, psychological distress and functional limitation were examined according to country of birth among 23,112 people with type 2 diabetes. Logistic regression modelling was used to compare the odds of poor outcomes between Australian-born and overseas-born participants, adjusting for potential confounding and mediating variables. Both age-sex and fully adjusted odds ratios (aORs) are reported.\n    \n\n\n          Results:\n        \n      \n      Nearly half of the people with diabetes in the sample reported hearing loss and high levels of functional limitations, a third reported poor overall health. Compared to people with diabetes born in Australia, people born in South East Europe, North Africa, the Middle East had significantly greater odds of poor outcomes across the majority of examined health-related factors, with the largest odds observed in the elevated level of psychological distress outcome (aOR=3.4 in North African and the Middle East group). Higher aORs of poor overall health, QoL, memory problems and poor eyesight, and lower aORs for hearing loss, were also found among those born in the Asian countries.\n    \n\n\n          Conclusions:\n        \n      \n      The results demonstrated significant ethnic disparity in the prevalence of health-related outcomes. These findings provide important context for the formulation of culturally sensitive secondary prevention strategies."
        },
        {
            "title": "Effect of Renin-Angiotensin-Aldosterone System Inhibitors on Short-Term Mortality After Sepsis: A Population-Based Cohort Study.",
            "abstract": "Antagonists of the renin-angiotensin-aldosterone system (RAAS), including ACEIs (angiotensin-converting enzyme inhibitors) and ARBs (angiotensin II receptor blockers), may prevent organ failure. We, therefore, investigated whether specific RAAS inhibitors are associated with reduced mortality in patients with sepsis.We conducted a population-based retrospective cohort study using multivariable propensity score-based regression to control for differences among patients using different RAAS inhibitors. A multivariable-adjusted Cox proportional-hazards regression model was used to determine the association between RAAS inhibitors and sepsis outcomes. To directly compare ACEI users, ARB users, and nonusers, a 3-way propensity score matching approach was performed. Results were pooled with previous evidence via a random-effects meta-analysis. A total of 52 727 patients were hospitalized with sepsis, of whom 7642 were prescribed an ACEI and 4237 were prescribed an ARB. Using propensity score-matched analyses, prior ACEI use was associated with decreased 30-day mortality (hazard ratio, 0.84 [95% CI, 0.75-0.94]) and 90-day mortality (hazard ratio, 0.83 [95% CI, 0.75-0.92]) compared with nonuse. Prior ARB use was associated with an improved 90-day survival (hazard ratio, 0.88 [95% CI, 0.83-0.94]). These results persisted in sensitivity analyses focusing on patients without cancer and patients with hypertension. By contrast, no beneficial effect was found for antecedent β-blockers exposure (hazard ratio, 0.99 [95% CI, 0.94-1.05]). The pooled estimates obtained from the meta-analysis was 0.71 (95% CI, 0.58-0.87) for prior use of ACEI/ARB.The short-term mortality after sepsis was substantially lower among those who were already established on RAAS inhibitor treatment when sepsis occurred."
        },
        {
            "title": "Vitamin D status and early age-related macular degeneration in postmenopausal women.",
            "abstract": "Objective:\n        \n      \n      The relationship between serum 25-hydroxyvitamin D (25[OH]D) concentrations (nmol/L) and the prevalence of early age-related macular degeneration (AMD) was investigated in participants of the Carotenoids in Age-Related Eye Disease Study.\n    \n\n\n          Methods:\n        \n      \n      Stereoscopic fundus photographs, taken from 2001 to 2004, assessed AMD status. Baseline (1994-1998) serum samples were available for 25(OH)D assays in 1313 women with complete ocular and risk factor data. Odds ratios (ORs) and 95% confidence intervals (CIs) for early AMD (n = 241) of 1287 without advanced disease were estimated with logistic regression and adjusted for age, smoking, iris pigmentation, family history of AMD, cardiovascular disease, diabetes, and hormone therapy use.\n    \n\n\n          Results:\n        \n      \n      In multivariate models, no significant relationship was observed between early AMD and 25(OH)D (OR for quintile 5 vs 1, 0.79; 95% CI, 0.50-1.24; P for trend = .47). A significant age interaction (P = .002) suggested selective mortality bias in women aged 75 years and older: serum 25(OH)D was associated with decreased odds of early AMD in women younger than 75 years (n = 968) and increased odds in women aged 75 years or older (n = 319) (OR for quintile 5 vs 1, 0.52; 95% CI, 0.29-0.91; P for trend = .02 and OR, 1.76; 95% CI, 0.77-4.13; P for trend = .05, respectively). Further adjustment for body mass index and recreational physical activity, predictors of 25(OH)D, attenuated the observed association in women younger than 75 years. Additionally, among women younger than 75 years, intake of vitamin D from foods and supplements was related to decreased odds of early AMD in multivariate models; no relationship was observed with self-reported time spent in direct sunlight.\n    \n\n\n          Conclusions:\n        \n      \n      High serum 25(OH)D concentrations may protect against early AMD in women younger than 75 years."
        },
        {
            "title": "Test of the National Death Index and Equifax Nationwide Death Search.",
            "abstract": "The authors compared the ability of the National Death Index and the Equifax Nationwide Death Search to ascertain deaths of participants in the Nurses' Health Study. Each service was sent information on 197 participants aged 60-68 years in 1989 whose deaths were reported by kin or postal authorities and 1,997 participants of the same age who were known to be alive. Neither service was aware of the authors' information regarding participants' vital status. The sensitivity of the National Death Index was 98 percent and that of Equifax was 79 percent. Sensitivity was similar for women aged 65-68 years; however, for women aged 61-64 years, the sensitivity of the National Death Index was 97.7 percent compared with 60.2 percent for Equifax. The specificity of both services was approximately 100 percent. The contrast between the sources of these databases and the matching algorithms they employ has implications for researchers and for those planning health data systems."
        },
        {
            "title": "Associations between diabetes self-management and microvascular complications in patients with type 2 diabetes.",
            "abstract": "Objectives:\n        \n      \n      Diabetes is a major public health problem that is approaching epidemic proportions globally. Diabetes self-management can reduce complications and mortality in type 2 diabetic patients. The purpose of this study was to examine associations between diabetes self-management and microvascular complications in patients with type 2 diabetes.\n    \n\n\n          Methods:\n        \n      \n      In this cross-sectional study, 562 Iranian patients older than 30 years of age with type 2 diabetes who received treatment at the Diabetes Research Center of the Endocrinology and Metabolism Research Institute of the Tehran University of Medical Sciences were identified. The participants were enrolled and completed questionnaires between January and April 2014. Patients' diabetes self-management was assessed as an independent variable by using the Diabetes Self-Management Questionnaire translated into Persian. The outcomes were the microvascular complications of diabetes (retinopathy, nephropathy, and neuropathy), identified from the clinical records of each patient. A multiple logistic regression model was used to estimate odds ratios (ORs) and 95% confidence intervals (CIs) between diabetes self-management and the microvascular complications of type 2 diabetes, adjusting for potential confounders.\n    \n\n\n          Results:\n        \n      \n      After adjusting for potential confounders, a significant association was found between the diabetes self-management sum scale and neuropathy (adjusted OR, 0.64; 95% CI, 0.45 to 0.92, p=0.01). Additionally, weak evidence was found of an association between the sum scale score of diabetes self-management and nephropathy (adjusted OR, 0.71; 95% CI, 0.47 to 1.05, p=0.09).\n    \n\n\n          Conclusions:\n        \n      \n      Among patients with type 2 diabetes, a lower diabetes self-management score was associated with higher rates of nephropathy and neuropathy."
        },
        {
            "title": "A global assessment of the gender gap in self-reported health with survey data from 59 countries.",
            "abstract": "Background:\n        \n      \n      While surveys in high-income countries show that women generally have poorer self-reported health than men, much less is known about gender differences in other regions of the world. Such data can be used to examine the determinants of sex differences.\n    \n\n\n          Methods:\n        \n      \n      We analysed data on respondents 18 years and over from the World Health Surveys 2002-04 in 59 countries, which included multiple measures of self-reported health, eight domains of functioning and presumptive diagnoses of chronic conditions. The age-standardized female excess fraction was computed for all indicators and analysed for five regional groups of countries. Multivariate regression models were used to examine the association between country gaps in self-reported health between the sexes with societal and other background characteristics.\n    \n\n\n          Results:\n        \n      \n      Women reported significantly poorer health than men on all self-reported health indicators. The excess fraction was 15 % for the health score based on the eight domains, 28 % for \"poor\" or \"very poor\" self-rated health on the single question, and 26 % for \"severe\" or \"extreme\" on a single question on limitations. The excess female reporting of poorer health occurred at all ages, but was smaller at ages 60 and over. The female excess was observed in all regions, and was smallest in the European high-income countries. Women more frequently reported problems in specific health domains, with the excess fraction ranging from 25 % for vision to 35 % for mobility, pain and sleep, and with considerable variation between regions. Angina, arthritis and depression had female excess fractions of 33, 32 and 42 % respectively. Higher female prevalence of the presumptive diagnoses was observed in all regional country groups. The main factors affecting the size of the gender gap in self-reported health were the female-male gaps in the prevalence of chronic conditions, especially arthritis and depression and gender characteristics of the society.\n    \n\n\n          Conclusions:\n        \n      \n      Large female-male differences in self-reported health and functioning, equivalent to a decade of growing older, consistently occurred in all regions of the world, irrespective of differences in mortality levels or societal factors. The multi-country study suggests that a mix of biological factors and societal gender inequalities are major contributing factors to gender gap in self-reported measures of health."
        },
        {
            "title": "Cytomegalovirus antigenemia surveillance in the treatment of cytomegalovirus disease in AIDS patients.",
            "abstract": "Objective:\n        \n      \n      Surveillance of quantitative cytomegalovirus (CMV) antigenemia among AIDS patients with CMV treated complications in order to determine its value in assessing the response to treatment and survival.\n    \n\n\n          Methods:\n        \n      \n      A longitudinal follow-up of antigenemia measurement at diagnosis, after induction therapy with ganciclovir or foscarnet, and every 3 months during maintenance therapy was carried out in 25 patients with CMV retinitis and in 8 with extraocular CMV disease. Positive antigenemia was defined as the presence of any amount of immunofluorescent pp65-positive leukocytes/10(5) cells.\n    \n\n\n          Results:\n        \n      \n      Mean antigenemia values were: 77+/-148/10(5) leukocytes at retinitis diagnosis; 45+/-114 after induction therapy; and 7+/-18 and 1.5+/-4 after 6 months and one year of therapy, respectively. Patients achieving undetectable antigenemia increased from 44% at baseline to 68% at postinduction and 80% during follow-up. Seven patients (28%) who remained free of relapses presented significant minor baseline antigenemias and became negative after induction therapy. Patients with extraocular disease showed erratic antigenemia values and absent therapeutic response. CMV blood cultures before and after induction therapy were positive in 39% and 21% of patients, respectively. Kaplan-Meier analysis revealed a significantly longer survival for patients with retinitis when compared to those with extraocular complications, and for patients with negative antigenemia after induction in comparison with those who failed to achieve it.\n    \n\n\n          Conclusions:\n        \n      \n      Low basal antigenemia and antigenemia clearance after induction therapy are variables directly related to good response to treatment and survival. Continuous surveillance of antigenemia during treatment could permit designing of individual strategies to obtain a better response."
        },
        {
            "title": "Longitudinal strain combined with delayed-enhancement magnetic resonance improves risk stratification in patients with dilated cardiomyopathy.",
            "abstract": "Objective:\n        \n      \n      Late gadolinium enhancement (LGE) on cardiac magnetic resonance (CMR) imaging has been reported to be associated with unfavourable outcomes; however, few studies have addressed the prognostic value of left ventricular (LV) deformation parameter indicated by global longitudinal strain (GLS) in two-dimensional speckle-tracking (2DST) echocardiography in patients with non-ischaemic dilated cardiomyopathy (DCM). This study aims to investigate whether the combination of GLS and LGE is useful in stratifying the risk in patients with DCM.\n    \n\n\n          Methods:\n        \n      \n      We studied 179 consecutive symptomatic patients with DCM (age, 61±15 years; 121 males; left ventricular ejection fraction (LVEF) 33%±9%; New York Heart Association (NYHA) class II: n=71, III: n=107, IV: n=1) who underwent CMR and echocardiography with conventional assessment and 2DST analysis.\n    \n\n\n          Results:\n        \n      \n      There were 40 rehospitalisations for heart failure, including 7 cardiac deaths and 2 implantations of LV assist device during follow-up (3.8±2.5 years). Univariable Cox proportional hazard regression analysis showed that NYHA class, blood pressure, B-type natriuretic peptide, LV end-diastolic and end-systolic volumes, LVEF, left atrium volume, GLS and LGE were significantly associated with long-term outcome. Multivariable analysis revealed that GLS and LGE were independently associated with long-term outcome (p<0.05, both). In additional analyses, we found independent associations between GLS and LV reverse remodelling after the optimal medical therapy, and between LGE and life-threatening arrhythmias (p<0.05, both).\n    \n\n\n          Conclusion:\n        \n      \n      Combining GLS and LGE could be useful for risk stratification and prognostic assessment in patients with DCM."
        },
        {
            "title": "Low Relative Lean Mass is Associated with Increased Likelihood of Abdominal Aortic Calcification in Community-Dwelling Older Australians.",
            "abstract": "Age-related loss of skeletal muscle is associated with increased risk of functional limitation and cardiovascular (CV) mortality. In the elderly abdominal aortic calcification (AAC) can increase CV risk by altering aortic properties which may raise blood pressure and increase cardiac workload. This study investigated the association between low muscle mass and AAC in community-dwelling older Australians. Data for this cross-sectional analysis were drawn from a 2010 sub-study of the Melbourne Collaborative Cohort Study in the setting of community-dwelling older adults. Three hundred and twenty-seven participants [mean age = 71 ± 6 years; mean BMI = 28 ± 5 kg/m(2); females n = 199 (62 %)] had body composition determined by dual-energy x-ray absorptiometry (DXA) and AAC determined by radiography. Participants were stratified into tertiles of sex-specific BMI-normalised appendicular lean mass (ALM). Those in the lowest tertile were considered to have low relative muscle mass. Aortic calcification score (ACS) was determined visually as the extent of calcification on the aortic walls between L1 and L4 vertebrae (range: 0-24). Severe AAC was defined as ACS ≥ 6. Prevalence of any AAC was highest in participants with low relative muscle mass (74 %) compared to the middle (65 %) and upper (53 %) tertiles (p trend = 0.006). The lower ALM/BMI tertile had increased odds (Odds ratio = 2.3; 95 % confidence interval: 1.1-4.6; p = 0.021) of having any AAC; and having more severe AAC (2.2; 1.2-4.0; p = 0.009) independent of CV risk factors, serum calcium and physical activity. AAC is more prevalent and severe in community-dwelling older adults with low relative muscle mass. Maintaining muscle mass could form part of a broader primary prevention strategy in reducing AAC."
        },
        {
            "title": "Can contrast-enhanced MR imaging predict survival in breast cancer?",
            "abstract": "Purpose:\n        \n      \n      To investigate the value of pre-operative contrast-enhanced MR imaging (CE-MRI) in predicting the disease-free and overall survival in breast cancer.\n    \n\n\n          Material and methods:\n        \n      \n      The study population consisted of 50 consecutive patients with histopathologically verified primary breast cancer who pre-operatively underwent CE-MRI examination between 1992 and 1993. A three-time point MR examination was performed where the enhancement rates (C1 and C2), signal enhancement ratio (SER=C1/C2) and washout (W=C1-C2) were calculated. The relation of these MR parameters to disease-free and overall survival was investigated. The median follow-up for surviving patients was 95 months. Univariate and multivariate statistical analyses were performed to evaluate the impact of different factors on prediction of survival.\n    \n\n\n          Results:\n        \n      \n      Of the MR parameters examined at univariate analysis, increased C1 (p=0.029), W (p=0.0081) and SER values (p=0.0081) were significantly associated with shorter disease-free survival, and only C1 (p=0.016) was related significantly to overall survival. Multivariate analysis for disease-free survival showed that the SER (p=0.014) and tumor size (p=0.001) were significant and independent predictors. Age (p=0.003), lymph node status (p=0.014), tumor size (p=0.039) and proliferating cell nuclear antigen index (p=0.053) remained independently associated with overall survival at multivariate analysis. C1 was not confirmed as an independent predictor of overall survival.\n    \n\n\n          Conclusion:\n        \n      \n      Our findings support the presumption that CE-MRI is useful in predicting the disease-free survival in patients with breast cancer."
        },
        {
            "title": "Validation of the pneumonia severity index. Importance of study-specific recalibration.",
            "abstract": "Objective:\n        \n      \n      To evaluate the predictive validity and calibration of the pneumonia severity-of-illness index (PSI) in patients with community-acquired pneumonia (CAP).\n    \n\n\n          Patients:\n        \n      \n      Randomly selected patients (n = 1,024) admitted with CAP to 22 community hospitals.\n    \n\n\n          Measurements and main results:\n        \n      \n      Medical records were abstracted to obtain prognostic information used in the PSI. The discriminatory ability of the PSI to identify patients who died and the calibration of the PSI across deciles of risk were determined. The PSI discriminates well between patients with high risk of death and those with a lower risk. In contrast, calibration of the PSI was poor, and the PSI predicted about 2.4 times more deaths than actually occurred in our population of patients with CAP.\n    \n\n\n          Conclusions:\n        \n      \n      We found that the PSI had good discriminatory ability. The original PSI overestimated absolute risk of death in our population. We describe a simple approach to recalibration, which corrected the overestimation in our population. Recalibration may be needed when transporting this prediction rule across populations."
        },
        {
            "title": "Exfoliative glaucoma and primary open-angle glaucoma: associations with death causes and comorbidity.",
            "abstract": "Purpose:\n        \n      \n      To investigate whether type of glaucoma or use of acetazolamide are associated with main cause of death and comorbidity.\n    \n\n\n          Material and methods:\n        \n      \n      The survival data, including date and cause of death, for 1147 patients with capsular or simple glaucoma who were ultimately hospitalized at the Eye Department, National Hospital, Oslo, between 1961 and 1970, were analysed. Binary logistic regression was carried out to investigate the patterns of death causes and comorbidity in subgroup analyses.\n    \n\n\n          Results:\n        \n      \n      Patients with exfoliative glaucoma (XFG) and those with primary open-angle glaucoma (POAG) showed no significant differences in rates of death caused by acute cerebrovascular diseases, cardiac diseases and cancer. Interestingly, we found that chronic cerebral diseases such as senile dementia, cerebral atrophy and chronic cerebral ischaemia (n = 81) were more common in patients with XFG than in those with POAG (p = 0.01) and in the group of acetazolamide users (p = 0.03). Patients with XFG had a higher probability of developing an acute cerebrovascular disease than patients with POAG (n = 228, p = 0.03).\n    \n\n\n          Conclusion:\n        \n      \n      In this retrospective study, we found that comorbidity with acute cerebrovascular disease and chronic cerebral diseases (senile dementia, cerebral atrophy and chronic cerebral ischaemia) were more common in patients with XFG than in patients with POAG. Prospective data are needed in order to conclude upon the associations found in this study."
        },
        {
            "title": "An Imaging Biomarker for Assessing Hepatic Function in Patients With Primary Sclerosing Cholangitis.",
            "abstract": "Background & aims:\n        \n      \n      We aimed to evaluate the potential of hepatobiliary phase magnetic resonance imaging (MRI) as parameter for assessment of hepatocellular function in patients with primary sclerosing cholangitis (PSC).\n    \n\n\n          Methods:\n        \n      \n      We collected data from 111 patients (83 male, 28 female; median, 44 years old), from March 2012 through March 2016, with a confirmed diagnosis of PSC who underwent MRI evaluation before and after injection (hepatobiliary phase) of a hepatocyte-specific contrast agent (gadoxetate disodium). Signal intensities were measured in each liver segment. Mean relative enhancement values were calculated and correlated with findings from liver functions tests, prognostic scoring systems (model for end-stage liver disease [MELD] score; Mayo risk score; Amsterdam-Oxford-PSC score), abnormalities detected by endoscopic retrograde cholangiopancreatography (using the Amsterdam cholangiographic classification system), and clinical endpoints (liver transplantation, cholangiocarcinoma, liver-related death). Our primary aim was to associate relative enhancement values with liver function and patient outcomes.\n    \n\n\n          Results:\n        \n      \n      Most patients had moderate-stage disease and had intermediate levels of risk (median MELD score, 8 and median Mayo score, 0.27). Clinical endpoints were reached by 21 patients (6 developed cholangiocarcinoma, 8 underwent liver transplantation, and 7 patients died). The highest levels of correlations were observed for relative enhancement 20 min after contrast injection and level of alkaline phosphatase (r = -0.636), bilirubin (r = -0.646), albumin (r = 0.538); as well as international normalized ratio (r = 0.456); MELD score (r = -0.587); Mayo risk score (r = -0.535), and Amsterdam-Oxford model score (r = -0.595) (P < .0001). Relative enhancement correlated with all clinical endpoints (all P < .05). A cutoff relative enhancement value of 0.65 identified patients with a clinical endpoint with 73.9% sensitivity 92.9% specificity (area under the receiver operating characteristic curve, 0.901; likelihood ratio, 10.34; P < .0001).\n    \n\n\n          Conclusions:\n        \n      \n      In an analysis of 111 patients with PSC, we found MRI-measured relative enhancement, using a hepatocyte-specific contrast agent, to identify patients with clinical outcomes with 73.9% sensitivity 92.9% specificity. Long-term, multicenter studies are needed to further evaluate this marker of PSC progression."
        },
        {
            "title": "Non-cytomegalovirus ocular opportunistic infections in patients with acquired immunodeficiency syndrome.",
            "abstract": "Purpose:\n        \n      \n      To report the incidence and clinical outcomes of non-cytomegalovirus (non-CMV) ocular opportunistic infections in patients with acquired immunodeficiency syndrome (AIDS) in the era of highly active antiretroviral therapy.\n    \n\n\n          Design:\n        \n      \n      Multicenter, prospective, observational study of patients with AIDS.\n    \n\n\n          Methods:\n        \n      \n      Medical history, ophthalmologic examination, and laboratory tests were performed at enrollment and every 6 months subsequently. Once an ocular opportunistic infection was diagnosed, patients were seen every 3 months for outcomes.\n    \n\n\n          Results:\n        \n      \n      At enrollment, 37 non-CMV ocular opportunistic infections were diagnosed: 16 patients, herpetic retinitis; 11 patients, toxoplasmic retinitis; and 10 patients, choroiditis. During the follow-up period, the estimated incidences (and 95% confidence intervals [CI]) of these were: herpetic retinitis, 0.007/100 person-years (PY) (95% CI 0.0004, 0.039); toxoplasmic retinitis, 0.007/100 PY (95% CI 0.004, 0.039); and choroiditis, 0.014/ 100 PY (95% CI 0.0025, 0.050). The mortality rates appeared higher among those patients with newly diagnosed or incident herpetic retinitis and choroiditis (rates = 21.7 deaths/100 PY [P = .02] and 12.8 deaths/100 PY [P = .04]), respectively, than those for patients with AIDS without an ocular opportunistic infection (4.1 deaths/100 PY); toxoplasmic retinitis did not appear to be associated with greater mortality (6.4/100 PY, P = .47). Eyes with newly diagnosed herpetic retinitis appeared to have a poor visual prognosis, with high rates of visual impairment (37.9/100 PY) and blindness (17.5/100 PY), whereas those outcomes in eyes with choroiditis appeared to be lower (2.3/100 PY and 0/100 PY, respectively).\n    \n\n\n          Conclusions:\n        \n      \n      Although uncommon, non-CMV ocular opportunistic infections may be associated with high rates of visual loss and/or mortality."
        },
        {
            "title": "Ranibizumab Plus Panretinal Photocoagulation versus Panretinal Photocoagulation Alone for High-Risk Proliferative Diabetic Retinopathy (PROTEUS Study).",
            "abstract": "Purpose:\n        \n      \n      Comparison of the efficacy of ranibizumab (RBZ) 0.5 mg intravitreal injections plus panretinal photocoagulation (PRP) versus PRP alone in the regression of the neovascularization (NV) area in subjects with high-risk proliferative diabetic retinopathy (HR-PDR) over a 12-month period.\n    \n\n\n          Design:\n        \n      \n      Prospective, randomized, multicenter, open-label, phase II/III study.\n    \n\n\n          Participants:\n        \n      \n      Eighty-seven participants (aged ≥18 years) with type 1/2 diabetes and HR-PDR (mean age, 55.2 years; 37% were female).\n    \n\n\n          Methods:\n        \n      \n      Participants were randomized (1:1) to receive RBZ+PRP (n = 41) or PRP monotherapy (n = 46). The RBZ+PRP group received 3 monthly RBZ injections along with standard PRP. The PRP monotherapy group received standard PRP between day 1 and month 2; thereafter, re-treatments in both groups were at the investigators' discretion.\n    \n\n\n          Main outcome measures:\n        \n      \n      The primary outcome was regression of NV total, on the disc (NVD) plus elsewhere (NVE), defined as any decrease in the area of NV from the baseline to month 12. Secondary outcomes included best-corrected visual acuity (BCVA) changes from baseline to month 12, time to complete NV regression, recurrence of NV, macular retinal thickness changes from baseline to month 12, need for treatment for diabetic macular edema, need for vitrectomy because of occurrence of vitreous hemorrhage, tractional retinal detachment or other complications of DR, and adverse events (AEs) related to treatments.\n    \n\n\n          Results:\n        \n      \n      Seventy-seven participants (88.5%) completed the study. Overall baseline demographics were similar for both groups, except for age. At month 12, 92.7% of participants in the RBZ+PRP group presented NV total reduction versus 70.5% of the PRP monotherapy participants (P = 0.009). The number of participants with NVD and NVE reductions was higher with RBZ+PRP (93.3% and 91.4%, respectively) versus PRP (68.8% and 73.7%, respectively), significant only for NVE (P = 0.048). Complete NV total regression was observed in 43.9% in the RBZ+PRP group versus 25.0% in the PRP monotherapy group (P = 0.066). At month 12, the mean BCVA was 75.2 letters (20/32) in the RBZ+PRP group versus 69.2 letters (20/40) in the PRP monotherapy group (P = 0.104). In the RBZ+PRP group, the mean number of PRP treatments over month 12 was 3.5±1.3, whereas in the PRP monotherapy group, it was 4.6±1.5 (P = 0.001). No deaths or unexpected AEs were reported.\n    \n\n\n          Conclusions:\n        \n      \n      Treatment with RBZ+PRP was more effective than PRP monotherapy for NV regression in HR-PDR participants over 12 months."
        },
        {
            "title": "Utility values in Singapore Chinese adults with primary open-angle and primary angle-closure glaucoma.",
            "abstract": "Purpose:\n        \n      \n      To evaluate utility values in Chinese glaucoma patients.\n    \n\n\n          Patient and methods:\n        \n      \n      Singapore Chinese residents (n = 213) with primary open-angle glaucoma or primary angle-closure glaucoma were recruited from a single tertiary ophthalmic center. Standard face-to-face interviews were conducted to ask about utility values (time trade-off and standard gamble for both death and blindness). Ocular information, including current visual acuity, intraocular pressure, visual field defect, and cup-disc ratio were also obtained.\n    \n\n\n          Results:\n        \n      \n      The mean time trade-off utility value was 0.88 (95% confidence interval 0.85, 0.91), and standard gamble for death and blindness were 0.94 (95% confidence interval 0.93, 0.96) and 0.95 (95% confidence interval 0.93, 0.97), respectively. Only 35.7% of patients were willing to trade time, and 34.3% willing to risk blindness in return for perfect vision. Both primary angle-closure glaucoma and primary open-angle glaucoma patients had similar utility values. After adjusting for age, gender, language spoken, educational level, and diagnosis, patients with better eye visual field PSD > 10 were 2.52 times (95% confidence interval 1.13, 5.61) more willing to trade time. In a multivariate model, the odds ratio of willingness to risk blindness for a complete hypothetical glaucoma cure was 9.88 (95% confidence interval 1.65, 59.23) for patients who had only visited an ophthalmologist 15 years or more ago, and 0.53 (95% confidence interval 0.27, 1.02) for patients who had previous trabeculectomy.\n    \n\n\n          Conclusion:\n        \n      \n      Most Chinese glaucoma patients in Singapore are not willing to trade time or risk blindness. Patients with worse visual fields in the better-seeing eye are more willing to trade time; whereas patients who have not seen an ophthalmologist for at least 15 years or who had no history of a previous trabeculectomy are more willing to risk blindness."
        },
        {
            "title": "Do we need a computed tomography examination in all patients with acute pancreatitis within 72 h after admission to hospital for the detection of pancreatic necrosis?",
            "abstract": "Background:\n        \n      \n      The aim of this prospective study was to define the role of an initial contrast-enhanced computed tomography (CT) obtained within 72 h after admission to hospital for determining the prognosis of acute pancreatitis and to investigate whether CT scans can be replaced by conventional prognostic parameters.\n    \n\n\n          Methods:\n        \n      \n      The study involves 231 patients admitted to the Lüneburg clinic with a first attack of acute pancreatitis from 1988 to 1995. In all of them, a contrast-enhanced CT was performed within 72 h of admission and scored according to Balthazar. The results were compared with the Ranson and Imrie laboratory prognostic scores and with parameters of the severity of the disease: the initial organ failure according to the Atlanta classification; days spent on intensive care unit or altogether in hospital; indication for artificial ventilation, dialysis and surgical intervention (necrosectomy); development of pancreatic pseudocysts; and mortality.\n    \n\n\n          Results:\n        \n      \n      Although there was a good statistical correlation between Ranson, Imrie, and Balthazar scores with the severity of the disease (P < 0.001 to P = 0.03), low and moderately raised Ranson (0-2, 0-5 points) and Imrie scores (0-1.0-3 points) failed to identify all patients with pancreatic necrosis with sufficient sensitivity rates (31.7; 78.0 and 39.0; 78.0%), positive (32.6; 25.3 and 75.0; 45.0%) and negative (91.0; 87.9 and 85.4; 84.8%) predictive values.\n    \n\n\n          Conclusions:\n        \n      \n      A contrast-enhanced CT on admission correlates significantly with the severity of the disease and cannot be replaced by conventional laboratory prognostic scores. The decision to use a CT cannot depend on the results of the Ranson/Imrie scores."
        },
        {
            "title": "Modification of the Association between Visual Impairment and Mortality by Physical Activity: A Cohort Study among the Korean National Health Examinees.",
            "abstract": "The association between visual impairment and higher mortality remains unclear. In addition, evidence is lacking on the interaction between visual function and physical activity on mortality. We used data of individuals with no disability or with visual impairment among those who participated in the National Health Screening Program in Korea in 2009 or 2010. We constructed Cox proportional hazard models adjusted for potential confounders to evaluate the independent association between visual impairment and mortality. More severe visual impairment was associated with higher all-cause mortality (p-value for trend = 0.03) and mortality due to cardiovascular diseases (p-value for trend = 0.02) and that due to other diseases (p-value for trend = 0.01). We found an interaction on an additive scale between visual impairment and no physical activity on all-cause mortality (relative excess risk due to interaction = 1.34, 95% confidence interval: 0.37, 2.30, p-value = 0.01). When we stratified the study population by physical activity, the association between visual impairment and mortality was only found among individuals who did not engage in regular physical activity (p-value for trend = 0.01). We found an independent association between visual impairment and mortality and modification of this association by physical activity."
        },
        {
            "title": "Laryngeal or hypopharyngeal squamous cell carcinoma: can follow-up CT after definitive radiation therapy be used to detect local failure earlier than clinical examination alone?",
            "abstract": "Purpose:\n        \n      \n      To determine if follow-up computed tomography (CT) after definitive radiation therapy for laryngeal or hypopharyngeal (laryngopharyngeal) carcinoma allows the detection of local failure earlier than clinical examination alone.\n    \n\n\n          Materials and methods:\n        \n      \n      Pre- and post-radiation therapy follow-up CT scans in 66 patients were reviewed retrospectively. All patients underwent definitive hyperfractionated radiation therapy and were followed up clinically for at least 2 years after its completion. Post-radiation therapy CT scans (N = 153) were evaluated for posttreatment changes with a three-point score: A score of 1 represented expected posttreatment changes; 2, focal mass with a maximal diameter of less than 1 cm and/or asymmetric obliteration of laryngeal tissue planes; or 3, focal mass with a maximal diameter equal to or greater than 1 cm or estimated tumor volume reduction of less than 50%. All patients underwent the first posttreatment CT study 1-6 months after therapy. New or progressive laryngeal cartilage changes were noted. The clinical impression of the larynx at the time of each follow-up CT scan was also recorded.\n    \n\n\n          Results:\n        \n      \n      In 12 of 29 (41%) patients with treatment failure at the primary site, follow-up CT scans were definite for local failure (score, 3) a mean of 5.5 months (median, 3.5 months; range, 1-17 months) before clinical examination results.\n    \n\n\n          Conclusion:\n        \n      \n      In many patients, follow-up CT shows local failure earlier than does clinical examination alone."
        },
        {
            "title": "Building momentum: an ethnographic study of inner-city redevelopment.",
            "abstract": "Objectives:\n        \n      \n      One factor contributing to the decay of inner-city areas, and to consequent excess mortality, is the massive loss of housing. This report studied the effects of a redevelopment project on social functioning in an inner-city community.\n    \n\n\n          Methods:\n        \n      \n      This ethnographic study included the following elements: a longitudinal study of 10 families living in renovated housing, repeated observations and photographing of the street scene, focus groups, and informal interviews with area residents. The project was located in the Bradhurst section of Harlem in New York City and was focused on a redevelopment effort sponsored by local congregations.\n    \n\n\n          Results:\n        \n      \n      Those who were able to move into newly renovated housing found that their living conditions were greatly improved. Neighborhood revitalization lagged behind the rehabilitation of individual apartment houses. This uneven redevelopment was a visual and sensory reminder of \"what had been.\" Residents missed the warmth and social support that existed in Harlem before its decline.\n    \n\n\n          Conclusions:\n        \n      \n      Rebuilding damaged housing contributes greatly to the well-being of inner-city residents. The current pace and scope of rebuilding are insufficient to restore lost vitality."
        },
        {
            "title": "Jejunostomy tube placement in refractory diabetic gastroparesis: a retrospective review.",
            "abstract": "Objectives:\n        \n      \n      Severe diabetic gastroparesis leading to recurrent episodes of diabetic ketoacidosis and frequent hospitalizations can be among the most disabling of all diabetic complications. Surgical placement of a jejunostomy tube (J-tube) beyond the affected stomach to deliver fluid, nutrients, and medication is one of the few therapeutic options remaining in the cohort of patients who have failed standard medical therapy. This study attempts to define the natural history of refractory diabetic gastroparesis and the risks and benefits of J-tube placement.\n    \n\n\n          Methods:\n        \n      \n      A total of 26 patients with diabetic gastroparesis requiring J-tube placement were identified between 1980 and 1994. Medical chart review and telephone follow-up were performed using standardized questionnaires.\n    \n\n\n          Results:\n        \n      \n      All patients had documented delayed gastric emptying, had failed medical therapy, and had been hospitalized on multiple occasions. Neuropathy, retinopathy, and nephropathy were observed in 88%, 81%, and 65% of patients, respectively. The mean duration of study follow-up was 47 months (1-130 months). The mean age of subjects at the time of J-tube placement was 31 yr, and a preponderance of female patients (73%) was noted. There were 23 major complications in 14 patients requiring surgery or hospitalization and 47 minor complications in 21 patients managed on an outpatient basis. The mean duration of J-tube use was 20 months. There were 10 deaths during follow-up, one related to J-tube placement. Retrospectively, 39% reported improved symptoms of nausea/vomiting (4% worsened), 52% reported fewer hospitalizations (4%, more frequent), 56% reported improved nutritional status (4% worsened), and 83% reported improved overall health (4% worsened) after J-tube placement. The improvement in overall health status was the only symptom that reached statistical significance.\n    \n\n\n          Conclusion:\n        \n      \n      Severe refractory gastroparesis is associated with multiple hospitalizations, a high incidence of concomitant diabetic complications, and a mortality rate of 38% at 4 yr. There is a high incidence of complications following J-tube placement in this population. Despite this, most patients retrospectively reported improved overall health after J-tube placement. Therefore, placement of a J-tube may be a workable option with acceptable perioperative morbidity and mortality rates in selected patients with severe diabetic gastroparesis who have failed medical therapy. A prospective study of J-tube placement and other available means of nutritional support is needed to demonstrate further the efficacy of this intervention in patients with severe diabetic gastroparesis."
        },
        {
            "title": "MRI Findings in Patients With Leukemia and Positive CSF Cytology: A Single-Institution 5-Year Experience.",
            "abstract": "Objective:\n        \n      \n      The purposes of this study were to describe the spectrum of MRI findings and determine the prognostic role of MRI in adults with acute leukemia with positive CSF cytology.\n    \n\n\n          Materials and methods:\n        \n      \n      In this retrospective study of 34 patients (19 women, 15 men; mean age, 51 years; range, 18-72 years) treated for CNS leukemia between 2006 and 2011, 31 (91%) contrast-enhanced brain and 14 (41%) spine MRI studies were reviewed by two radiologists to note patterns of enhancement. Interobserver agreement and correlation of enhancement with outcome were analyzed.\n    \n\n\n          Results:\n        \n      \n      MRI showed abnormal findings in 25 patients (74%). Pachymeningeal enhancement (n = 9/31, 29%), leptomeningeal enhancement (n = 6/31, 19%), cranial nerve enhancement (n = 9/31, 29%), masslike enhancement (n = 3/31, 10%), and spinal meningeal enhancement (n = 10/14, 71%) were identified. There was strong interobserver agreement (κ = 0.906). Survival rates were shorter to a statistically significant degree with pachymeningeal enhancement (median, 7 months; interquartile range [IQR], 5-8 months versus median, 26 months; IQR, 15 months to not reached; p = 0.004) and two or more sites of enhancement (median, 8 months; IQR, 3-13 months versus median, 19 months; IQR, 9 months to not reached; p = 0.046).\n    \n\n\n          Conclusion:\n        \n      \n      Brain or spine MRI examinations (or both) showed abnormal findings in nearly three-fourths of adults with acute leukemia with positive CSF cytology who were imaged for neurologic symptoms. Pachymeningeal enhancement and two or more sites of brain involvement were associated with shorter survival."
        },
        {
            "title": "Has blood glucose level measured on admission to hospital in a patient with acute pancreatitis any prognostic value?",
            "abstract": "Background:\n        \n      \n      Early detection of pancreatic necrosis allows better management of the disease. Contrast-enhanced computed tomography (CT) as the gold standard for detecting pancreatic necrosis is expensive.\n    \n\n\n          Aim of the study:\n        \n      \n      This study was to evaluate for the first time whether blood glucose estimation on hospital admission--a simple, cheap, readily available laboratory parameter--may detect pancreatic necrosis and have prognostic value in acute pancreatitis.\n    \n\n\n          Methods:\n        \n      \n      Single blood glucose estimation upon hospital admission was evaluated prospectively for detecting pancreatic necrosis and as a prognostic indicator. The study included 241 nondiabetic patients with a first attack of acute pancreatitis. All underwent CT within 72 h of admission.\n    \n\n\n          Results:\n        \n      \n      High blood glucose (> 125 mg/dl) correlated significantly with complex high clinical and biochemical prognostic scores (Ranson, Imrie), a high Balthazar score, pancreatic pseudocysts, and a long hospital stay, but not with organ failure, indication for artificial ventilation, dialysis, surgery, length of intensive care, and mortality. Pancreatic necrosis detection sensitivity of high blood glucose was 83%, specificity 49%, positive predictive value 28%, and negative predictive value 92%.\n    \n\n\n          Conclusion:\n        \n      \n      A patient with normal blood glucose on admission is unlikely to have pancreatic necrosis. Contrast-enhanced CT would not be needed unless the patient fails to improve."
        },
        {
            "title": "Review of the performance of methods to identify diabetes cases among vital statistics, administrative, and survey data.",
            "abstract": "Purpose:\n        \n      \n      The ability to identify prevalent cases of diagnosed diabetes is crucial to monitoring preventative care practices and health outcomes among persons with diagnosed diabetes.\n    \n\n\n          Methods:\n        \n      \n      We conducted a comprehensive literature review to assess and summarize the validity of various strategies for identifying individuals with diagnosed diabetes and to examine the factors influencing the validity of these strategies.\n    \n\n\n          Results:\n        \n      \n      We found that studies using either administrative data or survey data were both adequately sensitive (i.e., identified the majority of cases of diagnosed diabetes) and highly specific (i.e., did not identify the individuals as having diabetes if they did not). In contrast, studies based on cause-of-death data from death certificates were not sensitive, failing to identify about 60% of decedents with diabetes and in most of these studies, researchers did not report specificity or positive predictive value.\n    \n\n\n          Conclusions:\n        \n      \n      Surveillance is critical for tracking trends in diabetes and targeting diabetes prevention efforts. Several approaches can provide valuable data, although each has limitations. By understanding the limitations of the data, investigators will be able to estimate diabetes prevalence and improve surveillance of diabetes in the population."
        },
        {
            "title": "Long-term Follow-up of Cytomegalovirus Retinitis in Non-HIV Immunocompromised Patients: Clinical Features and Visual Prognosis.",
            "abstract": "Purpose:\n        \n      \n      To evaluate clinical features and long-term visual outcome of cytomegalovirus (CMV) retinitis in patients without human immunodeficiency virus (HIV) infection, and to determine factors that predict visual outcome.\n    \n\n\n          Design:\n        \n      \n      Retrospective cohort study.\n    \n\n\n          Methods:\n        \n      \n      Consecutive patients with CMV retinitis without HIV infection were reviewed. Main outcome measures included clinical features, proportion of eyes with 6-month and final visual acuity (VA) <20/70 and <20/400, and odds ratios of factors associated with poor visual outcome.\n    \n\n\n          Results:\n        \n      \n      A total of 20 eyes from 13 patients were included with a median follow-up time of 17 months. All had at least 6 months of follow-up except 1 patient who died from sepsis at 1 month. At presentation, 50% of eyes had VA <20/70 and 25% had VA <20/400. Zone 1 involvement occurred in 55% and vitreous haze ≥grade 2+ occurred in 25%. Recurrence occurred in 33.3% at a mean time of 6.4 ± 3.3 weeks after discontinuation of anti-CMV therapy. The retinal detachment rate was 21.7% per eye-year and mortality rate was 11.7% per person-year. At final visit, 60% had VA <20/70 and 35% had VA <20/400. Macular involvement was significantly associated with poor final VA <20/400 (odds ratio = 25.00, P = .016).\n    \n\n\n          Conclusions:\n        \n      \n      CMV retinitis without HIV infection was often aggressive at presentation. Significant intraocular inflammation was not uncommon. The long-term visual outcome was poor, especially in those with macular involvement."
        },
        {
            "title": "The depletion of susceptibles effect in the assessment of burden-of-illness: the example of age-related macular degeneration in the community-dwelling elderly population of Quebec.",
            "abstract": "Background:\n        \n      \n      With the aging of the population, age-related macular degeneration (AMD) is becoming a public health concern. Few studies have assessed its consequences on morbidity and mortality, and the findings are conflicting.\n    \n\n\n          Objectives:\n        \n      \n      To assess the risk of depression, fracture, institutionalization, and death among elderly patients with suspected exudative AMD and the impact of the depletion of susceptibles effect in a burden-of-illness study.\n    \n\n\n          Methods:\n        \n      \n      A population-based retrospective cohort study was conducted in the community-dwelling elderly population of Quebec. The cohort was assembled through the Quebec medical claims database (RAMQ). Among patients age 65 and older with a claim involving a diagnosis of AMD over the years 2000 to 2004, those with suspected exudative AMD (n=2,071) were retained, using fluorescein angiography as a marker. The reference cohort consisted of a sample of 16,932 elderly without a claim involving AMD or visual impairment.\n    \n\n\n          Results:\n        \n      \n      Suspected exudative AMD was associated with an increased risk of depression (hazard ratio HR=1.3, 95%CI 1.18-1.43) and fracture (HR=1.19, 95%CI 1.03-1.37), but a decreased risk of institutionalization (HR=0.55, 95%CI 0.42-0.71) and death (HR=0.68, 95%CI 0.59-0.78). After adjustment for the incident/prevalent status of the AMD, the association between suspected exudative AMD and institutionalization was no longer statistically significant (HR=0.75, 95%CI 0.5-1.12).\n    \n\n\n          Conclusions:\n        \n      \n      These findings enhance the need to detect visual loss and to consider patients' ability to adapt to AMD, to maintain their quality of life. Failure to account for duration of illness and the depletion of susceptibles effect may bias results of burden-of-illness studies."
        },
        {
            "title": "Vision and quality of life: development of methods for the VisQoL vision-related utility instrument.",
            "abstract": "Purpose:\n        \n      \n      To describe the methods and innovations used in constructing the VisQoL, a vision-related utility instrument for the health economic evaluation of eye care and rehabilitation programs.\n    \n\n\n          Methods:\n        \n      \n      The VisQoL disaggregates vision into six items. Utilities were estimated for item worst responses (the worst level for each item, with all other items at their best level) and VisQoL all-worst responses (all items at their worst level) using the time trade-off procedure. Time trade-off questions require people to imagine living a fixed number of years with a particular health condition and then indicate how many of those years of life they would be willing to trade to have perfect health. Where respondents indicated a health state was \"worse than death\" negative utilities were estimated. Time trade-off questions minimized the \"focusing effect,\" which occurs if respondents discount the fact that all other aspects of health are at their best when answering questions, by using pictorial and verbal aids.\n    \n\n\n          Results:\n        \n      \n      Item utilities were combined using a multiplicative model, and VisQoL model utilities placed on a scale where 0.00 and 1.00 represent full health and death, respectively. The VisQoL allows utilities to be calculated for a wide range of vision-related conditions.\n    \n\n\n          Conclusion:\n        \n      \n      The 6-item VisQoL has excellent psychometric properties and is specifically designed to be sensitive to vision-related quality of life. It is the first instrument to permit the rapid estimation of utility values for use in economic evaluations of vision-related programs."
        },
        {
            "title": "Cutaneous changes of nephrogenic systemic fibrosis: predictor of early mortality and association with gadolinium exposure.",
            "abstract": "Objective:\n        \n      \n      Nephrogenic systemic fibrosis (NSF) is a rapidly progressive, debilitating condition that causes cutaneous and visceral fibrosis in patients with renal failure. Little is known about its prevalence or etiology. The aim of this study was to establish the prevalence of NSF and associated risk factors\n    \n\n\n          Methods:\n        \n      \n      Two cohorts of patients were recruited from 6 outpatient hemodialysis centers and examined for cutaneous changes of NSF, which were defined using a scoring system based on hyperpigmentation, hardening, and tethering of skin on the extremities. Demographic data were gathered, mortality was followed up prospectively for 24 months, and gadolinium exposure was ascertained for a subgroup of patients in the second cohort.\n    \n\n\n          Results:\n        \n      \n      Examination reproducibility was 97% in cohort 1. In cohort 2, 25 (13%) of 186 patients demonstrated cutaneous changes of NSF. Twenty-four-month mortality following examination was 48% and 20% in patients with and those without cutaneous changes of NSF, respectively (adjusted hazard ratio 2.9, 95% confidence interval [95% CI] 1.4-5.9). Cutaneous changes of NSF were observed in 16 (30%) of 54 patients with prior exposure to gadopentetate dimeglumine contrast during imaging studies. Exposure to gadolinium-containing contrast was associated with an increased risk of developing cutaneous changes of NSF (odds ratio 14.7, 95% CI 1.9-117.0) compared with nonexposed patients.\n    \n\n\n          Conclusion:\n        \n      \n      In patients receiving hemodialysis, NSF is an underrecognized disorder that is associated with increased mortality. Exposure to gadolinium-containing contrast material appears to be a significant risk factor for the development of NSF."
        },
        {
            "title": "Suprasellar meningiomas--neurological and visual outcome at long-term follow-up in a homogeneous series of patients treated microsurgically.",
            "abstract": "Most of the previously published surgical series of suprasellar meningiomas have two disadvantages: (1) patients involved were treated within a relatively long time period, making analysis more difficult, (2) radiographic long term follow-up examinations with either CT- or MRI-scans were not performed. Both disadvantages were overcome in our retrospective clinical study, consisting of 50 consecutive patients with suprasellar meningiomas treated between 1982 and 1991. Radiological, ophthalmological, and neurological investigations were performed preoperatively, postoperatively and at long term follow-up (mean: 5.7 years). A radiologically confirmed radical tumour removal could be achieved in 84% of patients. Both, the peri-operative mortality (2%) and serious operative morbidity (6%) were low. However, 12% of patients developed late onset epilepsy. At long term follow-up, visual function was improved in 67%, unchanged in 9% and worsened in 24%. In more than 50% of patients the vision showed recovery over a longer time period than the first 10 days after operation. Radiographic control examinations revealed tumour recurrences in 2 patients (both asymptomatic) and progress of residual tumour in 5 patients (2 symptomatic, 3 asymptomatic). Since introduction of modern neurosurgery, a clear improvement in the surgical treatment of suprasellar meningiomas can be observed. However, the still long delay in diagnosing these tumours correctly prevents a further improvement of the ophthalmological results at long-term follow-up. Due to a relatively high rate of late onset epilepsy, anticonvulsive prophylaxis for 6 months seems to be justified. Regarding present preoperative diagnostic measures, ia-DSA seems only be indicated in patients with CT/MRI-scans, suspicious for tumourous narrowing or invasion of major cerebral arteries. In addition, we recommend radiographic control examinations at regular time intervals to confirm radical tumour removal and to detect the \"ideal\" point of time for renewed treatment."
        },
        {
            "title": "Clinical characteristics and prognosis of orbital invasive aspergillosis.",
            "abstract": "Purpose:\n        \n      \n      To evaluate the clinical characteristics and prognosis of orbital invasive aspergillosis, a major cause of morbidity and mortality in immunocompromised patients.\n    \n\n\n          Methods:\n        \n      \n      Review of the medical records of Severance Hospital, Yonsei University College of Medicine, from 1995 to 2007 revealed 15 patients with orbital invasive aspergillosis. A retrospective cohort study was conducted to evaluate the clinical characteristics, radiologic findings, associated underlying diseases, and prognosis. Risk factors for mortality were analyzed.\n    \n\n\n          Results:\n        \n      \n      A total of 15 cases of orbital invasive aspergillosis were included in this study. The mean age for all patients was 61.9 years. The most common underlying disease was diabetes mellitus and all patients had paranasal sinus infections. The most common ocular symptoms were visual disturbance, periorbital swelling, and periorbital pain. The mortality rate associated with invasive aspergillosis was 40%. According to univariate analysis, variables significantly associated with invasive aspergillosis-related mortality included fever and incorrect initial diagnosis.\n    \n\n\n          Conclusions:\n        \n      \n      Fever and incorrect initial diagnosis were found to be associated with high mortality rates in patients with orbital invasive aspergillosis. Further study is necessary to determine optimal strategies for early diagnosis and appropriate treatment."
        },
        {
            "title": "Associations between hearing impairment and mortality risk in older persons: the Blue Mountains Hearing Study.",
            "abstract": "Purpose:\n        \n      \n      To assess whether hearing loss predicts an increased risk of mortality.\n    \n\n\n          Methods:\n        \n      \n      The Blue Mountains Hearing Study examined 2956 persons (49+ years) during 1997 to 2000. The Australian National Death Index was used to identify deaths until 2005. Hearing loss was defined as the pure-tone average (0.5-4 kHz) of air-conduction hearing thresholds greater than 25 dB HL. Associations between hearing loss and mortality risk were estimated using Cox regression and structural equation modeling (SEM).\n    \n\n\n          Results:\n        \n      \n      When we used Cox regression, we discovered that hearing loss was associated with increased risk of cardiovascular (hazard ratio [HR] 1.36, 95% confidence interval [CI] 1.08-1.84) and all-cause (AC) mortality (HR 1.39, 95% CI 1.11-1.79) after adjustment for age and sex but not after multivariable adjustment. SEM pathway analysis, however, revealed a greater AC mortality risk (HR 2.58, 95% CI 1.64-4.05) in persons with hearing loss, which was mediated: cognitive impairment (HR 1.45, 95% CI 1.08-1.94) and walking disability (HR 1.63, 95% CI 1.24-2.15). These variables increased mortality both directly and indirectly through effects on self-rated health.\n    \n\n\n          Conclusions:\n        \n      \n      Hearing loss was associated with increased AC mortality via three mediating variables: disability in walking, cognitive impairment, and self-rated health. It is important to recognize that persons with combined disabilities are at increased risk of cardiovascular and AC mortality."
        },
        {
            "title": "Contrast-enhanced ultrasound -guided axillary lymph node core biopsy: Diagnostic accuracy in preoperative staging of invasive breast cancer.",
            "abstract": "Objectives:\n        \n      \n      To evaluate accuracy of contrast enhanced ultrasound (CEUS)-sentinel procedure followed by core biopsy (CB) and marking in patients with breast cancer. To compare the axillary metastatic tumour burden in patients with positive vs. negative CB results.\n    \n\n\n          Methods:\n        \n      \n      Two radiologists in our tertiary care hospital performed axillary CEUS sentinel procedures on consecutive US node negative breast cancer patients. The first enhancing lymph node (LN) was core biopsied and marked with a breast coil. The results were compared to final histopathology. We analysed the diagnostic performance of CEUS CB and its ability to detect patients with higher axillary burden (>2 metastasis).\n    \n\n\n          Results:\n        \n      \n      During the study period between January 2013 and December 2014, altogether 54 patients (mean age 60.4 years) were included in the statistical analysis. The sensitivity for CEUS CB was 66.7%, specificity 100%, PPV 100%, NPV 93.8% and overall accuracy 94.4%. The method correctly recognised all the axillae with higher tumour burdens (sensitivity 100%, N=3) and 59.3% of coils marking the LNs were discovered.\n    \n\n\n          Conclusion:\n        \n      \n      CEUS -guided axillary CB proved to be feasible and accurate procedure with moderate sensitivity and it clearly identified the higher axillary tumour burden. The coil marking of LNs as used cannot be recommended. In clinical routine, CEUS procedure might be recommended in selective patient populations."
        },
        {
            "title": "Plaque radiotherapy for juxtapapillary choroidal melanoma overhanging the optic disc in 141 consecutive patients.",
            "abstract": "Objective:\n        \n      \n      To evaluate tumor control with plaque radiotherapy for juxtapapillary choroidal melanoma that overhangs the optic disc.\n    \n\n\n          Methods:\n        \n      \n      Retrospective medical record review of 141 consecutive patients with data on complications of treatment, final visual acuity, visual loss, enucleation, tumor recurrence, metastasis, and death.\n    \n\n\n          Results:\n        \n      \n      The median patient age was 61 years. Presenting symptoms included reduced visual acuity in 72 eyes (51%), photopsia in 14 (10%), and visual field defect in 18 (13%); 35 patients (25%) were asymptomatic. The median tumor basal diameter was 11 mm and the median thickness was 5.2 mm. The tumor overhung 50% or less of the disc in 88 eyes (62%) and more than 50% of the disc in 53 eyes (38%). In 19 cases (13%), the tumor overhung the entire disc. All patients were treated with plaque radiotherapy, using a notched design in 126 eyes (89%) and a round design in 14 eyes (10%), with iodine 125 in 132 eyes (94%) and cobalt 60 in 9 eyes (6%). The median radiation dose to the tumor apex was 8500 cGy. Adjuvant transpupillary thermotherapy was used in 54 eyes (39%). During a mean follow-up of 56 months, complications included nonproliferative retinopathy in 61 eyes (51%), proliferative retinopathy in 26 (22%), maculopathy in 44 (37%), papillopathy in 57 (48%), neovascular glaucoma in 23 (19%), and vitreous hemorrhage in 48 (40%). A final visual acuity of 20/200 or worse was measured in 72 eyes (77%), and visual loss of more than 5 Snellen lines occurred in 59 eyes (63%). Enucleation was necessary in 27 eyes (23%). Tumor recurrence was found in 12 eyes (10%). Metastasis developed in 15 patients (13%) and death in 4 cases (3%).\n    \n\n\n          Conclusions:\n        \n      \n      Using plaque radiotherapy for choroidal melanoma overhanging the optic disc, local tumor control was achieved in 90% of cases. Tumor and radiation effects led to poor visual acuity in 77% of eyes. The metastatic rate was 13% and the mortality rate was 3%."
        },
        {
            "title": "Colorectal anastomotic leak: delay in reintervention after false-negative computed tomography scan is a reason for concern.",
            "abstract": "Background:\n        \n      \n      Early detection of anastomotic leakage (AL) after colorectal surgery followed by timely reintervention is of crucial importance. The aim of this study was to investigate the accuracy of computed tomography (CT) imaging for AL and the effects of delay in reintervention after a false-negative CT.\n    \n\n\n          Methods:\n        \n      \n      All files from patients who had colorectal surgery with primary anastomoses between 2009 and 2014 were reviewed. The predictive value of CT scanning for AL was determined and correlated with short-term postoperative patient outcomes. In addition, factors predictive of false-negative scans were assessed.\n    \n\n\n          Results:\n        \n      \n      Six hundred and twenty-eight patient files were reviewed. In total, a CT scan was performed in 127 patients. Overall, leakage was seen in 49 patients (7.8%). The positive and negative predictive values were 78 and 88%, respectively. Sensitivity was 73% and specificity 91%. In patients with a true-positive CT (n = 24), reintervention followed after a median interval of 0 days (IQR 1), whereas this was 1 day (IQR 2) in the false-negative group (n = 11) (p < 0.05). This was associated with a significantly increased mortality rate (1/24 = 4.2% vs 5/11 = 45.5%) (p < 0.005), an increased length of hospital stay [median 28 days (IQR 26) vs 54 days (IQR 20) (p < 0.05)].\n    \n\n\n          Conclusions:\n        \n      \n      Delayed reintervention after false-negative CT scanning is associated with a high mortality rate and a significant increase in length of hospital stay."
        },
        {
            "title": "Utility-based estimates of the relative morbidity of visual impairment and angina.",
            "abstract": "Purpose:\n        \n      \n      To quantify and compare the reduction in quality of life due to visual impairment and angina using patient preferences (utilities).\n    \n\n\n          Methods:\n        \n      \n      Using a standard time tradeoff method, we obtained utilities for current vision, monocular and binocular blindness, current angina, and moderate angina in 60 patients with both vision problems and angina pectoris who sought care at the National Eye Institute (NEI), National Naval Medical Center, or Barnes-Jewish Hospital. Patients were characterized clinically based on visual acuity and the Duke Activity Status Index (DASI). Patients also completed a seven-item version of the NEI Visual Functioning Questionnaire and the SF-36 Health Survey Questionnaire.\n    \n\n\n          Results:\n        \n      \n      Patients had a median visual acuity of 20/100 in the worst eye, 20/40 in the better eye, and a median DASI of 24.2 (0 = severe functional limitations due to anginal symptoms, 58.2 = no limitations). There was substantial variation in utilities among patients. The average utility for current vision (relative to ideal vision [= 1.0] and death [= 0.0]) was 0.82; the average utility for current angina (relative to no angina symptoms [= 1.0] and death [ = 0.0]) was 0.89. Among 26 patients with both visual impairment and recent anginal symptoms, the decrement in utility (on a scale ranging from ideal health [= 1.0] to death [= 0.0]) imposed by current visual impairment was greater than that imposed by current angina symptoms (0.146 versus 0.072, p=0.08, Wilcoxon signed rank test). The decrement in utility associated with binocular blindness was greater than the decrement associated with the symptoms of moderate angina (0.477 versus 0.039, p<0.0001).\n    \n\n\n          Conclusions:\n        \n      \n      Clinical status is not a surrogate for patient preferences regarding vision impairment or angina. There is substantial variation in utilities within the study population for both experienced and theoretical impairment states which is not explained by variations in clinical status. Some states of visual impairment may pose a greater quality of life burden than anginal symptoms. Because patient preferences for vision vary greatly, individual assessment is warranted for consideration in therapeutic decision making."
        },
        {
            "title": "Association between visual impairment and sleep duration: analysis of the 2009 National Health Interview Survey (NHIS).",
            "abstract": "Background:\n        \n      \n      Visual impairment (VI) is associated with increased mortality and health factors such as depression and cardiovascular disease. Epidemiologic studies consistently show associations between sleep duration with adverse health outcomes, but these have not systematically considered the influence of VI. The aim of this study was to ascertain the independent association between VI and sleep duration using the National Health Interview Survey (NHIS) data. We also examined whether race/ethnicity influenced these associations independently of sociodemographic and medical characteristics.\n    \n\n\n          Methods:\n        \n      \n      Our analysis was based on the 2009 NHIS, providing valid sleep and vision data for 29,815 participants. The NHIS is a cross-sectional household interview survey utilizing a multistage area probability design. Trained personnel from the US census bureau gathered data during face-to-face interview and obtained socio-demographic, self-reported habitual sleep duration and physician-diagnosed chronic conditions.\n    \n\n\n          Results:\n        \n      \n      The mean age of the sample was 48 years and 56% were female. Short sleep and long sleep durations were reported by 49% and 23% of the participants, respectively. Visual impairment was observed in 10%. Multivariate-adjusted logistic regression models showed significant associations between VI and short sleep (OR = 1.6, 95% CI = 1.5-1.9 and long sleep durations (OR = 1.6, 95% CI = 1.3-1.9). These associations persisted in multivariate models stratified by race-ethnic groups.\n    \n\n\n          Conclusion:\n        \n      \n      Visual impairment was associated with both short and long sleep durations. Analysis of epidemiologic sleep data should consider visual impairment as an important factor likely to influence the amount of sleep experienced habitually."
        },
        {
            "title": "Retinal arteriolar emboli and long-term mortality: pooled data analysis from two older populations.",
            "abstract": "Background and purpose:\n        \n      \n      To assess the relationship between retinal arteriolar emboli and mortality in older people.\n    \n\n\n          Methods:\n        \n      \n      Pooled data from 2 population-based cohort studies. At baseline, the Beaver Dam Eye Study (BDES) examined 4926 persons 43 to 86 years of age (1988 to 1990), and the Blue Mountains Eye Study (BMES) examined 3654 persons 49 to 97 years of age (1992 to 1994). Retinal arteriolar emboli were assessed by grading retinal photographs using standardized methods. Deaths and causes of death were determined from death certificates or Australian National Death Index. Cox regression models were used to estimate mortality hazard ratios (HRs) associated with emboli, adjusting for age, gender, body mass index, hypertension, diabetes, smoking, serum total cholesterol, high-density lipoprotein cholesterol, study site, and past histories of stroke, angina, and acute myocardial infarct.\n    \n\n\n          Results:\n        \n      \n      Of 8580 baseline participants, 8384 (98%) had retinal photographs available, and 111 showed retinal arteriolar emboli (BDES n=61; BMES n=50). Over 10 to 12 years, 2506 participants (30%) died, including 344 (4%) from stroke-related and 1315 (16%) from cardiovascular causes. The cumulative mortality rates were higher in participants with than without emboli (all-cause 56% versus 30%; stroke-related 12% versus 4.0%; cardiovascular 30% versus 16%). The increased mortality risk associated with emboli was independent of age, gender, other vascular risk factors, and past histories of stroke or heart disease for all-cause (multivariate-adjusted HR, 1.3; CI, 1.0 to 1.8) and stroke-related mortality (HR, 2.0; CI, 1.1 to 3.8) but not for cardiovascular mortality (HR, 1.2; CI, 0.8 to 1.7).\n    \n\n\n          Conclusions:\n        \n      \n      Our pooled data from 2 older populations suggest that retinal emboli predict a modest increase in all-cause and stroke-related mortality independent of cardiovascular risk factors."
        },
        {
            "title": "Use of routine CT-SCANS to detect severe postoperative complications after pancreato-duodenectomy.",
            "abstract": "Background:\n        \n      \n      To evaluate the performance of CT-scans performed one week after pancreato-duodenectomy (PD) to detect severe postoperative complications requiring an invasive treatment.\n    \n\n\n          Patients and methods:\n        \n      \n      This monocentric retrospective study was conducted on data collected between 2005 and 2013. Patients undergoing PD underwent CT-scan with IV contrast at the end of the first postoperative week. The results of the CT-scans were analyzed to evaluate the usefulness of this procedure. The main assessment criterion was the occurrence of type-III complication (or greater) according to the Dindo-Clavien classification.\n    \n\n\n          Results:\n        \n      \n      In total, 138 patients were included. The mortality rate was 2.2%. The postoperative complication rate was 57.2%. The pancreatic fistula rate was 19.6%; 46 patients (33.3%) presented with a severe complication. A total of 138 CT-scans were analyzed: 44 (31.8%) were abnormal, 94 (68.2%) were normal. Among patients with abnormal CT-scans, 17 (39%) presented with a severe complication requiring an invasive treatment. Among the 94 patients with normal CT-scans, 14 patients (15%) presented a severe postoperative complication. Evaluation of the performance of the CT-scans at the end of the first postoperative week found a sensitivity of 55%, a specificity of 75%, a positive predictive value of 39%, and a negative predictive value of 85%.\n    \n\n\n          Conclusion:\n        \n      \n      Systematic CT-scans performed at the end of the first postoperative week do not effectively detect severe complications after PD and do not help to prevent them."
        },
        {
            "title": "Clinical characteristics, organ failure, inflammatory markers and prediction of mortality in patients with community acquired bloodstream infection.",
            "abstract": "Background:\n        \n      \n      Community acquired bloodstream infection (CABSI) in low- and middle income countries is associated with a high mortality. This study describes the clinical manifestations, laboratory findings and correlation of SOFA and qSOFA with mortality in patients with CABSI in northern Vietnam.\n    \n\n\n          Methods:\n        \n      \n      This was a retrospective study of 393 patients with at least one positive blood culture with not more than one bacterium taken within 48 h of hospitalisation. Clinical characteristic and laboratory results from the first 24 h in hospital were collected. SOFA and qSOFA scores were calculated and their validity in this setting was evaluated.\n    \n\n\n          Results:\n        \n      \n      Among 393 patients with bacterial CABSI, approximately 80% (307/393) of patients had dysfunction of one or more organ on admission to the study hospital with the most common being that of coagulation (57.1% or 226/393). SOFA performed well in prediction of mortality in those patients initially admitted to the critical care unit (AUC 0.858, 95%CI 0.793-0.922) but poor in those admitted to medical wards (AUC 0.667, 95%CI 0.577-0.758). In contrast qSOFA had poor predictive validity in both settings (AUC 0.692, 95%CI 0.605-0.780 and AUC 0.527, 95%CI 0.424-0.630, respectively). The overall case fatality rate was 28%. HIV infection (HR = 3.145, p = 0.001), neutropenia (HR = 2.442, p = 0.002), SOFA score 1-point increment (HR = 1.19, p < 0.001) and infection with Enterobacteriaceae (HR = 1.722, p = 0.037) were independent risk factors for in-hospital mortality.\n    \n\n\n          Conclusions:\n        \n      \n      Organ dysfunction was common among Vietnamese patients with CABSI and associated with high case fatality. SOFA and qSOFA both need to be further validated in this setting."
        },
        {
            "title": "Benign positional vertigo: incidence and prognosis in a population-based study in Olmsted County, Minnesota.",
            "abstract": "A retrospective review of our population-based medical records linkage system for residents of Olmsted County, Minnesota, revealed 53 patients (34 women and 19 men; mean age, 51 years) with newly diagnosed benign positional vertigo in 1984. The age- and sex-adjusted incidence was 64 per 100,000 population per year (95% confidence interval, 46 to 81 per 100,000). The incidence of benign positional vertigo increased by 38% with each decade of life (95% confidence interval, 23 to 54%). One patient had an initial stroke during follow-up; thus, the relative risk for new stroke associated with benign positional vertigo was 1.62 (95% confidence interval, 0.04 to 8.98) in comparison with the expected occurrence based on incidence rates for an age- and sex-adjusted control population. The observed survival among the 53 Olmsted County residents with benign positional vertigo diagnosed in 1984 was not significantly different from that of an age- and sex-matched general population. Patients with benign positional vertigo seem to have a good prognosis."
        },
        {
            "title": "Infarct characterization and quantification by delayed enhancement cardiac magnetic resonance imaging is a powerful independent and incremental predictor of mortality in patients with advanced ischemic cardiomyopathy.",
            "abstract": "Background:\n        \n      \n      Infarct heterogeneity has been shown to be independently associated with adverse outcomes in previous smaller studies. However, it is unknown whether infarct characterization is an independent predictor of all-cause mortality in patients with advanced ischemic cardiomyopathy, after adjusting for clinical risk factors, severity of ischemic mitral regurgitation, incomplete revascularization, and device therapy.\n    \n\n\n          Methods and results:\n        \n      \n      A total of 362 patients with ischemic cardiomyopathy (left ventricular dysfunction with >70% stenosis in ≥1 epicardial coronary artery) underwent delayed hyperenhancement-magnetic resonance imaging and coronary angiography between 2002 and 2006. Total myocardial scar and peri-infarct (PI) area were measured using various threshold techniques. Multivariate survival analysis (primary end point of all-cause mortality) was conducted. One hundred fifty-seven deaths occurred during a mean 5.4-year follow-up (mean left ventricular ejection fraction, 23±9%; mean end-systolic volume index, 113±48 mL; mean total myocardial scar %, 25.5±16.0%; mean PI%, 5.7±2.9%). PI% (β=2.07; P<0.001) was an independent predictor of survival, independent of age, end-systolic volume, sex, mitral regurgitation, diabetes mellitus, dyslipidemia, coronary artery disease severity, implantable cardioverter defibrillator, and incomplete revascularization. PI% using 2 to 3 SD technique yielded the highest incremental prognostic power (χ(2) score 149).\n    \n\n\n          Conclusions:\n        \n      \n      In advanced ischemic cardiomyopathy, PI% is a powerful independent and incremental predictor of all-cause mortality. Infarct heterogeneity offers substantial further risk stratification when compared with quantification of total myocardial scar % alone even after adjusting for clinical risk factors, end-systolic volume index, mitral regurgitation, incomplete revascularization, and implantable cardioverter defibrillator implantation."
        },
        {
            "title": "Correlates of short- and long-term case fatality within an incident stroke population in Tanzania.",
            "abstract": "Background:\n        \n      \n      This study aimed to identify correlates of case fatality within an incident stroke population in rural Tanzania.\n    \n\n\n          Methods:\n        \n      \n      Stroke patients, identified by the Tanzanian Stroke Incidence Project, underwent a full examination and assessment around the time of incident stroke. Records were made of demographic data, blood pressure, pulse rate and rhythm, physical function (Barthel index), neurological status (communication, swallowing, vision, muscle activity, sensation), echocardiogram, chest X-ray and computed tomography (CT) head scan. Cases were followed up over the next 3 - 6 years.\n    \n\n\n          Results:\n        \n      \n      In 130 incident cases included in this study, speech, language and swallowing problems, reduced muscle power, and reduced physical function were all significantly correlated with case fatality at 28 days and 3 years. Age was significantly correlated with case fatality at 3 years, but not at 28 days post-stroke. Smoking history was the only significant correlate of case fatality at 28 days that pre-dated the incident stroke. All other significant correlates were measures of neurological recovery from stroke.\n    \n\n\n          Conclusions:\n        \n      \n      This is the first published study of the correlates of post-stroke case fatality in sub-Saharan Africa (SSA) from an incident stroke population. Case fatality was correlated with the various motor impairments resulting from the incident stroke. Improving poststroke care may help to reduce stroke case fatality in SSA."
        },
        {
            "title": "Healthy centenarians do not exist, but autonomous centenarians do: a population-based study of morbidity among Danish centenarians.",
            "abstract": "Objective:\n        \n      \n      To assess the prevalence of common illnesses in an unselected population of centenarians.\n    \n\n\n          Design:\n        \n      \n      A population-based survey.\n    \n\n\n          Setting:\n        \n      \n      Denmark.\n    \n\n\n          Participants:\n        \n      \n      All Danes who celebrated their 100th anniversary between April 1, 1995 and May 31, 1996: 276 persons.\n    \n\n\n          Measurements:\n        \n      \n      All participants (including proxies) were visited at their domicile for an interview (sociodemographic characteristics, activities of daily living, living conditions, need of assistance from other people, former health and current diseases, current medication) and a clinical examination (dementia screening test, heart and lung auscultation, neurological assessment, height and weight, electrocardiogram, arm and ankle blood pressure, assessment of hearing and vision capacity, a short physical performance test, bio-impedance, lung function test, blood test). Further health information was retrieved from medical files and national health registers.\n    \n\n\n          Results:\n        \n      \n      Seventy-five percent (207) of eligible subjects participated in the study. Cardiovascular disease was present in 149 (72%) subjects. Osteoarthritis (major joints) was present in 54%, hypertension (> or =140/ > or =90) in 52%, dementia in 51%, and ischemic heart disease in 28%. The mean number of illness was 4.3 (standard deviation (SD) 1.86). Only one subject was identified as being free from any chronic condition or illness. Sixty percent had been treated for illness with high mortality. In 25 autonomous (nondemented, functioning well physically, living at home) and 182 nonautonomous centenarians, comorbidities were equivalent.\n    \n\n\n          Conclusion:\n        \n      \n      Because they have a high prevalence of several common diseases and chronic conditions, Danish centenarians are not healthy. However, a minor proportion was identified as being cognitively intact and functioning well."
        },
        {
            "title": "Elevated plasma levels of soluble tumor necrosis factor receptor (sTNFRp60) reflect severity of acute pancreatitis.",
            "abstract": "Objective:\n        \n      \n      To investigate the role of activated leukocytes in acute pancreatitis, we measured soluble receptors of tumour necrosis factor alpha (sTNFR, p60 subtype) in plasma and evaluated the association of sTNFR with the clinical severity of the disease.\n    \n\n\n          Design:\n        \n      \n      Prospective, descriptive study.\n    \n\n\n          Setting:\n        \n      \n      A medical intensive care unit (ICU) in a university hospital.\n    \n\n\n          Patients:\n        \n      \n      25 consecutive ICU admissions of adult patients with acute pancreatitis.\n    \n\n\n          Measurements and results:\n        \n      \n      The clinical severity of the disease was assessed using weights for the worst 17 physiological abnormalities of the Acute Physiology and Chronic Health Evaluation III score over a 24-h period after admission. According to the sum of these weights (giving the Acute Physiology Score, APS) patients were divided into a group with mild pancreatitis (APS < 25) and into a group with severe pancreatitis (APS > or = 25). Soluble TNFR was determined in plasma using an enzyme-linked immunoadsorbent assay. In patients with clinically severe pancreatitis, plasma sTNFR concentrations of 8.8 (16) ng/ ml (median, interquartile range) were significantly higher when compared to patients with mild disease [2.7 (1.5) ng/ml; p < 0.0001]. The sensitivity and specificity of sTNFR plasma concentrations (cutoff point at 5 ng/ml) for the prediction of severe pancreatitis were 90 and 100%, respectively. A highly positive correlation between sTNFR and deviations of physiological parameters from normal (APS score) was demonstrated (r = 0.81). The development of multiple organ failure (MOF) and death was associated with significantly higher sTNFR levels when compared to patients without MOF and survivors [16.4 (17) vs 3.2 (2) ng/ml, p = 0.0014 and 16.0 (18) vs 3.3 (4) ng/ml, p = 0.016, respectively]. For evidence of necrotizing pancreatitis, plasma C-reactive protein concentrations were measured and a significant exponential regression was found with sTNFR (r = 0.77, p < 0.0001). Patients developing pancreatic necrosis, as demonstrated by contrast-enhanced computed tomography, had significantly higher sTNFR concentrations when compared to patients with edematous pancreatitis [9.1 (17) vs 3.2 (2) ng/ml, p = 0.0018).\n    \n\n\n          Conclusion:\n        \n      \n      The p60 subtype of soluble TNFR is elevated in the plasma of patients with clinically severe acute pancreatitis. This elevation is positively correlated to abnormalities in physiological parameters, development of MOF, and mortality. The association with pancreatic necrosis suggests that, by mediating the effects of TNF, TNFRp60 reflects inflammatory tissue damage leading to severe systemic complications."
        },
        {
            "title": "A practical guide to magnetic resonance vascular imaging: techniques and applications.",
            "abstract": "Magnetic resonance angiography is a technique used to image both central and peripheral arteries using contrast and noncontrast techniques. These techniques are similar in that a bright signal, which appears white within blood vessels, is generated and the background tissues, veins, and stationary tissues are dark. This allows for assessment of anatomy and vascular disease. Extracellular gadolinium-based contrast agents allow for excellent visualization of both central and peripheral arteries. Acquiring images during first pass is required for high-contrast images within arteries, thereby limiting contamination with contrast enhancement of veins and soft tissue. Contrast-enhanced techniques using time-resolved angiography and blood pool contrast agents minimize this temporal limitation. Noncontrast techniques eliminate the uncommon but potentially fatal complications associated with gadolinium contrast agents, such as nephrogenic systemic fibrosis. These techniques including phase contrast and time-of-flight sequences have inferior contrast resolution compared with contrast-enhanced techniques and are susceptible to artifacts, which can limit interpretation. The advantage, however, is the ability to assess vascular disease in patients with severe renal failure without the added risks of gadolinium contrast media. The aim of this review is to outline the different techniques available for imaging both the arterial and venous systems, their advantages and disadvantages, and the indications in vascular disease."
        },
        {
            "title": "Incremental value of extracellular volume assessment by cardiovascular magnetic resonance imaging in risk stratifying patients with suspected myocarditis.",
            "abstract": "Cardiovascular magnetic resonance imaging (CMR) has become a key investigative tool in patients with suspected myocarditis. However, the prognostic implications of T1 mapping, including extracellular volume (ECV) calculation, is less clear. Patients with suspected myocarditis who underwent CMR evaluation, including T1 mapping at our institution were included. CMR findings including late gadolinium enhancement (LGE), left ventricular ejection fraction (LVEF), native T1 mapping, and ECV calculation were associated with first major adverse cardiac events (MACE). MACE included a composite of all-cause death, heart failure hospitalization, heart transplantation, documented sustained ventricular arrhythmia, and recurrent myocarditis. One hundred seventy-nine patients with a mean age of 49 ± 15 years were identified. Seventy nine individuals (44%) were female. Mean LVEF was 48 ± 16. At a median follow-up of 4.1 [interquartile-range (IQR) 2.2-6.1] years, 22 (12%) patients experienced a MACE. Mean ECV (per 10%) was significantly associated with MACE (HR 2.09, 95% CI 1.07-4.08, p = 0.031). Presence of ECV ≥ 35% demonstrated significant univariable association with MACE (HR 3.3, 95% CI 1.43-7.97, p = 0.005) and such association was maintained when adjusted to LVEF (HR 3.42, 95% CI 1.42-7.94, p = 0.006). ECV ≥ 35% portended a greater than threefold increased hazards to MACE adjusted to LGE presence (HR 3.14, 95% CI 1.29-7.36, p = 0.012). In patients without LGE, ECV ≥ 35% portended a greater than sixfold increased hazards (HR 6.6, p = 0.010). In the multivariable model including age, LVEF and LGE size, only ECV ≥ 35% maintained its significant association with outcome. ECV calculation by CMR is a useful tool in the risk stratification of patients with clinically suspected myocarditis, incremental to LGE and LVEF."
        },
        {
            "title": "Prospective comparison of emergency physician-performed venous ultrasound and CT venography for deep venous thrombosis.",
            "abstract": "Background:\n        \n      \n      Venous thromboembolic disease is a major cause of mortality and morbidity.\n    \n\n\n          Objectives:\n        \n      \n      The aim of this study is to compare emergency physician-performed ultrasound (EPPU) of the lower extremities with CT venography (CTV) in emergency department (ED) patients undergoing workup for pulmonary embolism (PE).\n    \n\n\n          Methods:\n        \n      \n      This was a prospective study performed at a busy academic ED. Adult patients (>18) undergoing workup for PE were eligible for the study; enrollment was based on a convenience sample, during hours worked by the investigators. Study patients underwent EPPU of the lower extremities followed by CT angiogram (CTA) of the chest and CTV of the lower extremities. Sensitivity and specificity of the ultrasound examination were calculated using CTV as the gold standard.\n    \n\n\n          Results:\n        \n      \n      A total of 61 patients were enrolled. Of 61 patients, 50 (82%; 95% confidence interval [CI], 72%-91%) had negative workups; 11 (18%; 95% CI, 8%-27%) were noted to have PE on CTA; 6 (10%; 95% CI, 2%-17%) were noted to have lower extremity deep venous thrombosis (DVT) on both EPPU and CTV evaluation; whereas 1 patient was found to have an external iliac DVT on CTV, which was not noted on EPPU. All patients with DVT (by either EPPU or CTV) were found to have PE on CTA. Sensitivity and specificity of EPPU when compared to CTV in the diagnosis of DVT was 86% (95% CI, 42%-99%) and 100% (95% CI, 91%-100%), respectively.\n    \n\n\n          Conclusions:\n        \n      \n      Emergency physician-performed ultrasound produces results consistent with CTV in the diagnosis of femoropopliteal DVT. More proximal clots are not evaluated with EPPU and thus may result in a false negative."
        },
        {
            "title": "Validity of EuroQOL-5D, time trade-off, and standard gamble for age-related macular degeneration in the Singapore population.",
            "abstract": "Background/aims:\n        \n      \n      Utility values of age-related macular degeneration (AMD) in Asian patients are unknown. This study aims to assess utility values and construct validity of the EuroQOL-5D (EQ-5D), time trade-off (TTO), and standard gamble (SG) instruments in the Singapore multi-ethnic AMD population.\n    \n\n\n          Methods:\n        \n      \n      Cross-sectional, two-centre, institution-based study. Visual acuity (VA), clinical AMD severity, and utility scores on the EQ-5D, TTO, and SG were obtained from 338 AMD patients. VA was analysed in terms of the better-seeing eye (BEVA), worse-seeing eye (WEVA), and weighted average of both eyes (WVA). We evaluated SG on the perfect health-death (SG(death)) and binocular perfect vision-binocular blindness (SG(blindness)) scales. Construct validity was determined by testing a priorihypotheses relating the EQ-5D, TTO, and SG utility scores to VA and clinical AMD severity.\n    \n\n\n          Results:\n        \n      \n      The mean utilities on the EQ-5D, TTO, SG(death), and SG(blindness) were 0.89, 0.81, 0.86, and 0.90, respectively. EQ-5D scores correlated weakly with BEVA, WEVA, and WVA (Pearson's correlation coefficients -0.291, -0.247, and -0.305 respectively, P<0.001 for all). SG(death) and SG(blindness) demonstrated no correlation with BEVA, WEVA, or WVA (Pearson's correlation coefficients, range -0.06 to -0.125). TTO showed weak association only with WEVA and WVA (correlation coefficients -0.237, -0.228, P<0.0001), but not with BEVA (correlation coefficient -0.161). Clinical AMD severity correlated with EQ-5D and SG(death), but not with TTO and SG(blindness) (P=0.004, 0.002, 0.235, and 0.069, respectively).\n    \n\n\n          Conclusions:\n        \n      \n      AMD has a negative impact on utilities, although utility scores were high compared with Western cohorts. EQ-5D, TTO, and SG showed suboptimal construct validity, suggesting that health status utilities may not be sufficiently robust for cost-utility analyses in this population."
        },
        {
            "title": "How well do prediction equations predict? Using receiver operating characteristic curves and accuracy curves to compare validity and generalizability.",
            "abstract": "Although morbidity and mortality prediction equations are widely used in planning, clinical practice, and health risk appraisal, their validity and generalizability have been tested only in a limited way. Previous attempts lacked an absolute standard of performance and looked only at the equations' ability to predict who would become ill (sensitivity), not the equally important ability to predict who would remain healthy (specificity). We compared six all-cause mortality prediction equations using receiver operating characteristic curves and accuracy curves, which overcome the limitations of earlier methods and provide a concise visual representation of the results. We used equations from five prospective studies conducted in the United States (Tecumseh at 8 and 12 years of follow-up, Framingham, Chicago Gas, Chicago Western Electric, and Albany), each of which included cholesterol, smoking, and blood pressure as independent variables, to predict 12-year mortality in Tecumseh males age 40-54 years. Previous studies suggested that these equations predict equally well. Our analysis found that, although all predict better than chance, Albany, Chicago Western Electric, and Tecumseh at 8 years underestimate mortality. Receiver operating characteristic and accuracy curves are a promising technique for assessment of prediction equations."
        },
        {
            "title": "Enucleation versus preservation of blind eyes following plaque radiotherapy for choroidal melanoma.",
            "abstract": "Background:\n        \n      \n      Currently available information about patients with posterior uveal melanoma treated by plaque radiotherapy is insufficient to determine what to do about eyes that become blind as a consequence of the tumour and its treatment. Should they be enucleated, or is ocular preservation just as good in terms of survival?\n    \n\n\n          Methods:\n        \n      \n      We performed a retrospective survival analysis of secondary enucleation versus ocular preservation in patients with a posterior uveal melanoma treated by plaque radiotherapy whose irradiated eye became completely blind following that treatment. Of the 79 patients who fulfilled defined inclusion criteria, 25 underwent secondary enucleation of the blind eye, and 54 retained their irradiated blind eye.\n    \n\n\n          Results:\n        \n      \n      Most of the baseline demographic and tumour-related variables evaluated were similarly distributed between the subgroups. The 5-year, 10-year and 15-year all-cause death rates in the secondary enucleation subgroup were 24.7%, 51.5% and 52.0% respectively, and those in the ocular preservation subgroup were 7.4%, 32.9% and 48.1% respectively. In spite of the apparent slight difference between the curves, the difference was not statistically significant (p = 0.41, Mantel-Haenszel test).\n    \n\n\n          Interpretation:\n        \n      \n      Although a retrospective study of this type has several limitations, our results suggest that secondary enucleation is not likely to substantially improve survival of patients whose irradiated eye becomes totally blind following plaque radiotherapy for choroidal or ciliochoroidal melanoma."
        },
        {
            "title": "The association between cataract and mortality among older adults.",
            "abstract": "Context:\n        \n      \n      Previous research has suggested that persons with cataract have an increased risk of death.\n    \n\n\n          Objective:\n        \n      \n      To compare the mortality experience of patients with cataract who elect surgery, patients with cataract who do not elect surgery, and patients without cataract independent of potentially confounding risk factors.\n    \n\n\n          Design:\n        \n      \n      Cohort study.\n    \n\n\n          Setting:\n        \n      \n      Ophthalmology and optometry clinics affiliated with the Callahan Eye Foundation Hospital in Birmingham, Alabama.\n    \n\n\n          Patients:\n        \n      \n      384 persons with and without cataract.\n    \n\n\n          Main outcome measure:\n        \n      \n      Mortality.\n    \n\n\n          Results:\n        \n      \n      Of the 384 study subjects, 286 had cataract, of whom 200 elected to have cataract surgery. Patients with cataract who did and did not elect surgery had significantly higher mortality compared to those without cataract (crude mortality rate ratio (MRR) 3.9 (95% confidence interval (CI) 1.5-9.8) and 7.3 (95% CI 2.8-19.1), respectively). After adjustment for age, gender, race, education, chronic medical conditions, smoking, drinking, depression, and cognitive status, the no-surgery cataract group had an elevated mortality rate (MRR 3.2 (95% 1.2-9.0)), compared to the no-cataract group, with a borderline elevation in MR for the surgery group (MRR 2.0 (95% 0.8-5.9). Limiting the study population to non-diabetics or those without concurrent eye conditions (glaucoma, maculopathy, retinopathy) did not materially influence the adjusted MRRs although the precision of the estimates was reduced.\n    \n\n\n          Conclusions:\n        \n      \n      The results suggest that older persons with cataract, in particular those who decline surgery, have an increased risk of death, supporting the hypothesis that age-related cataract reflects systemic as well as localized ocular disease."
        },
        {
            "title": "Exercise echocardiography and cardiac magnetic resonance imaging to predict outcome in patients with hypertrophic cardiomyopathy.",
            "abstract": "Aims:\n        \n      \n      We have observed that wall motion abnormalities (WMAs) during exercise echocardiography (ExE) are associated to events in hypertrophic cardiomyopathy (HCM). Our objective was to evaluate ExE and cardiac magnetic resonance (CMR) to predict outcome in HCM.\n    \n\n\n          Methods and results:\n        \n      \n      ExE and CMR were performed in 148 patients with HCM. During follow-up (7.1 ± 2.7 years), there were 7 hard events (Hard-E) and 26 combined events (Comb-E). Exercise WMAs were observed in 13 patients (8.8%), perfusion defects in 10 (6.8%), and late gadolinium enhancement (LGE) in 48 (32.4%). WMAs were seen in 57% of patients with Hard-E vs. 6% without (P = 0.001) and in 23 and 6% with and without Comb-E (P = 0.005). Perfusion defects were also more frequent in patients with Hard-E than without (43 vs. 5%, P = 0.007) and in patients with Comb-E than without (23 vs. 5%, P = 0.002). LGE (g) was greater in patients with Comb-E than without [median (25th-75th percentile) 0 (0-21.1) vs. 0 (0-9.3) g P = 0.04]. Univariable predictors of Comb-E included NYHA class ≥2, peak double product, ΔWMSI, and CMR data. Peak double product [Hazard ratios (HR) = 0.99, confidence intervals (CI) 95% 0.99-0.99, P = 0.02] and ΔWMSI (HR = 404, CI 95% 12-13681, P = 0.001) remained independent predictors. Peak WMSI correlated with myocardial mass with LGE (r = 0.20, P = 0.02) and with perfusion defect area (r = 0.40, P < 0.001). LGE affecting ≥15% of the left ventricle was observed in 38% of patients with exercise WMAs vs. 12% without (P = 0.009).\n    \n\n\n          Conclusion:\n        \n      \n      CMR data are associated to exercise WMAs in patients with HCM. ExE and CMR may help to predict outcome in them."
        },
        {
            "title": "Association of Intravitreal Anti-Vascular Endothelial Growth Factor Therapy With Risk of Stroke, Myocardial Infarction, and Death in Patients With Exudative Age-Related Macular Degeneration.",
            "abstract": "Importance:\n        \n      \n      Current studies assessing the risk of stroke, myocardial infarction (MI), and death in patients undergoing intravitreal anti-vascular endothelial growth factor (VEGF) therapy are inconclusive. To our knowledge, no population-based studies have been performed to examine these potential risks.\n    \n\n\n          Objective:\n        \n      \n      To examine whether patients with exudative age-related macular degeneration (AMD) receiving intravitreal anti-VEGF injections have a higher incidence of MI, stroke, or death compared with control populations.\n    \n\n\n          Design, setting, and participants:\n        \n      \n      This population-based, retrospective cohort study included 504 patients from Olmsted County, Minnesota, identified through the Rochester Epidemiology Project (REP) database as receiving at least 1 intravitreal anti-VEGF injection for exudative AMD from January 1, 2004, to December 31, 2013. Three age- and sex-matched control groups of individuals who did not receive anti-VEGF treatment and were derived from the REP database were also studied: control individuals with exudative AMD in the era before anti-VEGF (January 1, 1990, to December 31, 2003), controls with dry AMD, and controls without AMD. Data analysis was performed from September 1, 2016, to September 1, 2017.\n    \n\n\n          Main outcomes and measures:\n        \n      \n      Five-year risk of stroke, MI, and death were assessed in patients compared with controls using Kaplan-Meier and multivariate analysis with Cox proportional hazards regression models.\n    \n\n\n          Results:\n        \n      \n      The study included 504 patients (321 female [63.7%]; mean [SD] age, 76.5 [10.0] years) who received at least 1 intravitreal anti-VEGF injection for exudative AMD during the study period. Kaplan-Meier analysis revealed a 5-year risk of 7.2% for stroke, 6.1% for MI, and 30.0% for death. Patients who received anti-VEGF had no increased risk of stroke or MI compared with controls with dry AMD (n = 504), controls with exudative AMD (n = 473), or controls without AMD (n = 504). There was an increased risk of mortality compared with controls with exudative AMD in the era prior to anti-VEGF therapy but not the other control groups on multivariate analysis (hazard ratio, 1.63; 95% CI, 1.30-2.04; P < .001).\n    \n\n\n          Conclusions and relevance:\n        \n      \n      This population-based study revealed that intravitreal anti-VEGF therapy for exudative AMD was not associated with consistent increases in the risk of stroke, MI, or death compared with no therapy in patients with or without AMD. It appears to be likely the cardiac events these patients experience are not attributable to their anti-VEGF therapy."
        },
        {
            "title": "The prognosis of CMV retinitis among patients with AIDS in Serbia.",
            "abstract": "Background:\n        \n      \n      Cytomegalovirus (CMV) end-organ diseases, including CMV retinitis, are major opportunistic events in terminal AIDS patients.\n    \n\n\n          Methods:\n        \n      \n      A retrospective study of 30 AIDS patients with CMV retinitis treated between 1997 and 2007 in Serbia was conducted to examine the prognosis and factors associated with survival.\n    \n\n\n          Results:\n        \n      \n      Eighteen (60%) patients survived the mean follow-up period of 46.4+/-36 months. Patients' sex, mode of HIV transmission or previous AIDS diagnosis did not affect survival. Bilateral CMV retinitis predicted dissemination of CMV disease and poor prognosis (OR 7.8, 95% CI 1.3-47.0, P=0.012), but was not associated with blindness (P=0.33). Among patients treated with HAART and CMV therapy the probability of surviving 10 years was 70%, while in those on CMV therapy alone, the median survival was 10 months (log rank P=0.00). However, HAART itself was not sufficient to prevent blindness and the major predictor of blindness was a baseline CD4 cell count of less than 50/microL (OR 6.8, 95% CI 1.1-41.8, P=0.03). After CMV disease, most patients suffered other opportunistic events regardless of HAART introduction.\n    \n\n\n          Conclusion:\n        \n      \n      Even in the HAART era patients with advanced immunodeficiency and CMV retinitis may not escape from the high risk mortality group, while survivors commonly lose sight."
        },
        {
            "title": "Clinical outcomes of progressive supranuclear palsy and multiple system atrophy.",
            "abstract": "Prognostic predictors have not been defined for progressive supranuclear palsy (PSP) and multiple system atrophy (MSA). Subtypes of both disorders have been proposed on the basis of early clinical features. We performed a retrospective chart review to investigate the natural history of pathologically confirmed cases of PSP and MSA. Survival data and several clinically relevant milestones, namely: frequent falling, cognitive disability, unintelligible speech, severe dysphagia, dependence on wheelchair for mobility, the use of urinary catheters and placement in residential care were determined. On the basis of early symptoms, we subdivided cases with PSP into 'Richardson's syndrome' (RS) and 'PSP-parkinsonism' (PSP-P). Cases of MSA were subdivided according to the presence or absence of early autonomic failure. Sixty-nine (62.7%) of the 110 PSP cases were classified as RS and 29 (26.4%) as PSP-P. Of the 83 cases of MSA, 42 (53.2%) had autonomic failure within 2 years of disease onset. Patients with PSP had an older age of onset (P < 0.001), but similar disease duration to those with MSA. Patients with PSP reached their first clinical milestone earlier than patients with MSA (P < 0.001). Regular falls (P < 0.001), unintelligible speech (P = 0.04) and cognitive impairment (P = 0.03) also occurred earlier in PSP than in MSA. In PSP an RS phenotype, male gender, older age of onset and a short interval from disease onset to reaching the first clinical milestone were all independent predictors of shorter disease duration to death. Patients with RS also reached clinical milestones after a shorter interval from disease onset, compared to patients with PSP-P. In MSA early autonomic failure, female gender, older age of onset, a short interval from disease onset to reaching the first clinical milestone and not being admitted to residential care were independent factors predicting shorter disease duration until death. The time to the first clinical milestone is a useful prognostic predictor for survival. We confirm that RS had a less favourable course than PSP-P, and that early autonomic failure in MSA is associated with shorter survival."
        },
        {
            "title": "Visual acuity and mortality in older people and factors on the pathway.",
            "abstract": "## PURPOSE\nTo examine vision as a predictor of mortality in older people and the role of mobility, depressed mood, chronic diseases, body mass index, physical activity and injurious accidents in this possible association.\n## METHODS\n223 persons aged 75 and 193 persons aged 80 years at the baseline participated in visual acuity measurements. Visual acuity (VA) of < 0.3 in the better eye was defined as visual impairment, VA of > or = 0.3 but < or = 0.5 as lowered vision and VA > 0.5 as normal VA. Death dates were received from the official register. Cox regression models were used to determine the relative risks of mortality and to study what factors lie on the pathway from poor vision to mortality.\n## RESULTS\nOver the 10-year follow-up, 107 (48%) persons aged 75 years and 138 (72%) aged 80 years at the baseline died. The risk for mortality among the 75-year-olds with lowered vision was 1.98 (95 % CI 1.25-3.13) and with visual impairment 1.90 (95% CI 1.12-3.20) compared to those with normal VA. Lower walking speed, physical inactivity, cardiovascular diseases, injurious accidents, diabetes and depressed mood each attenuated the risk markedly. Nevertheless, lowered vision remained a significant predictor of mortality even after including all these variables in the model. Among the 80-year-olds vision did not correlate with mortality.\n## CONCLUSIONS\nLowered vision and severe visual impairment predicted mortality in the 75-year-old but not 80-year-old population. The increased risk was partially explained by lower walking speed, physical inactivity, cardiovascular diseases, depressed mood, diabetes and injurious accidents.\n"
        },
        {
            "title": "Risk adjustment methods can affect perceptions of outcomes.",
            "abstract": "When comparing outcomes of medical care, it is essential to adjust for patient risk, including severity of illness. A variety of severity measures exist, but perceptions of outcomes may differ depending on how severity is defined. We used two severity-adjustment approaches to demonstrate that comparisons of outcomes across subgroups of patients can vary dramatically depending on how severity is assessed. We studied two approaches: model 1 was the admission MedisGroups score; model 2 was computed from age and 12 chronic conditions defined by diagnosis codes. Although common summary measures of model performance (R-squared and C) both suggested that model 1 is a better predictor of in-hospital death than model 2, the weaker model consistently produced more accurate expectations by payer class and age group. Using model 1 for severity adjustment suggested that Medicare patients did substantially worse than expected and Medicaid patients substantially better. In contrast, use of model 2 found Medicare patients doing as expected, but Medicaid patients faring poorly."
        },
        {
            "title": "Early hospital mortality prediction of intensive care unit patients using an ensemble learning approach.",
            "abstract": "Background:\n        \n      \n      Mortality prediction of hospitalized patients is an important problem. Over the past few decades, several severity scoring systems and machine learning mortality prediction models have been developed for predicting hospital mortality. By contrast, early mortality prediction for intensive care unit patients remains an open challenge. Most research has focused on severity of illness scoring systems or data mining (DM) models designed for risk estimation at least 24 or 48h after ICU admission.\n    \n\n\n          Objectives:\n        \n      \n      This study highlights the main data challenges in early mortality prediction in ICU patients and introduces a new machine learning based framework for Early Mortality Prediction for Intensive Care Unit patients (EMPICU).\n    \n\n\n          Materials and methods:\n        \n      \n      The proposed method is evaluated on the Multiparameter Intelligent Monitoring in Intensive Care II (MIMIC-II) database. Mortality prediction models are developed for patients at the age of 16 or above in Medical ICU (MICU), Surgical ICU (SICU) or Cardiac Surgery Recovery Unit (CSRU). We employ the ensemble learning Random Forest (RF), the predictive Decision Trees (DT), the probabilistic Naive Bayes (NB) and the rule-based Projective Adaptive Resonance Theory (PART) models. The primary outcome was hospital mortality. The explanatory variables included demographic, physiological, vital signs and laboratory test variables. Performance measures were calculated using cross-validated area under the receiver operating characteristic curve (AUROC) to minimize bias. 11,722 patients with single ICU stays are considered. Only patients at the age of 16 years old and above in Medical ICU (MICU), Surgical ICU (SICU) or Cardiac Surgery Recovery Unit (CSRU) are considered in this study.\n    \n\n\n          Results:\n        \n      \n      The proposed EMPICU framework outperformed standard scoring systems (SOFA, SAPS-I, APACHE-II, NEWS and qSOFA) in terms of AUROC and time (i.e. at 6h compared to 48h or more after admission).\n    \n\n\n          Discussion and conclusion:\n        \n      \n      The results show that although there are many values missing in the first few hour of ICU admission, there is enough signal to effectively predict mortality during the first 6h of admission. The proposed framework, in particular the one that uses the ensemble learning approach - EMPICU Random Forest (EMPICU-RF) offers a base to construct an effective and novel mortality prediction model in the early hours of an ICU patient admission, with an improved performance profile."
        },
        {
            "title": "Factors associated with macular thickness in the COMET myopic cohort.",
            "abstract": "Purpose:\n        \n      \n      To determine whether macular thickness is associated with ethnicity, gender, axial length (AL), and severity of myopia in a cohort of young adults from the Correction of Myopia Evaluation Trial (COMET).\n    \n\n\n          Methods:\n        \n      \n      Eleven years after their baseline visit, 387/469 (83%) subjects returned for their annual visit. In addition to the protocol-specific measures of spherical equivalent refractive error (SER) and AL, high-resolution macular imaging also was performed with optical coherence tomography (RTVue). From these scans, full-thickness values for the central (1 mm), parafoveal (1 to 3 mm), and perifoveal (3 to 5 mm) annular regions were calculated. Gender, ethnicity, AL, and SER were examined for associations with macular thickness using univariate and multivariable linear regression analyses.\n    \n\n\n          Results:\n        \n      \n      In the 377 subjects with usable data (mean age = 21.0 ± 1.3 years), the mean SER ± SD was -5.0 ± 1.9 D and mean AL was 25.4 ± 0.9 mm. Mean foveal thickness was 252.0 ± 20.1 μm in the center, 315.6 ± 14.0 μm in the parafovea, and 284.4 ± 12.9 μm in the perifovea. In the best-fit multivariable model that adjusted for gender, ethnicity, and AL, females had significantly thinner maculas than males for all three regions (p < 0.0001), with the largest difference in the center (12.8 μm, 95% confidence interval: 9.2 to 16.4). The effect of ethnicity was strongest in the central fovea, with African-Americans, Asians, Hispanics, and mixed ethnic groups having thinner maculas than whites (all p values < 0.005). Increased AL was significantly associated with slightly thicker central foveas (p = 0.001) and thinner parafoveal (p = 0.02) and perifoveal (p < 0.0001) regions.\n    \n\n\n          Conclusions:\n        \n      \n      In this ethnically diverse cohort of moderate and high myopes, females and African-Americans were found to have the thinnest central foveas. Whether such thinning in the macula as a young adult is a risk factor for future disease remains to be determined."
        },
        {
            "title": "Dual sensory impairment in older adults increases the risk of mortality: a population-based study.",
            "abstract": "Although concurrent vision and hearing loss are common in older adults, population-based data on their relationship with mortality is limited. This cohort study investigated the association between objectively measured dual sensory impairment (DSI) with mortality risk over 10 years. 2812 Blue Mountains Eye Study participants aged 55 years and older at baseline were included for analyses. Visual impairment was defined as visual acuity less than 20/40 (better eye), and hearing impairment as average pure-tone air conduction threshold greater than 25 dB HL (500-4000 Hz, better ear). Ten-year all-cause mortality was confirmed using the Australian National Death Index. After ten years, 64% and 11% of participants with DSI and no sensory loss, respectively, had died. After multivariable adjustment, participants with DSI (presenting visual impairment and hearing impairment) compared to those with no sensory impairment at baseline, had 62% increased risk of all-cause mortality, hazard ratio, HR, 1.62 (95% confidence intervals, CI, 1.16-2.26). This association was more marked in those with both moderate-severe hearing loss (>40 dB HL) and presenting visual impairment, HR 1.84 (95% CI 1.19-2.86). Participants with either presenting visual impairment only or hearing impairment only, did not have an increased risk of mortality, HR 1.05 (95% CI 0.61-1.80) and HR 1.24 (95% CI 0.99-1.54), respectively. Concurrent best-corrected visual impairment and moderate-severe hearing loss was more strongly associated with mortality 10 years later, HR 2.19 (95% CI 1.20-4.03). Objectively measured DSI was an independent predictor of total mortality in older adults. DSI was associated with a risk of death greater than that of either vision loss only or hearing loss alone."
        },
        {
            "title": "Non-standard vision measures predict mortality in elders: the Smith-Kettlewell Institute (SKI) study.",
            "abstract": "## PURPOSE\nTo determine which vision tests predict mortality within 10 years in a community-based elderly sample.\n## METHODS\nNine hundred residents of Marin County, California 58 to 101 years of age (mean 75 years at baseline), underwent a battery of tests, including high contrast acuity, low contrast acuity, low contrast/low luminance acuity, acuity in glare, contrast sensitivity, color vision, stereopsis, standard and attentional fields. The association between the vision tests and mortality within 10 years of baseline was assessed with Cox Proportional Hazards models controlling for age, sex, education level, depression, cognitive status and self-reported medical conditions.\n## RESULTS\nForty-three percent of the sample died within 10 years of baseline. When controlling for mortality-related covariates, impairment in any of the vision measures was associated with increased risk of death. However, non-standard vision measures (ie, impairment in low contrast/low luminance acuity, standard field integrity and the impact of the attentional task on field integrity) were more highly associated with mortality than standard high contrast acuity.\n## CONCLUSIONS\nIn agreement with other studies, we find that visual impairment is a significant predictor of death. However, the strongest relationship was found for measures other than high contrast acuity. These results suggest that non-standard vision measures may be more sensitive indicators of generalized aging in the most elderly.\n"
        },
        {
            "title": "Association of Nonarteritic Ischemic Optic Neuropathy With Obstructive Sleep Apnea Syndrome: Consequences for Obstructive Sleep Apnea Screening and Treatment.",
            "abstract": "Importance:\n        \n      \n      The prevalence of obstructive sleep apnea syndrome (OSAS) in patients with nonarteritic anterior ischemic optic neuropathy (NAION) and its influence on second eye involvement is not well known.\n    \n\n\n          Objective:\n        \n      \n      To evaluate the prevalence of OSAS in patients with NAION and risk factors of second eye involvement.\n    \n\n\n          Design, setting, and participants:\n        \n      \n      In this cohort study, we examined 118 patients with anterior ischemic optic neuropathy referred to a tertiary care center from January 1, 2003, through December 31, 2010.\n    \n\n\n          Exposures:\n        \n      \n      Patients underwent polysomnography to detect OSAS and were prospectively followed up to assess the risk of second eye involvement.\n    \n\n\n          Main outcomes and measures:\n        \n      \n      The prevalence of OSAS in patients with NAION and the risk of second eye involvement using survival analysis based on the presence of OSAS, indication for ventilation treatment with continuous positive airway pressure, and other potential ocular and systemic confounders.\n    \n\n\n          Results:\n        \n      \n      In 89 patients with NAION who underwent polysomnography, 67 (75%) had OSAS. Second eye involvement was found in 10 (13.7%) of 73 patients at 3 years: 8 (15.4%) of 52 patients with OSAS at 3 years and 2 (9.5%) of 21 patients without OSAS at 3 years; P = .04. In multivariate analysis, nonadherence to ventilation treatment with continuous positive airway pressure in patients with severe OSAS increased the risk of second eye involvement (hazard ratio, 5.54; 95% CI, 1.13-27.11; P = .04).\n    \n\n\n          Conclusions and relevance:\n        \n      \n      These results suggest that OSAS is common in patients with NAION and that polysomnography should be considered in these patients. These findings also suggest that patients with severe OSAS who are nonadherent to ventilation treatment with continuous positive airway pressure have an increased risk of second eye involvement."
        },
        {
            "title": "The economic costs of progressive supranuclear palsy and multiple system atrophy in France, Germany and the United Kingdom.",
            "abstract": "Progressive supranuclear palsy (PSP) and multiple system atrophy (MSA) are progressive disabling neurological conditions usually fatal within 10 years of onset. Little is known about the economic costs of these conditions. This paper reports service use and costs from France, Germany and the UK and identifies patient characteristics that are associated with cost. 767 patients were recruited, and 760 included in the study, from 44 centres as part of the NNIPPS trial. Service use during the previous six months was measured at entry to the study and costs calculated. Mean six-month costs were calculated for 742 patients. Data on patient sociodemographic and clinical characteristics were recorded and used in regression models to identify predictors of service costs and unpaid care costs (i.e., care from family and friends). The mean six-month service costs of PSP were €24,491 in France, €30,643 in Germany and €25,655 in the UK. The costs for MSA were €28,924, €25,645 and €19,103 respectively. Unpaid care accounted for 68-76%. Formal and unpaid costs were significantly higher the more severe the illness, as indicated by the Parkinson's Plus Symptom scale. There was a significant inverse relationship between service and unpaid care costs."
        },
        {
            "title": "Falls among institutionalized elderly in Alexandria.",
            "abstract": "Falls are a common geriatric problem causing considerable morbidity, mortality, and affecting the quality of life of many elderly people. A cross sectional study was conducted to determine the prevalence and risk factors of falls among elderly people living in geriatric institutions in Alexandria. The total sample included 103 elderly females and 62 elderly males from six institutions. All participants were subjected to interviewing questionnaire to collect data about history, circumstances, outcome of falls, previous falls and history of diseases and drug intake It included also data about activities of daily living. Anthropometric measurements, blood pressure, postural hypotension assessment, complete physical examination and Tinetti scale for balance and gait could be completed for a sub-sample. The prevalence of falls was 32.1%. Most of the falls occurred during the daytime (77.4%), mainly in the bedroom or in the way from bed to bathroom (37.7% each). The most likely causes were slip/trip (41.5%) followed by dizziness/vertigo (32.1%). Fractures occurred in 20.8% of falls. Advanced age (70-79, 80 years and above), history of three or more falls, history of disability from previous falls, history of visual problem, history of cardiac and antihypertensive drug use, and mild impairment of balance and gait were significant risk factors for falls in the univariate analysis. All these factors except for the impairment of balance and gait were also significant predictors of falls in the multivariate analysis."
        },
        {
            "title": "Longitudinal studies of dependence in daily life activities among elderly persons.",
            "abstract": "Ability in activities of daily living (ADL), use of assistive devices, and relation to functional limitations and impairments were studied among persons between 70 and 76 years of age within the Inter-Vention study of Elderly in Göteborg (IVEG) Sweden. An ADL index was developed including instrumental activities (I-ADLs) (cleaning, shopping, transportation and cooking), which was combined with Katz' Index of personal daily life activities (P-ADLs) (bathing, dressing, going to the toilet, transfer, continence and feeding). Independence of and dependence on assistance from another person was assessed and it was possible to classify performance according to an ordinal scale of ADL-steps. The reliability and validity of the scale were tested in an out-patient sample (n = 85) as well as in a population of 76-year-olds (n = 659) and were found to be sufficient (coefficients of reproducibility and scalability, internal consistency, inter- observer reliability, content, construct, and criterion validity). The \"Staircase of ADL\" can be used for observation and documentation of different levels of ability/disability for individuals, groups and for population studies. Most persons (83%) were independent in all activities at age 70 (n = 617). Among survivors followed longitudinally, the incidence of disability was 8% between 70 and 73 and 26% between 73 and 76 years of age. Dependence at age 70 could predict mortality as well as institutionalization. No sex differences were found in the proportion with overall disability. Assistance given by relatives dominated both at 70, 73 and 76 years of age. One fifth at age 70 and almost half of the population at age 76 used assistive devices (AD) in daily life activities, and the use was more frequent in women (52%) than men (37%) at age 76 (n = 595). During the studied age interval, 39% \"new users\" were found, while 22% were \"temporary users\". The usage rate was high and the effectiveness of ADs increased the person's ability to master the situation, especially evident as increased safety and reduction of effort in activities of daily living, implying a reduced degree of handicap. Physical impairments and functional limitations had a considerable impact on dependence in daily life activities as persons dependent in ADL had lower maximal walking speed, grip strength, knee extensor strength, stair-climbing capacity and forward reach than those who were independent in ADL (n = 602). Walking speed in both women and men and sight impairment in men had the greatest influence on dependence in ADL. Women and men who stayed independent over the period (70-76) had significantly higher maximal walking speed and knee extensor strength at the age of 70 than those who became dependent or were dependent on both occasions."
        },
        {
            "title": "Serum cystatin c is not superior to serum creatinine for early diagnosis of contrast-induced nephropathy in patients who underwent angiography.",
            "abstract": "Background:\n        \n      \n      Iodiated contrast-induced nephropathy (CIN) is a serious complication of contrast-enhanced imaging. The aim of this study was to evaluate the diagnostic sensitivities and specificities of serum cystatin C (sCys C) and serum creatinine (sCr) for CIN and to further investigate difference of the incidence, risk factors, and in-hospital and 3-month prognosis of CIN according to sCys C criteria and sCr criteria.\n    \n\n\n          Methods:\n        \n      \n      We prospectively evaluated 213 patients who underwent angiography. The sCr and sCys C concentrations were detected before and at 48 hours, 72 hours after the procedure. The incidence, risk factors, and in-hospital and 3-month prognosis of CIN were analyzed. Receiver operating characteristic curve (ROC) analysis was performed for sCr and sCys C 48 hours after procedure.\n    \n\n\n          Results:\n        \n      \n      The incidence of CIN was 24.4% (sCys C criteria) and 8% (sCr criteria). Diabetes mellitus, dehydration, and hypoalbuminemia were independent risk factors for CIN. Area under the ROC of sCys C 48 hours after procedure was not superior to sCr (0.715 vs 0.790, P=.178). The mortality of patients with CIN in sCr criteria increased significantly (P<.05).\n    \n\n\n          Conclusion:\n        \n      \n      In this study, the incidence and risk factors of CIN were related to diagnostic criteria. The sCys C was not superior to sCr for predicting CIN in the patients who underwent angiography."
        },
        {
            "title": "Sight-threatening retinopathy is associated with lower mortality in type 2 diabetic subjects: a 10-year observation study.",
            "abstract": "Aims:\n        \n      \n      To study associations between diabetic retinopathy and development of stroke, myocardial infarction and death in type 2 diabetic patients.\n    \n\n\n          Methods:\n        \n      \n      During a 10-year observation period, 363 type 2 diabetic patients (diagnosis > or =30 years of age) attending an outpatient clinic were studied regarding the prevalence and incidence of retinopathy and associated risk factors, i.e., (HbA(1c), blood pressure, albuminuria, plasma creatinine, age, sex and diabetes duration) in relation to the development of myocardial infarction, stroke and death. The degree of retinopathy was classified as no retinopathy, background or sight-threatening retinopathy, i.e., clinically significant macular edema, severe non-proliferative or proliferative retinopathy.\n    \n\n\n          Results:\n        \n      \n      During the study period, 62 patients had had myocardial infarction, 54 stroke and 99 patients died. Patients with sight-threatening retinopathy at baseline (n=41) had a 2.2-fold increased (p<0.01) risk for death compared to patients with no or background retinopathy, even when controlled for medical risk factors. When adjusted for medical risk factors, patients with no retinopathy at baseline (n=226) who remained without retinopathy or developed background retinopathy (n=187) during the study period, had a 3.6-fold increased risk for death (95% CI, 1.1, 11.8), (p=0.03), compared to patients who developed sight-threatening retinopathy (n=39), while the incidence of myocardial infarction did not differ. More patients who developed sight-threatening retinopathy were treated with ACE inhibitors than patients who did not (41% versus 24%; p=0.03).\n    \n\n\n          Conclusion:\n        \n      \n      Despite more medical risk factors, patients who developed sight-threatening retinopathy had lower mortality compared to patients with no or background retinopathy at follow-up. More patients who developed sight-threatening retinopathy were treated with ACE inhibitors but this seemed not to have influenced the lower mortality rate in this group, whereas the use of ACE inhibitors in patients who did not develop sight-threatening retinopathy was connected with lower mortality rate."
        },
        {
            "title": "Texture analysis of advanced non-small cell lung cancer (NSCLC) on contrast-enhanced computed tomography: prediction of the response to the first-line chemotherapy.",
            "abstract": "Objectives:\n        \n      \n      To assess whether tumour heterogeneity, quantified by texture analysis (TA) on contrast-enhanced computed tomography (CECT), can predict response to chemotherapy in advanced non-small cell lung cancer (NSCLC).\n    \n\n\n          Methods:\n        \n      \n      Fifty-three CECT studies of patients with advanced NSCLC who had undergone first-line chemotherapy were retrospectively reviewed. Response to chemotherapy was evaluated according to RECIST1.1. Tumour uniformity was assessed by a TA method based on Laplacian of Gaussian filtering. The resulting parameters were correlated with treatment response and overall survival by multivariate analysis.\n    \n\n\n          Results:\n        \n      \n      Thirty-one out of 53 patients were non-responders and 22 were responders. Average overall survival was 13 months (4-35), minimum follow-up was 12 months. In the adenocarcinoma group (n = 31), the product of tumour uniformity and grey level (GL*U) was the unique independent variable correlating with treatment response. Dividing the GL*U (range 8.5-46.6) into tertiles, lesions belonging to the second and the third tertiles had an 8.3-fold higher probability of treatment response compared with those in the first tertile. No association between texture features and response to treatment was observed in the non-adenocarcinoma group (n = 22). GL*U did not correlate with overall survival.\n    \n\n\n          Conclusions:\n        \n      \n      TA on CECT images in advanced lung adenocarcinoma provides an independent predictive indicator of response to first-line chemotherapy."
        },
        {
            "title": "Twenty-year outcomes in patients with newly diagnosed glaucoma: mortality and visual function.",
            "abstract": "Background/aims:\n        \n      \n      To determine the mortality within 20 years of diagnosis of chronic open-angle glaucoma (COAG) and visual acuity and visual field progression of a cohort followed for 20 years.\n    \n\n\n          Methods:\n        \n      \n      Twenty years following the diagnosis of COAG in 68 of 436 (16%) patients seen in a glaucoma case-finding clinic, visual and mortality outcomes were audited from medical records. Causes of death were obtained from general practitioner records and death certificates. Probability of death was calculated using a Kaplan-Meier survival curve. The visual field of each eye of survivors was graded using a nine-stage severity scale. Visual outcome was analysed at the 20-year follow-up visit.\n    \n\n\n          Results:\n        \n      \n      From 68, 14 (21%) were lost to follow-up. In the remaining 54, 20 (37%) were alive 20 years after diagnosis. Of 63% who died, mean age of death was 84 years, most commonly due to vascular disease. Mean age at presentation of those who died was 73.7 years versus 63.2 years for survivors (P=0.001). The median time to death was 16 years. On visual field analysis, nearly half (48.9%) of eyes did not deteriorate, but 28.3% eyes deteriorated by more than two stages. Those who died had worse final visual acuity than survivors (P<0.001). Three who died were registered severely visually impaired mainly from macular disease, but no survivors were registered (P<0.001).\n    \n\n\n          Conclusion:\n        \n      \n      In this cohort, approximately two-thirds of patients with glaucoma died within 20 years of diagnosis. In most older patients with glaucoma, the overall goal of preventing visual handicap and blindness is achievable 20 years after diagnosis."
        },
        {
            "title": "Associations between Physical Activity and Comorbidities in People with COPD Residing in Spain: A Cross-Sectional Analysis.",
            "abstract": "There is a high prevalence of comorbidities among patients with chronic obstructive pulmonary disease (COPD). Comorbidities are likely common in patients with any COPD degree and are associated with increased mortality. The aim of this study was to determine the prevalence of thirty-one different COPD comorbidities and to evaluate the association between physical activity (PA) levels in people with COPD residing in Spain. Cross-sectional data from the Spanish National Health Survey 2017 were analysed. A total of 601 adults (52.2% females) with COPD aged 15 to 69 participated in this study. PA (exposure) was measured with the International Physical Activity Questionnaire (IPAQ) short form and comorbidities (outcomes) were self-reported in response to the question \"Have you ever been diagnosed with…?\" Multivariable logistic regression, in three different models, was used to assess this association. Results showed a high prevalence of comorbidities (94%), these being chronic lumbar back pain (38.9%), chronic allergy (34.8%), arthrosis (34.1%), chronic cervical back pain (33.3%), asthma (32.9%) and hypertension (32.8%) the most prevalent. Low PA level was significantly associated with urinary incontinence (2.115[1.213-3.689]), chronic constipation (1.970[1.119-3.459]), cataracts (1.840[1.074-3.153]), chronic anxiety (1.508[1.002-2.269]) and chronic lumbar back pain (1.489[1.044-2.125]). Therefore, people with COPD should increase their PA levels in order to reduce their risk of comorbidities and increase their quality of life."
        },
        {
            "title": "Ten-year follow-up of helium ion therapy for uveal melanoma.",
            "abstract": "Purpose:\n        \n      \n      To examine the results of helium ion irradiation in 218 patients with uveal melanoma treated more than 10 years ago.\n    \n\n\n          Method:\n        \n      \n      A retrospective review was made of 218 patients (218 eyes) treated with helium ion radiation for uveal melanoma between 1978 and 1984.\n    \n\n\n          Results:\n        \n      \n      After helium ion irradiation, 208 (95.4%) of 218 eyes had local tumor control. Ten years after irradiation, 46 (22.4%) of 218 eyes had been enucleated; the majority of enucleations (37 of 46) resulted from anterior ocular segment complications. Ten years after radiation, 102 (46.8%) of the 218 patients were dead; 51 had non-melanoma-related deaths and 51 had died of metastatic melanoma. Best-corrected visual acuity after radiation was greater than 20/40 in 21 (23%) of 93 eyes of the patients who were alive and who had retained their eyes 10 or more years after treatment. In patients with tumors less than 6 mm in height and more than 3 mm distant from the nerve or the fovea, 13 (72%) of 18 retained visual acuity greater than 20/40. In contrast, only 11% of the patients with either thicker tumors or those close to the nerve or fovea retained that level of visual acuity.\n    \n\n\n          Conclusions:\n        \n      \n      Helium ion irradiation of uveal melanoma is associated with good local tumor control and reasonable retention of the treated eye 10 years after treatment. In eyes with tumors less than 6 mm in thickness and more than 3 mm distant from the optic nerve and fovea, many retain excellent vision."
        },
        {
            "title": "Beyond AREDS Formulations, What Is Next for Intermediate Age-Related Macular Degeneration (iAMD) Treatment? Potential Benefits of Antioxidant and Anti-inflammatory Apocarotenoids as Neuroprotectors.",
            "abstract": "Age-related macular degeneration (AMD) is the commonest cause of severe visual loss and blindness in developed countries among individuals aged 60 and older. AMD slowly progresses from early AMD to intermediate AMD (iAMD) and ultimately late-stage AMD. Late AMD encompasses either neovascular AMD (nAMD) or geographic atrophy (GA). nAMD is defined by choroidal neovascularization (CNV) and hemorrhage in the subretinal space at the level of the macula. This induces a rapid visual impairment caused by the death of photoreceptor cells. Intravitreal injection of anti-vascular endothelial growth factor (VEGF) antibodies is the standard treatment of nAMD but adds to the burden of patient care. GA is characterized by slowly expanding photoreceptor, and retinal pigment epithelium (RPE) degeneration patches progressively leading to blindness. There is currently no therapy to cure GA. Late AMD continues to be an unmet medical need representing a major health problem with millions of patients worldwide. Oxidative stress and inflammation are recognized as some of the main risk factors to developing late AMD. The antioxidant formulation AREDS (Age-Related Eye Disease Studies), contains β-carotene, which has been replaced by lutein and zeaxanthin in AREDS2, are given to patients with iAMD but have a limited effect on the incidence of nAMD and GA. Thus, to avoid or slowdown the development of late stages of AMD (nAMD or GA), new therapies targeting iAMD are needed such as crocetin obtained through hydrolysis of crocin, an important component of saffron (Crocus sativus L.), and norbixin derived from bixin extracted from Bixa orellana seeds. We have shown that these apocarotenoids preserved more effectively RPE cells against apoptosis following blue light exposure in the presence of A2E than lutein and zeaxanthin. In this review, we will discuss the potential use of apocarotenoids to slowdown the progression of iAMD, to reduce the incidence of both forms of late AMD."
        },
        {
            "title": "Predictors of diabetes self-management in older adults receiving chemotherapy.",
            "abstract": "Background:\n        \n      \n      Cancer patients with diabetes have higher mortality rates and are more likely to develop infections, and be hospitalized during treatment. Hyperglycemia has been hypothesized as one of the factors associated with this increased risk. Diabetes self-management is one of the essential elements used by patients to maintain glucose levels.\n    \n\n\n          Objective:\n        \n      \n      This exploratory study seeks to develop an understanding of the impact cancer treatment can have on overall diabetes self-management and how individual, clinical, and behavioral characteristics may influence or predict the level of diabetes self-management in adults who are undergoing chemotherapy for a solid tumor cancer.\n    \n\n\n          Methods:\n        \n      \n      This study was conducted at 8 community-based cancer centers in Michigan and Ohio and used a written, self-administered survey at baseline and a phone survey 8 weeks later.\n    \n\n\n          Results:\n        \n      \n      Diabetes self-management significantly decreased (P < .001), and the level of symptom severity significantly increased (P < .001) after patients were on chemotherapy for a minimum of 8 weeks. The level of symptom severity and diabetes self-efficacy were significantly predictive of the performance of diabetes self-management activities.\n    \n\n\n          Conclusions:\n        \n      \n      Chemotherapy and associated symptoms can have a negative impact on the performance of diabetes self-management activities in adults with both diabetes and cancer, increasing the risk for hyperglycemia and development of complications.\n    \n\n\n          Implications for practice:\n        \n      \n      Oncology nurses need to be aware of the impact cancer treatment can have on the performance of diabetes self-management activities in adults. Future research needs to test interventions that may assist patients with diabetes and cancer in managing both diseases."
        },
        {
            "title": "Comparison of score-based prediction of 90-day mortality after liver resection.",
            "abstract": "Background:\n        \n      \n      Indications for liver surgery are expanding fast and complexity of procedures increases. Preoperative mortality risk assessment by scoring systems is debatable. A previously published externally validated Mortality Risk Score allowed easy applicable and precise prediction of postoperative mortality. Aim of the study was to compare the performance of the Mortality Risk Score with the standard scores MELD and P-POSSUM.\n    \n\n\n          Methods:\n        \n      \n      Data of 529 patients undergoing liver resection were analysed. Mortality Risk Score, the labMELD Score and the P-POSSUM Scores (PS, OS, P-POSSUM mortality %) were calculated. The ROC curves of the three scoring systems were computed and the areas under the curve (C-index) were calculated using logistic regression models. Comparisons between the ROC curves were performed using the corresponding Wald tests.\n    \n\n\n          Results:\n        \n      \n      Internal validation confirmed that the risk model was predictive for a 90-day mortality rate with a C-index of 0.8421. The labMELD Score had a C-index of 0.7352 and the P-POSSUM system 0.6795 (PS 0.6953, OS 0.5413). The 90-day mortality rate increased with increasing labMELD values (p < 0.0001). Categorized according to the Mortality Risk Score Groups the labMELD Score showed a linear increase while the POSSUM Scores showed variable results.\n    \n\n\n          Conclusions:\n        \n      \n      By accurately predicting the risk of postoperative mortality after liver surgery the Mortality Risk Score should be useful at the selection stage. Prediction can be adjusted by use of the well-established labMELD Score. In contrast, the performance of standard P-POSSUM Scores is limited."
        },
        {
            "title": "Visual deterioration during pregnancy due to skull base tumors compressing the optic apparatus.",
            "abstract": "Intracranial tumors may rapidly enlarge during pregnancy. When the tumor abuts the optic apparatus, tumor growth may cause visual deterioration. The decisions regarding the management of these tumors should take into consideration visual function, fetal and maternal safety, and the ability for total resection of the tumor. The objective of the study was to describe our experience and to establish principles for management of intracranial tumors compressing the optic apparatus that present during pregnancy or in the early post partum period. A retrospective case-series review was conducted. Women who presented with visual deterioration either during pregnancy or in the early post partum period due to an intracranial tumor were included. Neurosurgical and obstetrical data were collected from the patients' hospital files and outpatient clinic records. Between 2005 and 2011, nine pregnant women with visual deterioration were diagnosed and treated. Of them, four underwent a neurosurgical procedure during pregnancy. Of the five patients who underwent surgery for tumor resection after delivery, three required urgent cesarean section either due to acute visual deterioration or obstetrical reasons. There was no maternal or fetal mortality and a good overall neonatal outcome was achieved. Improvement in visual acuity and visual fields was achieved in all patients. Postoperative complications included two cases of CSF leak, which resolved after treatment. Visual deterioration during pregnancy due to tumors that compress the optic apparatus requires treatment by a multi-disciplinary team. Surgery is well tolerated by mother and fetus during early and midpregnancy; thus, in cases where visual deterioration is detected, delay of surgery is not justified."
        },
        {
            "title": "Predicting tumor responses and patient survival in chemoradiotherapy-treated patients with non-small-cell lung cancer using dynamic contrast-enhanced integrated magnetic resonance-positron-emission tomography.",
            "abstract": "Purpose:\n        \n      \n      We investigated whether radiologic parameters by dynamic contrast-enhanced (DCE) integrated magnetic resonance-positron-emission tomography (MR-PET) predicts tumor response to treatment and survival in non-metastatic non-small-cell lung cancer (NSCLC) patients receiving chemoradiotherapy (CRT).\n    \n\n\n          Methods:\n        \n      \n      Patients underwent DCE integrated MR-PET imaging 1 week before CRT. The following parameters were analyzed: primary tumor size, gross tumor volume, maximal standardized uptake value (SUVmax), total lesion glycolysis (TLG), apparent diffusion coefficient (ADC), volume transfer constant (Ktrans), reverse reflux rate constant (kep), extracellular extravascular volume fraction (ve), blood plasma volume fraction (vp), and initial area under the time-concentration curve defined over the first 60 s post-enhancement (iAUC60). CRT responses were defined using the revised Response Evaluation Criteria in Solid Tumors (RECIST) guideline (version 1.1).\n    \n\n\n          Results:\n        \n      \n      Thirty patients were included. Non-responders demonstrated higher baseline TLG (p = 0.012), and lower baseline Ktrans (p = 0.020) and iAUC60 (p = 0.016) compared to responders, indicating the usefulness of DCE integrated MR-PET to predict treatment responses. Receiver operating characteristic curve indicated that TLG has the best differentiation capability to predict responders. By setting the threshold of TLG to 277, the sensitivity, specificity, and accuracy were 66.7%, 83.3%, and 75.0%, respectively, with an area under the curve of 0.776. The median follow-up time was 19.6 (range 7.8-32.0) months. In univariate analyses, baseline TLG >277 (p = 0.005) and baseline Ktrans <254 (10-3 min-1; p = 0.015) correlated with poor survival after CRT. In multivariate analysis, baseline TLG >277 remained the significant factor in predicting progression (p = 0.012) and death (p = 0.031).\n    \n\n\n          Conclusions:\n        \n      \n      The radiologic parameters derived from DCE integrated MR-PET scans are useful for predicting treatment response in NSCLC patients treated with CRT; furthermore, these parameters are correlated with clinical and survival outcomes including tumor progression and death."
        },
        {
            "title": "What predicts mortality in Parkinson disease?: a prospective population-based long-term study.",
            "abstract": "Objective:\n        \n      \n      To identify independent risk factors of mortality in a community-based Parkinson disease (PD) cohort during prospective long-term follow-up.\n    \n\n\n          Methods:\n        \n      \n      A community-based prevalent sample of 230 patients with PD from southwestern Norway was followed prospectively with repetitive assessments of motor and nonmotor symptoms from 1993 to 2005. Information on vital status until October 20, 2009, was obtained from the National Population Register in Norway. Cox proportional hazards models were applied to identify independent predictors of mortality during follow-up. Chronological age, Unified Parkinson's Disease Rating Scale (UPDRS) motor score, levodopa equivalent dose, probable REM sleep behavior disorder, psychotic symptoms, dementia, and use of antipsychotics were included as time-dependent variables, and age at onset (AAO) and sex as time-independent variables.\n    \n\n\n          Results:\n        \n      \n      Of 230 patients, 211 (92%) died during the study period. Median survival time from motor onset was 15.8 years (range 2.2-36.6). Independent predictors of mortality during follow-up were AAO (hazard ratio [HR] 1.40 for 10-years increase, p = 0.029), chronological age (HR 1.51 for 10-years increase, p = 0.043), male sex (HR 1.63, p = 0.001), UPDRS motor score (HR 1.18 for 10-point increase, p < 0.001), psychotic symptoms (HR 1.45, p = 0.039), and dementia (HR 1.89, p = 0.001).\n    \n\n\n          Conclusions:\n        \n      \n      This population-based long-term study demonstrates that in addition to AAO, chronological age, motor severity, and dementia, psychotic symptoms independently predict increased mortality in PD. In contrast, no significant impact of antipsychotic or antiparkinsonian drugs on survival was observed in our PD cohort. Early prevention of motor progression and development of psychosis and dementia may be the most promising strategies to increase life expectancy in PD."
        },
        {
            "title": "The International Intravitreal Bevacizumab Safety Survey: using the internet to assess drug safety worldwide.",
            "abstract": "Aim:\n        \n      \n      Off-label intravitreal injections of bevacizumab (Avastin) have been given for the treatment of neovascular and exudative ocular diseases since May 2005. Since then, the use of intravitreal bevacizumab has spread worldwide, but the drug-related adverse events associated with its use have been reported only in a few retrospective reviews. The International Intravitreal Bevacizumab Safety Survey was initiated to gather timely information regarding adverse events from doctors around the world via the internet.\n    \n\n\n          Methods:\n        \n      \n      An internet-based survey was designed to identify adverse events associated with intravitreal bevacizumab treatment. The survey web address was disseminated to the international vitreoretinal community via email. Rates of adverse events were calculated from participant responses.\n    \n\n\n          Results:\n        \n      \n      70 centres from 12 countries reported on 7113 injections given to 5228 patients. Doctor-reported adverse events included corneal abrasion, lens injury, endophthalmitis, retinal detachment, inflammation or uveitis, cataract progression, acute vision loss, central retinal artery occlusion, subretinal haemorrhage, retinal pigment epithelium tears, blood pressure elevation, transient ischaemic attack, cerebrovascular accident and death. None of the adverse event rates exceeded 0.21%.\n    \n\n\n          Conclusion:\n        \n      \n      Intravitreal bevacizumab is being used globally for ocular diseases. Self-reporting of adverse events after intravitreal bevacizumab injections did not show an increased rate of potential drug-related ocular or systemic events. These short-term results suggest that intravitreal bevacizumab seems to be safe."
        },
        {
            "title": "Mortality and hospital morbidity of working-age blind.",
            "abstract": "Aim:\n        \n      \n      Determine whether blindness in people aged 18-65 years was associated with increased rates of mortality, hospitalisation and length of stay.\n    \n\n\n          Methods:\n        \n      \n      A retrospective matched cohort study of legally blind people and normally sighted controls, aged 18-65 years, comparing mortality rates and hospital morbidity records.\n    \n\n\n          Results:\n        \n      \n      Together, 419 blind and 419 controls accumulated 12 258 hospital separations over the 11-year study period. The blind had an age-specific mortality rate seven times greater (12/1000 person years) than the general population (1.8/1000 person years) (p<0.001). Blindness was recorded as a comorbid condition for 76 (22%) blind individuals, on just 255 (2.3%) hospital separation records. Psychiatric, mental or behavioural conditions were the most frequently recorded diagnoses, after dialysis and endocrine conditions. After adjusting for comorbidities, the blind cohort had 1.5 times more hospital separations (p=0.007, 95% CI 1.1 to 2.0) and 2.2 times more bed days (p=0.016, 95% CI 1.4 to 4.1) compared with the control cohort.\n    \n\n\n          Conclusions:\n        \n      \n      Recognition and acknowledgement of in-patients' blind status may assist in understanding the frequent and extended health service utilisation rates. Encouraging and promoting the uptake and access to rehabilitation support services would be measures that may reduce the health service burden of blindness, the incidence of depression and other mental health problems."
        },
        {
            "title": "Excess mortality associated with blindness in leprosy patients in Korea.",
            "abstract": "Vision loss and blindness are potential complications of leprosy. There is little data available to indicate the impact of eye complications on life expectancy and quality of life. We sought to determine the relative risk of death in blind leprosy patients compared to nonblind leprosy patients. A population-based ocular survey of 510 mycobacteriologically negative leprosy patients in rural South Korea, conducted in 1988, formed the study population. After a 7-year period patients were traced to determine their status (alive, dead, lost to follow up). Blind patients showed a 4.8-fold risk of death, even after adjusting for other factors, compared to nonblind patients. Young blind leprosy patients had the highest relative risk of death. Excess mortality was not associated with any specific cause of blindness, ocular pathology, or type of disease. Findings from our study suggest that all leprosy patients with ocular disabilities (including those released from antileprosy treatment) should be targeted to receive eye care to prevent vision loss. Particular emphasis should be placed on young patients."
        },
        {
            "title": "Tumor, node, metastasis classification of malignant ciliary body and choroidal melanoma evaluation of the 6th edition and future directions.",
            "abstract": "Purpose:\n        \n      \n      The tumor, node, metastasis classification of malignant uveal melanoma has been revised. We evaluated how the 6th edition (TNM6) improves on the previous one (TNM5).\n    \n\n\n          Design:\n        \n      \n      Population-based, retrospective, cross-sectional study.\n    \n\n\n          Participants:\n        \n      \n      Two hundred eighty-nine consecutive patients who had a ciliary body and choroidal melanoma treated in the district of the Helsinki University Central Hospital, Finland, between 1962 and 1981.\n    \n\n\n          Methods:\n        \n      \n      Tumor dimensions, ciliary body involvement, and extraocular extension were evaluated from histopathologic sections and pathology reports. Tumors were assigned into categories and stages according to TNM6, TNM5, and 2 previously proposed size classifications.\n    \n\n\n          Main outcome measures:\n        \n      \n      Proportion of tumors classified in each category and melanoma-specific survival by category and stage.\n    \n\n\n          Results:\n        \n      \n      Of the 289 melanomas, 5% were classified as pT1, 63% as pT2, 22% as pT3, and 7% as pT4 according to TNM6. The corresponding percentages based on TNM5 were 8%, 17%, 63%, and 10%. Of pT2 tumors in TNM6, 4% came from pT1, 65% from pT3, and 4% from pT4 category of TNM5. Of 28 melanomas with extraocular growth, 29% were classified as pT2 in TNM6 rather than pT4 in TNM5. The 10-year survival estimate was 2 percentage points lower for pT1, 7 percentage points higher for pT2, 17 percentage points lower for pT3, and 13 percentage points lower for pT4 by TNM6 compared with TNM5; TNM6 (P<0.0001) and the modified alternative size classifications (P = 0.0022 and P = 0.0026) divided tumors according to prognosis better than TNM5. The 10-year survival for stage I, II, and III tumors was 2 percentage points lower, 7 points higher, and 23 points lower by TNM6, which was not better than TNM5 in separating patients according to prognosis (P = 0.47). The alternative size classifications provided more equal categories and fitted the data set better than TNM5 regarding prognosis.\n    \n\n\n          Conclusions:\n        \n      \n      TNM6 is an improvement over TNM5 in some, but not all, respects. Areas for development include taking into account ciliary body involvement and extraocular extension in more detail and combining into each stage tumor categories with similar rather than different prognosis. An evidence-based, multicenter approach would be beneficial."
        },
        {
            "title": "Risk assessment scores for patients with upper gastrointestinal bleeding and their use in clinical practice.",
            "abstract": "Upper gastrointestinal bleeding (UGIB) is a common cause for emergency admission to hospital representing a significant clinical as well as economic burden. UGIB encompasses a wide range of severities from life-threatening exsanguination to minor bleeding that may not require hospital admission. Patients with UGIB are often initially assessed and managed by junior doctors and non-gastroenterologists. Several risk scores have been created for the assessment of these patients, some requiring endoscopic data for calculation and others that are calculable from clinical data alone. A key question in clinical practice is how to accurately identify patients with UGIB at high risk of adverse outcome. Patients considered high risk are more likely to experience adverse outcomes and will require urgent intervention. In contrast, those patients with UGIB who are considered to be low risk could potentially be managed on an outpatient basis. The Glasgow Blatchford Score (GBS) appears best at identifying patients at low risk of requiring intervention or death and therefore may be best for use in clinical practice, allowing outpatient management in low risk cases. There has been some debate as to the optimal GBS cut-off score for safely identifying this low-risk group. Many guidelines suggest that patients with a GBS of zero can be safely managed as outpatients, but more recent studies have suggested that this threshold could potentially be safely increased to ≤1. Most other patients require inpatient endoscopy within 24 h and the full Rockall score remains important for risk assessment following endoscopy, particularly as it includes the endoscopic diagnosis. A minority of patients will require emergency endoscopy following resuscitation, but at present there is no evidence that risk scores can accurately identify this very high-risk group. Studies have shown the latest risk assessment score, the AIMS65, looks promising in the prediction of mortality. However, to date there is no data on the use of the AIMS65 in identifying low risk patients for possible outpatient management."
        },
        {
            "title": "Association of Visual Impairment and All-Cause 10-Year Mortality Among Indigenous Australian Individuals Within Central Australia: The Central Australian Ocular Health Study.",
            "abstract": "## IMPORTANCE\nIt is well established from different population-based studies that visual impairment is associated with increased mortality rate. However, to our knowledge, the association of visual impairment with increased mortality rate has not been reported among indigenous Australian individuals.\n## OBJECTIVE\nTo assess the association between visual impairment and 10-year mortality risk among the remote indigenous Australian population.\n## DESIGN, SETTING, AND PARTICIPANTS\nProspective cohort study recruiting indigenous Australian individuals from 30 remote communities located within the central Australian statistical local area over a 36-month period between July 2005 and June 2008. The data were analyzed in January 2017.\n## EXPOSURES\nVisual acuity, slitlamp biomicroscopy, and fundus examination were performed on all patients at recruitment. Visual impairment was defined as a visual acuity of less than 6/12 in the better eye.\n## MAIN OUTCOMES AND MEASURES\nMortality rate and mortality cause were obtained at 10 years, and statistical analyses were performed. Hazard ratios for 10-year mortality with 95% confidence intervals are presented.\n## RESULTS\nOne thousand three hundred forty-seven patients were recruited from a total target population number of 2014. The mean (SD) age was 56 (11) years, and 62% were women. The total all-cause mortality was found to be 29.3% at 10 years. This varied from 21.1% among those without visual impairment to 48.5% among those with visual impairment. After adjustment for age, sex, and the presence of diabetes and hypertension, those with visual impairment were 40% more likely to die (hazard ratio, 1.40; 95% CI, 1.16-1.70; P = .001) during the 10-year follow-up period compared with those with normal vision.\n## CONCLUSIONS AND RELEVANCE\nBilateral visual impairment among remote indigenous Australian individuals was associated with 40% higher 10-year mortality risk compared with those who were not visually impaired. Resource allocation toward improving visual acuity may therefore aid in closing the gap in mortality outcomes between indigenous and nonindigenous Australian individuals.\n"
        },
        {
            "title": "Activated Retinal Pigment Epithelium, an Optical Coherence Tomography Biomarker for Progression in Age-Related Macular Degeneration.",
            "abstract": "Purpose:\n        \n      \n      To summarize and contextualize recent histology and clinical imaging publications on retinal pigment epithelium (RPE) fate in advanced age-related macular degeneration (AMD); to support RPE activation and migration as important precursors to atrophy, manifest as intraretinal hyperreflective foci in spectral-domain optical coherence tomography (SDOCT).\n    \n\n\n          Methods:\n        \n      \n      The Project MACULA online resource for AMD histopathology was surveyed systematically to form a catalog of 15 phenotypes of RPE and RPE-derived cells and layer thicknesses in advanced disease. Phenotypes were also sought in correlations with clinical longitudinal eye-tracked SDOCT and with ex vivo imaging-histopathology correlations in geographic atrophy (GA) and pigment epithelium detachments (PED).\n    \n\n\n          Results:\n        \n      \n      The morphology catalog suggested two main pathways of RPE fate: basolateral shedding of intracellular organelles (apparent apoptosis in situ) and activation with anterior migration. Acquired vitelliform lesions may represent a third pathway. Migrated cells are packed with RPE organelles and confirmed as hyperreflective on SDOCT. RPE layer thickening due to cellular dysmorphia and thick basal laminar deposit is observed near the border of GA. Drusenoid PED show a life cycle of slow growth and rapid collapse preceded by RPE layer disruption and anterior migration.\n    \n\n\n          Conclusions:\n        \n      \n      RPE activation and migration comprise an important precursor to atrophy that can be observed at the cellular level in vivo via validated SDOCT. Collapse of large drusen and drusenoid PED appears to occur when RPE death and migration prevent continued production of druse components. Data implicate excessive diffusion distance from choriocapillaris in RPE death as well as support a potential benefit in targeting drusen in GA."
        },
        {
            "title": "Long term effect on intraocular pressure of phacotrabeculectomy compared to trabeculectomy.",
            "abstract": "Aim:\n        \n      \n      To compare the long term mean intraocular pressure (IOP) reduction after non-augmented single site phacotrabeculectomy with that after trabeculectomy and to determine the relation between preoperative IOP and IOP reduction.\n    \n\n\n          Methods:\n        \n      \n      A group of 44 consecutive patients with chronic open angle glaucoma who underwent phacotrabeculectomy were matched to a trabeculectomy control group and the results of surgery were compared. Linear regression analysis of preoperative IOP and IOP reduction was undertaken.\n    \n\n\n          Results:\n        \n      \n      The mean IOP reduction was significantly less in the phacotrabeculectomy group (6.7 (SD 2.1) mm Hg) than in the trabeculectomy group (11.0 (1.4) mm Hg) (p=0.0017). There was a significant difference in surgical success between the groups. The preoperative IOP was significantly related to the postoperative reduction in IOP in both groups (p<0.001).\n    \n\n\n          Conclusions:\n        \n      \n      In elderly white patients with chronic open angle glaucoma, phacotrabeculectomy is not as effective as trabeculectomy in reducing IOP. In both procedures the magnitude of IOP reduction is proportional to the preoperative IOP."
        },
        {
            "title": "A prospective study of dehydroepiandrosterone sulfate and cognitive function in an older population: the Rancho Bernardo Study.",
            "abstract": "Objective:\n        \n      \n      To determine whether low plasma dehydroepiandrosterone sulfate (DHEAS) levels predict poor cognitive function in the elderly.\n    \n\n\n          Design:\n        \n      \n      A prospective, population-based study with periodic clinical evaluations and 100% follow-up for vital status.\n    \n\n\n          Setting:\n        \n      \n      Rancho Bernardo, California\n    \n\n\n          Patients:\n        \n      \n      270 men and 167 women (80% of surviving, local, age-eligible subjects) from the Rancho Bernardo cohort who had plasma obtained for DHEAS assays in 1972 to 1974 and screening for dementia in 1988 to 1991.\n    \n\n\n          Measurements:\n        \n      \n      DHEAS levels were measured by radioimmunoassay. There were five interviewer-administered standard screening tests of cognitive function: Mini-Mental Status Examination, Buschke selective reminding test, Trails B, category fluency, and Heaton Visual Reproduction test.\n    \n\n\n          Results:\n        \n      \n      DHEAS levels were higher in men than women and decreased with age in both sexes. There were no significant differences in age-adjusted DHEAS levels in the percent of men or women with categorically impaired performance on any test. When analyzed as a continuous variable, DHEAS levels were significantly correlated with only one test, the Bushke, and only in women. Low baseline DHEAS levels were not associated with any mention of dementia on death certificates or with non-participation of survivors. Low levels of DHEAS predicted mortality in men more than in women such that men were more likely to have died before cognitive function testing than women.\n    \n\n\n          Conclusion:\n        \n      \n      The single DHEAS-memory association, restricted to women, is most likely to be spurious, consequent to multiple comparisons. We cannot exclude a true effect of low DHEAS, restricted to women and reflecting their better survival than men."
        },
        {
            "title": "The prediction of disability by self-reported physical frailty components of the Tilburg Frailty Indicator (TFI).",
            "abstract": "Disability is an important health outcome for older persons; it is associated with impaired quality of life, future hospitalization, and mortality. Disability also places a high burden on health care professionals and health care systems. Disability is regarded as an adverse outcome of physical frailty. The main objective of this study was to assess the predictive validity of the eight individual self-reported components of the physical frailty subscale of the TFI for activities of daily living (ADL) and instrumental activities of daily living (IADL) disability. This longitudinal study was carried out with a sample of Dutch citizens. At baseline the sample consisted at 429 people aged 65 years and older and a subset of all respondents participated again two and a half years later (N=355, 83% response rate). The respondents completed a web-based questionnaire comprising the TFI and the Groningen Activity Restriction Scale (GARS) for measuring disability. Five components together (unintentional weakness, weakness, poor endurance, slowness, low physical activity), referring to the phenotype of Fried et al., predicted disability, even after controlling for previous disability and other background characteristics. The other three components of the physical frailty subscale of the TFI (poor balance, poor hearing, poor vision) together did not predict disability. Low physical activity predicted both total and ADL disability, and slowness both total and IADL disability. In conclusion, self-report assessment using the physical subscale of the TFI aids the prediction of future ADL and IADL disability in older persons two and a half years later."
        },
        {
            "title": "Last neurologic event is associated with risk of in-hospital stroke or death after carotid endarterectomy or carotid artery stenting: Secondary data analysis of the German statutory quality assurance database.",
            "abstract": "Objective:\n        \n      \n      We sought to analyze the association between last neurologic event and the risk of stroke or death among patients treated with carotid endarterectomy (CEA) or carotid artery stenting (CAS) under routine conditions in Germany.\n    \n\n\n          Methods:\n        \n      \n      Secondary data analysis was performed based on the German statutory quality assurance database for carotid procedures. A total of 144,347 patients treated by CEA and 14,794 patients treated by CAS were included in the analysis. Primary outcome was any in-hospital stroke or death. To analyze the association between the last neurologic event and outcome, multilevel multivariable regression analysis was performed.\n    \n\n\n          Results:\n        \n      \n      In patients treated by CEA, raw risk for any in-hospital stroke or death was 2.0% (2923/144,347), with a risk of 1.4% in asymptomatic and 3.0% in symptomatic patients. In patients treated by CAS, raw risk for any in-hospital stroke or death was 3.6% (538/14,794), with a risk of 1.7% in asymptomatic and 6.1% in symptomatic patients. Regression analysis revealed that increasing severity of last neurologic event was significantly associated with an increasing risk of any in-hospital stroke or death in patients treated by both CEA and CAS (P < .004). However, the risk of any stroke or death did not significantly differ between asymptomatic patients and patients with amaurosis fugax before CEA or CAS (P = .219 for CEA, P = .124 for CAS).\n    \n\n\n          Conclusions:\n        \n      \n      Increasing severity of last neurologic event is associated with an increasing risk of any in-hospital stroke or death in patients treated by CEA and CAS. The risk of any stroke or death did not differ between asymptomatic patients and patients with amaurosis fugax."
        },
        {
            "title": "Diabetic microvascular complications: can patients at risk be identified? A review.",
            "abstract": "People with diabetes have an increased risk of developing microvascular complications, diabetic retinopathy, diabetic nephropathy and diabetic neuropathy, which, if undetected or left untreated, can have a devastating impact on quality of life and place a significant burden on health care costs. In addition, diabetic microvascular complications can reduce life expectancy. The strongest risk factors are glycaemic control and diabetes duration; however, other modifiable risk factors such as hypertension, hyperlipidaemia and smoking, and unmodifiable risk factors including age at onset of diabetes and genetic factors may all play a part. Along with the presence of external risk factors, some associations have also been noted between diabetic microvascular complications themselves. There is evidence that diabetic retinopathy in association with increased blood pressure is an important risk factor for diabetic nephropathy progression. Significant correlations have also been shown between the presence of diabetic peripheral neuropathy and the presence of background or proliferative diabetic retinopathy. Clinical trials are currently in progress looking at a number of approaches to designing treatments to prevent the adverse effects of hyperglycaemia. It is essential however, that risk factors associated with the progression and development of diabetic microvascular complications are detected and treated at an early stage in order to further reduce morbidity and mortality. Considering all three complications as interrelated may well facilitate early detection of microvascular disease. Despite good long-term glycaemic and blood pressure control, diabetes remains a major cause of blindness, renal failure and amputations. As the incidence of diabetes continues to rise, the burden of diabetic microvascular complications will increase in future, hence the need for early detection. Considering the microvascular complications of diabetes as related, and enquiring proactively about complications, may well facilitate early detection of microvascular disease."
        },
        {
            "title": "Intraventricular hemorrhage volume predicts poor outcomes but not delayed ischemic neurological deficits among patients with ruptured cerebral aneurysms.",
            "abstract": "Background:\n        \n      \n      Intraventricular hemorrhage (IVH) predicts worse outcomes following aneurysmal subarachnoid hemorrhage (SAH). One potential mechanism is that IVH predisposes to the development of delayed ischemic neurological deficits (DINDs). No previous studies have evaluated the association between IVH volume (in milliliters) and subsequent development of DINDs or poor outcomes.\n    \n\n\n          Objective:\n        \n      \n      To assess the association between the volume of IVH and the subsequent development of DINDs, delayed cerebral infarction, death, and poor neurological outcomes, specifically among patients with concomitant SAH and IVH.\n    \n\n\n          Methods:\n        \n      \n      We performed a cohort study involving 152 consecutive patients with concomitant SAH and IVH. To determine volume of IVH, we used the IVH Score, shown to correlate well with computerized volumetric assessment. To determine the relative quantity of subarachnoid blood, we applied the SAH Sum Score. Multivariate logistic regression was used to adjust for potential confounders.\n    \n\n\n          Results:\n        \n      \n      There was no significant association between IVH volume and the development of DINDs or delayed infarction. In contrast, patients with poor neurological outcomes had significantly larger baseline IVH volume (mean, 11.8 mL vs 3.8 mL, P = .001). In the multivariate analysis, IVH volume was an independent predictor of poor outcomes (OR per mL: 1.11 [1.04-1.18]). Patients in the highest quartile for IVH volume were far more likely to progress to poor outcome compared with those in the lowest quartile (OR 4.09 [1.32-12.65]). Interobserver agreement in the determination of IVH Score was moderate to good.\n    \n\n\n          Conclusions:\n        \n      \n      IVH volume is an independent predictor of poor neurological outcomes, even after adjusting for the amount of subarachnoid blood. The pathophysiology of this association does not appear to involve an increased risk of DINDs or delayed infarction. Measures aimed at accelerating IVH clearance, such as intraventricular thrombolysis, merit further evaluation."
        },
        {
            "title": "Visual impairment and unintentional injury mortality: the National Health Interview Survey 1986-1994.",
            "abstract": "Purpose:\n        \n      \n      To examine the relationship between reported visual impairment and unintentional injury mortality.\n    \n\n\n          Design:\n        \n      \n      Mortality linkage study of a population-based survey.\n    \n\n\n          Methods:\n        \n      \n      Mortality linkage through 1997 of 116,796 adult participants, aged 18 years and older, from the 1986 to 1994 National Health Interview Survey was analyzed with respect to reported visual impairment using Cox regression models. The average follow-up was 7.0 years, and 295 unintentional injury deaths were identified. After controlling for survey design, age, sex, and the presence and number of eye diseases, participants with severe, bilateral visual impairment were at increased risk of death relative to participants without visual impairment (hazard ratio: 7.4; 95% confidence interval: 3.0-17.8).\n    \n\n\n          Conclusions:\n        \n      \n      Our data provide evidence that severe, bilateral visual impairment is associated with an increased risk of unintentional mortality among adults in the United States."
        },
        {
            "title": "A prospective study on the natural history of multiple sclerosis: clues to the conduct and interpretation of clinical trials.",
            "abstract": "The study's objectives were to assess the predictive significance of different sets of demographic, clinical and extraclinical variables in identifying multiple sclerosis patients with various risk levels of worsening during the follow-up, in order to provide clues to inclusion criteria and selection of primary clinical end-points in therapeutic trials. Two hundred and twenty-four patients at their first diagnosis of multiple sclerosis admitted to our Department between 1983 and 1990 were prospectively followed-up until the end of 1996. We considered as end-points time to reach non-reversible disability levels corresponding to EDSS scores of 4.0 and 6.0 and the beginning of a secondary progressive phase in the relapsing-remitting subgroup of patients. For the statistical treatment of our data we used the Kaplan-Meier survival curves and the Cox regression analysis. An initially progressive course and higher basal EDSS scores proved to be the best predictors of unfavorable prognosis; a greater number of functional systems involved at onset as well as higher residual deficits in pyramidal, visual, sphincteric and cerebellar systems were other factors predictive of a poor outcome, whereas sensory system involvement turned out to be favorable. In the relapsing-remitting subgroup, a longer first inter-attack interval was associated with a better prognosis; however, overall number of relapses in the first two years of the disease was of no prognostic value. The presence of oligoclonal banding in the cerebrospinal fluid and a cerebral MRI 'strongly suggestive' or 'suggestive' of MS in the early phases of the disease were associated with a higher probability of a worse outcome."
        },
        {
            "title": "Endophthalmitis after penetrating keratoplasty.",
            "abstract": "Purpose:\n        \n      \n      To determine the incidence of endophthalmitis after penetrating keratoplasty (PK) and patient and donor risk factors.\n    \n\n\n          Design:\n        \n      \n      Retrospective cohort study using national transplant registry data.\n    \n\n\n          Participants:\n        \n      \n      All corneal transplant recipients (n = 11 320) registered on the United Kingdom Transplant Registry undergoing their first PK between April 1999 and December 2006.\n    \n\n\n          Methods:\n        \n      \n      Patients who developed endophthalmitis were identified on the transplant registry. In addition, cases where the fellow cornea from the same donor had been transplanted were included. Clinical information regarding donor and recipient characteristics, surgical details, and postoperative outcomes were collected and analyzed. In cases where endophthalmitis was reported, the diagnosis was verified by a follow-up supplementary questionnaire to the surgeon. Logistic regression was used to investigate differences in the factors associated with the development of endophthalmitis.\n    \n\n\n          Main outcome measures:\n        \n      \n      Incidence of endophthalmitis and graft survival.\n    \n\n\n          Results:\n        \n      \n      The overall incidence of endophthalmitis occurring after primary PK in the UK was 0.67%. The incidence of endophthalmitis occurring within 6 weeks of surgery was 0.16%. Graft survival after endophthalmitis was 27% (95% confidence interval, 16-38) at 5 years, with a mean best-corrected visual acuity of 1.13 (logarithm of the minimum angle of resolution) for surviving grafts. Factors associated with endophthalmitis were donor cause of death (infection), high-risk cases, and indication for corneal transplantation.\n    \n\n\n          Conclusion:\n        \n      \n      Endophthalmitis remains a serious issue, with those affected having reduced graft survival and poor visual outcomes. Management of the identified recipient and donor risk factors are important to reduce endophthalmitis risk. In particular, the increased incidence of endophthalmitis when the donor dies of infection requires further explanation and review of current donor eye retrieval and eye bank practices. The delayed presentation of endophthalmitis cases also raises questions regarding possible sequestration of microbes within the corneal tissue and the effect of antimicrobials in storage media."
        },
        {
            "title": "Prediction of severe, persistent activity-of-daily-living disability in older adults.",
            "abstract": "In a prospective cohort of nondisabled adults aged 65 years or more in the Established Populations for Epidemiologic Studies of the Elderly (1981-1987 and 1985-1992), we used a competing risk approach to predict the 5-year risk of severe, persistent activities-of-daily-living (ADLs) disability, defined as dependence in ≥3 ADLs for 2 consecutive annual interviews or for 1 interview followed by death in the subsequent year. During 5 years, 6.8% developed severe, persistent ADL dependence, and 14.6% died without severe, persistent ADL dependence in the derivation cohort (n = 8,301); the corresponding percentages were 6.8% and 15.8% in the validation cohort (n = 4,177). A model based on age, current employment, visual impairment, self-rated health, diabetes mellitus, history of stroke or brain hemorrhage, cognitive function, and self-reported physical function showed good calibration. Discrimination, assessed by C statistics, for <70, 70-74, 75-79, and ≥80 years, was 0.75, 0.74, 0.65, and 0.66 in the derivation cohort and 0.70, 0.72, 0.70, and 0.65 in the validation cohort, respectively. In conclusion, a simple risk score based on routinely available clinical information can predict severe, persistent disability in 5 years. Future studies should examine whether physical performance measures can further improve prediction in the oldest old."
        },
        {
            "title": "Brief update on the burden of diabetes in South Carolina.",
            "abstract": "Diabetes is a serious disease, which is often accompanied by complications, such as blindness, kidney failure, heart attacks, strokes and amputations. High blood pressure and abnormal cholesterol levels are frequent comorbidities. Diabetes has an immense impact on public health and medical care. In South Carolina (SC), medical costs rise with increased duration of the disease, and lifespan is shortened by 5 to 10 years in most patients. To describe the burden of diabetes in SC, we examined the public health surveillance systems available to estimate the prevalence, mortality and hospitalization rates and some disability statistics and hospital charges. Diabetes is the 7 leading cause of death in SC, directly or indirectly claiming more than 3,000 lives annually, and the 5 leading cause of death in blacks, claiming about 1,200 black lives each year. Minorities, predominantly blacks, experienced a substantially higher death rate and more years of potential life lost than whites. The racial disparity in mortality has widened over the past 10 years. People with diabetes are at increased risk for blindness, lower extremity amputation, kidney failure, nerve disease, hypertension, ischemic heart disease and stroke. Approximately 450,000 South Carolinians are affected by diabetes, many of whom were still undiagnosed in 2010. One of every 5 patients in a SC hospital has diabetes, and 1 in every 10 visits to a SC emergency room is diabetes related. The total charges for diabetes and diabetes-related hospitalizations and emergency room visits were more than $4.2 billion in 2010."
        },
        {
            "title": "[Perspectives of the digital mammography platform].",
            "abstract": "In Europe one out of every nine women suffers from breast cancer during her lifetime. Since the introduction of mammography screening programs more breast cancers are being diagnosed when they are still small and early stage cancers with a favourable prognosis. The introduction of digital mammography systems has led to a continuous reduction of breast cancer mortality especially in specific patient subgroups. Furthermore, the digital mammography platform enables the development of new, innovative breast imaging methods to increase sensitivity and decrease breast cancer mortality. This digital mammography platform includes digital breast tomosynthesis, digital contrast medium mammography and digital contrast medium breast tomosynthesis as well as fused data sets from digital mammography with ultrasound or MRI. The following article summarizes these new applications, describes the strengths of the digital platform and illustrates the potential advantages of an improved breast cancer diagnosis by digital mammography."
        },
        {
            "title": "How does difficulty communicating affect the social relationships of older adults? An exploration using data from a national survey.",
            "abstract": "Healthy social relationships are important for maintaining mental and physical health in later life. Less social support, smaller social networks, and more negative social interactions have been linked to depression, poorer immune functioning, lower self-rated health, increased incidence of disease, and higher mortality. Overwhelming evidence suggests that communication disorders adversely affect social relationships. Much less is known about whether some or all aspects of social relationships are negatively affected by a communication disorder. The relative impact of a communication disorder on social relationships, as compared to other kinds of disability, is also poorly understood. Data were analyzed from a representative national sample of community-dwelling adults aged 65 and older living in the continental United States (n=742). Results from multiple regressions indicated that difficulty communicating was significantly associated with several parameters of social relationships even after controlling for age, gender, partnership status, health, functional limitations, and visual impairment. Communication difficulty was a significant predictor of smaller social network size, fewer positive social exchanges, less frequent participation in social activities, and higher levels of loneliness, but was not a significant predictor of negative social exchanges. These findings suggest that communication disorders may place older adults at increased risk for mental and physical health problems because of social isolation, reduced social participation, and higher rates of loneliness. In addition, it appears that communication disorders may have a greater impact on positive, rather than negative, aspects of social relationships.\n    \n\n\n          Learning outcomes:\n        \n      \n      As a result of this activity, the following learning outcomes will be realized: Readers will be able to (1) describe changes in the social relationships of older adults that occur as part of normal aging, (2) identify the aspects of social relationships that were significantly impacted by a communication difficulty, and (3) discuss possible reasons for these findings including potential clinical implications."
        },
        {
            "title": "Long-term outcome after photocoagulation for proliferative diabetic retinopathy.",
            "abstract": "One-hundred and forty patients with 182 treated eyes were followed for up to 10 years after photocoagulation for proliferative diabetic retinopathy. Sixty-eight patients were still alive and under review after 10 years. Mortality was 33% at 10 years and the survivors were younger when treated and had lower systolic and diastolic blood pressures, a lower urea and creatinine and a lower prevalence of proteinuria and ECG evidence of ischaemia at baseline. Sixty-nine percent of all patients and 82% of those followed up for 10 years maintained good vision (6/12 or better) in their better eye at the last follow-up. Visual deterioration occurred mostly in the first 2 years after treatment and risk factors for poor final vision were poor vision at baseline, severity of disc new vessels, and age at presentation. It is concluded that the short-term beneficial effect of photocoagulation is maintained over long periods of follow-up."
        },
        {
            "title": "Residual Associations of Inflammatory Markers with eGFR after Accounting for Measured GFR in a Community-Based Cohort without CKD.",
            "abstract": "Background and objectives:\n        \n      \n      eGFR on the basis of creatinine (eGFRcre) associates differently with cardiovascular disease and mortality than eGFR on the basis of cystatin C (eGFRcys). This may be related to risk factors affecting the level of creatinine and cystatin C along non-GFR pathways, which may confound the association between eGFR and outcome. Nontraditional risk factors are usually not measured in epidemiologic studies of eGFR and cannot be adjusted for to reduce confounding. We examined whether the inflammatory markers soluble TNF receptor type 2 (sTNFR2), C-reactive protein (CRP), and fibrinogen associated differently with eGFR than with measured GFR (mGFR).\n    \n\n\n          Design, setting, participants, & measurements:\n        \n      \n      GFR was measured by iohexol clearance in 1627 middle-aged participants without kidney disease, diabetes, or cardiovascular disease enrolled in the Renal Iohexol Clearance Survey Study from the Sixth Tromsø Study between 2007 and 2009. Generalized estimating equations were used to assess the residual associations between eGFR (eGFRcre, eGFRcys, and eGFR on the basis of creatinine and cystatin C) and the inflammatory markers relative to mGFR.\n    \n\n\n          Results:\n        \n      \n      sTNFR2, CRP, and fibrinogen were associated with a higher eGFRcre after accounting for mGFR in multivariable-adjusted models (2.63 ml/min per 1.73 m(2); 95% confidence interval [95% CI], 2.1 to 3.2 per SD increase in sTNFR2, 0.93 ml/min per 1.73 m(2); 95% CI, 0.3 to 1.5 per SD increase in log CRP, and 1.19 ml/min per 1.73 m(2); 95% CI, 0.6 to 1.8 per SD increase in fibrinogen). sTNFR2 and CRP were inversely associated with eGFRcys (-1.4 ml/min per 1.73 m(2); 95% CI, -2.1 to -0.6 per SD increase in sTNFR2, and -0.76 ml/min per 1.73 m(2); 95% CI, -1.4 to -0.1 per SD increase in log CRP).\n    \n\n\n          Conclusions:\n        \n      \n      eGFRcre and eGFRcys are associated with inflammatory factors after accounting for mGFR but in opposite directions. These non-GFR-related associations may bias risk estimates by eGFR and, in part, explain the different risks predicted by eGFRcre and eGFRcys in longitudinal studies."
        },
        {
            "title": "Myocardial viability detected by myocardial contrast echocardiography--prognostic value in patients after myocardial infarction.",
            "abstract": "Objective:\n        \n      \n      This study aimed to assess the role of myocardial contrast echocardiography (MCE) as a predictor of cardiac events and death in patients with acute myocardial infarction (AMI).\n    \n\n\n          Methods:\n        \n      \n      Eighty-six patients underwent primary percutaneous coronary angioplasty for AMI. Segmental perfusion was estimated by MCE in real time at mean 5 days after PCI using low MI (0.3) after 0.3-0.5 ml bolus injection of intravenous Optison. MCE was scored semiquantitatively as: (1) normal perfusion (homogenous contrast effect), (2) partial perfusion (patchy myocardial contrast enhancement), (3) lack of perfusion (no visible contrast effect). A contrast score index (CSI) was calculated as the sum of MCE scores in each segment divided by the total number of segments. The patients were followed up for cardiac events and death.\n    \n\n\n          Results:\n        \n      \n      A CSI of >1.68 was taken to be a predictor of cardiac events and death. Death occurred only in patients with CSI >1.68. Patients with CSI >1.68 had a significantly (P = 0.03) higher incidence of cardiac death or cardiac events (75%) compared to those with CSI <1.68 (27%). The absence of residual perfusion within the infarct zone was an independent predictor of death and cardiac events (P = 0.02).\n    \n\n\n          Conclusions:\n        \n      \n      The absence of residual myocardial viability in the infarct zone supplied by an infarct-related artery is a powerful predictor of cardiac events in patients after AMI."
        },
        {
            "title": "Prediction of mortality by logistic regression analysis in patients with postoperative enterocutaneous fistulae.",
            "abstract": "The contrasting results of treatment of patients with postoperative enterocutaneous fistulae reflect the heterogeneity of the disease and depend on the patient's condition and the characteristics of the fistulae. For this reason, the use of a prognostic index, which enables such patients to be classified according to their risk of death, could be useful. In this study we propose a prognostic index based on a logistic regression analysis, obtained by using two (APACHE II score and serum albumin concentration) of the eight risk factors that have been retrospectively analysed in a series of 70 patients with postoperative enterocutaneous fistulae treated in our surgical department since 1981. The logistic regression equation indicates that patients with a probability of dying of less than 0.35 have a good prognosis, with a sensitivity of 90 per cent, a specificity of 90 per cent, a negative predictive value of 79 per cent, a positive predictive value of 96 per cent and an accuracy of 90 per cent. The predictive performance of the index has also been evaluated in a group of 17 patients studied prospectively, and this confirms the sensitivity and specificity of the model. This postoperative enterocutaneous fistulae index could be a helpful tool in clinical trials and surgical audit."
        },
        {
            "title": "Current management of glaucoma and the need for complete therapy.",
            "abstract": "Glaucoma is a long-term ocular neuropathy defined by optic disc or retinal nerve fiber structural abnormalities and visual field abnormality. Primary open-angle glaucoma is the most common type of glaucoma. Currently available treatments, initiated in a stepwise process, focus on intraocular pressure (IOP) reduction, and initially include topical drug therapy (single then multidrug combinations), followed by laser then surgical treatment. Topical prostaglandin analogues or beta-adrenergic receptor blockers are first used, followed by alpha-agonists or topical carbonic anhydrase inhibitors, and infrequently, cholinergic agonists and oral therapy. Limitations to existing topical IOP-reducing medications include continued disease progression in glaucoma patients with normal IOP, treatment failure, and low rates of compliance and persistence. Therapeutic agents under investigation include neuroprotectants, which target the disease process manifested by death of retinal ganglion cells, axonal loss, and irreversible loss of vision. Neuroprotectants may be used alone or in combination with IOPreducing therapy (a treatment strategy called complete therapy). Memantine, an N-methyl-D-aspartate receptor blocker currently approved for dementia, is the neuroprotectant farthest along in the process seeking regulatory approval for glaucoma treatment and has a favorable safety profile because of its selective mechanism of action. Several other neuroprotectants are in early stage investigation. Complete therapy provides hope for improved outcomes by reducing the significant morbidity and economic consequences that occur as a result of neurodegeneration and disease progression."
        },
        {
            "title": "A Phase IIIb study to evaluate the safety of ranibizumab in subjects with neovascular age-related macular degeneration.",
            "abstract": "Objective:\n        \n      \n      To evaluate the safety and efficacy of intravitreal ranibizumab in a large population of subjects with neovascular age-related macular degeneration (AMD).\n    \n\n\n          Design:\n        \n      \n      Twelve-month randomized (cohort 1) or open-label (cohort 2) multicenter clinical trial.\n    \n\n\n          Participants:\n        \n      \n      A total of 4300 subjects with angiographically determined subfoveal choroidal neovascularization (CNV) secondary to AMD.\n    \n\n\n          Methods:\n        \n      \n      Cohort 1 subjects were randomized 1:1 to receive 0.3 mg (n = 1169) or 0.5 mg (n = 1209) intravitreal ranibizumab for 3 monthly loading doses. Dose groups were stratified by AMD treatment history (treatment-naïve vs. previously treated). Cohort 1 subjects were retreated on the basis of optical coherence tomography (OCT) or visual acuity (VA) criteria. Cohort 2 subjects (n = 1922) received an initial intravitreal dose of 0.5 mg ranibizumab and were retreated at physician discretion. Safety was evaluated at all visits.\n    \n\n\n          Main outcome measures:\n        \n      \n      Safety outcomes included the incidence of ocular and nonocular adverse events (AEs) and serious adverse events (SAEs). Efficacy outcomes included changes in best-corrected VA over time.\n    \n\n\n          Results:\n        \n      \n      Some 81.7% of cohort 1 subjects and 49.9% of cohort 2 subjects completed the 12-month study. The average total number of ranibizumab injections was 4.9 for cohort 1 and 3.6 for cohort 2. The incidence of vascular and nonvascular deaths during the 12-month study was 0.9% and 0.7% in the cohort 1 0.3 mg group, 0.8% and 1.5% in the cohort 1 0.5 mg group, and 0.7% and 0.9% in cohort 2, respectively. The incidence of death due to unknown cause was 0.1% in both cohort 1 dose groups and cohort 2. The number of vascular deaths and deaths due to unknown cause did not differ across cohorts or dose groups. Stroke rates were 0.7%, 1.2%, and 0.6% in the 0.3 mg and 0.5 mg groups and cohort 2, respectively. At month 12, cohort 1 treatment-naïve subjects had gained an average of 0.5 (0.3 mg) and 2.3 (0.5 mg) VA letters and previously treated subjects had gained 1.7 (0.3 mg) and 2.3 (0.5 mg) VA letters.\n    \n\n\n          Conclusions:\n        \n      \n      Intravitreal ranibizumab was safe and well tolerated in a large population of subjects with neovascular AMD. Ranibizumab had a beneficial effect on VA. Future investigations will seek to establish optimal dosing regimens for persons with neovascular AMD.\n    \n\n\n          Financial disclosure(s):\n        \n      \n      Proprietary or commercial disclosure may be found after the references."
        },
        {
            "title": "Statistical process control for monitoring standardized mortality ratios of a classification tree model.",
            "abstract": "Objectives:\n        \n      \n      The ratio of observed to expected mortality (standardized mortality ratio, SMR), is a key indicator of quality of care. We use PreControl Charts to investigate SMR behavior over time of an existing tree-model for predicting mortality in intensive care units (ICUs) and its implications for hospital ranking. We compare the results to those of a logistic regression model.\n    \n\n\n          Methods:\n        \n      \n      We calculated SMRs of 30 equally-sized consecutive subsets from a total of 12,143 ICU patients aged 80 years or older and plotted them on a PreControl Chart. We calculated individual hospital SMRs in 2009, with and without repeated recalibration of the models on earlier data.\n    \n\n\n          Results:\n        \n      \n      The overall SMR of the tree-model was stable over time, in contrast to logistic regression. Both models were stable after repeated recalibration. The overall SMR of the tree on the whole validation set was statistically significantly different (SMR 1.00 ± 0.012 vs. 0.94 ± 0.01) and worse in performance than the logistic regression model (AUC 0.76 ± 0.005 vs. 0.79 ± 0.004; Brier score 0.17 ± 0.012 vs. 0.16 ± 0.010). The individual SMRs' range in 2009 was 0.53-1.31 for the tree and 0.64-1.27 for logistic regression. The proportion of individual hospitals with SMR >1, hinting at poor quality of care, reduced from 38% to 29% after recalibration for the tree, and increased from 15% to 35% for logistic regression.\n    \n\n\n          Conclusions:\n        \n      \n      Although the tree-model has seemingly a longer shelf life than the logistic regression model, its SMR may be less useful for quality of care assessment as it insufficiently responds to changes in the population over time."
        },
        {
            "title": "Bidirectional association between the risk of comorbidities and the diagnosis of retinal vein occlusion in an elderly population: a nationwide population-based study.",
            "abstract": "Background:\n        \n      \n      Retinal vein occlusion (RVO) is the second most common retinal vascular disease, with peak incidence at 70years of age. However, the bidirectional association between the risk of comorbidities and the diagnosis of RVO in this population is uncertain.\n    \n\n\n          Methods:\n        \n      \n      A population-based cohort of 1,784,960 patients 70years of age and older retrieved from the Taiwan National Health Insurance Research Database between 2000 and 2010. Risks of comorbidities were assessed 5years before and after the diagnosis of RVO.\n    \n\n\n          Results:\n        \n      \n      In our study, 3393 subjects had central RVO (CRVO) and 6688 subjects had branch RVO (BRVO). Before the diagnosis of RVO, patients showed increased risks for the following comorbidities: hypertension (odds ratio [OR]=1.83, 95% confidence interval [CI], 1.74-1.93), dyslipidemia (OR=1.29, [1.23-1.35]), DM (OR=1.29, [1.23-1.35]), liver disease (OR=1.22, [1.16-1.29]), renal disease (OR=1.30, [1.23-1.37]), and cerebrovascular disease (OR=1.16, [1.11-1.21]). After the diagnosis of RVO, patients were at greater risk of developing DM (adjusted hazard ratio [AHR]=1.12, [1.06-1.19]), PAD (AHR=1.17, [1.08-1.27]), and MACE (AHR=1.35, [1.25-1.46]); however, the risk of all-cause mortality was unchanged. Elderly patients with CRVO had a significantly higher risk of all-cause mortality (AHR=1.09, [1.02-1.17]), whereas patients with BRVO showed no significant differences in mortality.\n    \n\n\n          Conclusion:\n        \n      \n      This study suggests bidirectional association between the risk of comorbidities and the diagnosis of RVO in an elderly population."
        },
        {
            "title": "Age-related macular degeneration and mortality: the Melbourne Collaborative Cohort Study.",
            "abstract": "AimsTo assess associations between features of age-related macular degeneration (AMD) and mortality.MethodsA total of 21 129 participants from the Melbourne Collaborative Cohort Study aged 47-85 years (60% female) were assessed for AMD (2003-2007). Mortality data to December 31, 2012 were obtained through linkage with the National Death Index. Associations were assessed using Cox regression, adjusting for age, sex, smoking, region of birth, education, physical activity, diet and alcohol.ResultsLate AMD was identified in 122 (0.6%) participants, including those with choroidal neovascularisation (n=55, 0.3%), geographic atrophy (n=87, 0.4%) and reticular pseudodrusen (n=87, 0.4%). After a median follow-up period of 8.1 years, 1669 (8%) participants had died, including those from cardiovascular diseases (386), tobacco-related cancers (179), and neurodegenerative disease (157). There was evidence of an increased rate of all-cause mortality for those with choroidal neovascularisation (Hazard Ratio (HR) 1.71 95% CI 1.06-2.76) and geographic atrophy (HR 1.46 95% CI 0.99-2.16). Choroidal neovascularisation was also associated with an increased rate of cardiovascular mortality (HR 3.16 95% CI 1.62-6.15) and geographic atrophy was associated with an increased rate of death from tobacco-related cancer (HR 2.86 95% CI 1.15-7.09). Weak evidence was also present for an association between choroidal neovascularisation and death from neurodegenerative disease (HR 2.49 95% CI 0.79-7.85). Neither reticular pseudodrusen nor the earlier stages of AMD were associated with mortality.ConclusionsLate AMD is associated with an increased rate of all-cause mortality. Choroidal neovascularisation and geographic atrophy were associated with death from cardiovascular disease and tobacco-related cancer, respectively."
        },
        {
            "title": "Cognitive Performance Concomitant With Vision Acuity Predicts 13-Year Risk for Mortality.",
            "abstract": "Objective: To assess the joint impact of cognitive performance and visual acuity on mortality over 13-year follow-up in a representative US sample. Methods: Data from National Health and Nutrition Examination Survey (NHANES) participants (≥18 years old) were linked with the death record data of the National Death Index (NDI) with mortality follow-up through December 31, 2011. Cognitive performance was evaluated by the Digit Symbol Substitution Test (DSST) and cognitive performance impairment was defined as the DSST score equal to or less than the median value in the study population. Visual impairment (VI) was defined as presenting visual acuity worse than 20/40 in the better-seeing eye. Risks of all-cause and specific-cause mortality were estimated with Cox proportional hazards models after adjusting for confounders. Results: A total of 2,550 participants 60 years and older from two waves of (NHANES, 1999-2000, 2001-2002) were included in the current analysis. Over a median follow-up period of 9.92 years, 952 (35.2%) died of all causes, of whom 239 (23.1%), 224 (24.0%), and 489 (52.9%) died from cardiovascular disease (CVD), cancer, and non-CVD/non-cancer mortality, respectively. Cognitive performance impairment and VI increased the odds for mortality. Co-presence of VI among cognitive impaired elderly persons predicted nearly a threefold increased risk of all-cause mortality [hazard ratios (HRs), 2.74; 95% confidence interval (CI), 2.02-3.70; P < 0.001) and almost a fourfold higher risk of non-CVD/non-cancer mortality (HR, 3.72; 95% CI, 2.30-6.00; P < 0.001) compared to having neither impairment. Conclusion: People aged 60 years and over with poorer cognitive performance were at higher risk of long-term mortality, and were especially vulnerable to further mortality when concomitant with VI. It is informative for clinical implication in terms of early preventive interventions."
        },
        {
            "title": "Carotid arterial surgery using local anesthesia: a private practice retrospective study.",
            "abstract": "Carotid endarterectomy has recently become one of the more controversial operations. The tremendous increase in the number of endarterectomies performed, coupled with the apparent increase in morbidity and mortality associated with this operation in some studies, have brought into question the indications and results of the procedure. The potential for complications from the procedure itself, as well as increased morbidity and mortality from surgery on the elderly, make carotid endarterectomy a dangerous operation that must be done carefully and thoughtfully. The authors have performed carotid endarterectomies exclusively under local anesthesia to more closely evaluate the neurologic status of the patient. They believe that the operation performed in this manner obviates the use of a shunt and its inherent complications in greater than 80 per cent of the patients. This, coupled with the fact that many of the patients also have severe cardiac disease and the use of local anesthesia causes less hemodynamic changes and stress, should make carotid endarterectomy under local anesthesia the preferred approach."
        },
        {
            "title": "Contrast enhanced CT versus integrated PET-CT in pre-operative nodal staging of non-small cell lung cancer.",
            "abstract": "Purpose:\n        \n      \n      The purpose of this study was to retrospectively evaluate the effectiveness of integrated positron emission tomography-computed tomography (PET-CT) in comparison with contrast enhanced CT (CE-CT) in pre-operative staging of non-small cell lung cancer (NSCLC) by using surgical and pathological findings as the reference standard.\n    \n\n\n          Materials and methods:\n        \n      \n      From August 2008 to August 2009, 57 consecutive patients (50 males and 7 females; mean age, 59 years; range, 38-79 years) with NSCLC underwent conventional pre-operative lung cancer staging using clinical data and CE-CT of the chest, and integrated whole-body fluorine-18 fluorodeoxyglucose PET-CT studies. Histopathological results served as the reference standard.\n    \n\n\n          Results:\n        \n      \n      Forty-eight of the 57 patients (84%) had no lymph node involvement (N0), five (9%) were found to have N1 disease, and four (7%) had N2 disease. There was a significant difference between CE-CT and PET-CT for nodal staging of N0 disease (P < 0.05). Sensitivity, specificity, positive predictive value, negative predictive value, and accuracy of hilar and mediastinal lymph node staging were 56%, 73%, 28%, 90%, and 70%, with CE-CT, respectively; and 78%, 92%, 64%, 96%, and 89% with PET-CT, respectively.\n    \n\n\n          Conclusion:\n        \n      \n      Integrated PET-CT is more accurate than CE-CT for lymph node staging in NSCLC."
        },
        {
            "title": "PCA3 urinary test versus 1H-MRSI and DCEMR in the detection of prostate cancer foci in patients with biochemical alterations.",
            "abstract": "Aim:\n        \n      \n      To compare the prostate antigen 3 (PCA3) test with (1)H-magnetic resonance spectroscopic imaging ((1)H-MRSI) and dynamic contrast-enhanced magnetic resonance imaging (DCEMR) combined examination in the detection of prostate tumor foci in patients with persistently elevated prostate-specific antigen (PSA) levels and prior negative random transrectal ultrasound (TRUS)-guided biopsy.\n    \n\n\n          Patients and methods:\n        \n      \n      Forty-three patients with a first random biopsy negative for prostate adenocarcinoma, persistent elevated PSA and negative digital rectal examination were recruited. All the patients were submitted to MRSI examination (MRSI-DCEMR) and were submitted to an attentive prostate massage in order to perform PCA3 assay. Afterwards, 10-core laterally-directed random TRUS-guided prostate biopsy was performed.\n    \n\n\n          Results:\n        \n      \n      The overall sensitivity and specificity of a PCA3 score ≥35 for positive biopsy were 76.9% and 66.6%, respectively, with a positive predictive value (PPV) of 80% and a negative predictive value (NPV) of 62.5%; as for MRSI sensitivity and specificity were, respectively, 92.8% and 86.6% with a PPV of 92.8% and a NPV of 86.6%. Receiver operating characteristic (ROC) analysis rates were 0.755 for PCA3 and 0.864 for MRSI.\n    \n\n\n          Conclusion:\n        \n      \n      Combined MRSI/DCEMR can better improve the cancer detection rate in patients with prior negative TRUS-guided biopsy and altered PSA serum levels than PCA3. Optimization of MRSI will allow more precise diagnosis of local invasion and improved bioptical procedures."
        },
        {
            "title": "[Characteristics and predictive value of quality of life in a French cohort of angina patients].",
            "abstract": "The study of quality of life (QoL) in a French cohort of patients suffering from angina pectoris was one of the objectives of the ELAN longitudinal study. It concerned 3,954 subjects (76% males) mean age: 67 +/- 11 years, followed up by 613 cardiologists which were invited to complete a series of baseline sociodemographic and clinical data and to answer a series of questions upon one year outcome (3,261 medical records available). QoL was assessed at baseline via a self-administered 12-item general questionnaire, the Short-Form 12 (SF-12), enabling to compute a mental component summary (CS-12) and a physical component summary (PCS-12) score. Mean MCS-12 in the ELAN cohort (49 +/- 7.5) was very close to the standards derived from general American population (50 +/- 10) or to the data available in a general French population (51.2 +/- 7.4). Whereas mean PCS-12 was hardly lower (about one standard deviation) in comparison with general American population (50 +/- 10) or with a general French population (48.4 +/- 9.4). QoL was higher in males and linked to age in a contrasted way (higher MCS-12 and lower PCS-12 in elderly; p < 0.0001). It depended on the clinical condition (lower MCS-12 associated with mixed-type angina pectoris or with more severe angina and with persistent smoking; lower PCS-12 associated with mixed type or more severe angina, with cardiac failure episodes, arteritis obliterans, stroke antecedents or left ventricular hypertrophy). Both scores were negatively correlated, in multivariate regression analysis, with the severity of persisting angina at one year, after controlling for the severity of baseline angina and the other confounding variables. Above all, MCS-12 and especially PCS-12, predicted major coronary events at one year (death, myocardial infarction, angioplasty, coronary by-pass surgery). In a multivariate logistic regression analysis, low baseline PCS-12 was associated with higher risk for cardiovascular death at one year (OR = 2.44; 95% CI = 1.25-4.74; p < 0.01). These results confirm the clinical validity of SF-12 (cross sectional stage of the study) and stress its prognostic value independent from the other risk factors (longitudinal stage of the study)."
        },
        {
            "title": "Long-Term Survival Rates of Patients Undergoing Vitrectomy for Proliferative Diabetic Retinopathy.",
            "abstract": "Purpose:\n        \n      \n      Reported 5-year survival rates in patients undergoing vitreous surgery for proliferative diabetic retinopathy (PDR) range from 68-95%. Studies relating survival rates to medical baseline characteristics predate the millennium. This study aimed to update data on life expectancy of patients undergoing vitrectomy for PDR and identify baseline factors which may influence survival.\n    \n\n\n          Methods:\n        \n      \n      A retrospective cohort study of consecutive patients who underwent their first pars-plana vitrectomy for PDR between April 2004 and May 2005 was performed. Survival status on 1 May 2012 was the primary endpoint. The Kaplan-Meier life table method was used to determine survival rates. Univariate and multiple variable Cox proportional hazards regressions were used to identify risk factors for mortality.\n    \n\n\n          Results:\n        \n      \n      A total of 148 patients were included in the study, with a mean age of 54 years (range 20-80 years) at time of surgery. The 3-, 5- and 7-year survival rates were 94%, 86% and 77%, respectively (95% confidence interval, CI, 88-97%, 79-91% and 68-84%, respectively). Renal failure was the most common cause of death. The presence of limb ulcers at baseline was the most important prognostic indicator for mortality, with a hazard ratio of 3.13 (95% CI 1.46-6.71, p = 0.003) and a survival rate at 5 years reduced to 79%.\n    \n\n\n          Conclusion:\n        \n      \n      The 5-year survival rate remains comparable to those reported 20 years ago despite a lowering in threshold for vitrectomy and increased health awareness. Limb ulcers are strongly associated with increasing mortality. Clinicians should remain mindful of the systemic associations of diabetes particularly in advanced retinal disease."
        },
        {
            "title": "AGE-RELATED MACULAR DEGENERATION AND THE RISK OF ALL-CAUSE AND CARDIOVASCULAR MORTALITY: A Meta-Analysis of Cohort Studies.",
            "abstract": "Purpose:\n        \n      \n      We evaluated the association between age-related macular degeneration (AMD) and the risk of all-cause and cardiovascular mortality by meta-analyses of data from prospective studies.\n    \n\n\n          Methods:\n        \n      \n      A literature search was performed in PubMed, Web of Science, Embase, Cocharne Library, and China National Knowledge Infrastructure for relevant articles published up to December 2016. We estimated hazard ratios with 95% confidence intervals with fixed-effect models and conducted meta-regression to explore the potential sources of heterogeneity. Small-study effect was estimated by Egger's test and funnel plot.\n    \n\n\n          Results:\n        \n      \n      We identified 13 population-based prospective cohort studies that examined the relationship between AMD and all-cause and cardiovascular mortality. Overall, the hazard ratios (95% confidence intervals) of all-cause mortality and cardiovascular mortality associated with any AMD were 1.15 (1.05-1.27) and 1.05 (95% confidence intervals: 0.87-1.26), respectively. The risk of all-cause mortality and cardiovascular mortality associated with early AMD were 1.08 (1.00-1.18) and 1.05 (0.89-1.24), and the associations with late AMD were 1.23 (1.11-1.36) and 1.28 (1.04-1.57), respectively. No evidence of small-study effect was found.\n    \n\n\n          Conclusion:\n        \n      \n      This meta-analysis indicated that AMD, especially late AMD, was associated with increased risk of all-cause mortality and cardiovascular mortality based on comparisons with people who did not have AMD and who were of similar age and sex."
        },
        {
            "title": "Dynamic-enhanced MRI predicts metastatic potential of invasive ductal breast cancer.",
            "abstract": "Background:\n        \n      \n      Dynamic magnetic resonance imaging (MRI) has improved the detection of breast malignancies. The method is based on estimating the velocity of contrast enhancement taking into account increased angiogenesis in tumor. Microvessel density correlates with breast carcinoma metastasis. Thus, we hypothesized that contrast enhancement on MRI correlates with metastasis in breast cancer patients. The present study attempts to clarify the quantitative assessment of dynamic data, and examines the correlation between MRI enhancement and breast carcinoma metastasis.\n    \n\n\n          Methods:\n        \n      \n      The subjects consisted of 31 patients with invasive ductal breast cancer. Twenty patients were disease free for five years (group A), and eleven patients suffered from metastatic disease at distant sites concurrently or postoperatively (group B). Dynamic MRI was performed preoperatively using a 1.5T system in all cases. Using the dynamic data, the signal intensity (SI)ratio and SI index were determined and analyzed retrospectively taking into account the presence of distant metastases.\n    \n\n\n          Results:\n        \n      \n      The values of the SI ratio were 2.2+/-0.7 in group A and 2.3+/-0.4 in group B, respectively, with no significant difference seen between the groups. The SI index value was significantly higher in group B (28.5+/-32.8) than in group A (10.3+/-5.5, p<0.05).\n    \n\n\n          Conclusions:\n        \n      \n      The current series suggests that the SI index could distinguish patients with high risk of distant metastasis from disease free patients, preoperatively. If a suitable borderline value were established, the quantitative dynamic parameter determined by MRI may be useful for predicting the prognosis of breast cancer patients."
        },
        {
            "title": "Radiotherapy for orbital lymphoma : outcome and late effects.",
            "abstract": "Purpose:\n        \n      \n      To analyze the effectiveness of radiotherapy in the management of orbital non-Hodgkin's lymphoma (NHL).\n    \n\n\n          Patients and methods:\n        \n      \n      42 patients (median age 64.5 years) were reviewed retrospectively. The median follow-up period was 58 months. 26 patients had stage IE orbital lymphoma (22 indolent, four aggressive NHLs). 16 patients had advanced NHLs in stages II-IV with orbital involvement (eleven indolent, five aggressive NHLs). The median radiation dose was 40 Gy (20-46 Gy) for indolent lymphoma and 44 Gy (20-48 Gy) for aggressive lymphoma. Patients with stage IE were treated with at least 30 Gy.\n    \n\n\n          Results:\n        \n      \n      The 5-year local control rate for patients with stage I was 100%, the 5-year overall survival 91%. Two distant relapses were found, but no lymphoma-related death was detected. The 5-year local control rate for patients in stages II, III, and IV was 80%. Two local failures were detected. The 5-year overall survival for the advanced stages was 47%, nine patients with stages III and IV died due to systemic progression of lymphoma. Acute, radiotherapy-related complications grade 3/4 were not observed. Late effects grade 1/2 were documented in 45%. Six patients, treated with doses of > 36 Gy, developed grade 3 complications (four cataract, two dryness).\n    \n\n\n          Conclusion:\n        \n      \n      Radiotherapy alone yields excellent local control and overall survival rates in orbital lymphoma stage IE. Local irradiation is also well tolerated and effective in advanced NHL stages with orbital infiltration. Doses of > 36 Gy resulted in an increase of late complications."
        },
        {
            "title": "Mortality and cataract: findings from a population-based longitudinal study.",
            "abstract": "The study was carried out in a rural population in central India. A random sample of 11 village communities provided 1020 persons aged 40-64 years, who were examined in 1982 and again reassessed in 1986. Statistical analysis, based on the Mantel-Haenszel method for stratified data, showed increased mortality in persons who had central lens opacities, compared with those who had trivial or no central lens opacities. The significant age-adjusted death ratio was just over 2 (2.2), as were the age/sex-adjusted and age/vision-adjusted estimates, which indicate doubling of mortality in the cataract cohort. Multiple regression analysis using the Cox proportional-hazards model gave very similar results. Statistical tests for homogeneity of death ratios across the various age/sex/vision strata were carried out, and the observed association between cataract and mortality was found to be consistent, both in males and in females, in the youngest and oldest age groups, and among those with adequate vision of 6/18 or better as well as among persons with serious visual impairment. There were no known diabetics in the study sample, which came from what could reasonably be regarded as a non-diabetic population."
        },
        {
            "title": "Estimating incidence of vision-reducing cataract in Africa: a new model with implications for program targets.",
            "abstract": "Objective:\n        \n      \n      To estimate the incidence of vision-reducing cataract in sub-Saharan Africa and use these data to calculate cataract surgical rates (CSR) needed to eliminate blindness and visual impairment due to cataract.\n    \n\n\n          Methods:\n        \n      \n      Using data from recent population-based, standardized, rapid-assessment surveys, we calculated the age-specific prevalence of cataract (including operated and unoperated eyes) from surveys in 7 \"districts\" across Africa. This was done at 3 levels of visual acuity. Then we used the age-specific prevalence data to develop a model to estimate age-specific incidence at different visual acuities, taking into account differences in mortality rates between those with cataract compared with those without. The model included development of opacity in the first eye and second eye of people older than 50 years. The incidence data were used to calculate target cataract surgical rates.\n    \n\n\n          Results:\n        \n      \n      Incidence and CSR needs varied significantly in different sites and were lower in some than expected. Cataract surgical rates may depend on genetic, environmental, or cultural variations and will vary with population structure, which is not uniform across Africa.\n    \n\n\n          Conclusion:\n        \n      \n      Africa should not be viewed as homogeneous in terms of cataract incidence or CSR needed. These CSR calculations should be useful for more appropriate planning of human resources and service delivery on the continent. The methodology can be applied to other population-based data as they become available to determine appropriate CSR targets."
        },
        {
            "title": "Non-small cell lung cancer: detection of early response to chemotherapy by using contrast-enhanced dynamic and diffusion-weighted MR imaging.",
            "abstract": "Purpose:\n        \n      \n      To evaluate the ability of dynamic contrast material-enhanced and diffusion-weighted (DW) magnetic resonance (MR) imaging to help detect early response to chemotherapy in patients with non-small cell lung cancer (NSCLC).\n    \n\n\n          Materials and methods:\n        \n      \n      This study was approved by the institutional review board, and written informed consent was obtained from all subjects. Twenty-eight patients with stage IIIB or IV NSCLC (17 women, 11 men; mean age, 64.8 years) who underwent chemotherapy were enrolled. All patients underwent MR imaging before and after the first course of chemotherapy. The time to peak enhancement, maximum enhancement ratio, and washout ratio were determined from the time-signal intensity curves of dynamic contrast-enhanced MR images. The apparent diffusion coefficient (ADC) of each lung carcinoma was calculated from DW MR images. The responses of these parameters to the first course of chemotherapy and the pretreatment ADC itself were compared with final tumor size reduction by using the Pearson correlation coefficient. Kaplan-Meier curves of progression-free survival and overall survival were generated, and comparisons between the group with a good response of the significant parameter (upper 50th percentile) and that with a poor response of the significant parameter (lower 50th percentile) were performed by using a two-sided log-rank test.\n    \n\n\n          Results:\n        \n      \n      Significant correlation was found only between early ADC change and final tumor size reduction rate (r(2) = 0.41, P = .00025). The median progression-free survival for the group with a good increase in ADC was 12.1 months, and that for the group with a stable or decreased ADC was 6.67 months (P = .021), while median overall survival was 22.4 and 12.3 months, respectively (P = .048).\n    \n\n\n          Conclusion:\n        \n      \n      ADC seems to be a promising tool for monitoring the early response to or predicting prognosis after chemotherapy of NSCLC."
        },
        {
            "title": "Prediction of fluoroscopic angulations for transcatheter aortic valve implantation by CT angiography: influence on procedural parameters.",
            "abstract": "Aims:\n        \n      \n      Repeated angiograms to achieve an exactly orthogonal visualization of the aortic valve plane can substantially contribute to the total contrast amount required for transcatheter aortic valve implantation (TAVI). We investigated whether pre-procedural identification of an optimal fluoroscopic projection by cardiac computed tomography (CT) can significantly reduce the amount of a procedure-related contrast agent compared with angiographic determination of suitable angulations.\n    \n\n\n          Methods and results:\n        \n      \n      Eighty consecutive patients (81 ± 5 years, 55% male) with symptomatic severe aortic valve stenosis and normal renal function who underwent cardiac CT prior to TAVI were prospectively randomized. In 40 patients, a CT-predicted suitable angulation was used for the first aortic angiogram (CT cohort); in the other 40 patients, the first aortogram was acquired at LAO 10°/cranial 10 (angiography cohort). Additional aortograms were performed if no satisfactory view of the aortic valve plane was obtained. The number of aortograms needed to achieve a satisfactory fluoroscopic projection (1.2 ± 0.6 vs. 3.2 ± 1.7; P < 0.001) and the total amount of contrast agent per TAVI procedure were significantly lower in the CT cohort (95 ± 21 vs. 125 ± 36 mL; P < 0.001). Incidence of acute kidney injury was not significantly different. There was no significant difference regarding radiation dose, time of procedure, degree of post-procedural aortic regurgitation, complications and 30-day mortality between the cohorts.\n    \n\n\n          Conclusion:\n        \n      \n      Pre-procedural identification of a suitable fluoroscopic projection by cardiac CT significantly reduces a procedural contrast agent volume required for TAVI."
        },
        {
            "title": "Five-year incidence and progression of diabetic retinopathy in a defined older population: the Blue Mountains Eye Study.",
            "abstract": "Aims:\n        \n      \n      To determine 5-year incidence and progression of diabetic retinopathy in an older Australian population-based cohort.\n    \n\n\n          Methods:\n        \n      \n      During the period 1992-1994, the Blue Mountains Eye Study examined 3654 residents aged 49+years (82.4% of those eligible), living in two urban postcode areas, west of Sydney, Australia. Participants were subsequently invited to attend 5-year follow-up exams. After excluding 543 (14.8%) who died during the follow-up period, 2334 persons (75.0%) were re-examined during 1997-1999. The examination included a comprehensive questionnaire, blood pressure measurement, standardised refraction, Zeiss stereo retinal photographs, and estimation of fasting blood glucose. Diabetic retinopathy was graded from the retinal photographs, using the modified Early Treatment Diabetic Retinopathy Scale classification (15-step scale).\n    \n\n\n          Results:\n        \n      \n      Of participants with diabetes diagnosed at baseline, 150 were re-examined, including 139 with gradable fundus photographs. The cumulative 5-year incidence of diabetic retinopathy was 22.2% before 95% confidence interval (CI) 14.1-32.2%. Retinopathy progression (1+ steps) was documented in 25.9% (95% CI 18.8-34.0%) of participants with retinopathy and gradable photographs at both visits; in 58.3% of these cases, a 2+ -step progression was documented. Progression to proliferative retinopathy occurred in only 4.1% of those with retinopathy at baseline. The only baseline risk factors associated with retinopathy progression, after adjusting for age and gender, were increase in fasting blood glucose, odds ratio (OR) 1.2 (95% CI 1.1-1.4)/mmol/l, and increase in diabetes duration, OR 2.3 (95% CI 1.0-5.3)/10 years.\n    \n\n\n          Conclusions:\n        \n      \n      These data provide 5-year cumulative incidence of diabetic retinopathy in a defined older population. Increase in diabetes duration and elevated baseline fasting blood glucose level predicted retinopathy incidence."
        },
        {
            "title": "Correlation between retinal vein occlusion and cancer - a nationwide Danish cohort study.",
            "abstract": "Purpose:\n        \n      \n      To explore the association between retinal vein occlusion (RVO) and incident cancer.\n    \n\n\n          Methods:\n        \n      \n      All Danish citizens with a first-time diagnosis of RVO and no previous diagnosis of cancer in the period from 1 January 2004 to 31 December 2014 were included. Five likewise cancer-free, age- and gender-matched controls were included in a control cohort. All were followed up for 5 years or until either first diagnosis of cancer or death. Proportional hazards models with adjustment for age, gender, year of diagnosis and covariates and death as competing risk were used to estimate the risk of being diagnosed with cancer.\n    \n\n\n          Results:\n        \n      \n      There were 7963 RVO patients without cancer at the time of diagnosis, and all could be matched to likewise cancer-free controls. Half of RVO patients were male, and the median age at RVO diagnosis was 70 years (61-79). The control cohort was similar in terms of gender and age. The risk of cancer within 1 year was 1.8 among RVO patients and 1.5 among controls. The crude risk of cancer was 1.22 (1.11;1.34) and upon full adjustment 1.15 (1.05;1.27). No time dependency was detected, and the types of cancer developed in RVO patients and controls were similar.\n    \n\n\n          Conclusion:\n        \n      \n      Retinal vein occlusion (RVO) diagnosis is associated with an increased risk of being diagnosed with cancer. This risk is likely to reflect shared risk factors rather than a causal association."
        },
        {
            "title": "Increased plasma concentrations of soluble tumor necrosis factor receptors in sepsis syndrome: correlation with plasma creatinine values.",
            "abstract": "Objectives:\n        \n      \n      Tumor necrosis factor (TNF) is an important mediator in the complex pathophysiology of sepsis syndrome. Although a positive correlation with mortality rate has been demonstrated, TNF has not been found consistently in sepsis. Since prolonged increases in soluble TNF receptor concentrations were demonstrated after endotoxin and TNF administration, we investigated whether the measurement of TNF receptor concentrations could provide a better indicator of disease than plasma TNF and interleukin (IL)-6 concentrations.\n    \n\n\n          Design:\n        \n      \n      Prospective analysis.\n    \n\n\n          Setting:\n        \n      \n      General intensive care unit (ICU) of a university hospital.\n    \n\n\n          Patients:\n        \n      \n      Twenty-six patients with sepsis syndrome and proven bacteremia.\n    \n\n\n          Measurements and main results:\n        \n      \n      Plasma peak concentrations of the soluble 55-kilodalton molecular weight TNF receptor were significantly higher (p < .005) in nonsurvivors compared with survivors of sepsis syndrome, whereas the difference in peak concentrations of the soluble 75-kilodalton TNF receptor did not reach significance (p = .06). In contrast to TNF peak concentrations (p = .14), significantly higher (p < .05) IL-6 peak concentrations were measured in nonsurvivors. Besides the positive correlation between the soluble 55-kilodalton TNF receptor and the soluble 75-kilodalton TNF receptor (r2 = .68; p < .0001), peak concentrations of both soluble 55-kilodalton TNF receptor and 75-kilodalton TNF receptor correlated significantly with plasma creatinine values, an indicator of renal function (r2 = .60; p < .0001 and r2 = .44; p < .001, respectively). Plasma creatinine concentrations were significantly higher in nonsurvivors (p < .001).\n    \n\n\n          Conclusions:\n        \n      \n      In the population studied, plasma-soluble TNF receptor concentrations correlated with outcome as well as with plasma creatinine concentrations. The data presented suggest that increased plasma-soluble TNF receptor concentrations in patients with sepsis syndrome are merely the result of renal failure complicating sepsis, and are similarly correlated with mortality rate."
        },
        {
            "title": "MRI of avascular necrosis of bone.",
            "abstract": "Avascular necrosis (AVN) is characterized by death of both trabecular bone as well as bone marrow elements. Weight-bearing bone becomes mechanically weakened and may eventually collapse, secondarily leading to osteoarthritis and debilitating pain. Early diagnosis and treatment of this entity are crucial because it affects relatively young individuals, and treatment options for advanced disease are limited. Magnetic resonance imaging (MRI) has emerged as the modality of choice for the evaluation of avascular necrosis of bone. We will discuss applications of MRI for early diagnosis, for monitoring therapy, and for its potential role in assessing individuals at risk of AVN. Although bone scintigraphy using single photon emission computed tomography (CT) may be nearly as accurate as MRI, MRI offers a more specific diagnosis in the patient who presents with hip pain of uncertain etiology. In addition, lesion size and location can be more easily assessed on magnetic resonance images, and this has been shown to relate to prognosis and need for treatment. We will review the pathophysiologic mechanisms of AVN and the current use of MRI in the diagnosis of this condition."
        },
        {
            "title": "Occupational class and ischemic heart disease mortality in the United States and 11 European countries.",
            "abstract": "Objectives:\n        \n      \n      Twelve countries were compared with respect to occupational class differences in ischemic heart disease mortality in order to identify factors that are associated with smaller or larger mortality differences.\n    \n\n\n          Methods:\n        \n      \n      Data on mortality by occupational class among men aged 30 to 64 years were obtained from national longitudinal or cross-sectional studies for the 1980s. A common occupational class scheme was applied to most countries. Potential effects of the main data problems were evaluated quantitatively.\n    \n\n\n          Results:\n        \n      \n      A north-south contrast existed within Europe. In England and Wales, Ireland, and Nordic countries, manual classes had higher mortality rates than nonmanual classes. In France, Switzerland, and Mediterranean countries, manual classes had mortality rates as low as, or lower than, those among nonmanual classes. Compared with Northern Europe, mortality differences in the United States were smaller (among men aged 30-44 years) or about as large (among men aged 45-64 years).\n    \n\n\n          Conclusions:\n        \n      \n      The results underline the highly variable nature of socioeconomic inequalities in ischemic heart disease mortality. These inequalities appear to be highly sensitive to social gradients in behavioral risk factors. These risk factor gradients are determined by cultural as well as socioeconomic developments."
        },
        {
            "title": "PET imaging may provide a novel biomarker and understanding of right ventricular dysfunction in patients with idiopathic pulmonary arterial hypertension.",
            "abstract": "Background:\n        \n      \n      The clinical course in pulmonary arterial hypertension (PAH) is variable, and there is limited information on the determinants and progression of right ventricular (RV) dysfunction. The objective is to develop PET metabolic imaging of the RV as a noninvasive tool in patients with PAH.\n    \n\n\n          Methods and results:\n        \n      \n      We performed PET scanning in 16 patients with idiopathic PAH (age, 41±14 years, 82% women) using (13)N-NH(3) for perfusion imaging and (18)F-fluorodeoxyglucose for metabolic imaging. The myocardium was divided into 6 regions of interest (3 left ventricular [LV], 3 RV), and time-activity curves were generated. A 2- compartment model was used to calculate myocardial blood flow (MBF), and Patlak analysis was used to calculate the rate of myocardial glucose uptake (MGU). All patients underwent cardiac catheterization, cardiac MRI, and cardiopulmonary exercise testing with gas exchange. MBF, MGU, and the ratio of RV/LV MGU were correlated to clinical parameters. Pulmonary artery (PA) pressure was 79±19/30±8 mm Hg (mean, 48±10 mm Hg). MBF was 0.84±0.33 mL/g per minute for the LV and 0.45±0.14 mL/g per minute for the RV. Mean MGU was 136±72 nmol/g per minute for the LV and 96±69 nmol/g per minute for the RV. The ratio of RV/LV MGU correlated significantly with PA systolic (r=0.75, P=0.0085) and mean (r=0.87, P=0.001) pressure and marginally with maximum oxygen consumption (r=-0.59, P=0.05). RV free wall MGU also correlated well with mean PA pressure (r=0.66, P=0.03).\n    \n\n\n          Conclusions:\n        \n      \n      PET scanning with (13)N-NH(3) and (18)F-fluorodeoxyglucose is a feasible modality for quantifying RV blood flow and metabolism in patients with idiopathic PAH."
        },
        {
            "title": "[A gender perspective analysis of perception and practices in coronary disease in women from northern Mexico].",
            "abstract": "Objective:\n        \n      \n      To analyze the perception of coronary risk and health care practices in a group of Mexican women, from a gendered perspective.\n    \n\n\n          Materials and methods:\n        \n      \n      Mixed methods: survey of 140 women; nine in-depth interviews to women with coronary disease; eight semi-structured interviews to physicians.\n    \n\n\n          Analysis:\n        \n      \n      proportions contrast for quantitative data; and procedures of grounded theory for qualitative information.\n    \n\n\n          Results:\n        \n      \n      More than 50% of women don't know their coronary risk and how to reduce it. Despite having information about heart disease, vulnerable women with chest pain sought medical attendance less than non-vulnerable women (p=0.0l); and are blamed by physicians. Women consider they lack sufficient information about how to reduce the risk of coronary disease, and blame themselves when ill.\n    \n\n\n          Conclusions:\n        \n      \n      There are vulnerability conditions in women that modulate a low perception of their being at risk, and the scarcity of health care practices."
        },
        {
            "title": "Modified criteria for the systemic inflammatory response syndrome improves their utility following cardiac surgery.",
            "abstract": "Background:\n        \n      \n      Debate remains regarding whether the systemic inflammatory response syndrome (SIRS) identifies patients with clinically important inflammation. Defining criteria may be disproportionately sensitive and lack specificity. We investigated the incidence and evolution of SIRS in a homogenous population (following cardiac surgery) over 7 days to establish the relationship between SIRS and outcome, modeling alternative permutations of the criteria to increase their discriminatory power for mortality, length of stay, and organ dysfunction.\n    \n\n\n          Methods:\n        \n      \n      We conducted a retrospective analysis of prospectively collected data from a cardiothoracic ICU. Consecutive patients requiring ICU admission for the first time after cardiac surgery (N = 2,764) admitted over a 41-month period were studied.\n    \n\n\n          Results:\n        \n      \n      Concurrently, 96.2% of patients met the standard two criterion definition for SIRS within 24 h of ICU admission. Their mortality was 2.78%. By contrast, three or four criteria were more discriminatory of patients with higher mortality (4.21% and 10.2%, respectively). A test dataset suggested that meeting two criteria for at least 6 consecutive h may be the best model. This had a positive and negative predictive value of 7% and 99.5%, respectively, in a validation dataset. It performed well at predicting organ dysfunction and prolonged ICU admission.\n    \n\n\n          Conclusions:\n        \n      \n      The concept of SIRS remains valid following cardiac surgery. With suitable modification, its specificity can be improved significantly. We propose that meeting two or more defining criteria for 6 h could be used to define better populations with more difficult clinical courses following cardiac surgery. This group may merit a different clinical approach."
        },
        {
            "title": "Gender-specific associations of vision and hearing impairments with adverse health outcomes in older Japanese: a population-based cohort study.",
            "abstract": "Background:\n        \n      \n      Several epidemiological studies have shown that self-reported vision and hearing impairments are associated with adverse health outcomes (AHOs) in older populations; however, few studies have used objective sensory measurements or investigated the role of gender in this association. Therefore, we examined the association of vision and hearing impairments (as measured by objective methods) with AHOs (dependence in activities of daily living or death), and whether this association differed by gender.\n    \n\n\n          Methods:\n        \n      \n      From 2005 to 2006, a total of 801 residents (337 men and 464 women) aged 65 years or older of Kurabuchi Town, Gunma, Japan, participated in a baseline examination that included vision and hearing assessments; they were followed up through September 2008. Vision impairment was defined as a corrected visual acuity of worse than 0.5 (logMAR = 0.3) in the better eye, and hearing impairment was defined as a failure to hear a 30 dB hearing level signal at 1 kHz in the better ear. Information on outcomes was obtained from the town hall and through face-to-face home visit interviews. We calculated the risk ratios (RRs) of AHOs for vision and hearing impairments according to gender.\n    \n\n\n          Results:\n        \n      \n      During a mean follow-up period of 3 years, 34 men (10.1%) and 52 women (11.3%) had AHOs. In both genders, vision impairment was related to an elevated risk of AHOs (multi-adjusted RR for men and women together = 1.60, 95% CI = 1.05-2.44), with no statistically significant interaction between the genders. In contrast, a significant association between hearing impairment and AHOs (multi-adjusted RR = 3.10, 95% CI = 1.43-6.72) was found only in the men.\n    \n\n\n          Conclusion:\n        \n      \n      In this older Japanese population, sensory impairments were clearly associated with AHOs, and the association appeared to vary according to gender. Gender-specific associations between sensory impairments and AHOs warrant further investigation."
        },
        {
            "title": "The usefulness and costs of routine contrast studies after laparoscopic sleeve gastrectomy for detecting staple line leaks.",
            "abstract": "Background:\n        \n      \n      Although laparoscopic sleeve gastrectomy (LSG) has been shown to be a safe and effective treatment for severe obesity (body mass index ≥ 35), staple line leaks remain a major complication and account for a substantial portion of the procedure's morbidity and mortality. Many centres performing LSG routinely obtain contrast studies on postoperative day 1 for early detection of staple line leaks. We examined the usefulness of Gastrografin swallow as an early detection test for staple line leaks on postoperative day 1 after LSG as well as the associated costs.\n    \n\n\n          Methods:\n        \n      \n      We conducted a retrospective review of a prospectively collected database that included 200 patients who underwent LSG for severe obesity between 2011 and 2014. Primary outcome measures were the incidence of staple line leaks and the results of Gastrografin swallow tests. We obtained imaging costs from appropriate hospital departments.\n    \n\n\n          Results:\n        \n      \n      Gastrografin swallow was obtained on postoperative day 1 for all 200 patients who underwent LSG. Three patients (1.5%) were found to have staple line leaks. Gastrograffin swallows yielded 1 true positive result and 2 false negatives. The false negatives were subsequently diagnosed on computed tomography (CT) scan. The sensitivity of Gastrografin swallow in this study was 33%. For 200 patients, the total direct cost of the Gastrografin swallows was $35 000.\n    \n\n\n          Conclusion:\n        \n      \n      The use of routine upper gastrointestinal contrast studies for early detection of staple line leaks has low sensitivity and is costly. We recommend selective use of CT instead."
        },
        {
            "title": "Safe limits of contrast vary with hydration volume for prevention of contrast-induced nephropathy after coronary angiography among patients with a relatively low risk of contrast-induced nephropathy.",
            "abstract": "Background:\n        \n      \n      Few studies have investigated the safe limits of contrast to prevent contrast-induced nephropathy (CIN) based on hydration data. We aimed to investigate the relative safe maximum contrast volume adjusted for hydration volume in a population with a relatively low risk of CIN.\n    \n\n\n          Methods and results:\n        \n      \n      The ratios of contrast volume-to-creatinine clearance (V/CrCl) and hydration volume to body weight (HV/W) were determined in patients undergoing cardiac catheterization. Receiver-operator characteristic curve analysis based on the maximum Youden index was used to identify the optimal cutoff for V/CrCl in all patients and in HV/W subgroups. Eighty-six of 3273 (2.6%) patients with mean CrCl 71.89±27.02 mL/min developed CIN. Receiver-operator characteristic curve analysis indicated that a V/CrCl ratio of 2.44 was a fair discriminator for CIN in all patients (sensitivity, 73.3%; specificity, 70.4%). After adjustment for other confounders, V/CrCl >2.44 continued to be significantly associated with CIN (adjusted odds ratio, 4.12; P<0.001) and the risk of death (adjusted hazard ratio, 2.62; P<0.001). The mean HV/W was 12.18±7.40. We divided the patients into 2 groups (HV/W ≤12 and >12 mL/kg). The best cutoff value for V/CrCl was 1.87 (sensitivity, 67.9%; specificity, 64.4%; adjusted odds ratio, 3.24; P=0.011) in the insufficient hydration subgroup (HV/W, ≤12 mL/kg; CIN, 1.32%) and 2.93 (sensitivity, 69.0%; specificity, 65.0%; adjusted odds ratio, 3.04; P=0.004) in the sufficient hydration subgroup (HV/W, >12 mL/kg; CIN, 5.00%).\n    \n\n\n          Conclusions:\n        \n      \n      The V/CrCl ratio adjusted for HV/W may be a more reliable predictor of CIN and even long-term outcomes after cardiac catheterization. We also found a higher best cutoff value for V/CrCl to predict CIN in patients with a relatively sufficient hydration status, which may be beneficial during decision-making about contrast dose limits in relatively low-risk patients with different hydration statuses."
        },
        {
            "title": "Association of Dietary Magnesium Intake with Fatal Coronary Heart Disease and Sudden Cardiac Death.",
            "abstract": "Background: Postmenopausal women represent the highest population-based burden of cardiovascular disease, including sudden cardiac death (SCD). Our understanding of the etiology and risk factors contributing to fatal coronary heart disease (CHD) and SCD, particularly among women, is limited. This study examines the association between dietary magnesium intake and fatal CHD and SCD. Materials and Methods: We examined 153,569 postmenopausal women who participated in the Women's Health Initiative recruited between 1993 and 1998. Magnesium intake at baseline was assessed using a validated food frequency questionnaire, adjusting for energy via the residual method. Fatal CHD and SCD were identified over an average follow-up of 10.5 years. Results: For every standard deviation increase in magnesium intake, there was statistically significant risk reduction, after adjustment for confounders, of 7% for fatal CHD (hazard ratio [HR] 0.93, 95% confidence interval [CI] 0.89-0.97), and 18% risk reduction for SCD (HR 0.82, 95% CI 0.58-1.15) the latter of which did not reach statistical significance. In age-adjusted quartile analysis, women with the lowest magnesium intake (189 mg/day) had the greatest risk for fatal CHD (HR 1.54, 95% CI 1.40-1.69) and SCD (HR 1.70, 95% CI 0.94-3.07). This association was attenuated in the fully adjusted model, with HRs of 1.19 (95% CI 1.06-1.34) for CHD and 1.24 (95% CI 0.58-2.65) for SCD for the lowest quartile of magnesium intake. Conclusions: This study provides evidence of a potential inverse association between dietary magnesium and fatal CHD and a trend of magnesium with SCD in postmenopausal women. Future studies should confirm this association and consider clinical trials to test whether magnesium supplementation could reduce fatal CHD in high-risk individuals."
        },
        {
            "title": "Electrocardiographic ST-segment elevation myocardial infarction in critically ill patients: an observational cohort analysis.",
            "abstract": "Objective:\n        \n      \n      To investigate the specificity of the electrocardiographic diagnosis of ST-segment elevation myocardial infarction in the critical care unit setting.\n    \n\n\n          Design:\n        \n      \n      Retrospective observational cohort analysis.\n    \n\n\n          Setting:\n        \n      \n      An 880-bed tertiary care teaching hospital with 120 intensive care unit beds.\n    \n\n\n          Patients:\n        \n      \n      The population included medical, surgical, trauma, and neurosurgical intensive care unit patients.\n    \n\n\n          Interventions:\n        \n      \n      Electrocardiograms were systematically collected to include all consecutive recordings over a 15-month period in which the interpretation software indicated ***ACUTE MI***. Patient demographics, markers of intensive care unit complexity, and hospital mortality were ascertained. The electrocardiograms were then further evaluated by a blinded, board-certified cardiologist for agreement or disagreement with the interpretation software. Serum troponin measurements obtained within 96 hrs of electrocardiogram acquisition were used to determine the likelihood of myocardial infarction.\n    \n\n\n          Measurements and main results:\n        \n      \n      Over the 15-month study period, the interpretation software diagnosed ST-segment elevation myocardial infarction in 67 of 2243 intensive care unit patients (2.99%) who had an electrocardiogram performed. In the final study population of 46 cases with electrocardiographic ST-segment elevation myocardial infarction, 85% had peak troponin elevation<5 ng/mL, a strong suggestion against clinical ST-segment elevation myocardial infarction. The cardiologist agreed with the computer interpretation in 39% (18 of 46) of cases, but of those 18 patients, only six showed a significant rise in the troponin level. The cardiologist disagreed with the computer interpretation in 60.9% (28 of 46) of cases and of those, one patient had a marked elevation of the cardiac troponin.\n    \n\n\n          Conclusions:\n        \n      \n      ST-segment elevation myocardial infarction in the intensive care unit is a relatively common electrocardiographic reading both by standard interpretation software and by expert evaluation. In contrast to nonintensive care unit patients who present with chest pain, the electrocardiographic ST-segment elevation myocardial infarction diagnosis seems to be a nonspecific finding in the intensive care unit that is frequently the result of a variety of nonischemic processes. The vast majority of such patients do not have frank ST-segment elevation myocardial infarction."
        },
        {
            "title": "Visual prognosis of AIDS patients with cytomegalovirus retinitis.",
            "abstract": "A prospective study of visual acuity (VA) was performed in a cohort of 147 AIDS patients with cytomegalovirus retinitis (CMVR). Patients were treated according to standard regimes, and corrected VA was recorded at regular intervals from presentation until death. Follow-up was 6 weeks to 5 years (mean 30 weeks). Fifty patients (34%) had bilateral CMVR at initial presentation; at death 81 patients (55%) had bilateral disease. Thirty-one eyes initially uninfected developed CMVR during follow-up. Of 228 infected eyes, VA at presentation was 6/12 or better in 182 eyes (80%) and 6/60 or better in 215 eyes (94%) At death, VA was 6/12 or better in 112 eyes (49%) and 6/60 or better in 171 eyes (75%). VA in the better eye at death was 6/12 or better in 113 of 147 patients (77%), 6/24 or better in 135 patients (92%) and worse than 6/60 in only 7 patients (5%). Treatment of AIDS-related CMVR minimises loss of vision and may protect previously uninfected eyes, prolonging visual independence."
        },
        {
            "title": "Impact of Fluorine-18 2-Fluoro-2-Deoxy-D-Glucose Uptake on Preoperative Positron Emission Tomography/Computed Tomography in the Lymph Nodes of Patients with Primary Colorectal Cancer.",
            "abstract": "Background/aims:\n        \n      \n      Although the diagnostic value of fluorine-18 2-fluoro-2-deoxy-D-glucose (F-18-FDG) positron emission tomography/computed tomography (F-18-FDG-PET/CT) in patients with colorectal cancer (CRC) has been reported, the association between the F-18-FDG uptake in metastatic lymph nodes (FDGLN) and clinicopathological variables has not been fully investigated. We evaluated the diagnostic value of F-18-FDG-PET/CT in detecting LN metastasis from CRC, and the relationship between F-18-FDG-PET/CT-detecting LN metastasis and prognosis.\n    \n\n\n          Methods:\n        \n      \n      We retrospectively analyzed the medical records of 370 patients who underwent preoperative F-18-FDG-PET/CT, followed by surgical resection for CRC between January 2007 and December 2010. We analyzed the sensitivity, specificity, and accuracy of F-18-FDG-PET/CT and CT in diagnosing metastatic LNs. Survival was analyzed in 115 patients with stage III CRC.\n    \n\n\n          Results:\n        \n      \n      The sensitivity, specificity, and accuracy for detecting metastatic LNs using F-18-FDG-PET/CT were 56.8, 90.3, and 74.2%, and those for contrast-enhanced CT were 38.4, 95.5, and 65.0%, respectively. The accuracy of F-18-FDG-PET/CT was significantly associated with tumor depth and lymphatic involvement. In the survival analysis, cancer-specific survival and the disease-free survival were significantly shorter in patients with stage III CRC with FDGLN than in those without FDGLN.\n    \n\n\n          Conclusion:\n        \n      \n      F-18-FDG-PET/CT had low sensitivity and high specificity for detecting metastatic LNs from CRC. FDGLN independently predicted poor prognosis in patients with stage III CRC."
        },
        {
            "title": "Cytomegalovirus retinitis: diagnosis and treatment.",
            "abstract": "Cytomegalovirus (CMV) is a non-pathogenic organism in the immunocompetent, but is a major cause of morbidity and mortality amongst patients with AIDS, and the retina is the commonest site of infection. If left untreated, patients with CMVR will develop disease in their second eye and ultimately become blind. However, with correct diagnosis and treatment useful vision can be maintained in the majority of cases. Fifteen to 20% of patients with AIDS will contract cytomegalovirus retinitis (CMVR) and this may be the AIDS-defining diagnosis though more commonly it occurs months after the diagnosis of AIDS. Given the increasing number of HIV positive patients and their longer survival, it is likely that CMVR will become an increasingly prevalent condition. In these patients loss of sight from CMVR has devastating consequences in terms of loss of independence and quality of life and therefore ophthalmologists and physicians should be aware of the presenting characteristics of CMVR, be familiar with therapy and its complications, and be able to recognize relapsing infection."
        },
        {
            "title": "Ocular adnexal lymphoma staging and treatment: American Joint Committee on Cancer versus Ann Arbor.",
            "abstract": "Purpose:\n        \n      \n      To evaluate the prognostic utility of the American Joint Committee on Cancer (AJCC) staging system for ocular adnexal lymphoma (OAL). \n    \n\n\n          Methods:\n        \n      \n      A multicenter, consecutive case series of patients with biopsy-proven conjunctival, orbit, eyelid, or lacrimal gland/sac lymphoma was performed. The electronic pathology and clinical records were reviewed for new or recurrent cases of ocular adnexal lymphoma. The main outcome measures included pathology and clinical staging (AJCC and Ann Arbor systems), treatment, and recurrence (local and systemic). Statistical analysis included demographic evaluations and the Kaplan-Meier survival probability method. \n    \n\n\n          Results:\n        \n      \n      Extranodal marginal zone B-cell lymphoma of mucosa-associated lymphoid tissue were the most common (n=60/83, 72%). The most common Ann Arbor clinical stages were IE (76%) followed by IIE (17%) and IIIE (7%). Pathology identified 13 cases (15%) that were upstaged to group IV (p=0.017). Similarly, AJCC clinical stages were cT1NOMO (21.7%), cT2NOMO (44.6%), cT3N0M0 (5%), and cT4NOMO (2.4%). Local control was achieved in 75% of treated patients. There were 19 local recurrences from which 14 (74%) belonged to the non-radiation treatment groups. Lower-risk groups (T1 and T2 without lymph node involvement or metastatic disease of AJCC and IE of Ann Arbor) had longer disease-free survival than the higher-risk groups (AJCC T1, T2 with nodal involvement or metastatic disease, T3, and T4 as well as Ann Arbor II, III, and IV). The overall mean follow-up was 43.3 months (range 6-274). \n    \n\n\n          Conclusions:\n        \n      \n      Regardless of stage, recurrence and disease-free survival were more closely related to treatment and histopathology rather than tumor size or site-specific location."
        },
        {
            "title": "Objective Sepsis Surveillance Using Electronic Clinical Data.",
            "abstract": "Objective:\n        \n      \n      To compare the accuracy of surveillance of severe sepsis using electronic health record clinical data vs claims and to compare incidence and mortality trends using both methods.\n    \n\n\n          Design:\n        \n      \n      We created an electronic health record-based surveillance definition for severe sepsis using clinical indicators of infection (blood culture and antibiotic orders) and concurrent organ dysfunction (vasopressors, mechanical ventilation, and/or abnormal laboratory values). We reviewed 1,000 randomly selected medical charts to characterize the definition's accuracy and stability over time compared with a claims-based definition requiring infection and organ dysfunction codes. We compared incidence and mortality trends from 2003-2012 using both methods.\n    \n\n\n          Setting:\n        \n      \n      Two US academic hospitals.\n    \n\n\n          Patients:\n        \n      \n      Adult inpatients.\n    \n\n\n          Results:\n        \n      \n      The electronic health record-based clinical surveillance definition had stable and high sensitivity over time (77% in 2003-2009 vs 80% in 2012, P=.58) whereas the sensitivity of claims increased (52% in 2003-2009 vs 67% in 2012, P=.02). Positive predictive values for claims and clinical surveillance definitions were comparable (55% vs 53%, P=.65) and stable over time. From 2003 to 2012, severe sepsis incidence imputed from claims rose by 72% (95% CI, 57%-88%) and absolute mortality declined by 5.4% (95% CI, 4.6%-6.7%). In contrast, incidence using the clinical surveillance definition increased by 7.7% (95% CI, -1.1% to 17%) and mortality declined by 1.7% (95% CI, 1.1%-2.3%).\n    \n\n\n          Conclusions:\n        \n      \n      Sepsis surveillance using clinical data is more sensitive and more stable over time compared with claims and can be done electronically. This may enable more reliable estimates of sepsis burden and trends."
        },
        {
            "title": "Cataract-Related Visual Impairment Corrected by Cataract Surgery and 10-Year Mortality: The Liwan Eye Study.",
            "abstract": "Purpose:\n        \n      \n      To assess 10-year mortality in people who had undergone cataract surgery with no residual visual impairment (VI) and those who had persistent VI due to cataract using a population-based cohort.\n    \n\n\n          Methods:\n        \n      \n      The Liwan Eye Study is a 10-year longitudinal study commenced in 2003. According to the World Health Organization, presenting VI was defined as visual acuity less than 20/63 in the better-seeing eye. History of cataract surgery was defined as cataract surgery performed on either eye. Information on the date of surgery was recorded. Dates of death occurring between baseline and April 30, 2014 were obtained from the National Death Index data. Information on socioeconomic factors was obtained from questionnaire interviews. Cox proportional hazards regression models were used to assess the hazard ratios (HRs) and 95% confidence intervals (CIs).\n    \n\n\n          Results:\n        \n      \n      Fifty-nine participants had undergone cataract surgery without residual VI and 67 participants had persistent cataract-related VI. The 10-year mortality rate for participants who had undergone cataract surgery without residual VI was statistically significant lower than that in participants who had VI due to cataract based on log-rank test (32.2% vs. 64.2%; P = 0.002). This finding remained significant in the unadjusted Cox proportional hazards model (HR, 0.43; 95% CI, 0.25-0.74; P = 0.002). After adjusting for age, sex, history of diabetes, and hypertension, body mass index (BMI), education level, and personal income, participants with cataract surgery and no residual VI did not have a higher chance of survival than participants with persistent VI due to cataract (HR, 0.56; 95% CI, 0.26-1.20; P = 0.136).\n    \n\n\n          Conclusions:\n        \n      \n      Cataract-related VI corrected by cataract surgery was not associated with better survival after adjusting for a number of possible confounders. Given our sample size is relatively small and limited power, further studies with larger sample are needed."
        },
        {
            "title": "Otago glaucoma surgery outcome study: long-term results of trabeculectomy--1976 to 1995.",
            "abstract": "Objective:\n        \n      \n      To provide data on the long-term results of trabeculectomy performed in the province of Otago, New Zealand.\n    \n\n\n          Design:\n        \n      \n      Retrospective noncomparative case series.\n    \n\n\n          Participants:\n        \n      \n      A total of 289 eyes of 193 patients (excluding 4 eyes lost to follow-up soon after operation); all trabeculectomies performed for the first time on cases of primary glaucoma from 1976 through 1995.\n    \n\n\n          Intervention:\n        \n      \n      Standard Cairns trabeculectomy.\n    \n\n\n          Main outcome measures:\n        \n      \n      Intraocular pressure, visual acuity, visual field damage.\n    \n\n\n          Results:\n        \n      \n      Trabeculectomy was effective in controlling intraocular pressure at a level of 21 mmHg or less, with probabilities of 0.93 (95% confidence interval [CI], 0.90-0.97), 0.87 (95% CI, 0.82-0.93), and 0.85 (95% CI, 0.77-0.92) at 5, 10, and 15 years, respectively, after surgery. The mean visual acuity improved from 20/60 to 20/40 immediately after trabeculectomy but then declined steadily over the postoperative years. The decline in visual acuity led to blindness in 47 eyes. The Kaplan-Meier estimated probability of retaining useful vision (visual acuity > 20/400 and visual field > 5 degrees radius) in the overall group was 0.87 (95% CI, 0.79-0.91), 0.72 (95% CI, 0.60-0.79), and 0.6 (95% CI, 0.43-0.69) at 5, 10, and 15 years, respectively, after surgery. Those eyes that had good preoperative visual acuity (visual acuity > or = 20/30) had a significantly better chance of retaining useful vision (P = 0.02).\n    \n\n\n          Conclusions:\n        \n      \n      The intraocular pressure was well controlled by trabeculectomy, but a steady long-term decline in visual acuity and visual field occurred, decreasing the probability of an eye retaining useful vision up to the time of death to approximately 0.6."
        },
        {
            "title": "[Long-term surgical results of initial trabeculotomy combined with sinusotomy performed inferiorly].",
            "abstract": "Purpose:\n        \n      \n      To evaluate retrospectively the long-term effects of initial trabeculotomy combined with sinusotomy performed inferiorly.\n    \n\n\n          Patients and method:\n        \n      \n      Enrolled were 128 eyes of 100 patients who received initial glaucoma surgery. In 36 eyes, the removal of Schlemm's canal endothelium was also performed (removed group). The results were compared with the intact group\n    \n\n\n          Results:\n        \n      \n      In the primary open angle glaucoma (POAG), mean intraocular pressure (IOP) at 3 years after surgery was 14.6 (intact) and 15.4 mmHg (removed). Kaplan-Meier life-table analysis showed that qualified success rates for the intact group at 8 years were 62.2% and for the removed group at 5 years 45.2% defined by 20 mmHg or lower. The results in developmental glaucoma (DG) were similar to those in POAG. No statistical differences in postoperative IOP between the intact and removed groups were seen in either POAG or DG. In exfoliation glaucoma (XFG), mean IOPs for the intact group at 3 years were 17.3 mmHg and for the removed group at 2 years 15.4 mmHg. The success rates for the intact group at 3.5 years were 25.2% and for the removed group at 4.5 years 64.3%. The results in the intact group were worse than in the POAG patients. Although visual disturbance was seen in 13% of the patients, the major cause was the progression of the cataracts.\n    \n\n\n          Conclusions:\n        \n      \n      The long-term results were the same as those of previous reports on surgery performed superiorly, including the frequency of visual disturbance. However the removal of Schlemm's canal endothelium is necessary in XFG for better IOP control."
        },
        {
            "title": "High-sensitivity C-reactive protein and cystatin C independently and jointly predict all-cause mortality among the middle-aged and elderly Chinese population.",
            "abstract": "Studies investigating the relationship between high-sensitivity C-reactive protein (hs-CRP), cystatin C, and all-cause mortality yielded inconsistent results. Moreover, the joint effect of hs-CRP and cystatin C on mortality risk is largely unknown for the general population. In this study, we examined the associations between hs-CRP, cystatin C, and all-cause mortality using data from the China Health and Retirement Longitudinal Study (CHARLS). Middle-aged and elderly participants with complete data were enrolled for a 4-year follow-up of total mortality and plasma levels of hs-CRP (n = 11,409) and cystatin C (n = 8680). In study population, the highest quartiles of hs-CRP and cystatin C were significantly associated with increased total mortality risk compared with the lowest quartile, and adjusted hazard ratios (95% confidence intervals) were 2.08 (1.49-2.91) and 1.97 (1.33-2.94) for hs-CRP and cystatin C, respectively. Remarkably, the adjusted hazard ratio (95% confidence interval) of the co-occurrence of elevated hs-CRP and increased cystatin C was 4.17 (2.94-5.92), in contrast to each elevation alone: 1.89 (1.45-2.47) for hs-CRP and 2.08 (1.46-2.97) for cystatin C. Moreover, a subgroup analysis by gender yielded similar associations. Lastly, the addition of hs-CRP and cystatin C to conventional factors significantly improved risk prediction of total mortality (net reclassification index 0.3622, P < 0.0001; integrated discrimination improvement 0.0354, P < 0.0001). Taken together, findings suggest that plasma hs-CRP and cystatin C serve as independent predictors of all-cause mortality among the middle-aged and elderly Chinese population. Furthermore, the combination of hs-CRP and cystatin C could predict overall mortality better than each component individually."
        },
        {
            "title": "Quantification of gadolinium in fresh skin and serum samples from patients with nephrogenic systemic fibrosis.",
            "abstract": "Background:\n        \n      \n      Nephrogenic systemic fibrosis (NSF) is a rare, potentially fatal fibrosing disorder associated with renal insufficiency and gadolinium (Gd)-based contrast exposure. The cause remains unknown. To date, all efforts to investigate skin Gd concentrations in patients with NSF have been performed on paraffin-embedded samples, and Gd deposition has not been correlated with disease activity by a statistically significant analysis.\n    \n\n\n          Objective:\n        \n      \n      We sought to: (1) quantify Gd concentration in fresh tissue skin biopsy specimens; (2) quantify and compare synchronous Gd concentration of affected skin and unaffected skin in patients with NSF (n = 13) with a control group (n = 13); and (3) quantify serum Gd.\n    \n\n\n          Methods:\n        \n      \n      We used inductively coupled plasma mass spectrometry.\n    \n\n\n          Results:\n        \n      \n      In patients with NSF, the mean ratio of paired Gd concentrations of affected skin to unaffected skin was 23.1, ranging from 1.2 to 88.9. Mean serum Gd concentrations in patients with NSF were 4.8 ng/mL, which is more than 10 times the level in control patients. A statistically significant correlation existed between serum and affected skin Gd concentrations (r(2) = .74, P < .0001).\n    \n\n\n          Limitations:\n        \n      \n      Because of the feasibility of this study, the main limitation was the small sample size (n = 13 affected and 13 control).\n    \n\n\n          Conclusions:\n        \n      \n      Determination of Gd concentrations in fresh skin samples and serum using inductively coupled plasma mass spectrometry demonstrates significant differences in the amounts of Gd in involved versus nonlesional skin of patients with NSF. This supports the role of differential free Gd deposition from Gd-based contrast in the pathogenesis of NSF."
        },
        {
            "title": "Population screening for colorectal cancer, the goals and means.",
            "abstract": "The causes of colorectal cancer are complex and in most cases obscure, making primary prevention impossible at present. Secondary prevention by finding and treating early asymptomatic cancers may possibly reduce mortality from this very common cancer. Results from conventional treatment have changed little during recent decades and are unsatisfactory, with more than half of the patients dying from the disease. The incidence has increased during recent years in many countries, making it vital to evaluate possible benefits from screening. This review considers different methods of screening for colorectal cancer and includes an overview of continuing European controlled randomised trials with the faecal occult blood test, Haemoccult-II. No final evaluation is possible, but advantages and drawbacks of different strategies are discussed. Assuming that the goal of reducing mortality is achieved, several other problems remain unsolved: the organisation of screening, the training of doctors in endoscopy, cost benefit and cost effectiveness all of which will have to be solved before a population screening can be recommended. Present screening tools are not ideal and we have to continue the search for better markers of early colorectal cancers and even possible precursors like adenomas."
        },
        {
            "title": "Clinical and angiographic factors predicting fractional flow reserve and explaining the visual-functional mismatch in patients with intermediate coronary artery stenosis.",
            "abstract": "Background:\n        \n      \n      Visual-functional mismatch between coronary angiography and fractional flow reserve (FFR) has been reported, and the underlying reason remains poorly understood. Therefore, the relationship between angiographic measurements and FFR was evaluated, and predictors for FFR in intermediate coronary artery stenosis were determined.\n    \n\n\n          Methods:\n        \n      \n      Consecutive 314 patients (405 lesions) with a lesion of 30-80% angiographic diameter stenosis who underwent invasive FFR were recruited. The myocardial area supplied by the coronary artery distal to the stenosis was evaluated using a modified version of the Bypass Angioplasty Revascularization Investigation (BARI) score. Participants underwent follow-up, and major cardiovascular events (MACE), including all-cause death, myocardial infarction (MI), and unplanned revascularization were recorded.\n    \n\n\n          Results:\n        \n      \n      Although % diameter stenosis was correlated with FFR (R = 0.279, P < 0.001), diameter stenosis-FFR mismatch was observed in 37.8% of the lesions. Although FFR values were not associated with clinical factors, such as age, sex, and comorbidities, it was correlated with minimal lumen diameter (MLD), diffuse lesion, presence of proximal lesion, and BARI score. In addition, the lesions in left anterior descending (LAD) coronary artery showed low FFR values compared with those in the left circumflex coronary artery or right coronary artery. In multivariate logistic analysis, MLD (β coefficient = 0.330), diffuse lesion (β coefficient = -0.266), proximal lesion (β coefficient = -0.144), BARI score (β coefficient = -0.219), and LAD lesion (β coefficient = -0.293) were all independent predictors for FFR value. The estimated FFR value based on these factors showed smaller mismatch and higher sensitivity. No difference was observed in the event rates for MACE and MI or revascularization between the FFR-guided and estimated FFR-guided strategies.\n    \n\n\n          Conclusions:\n        \n      \n      MLD, diffuse lesion, proximal lesion, BARI score, and lesion vessel were independent predictors for FFR in intermediate coronary stenosis. Not only the extent of local lesion stenosis but also the amount of myocardial supply and the lesion location may determine the physiological significance and explain the visual-functional mismatch. The estimation of FFR by these factors may be useful in clinical practice."
        },
        {
            "title": "Lens changes and survival in a population-based study.",
            "abstract": "The Framingham Heart Study was begun in 1948 to study factors associated with cardiovascular disease. Participants have been reexamined approximately every two years. From 1973 to 1975, the Framingham Eye Study examined the eyes of available Heart Study participants. We used information about nearly 2000 persons from these population-based studies to investigate the relation of lens changes to survival. Follow-up ranged from five to eight years; 312 persons (16 per cent) died. Proportional-hazards regression analyses indicated an overall association of lens changes and decreased survival (P = 0.01), but detailed investigation showed (1) no association of lens changes and decreased survival among persons without diabetes (P = 0.29) and (2) a significant association of lens changes and decreased survival among persons with diabetes (P = 0.001). Diabetic persons with lens changes had an estimated death rate more than twice that of diabetics without lens changes. The duration of diabetes and degree of retinopathy were not associated with survival in this group, most of whom had adult-onset diabetes of short duration. We conclude that lens changes are earlier predictors of death in diabetics than these more traditional variables."
        },
        {
            "title": "Long-term survival rates of patients undergoing vitrectomy for diabetic retinopathy in an Australian population: a population-based audit.",
            "abstract": "Importance:\n        \n      \n      Five-year survival rates in patients undergoing vitrectomy for diabetic retinopathy (DR) vary from 68% to 95%. No study has been conducted in an Australian population.\n    \n\n\n          Background:\n        \n      \n      We aimed to determine the survival rates of patients undergoing diabetic vitrectomy in an Australian population.\n    \n\n\n          Design:\n        \n      \n      Retrospective audit, tertiary centre hospitals and private practices.\n    \n\n\n          Participants:\n        \n      \n      All individuals in South Australia and the Northern Territory who underwent their first vitrectomy for diabetic complications between January 1, 2007 and December 31, 2011.\n    \n\n\n          Methods:\n        \n      \n      An audit of all eligible participants has been completed previously. Survival status as of July 6, 2018 and cause of death were obtained using SA/NT DataLink. Kaplan-Meier survival curves and multivariate cox-regressions were used to analyse survival rates and identify risk factors for mortality.\n    \n\n\n          Main outcome measures:\n        \n      \n      Five-, seven- and nine-year survival rates.\n    \n\n\n          Results:\n        \n      \n      The 5-, 7- and 9-year survival rates were 84.4%, 77.9% and 74.7%, respectively. The most common cause of death was cardiovascular disease. Associated with increased mortality independent of age were Indigenous ethnicity (HR = 2.04, 95% confidence interval [CI]: 1.17-3.57, P = 0.012), chronic renal failure (HR = 1.76, 95% CI: 1.07-2.89, P = 0.026) and renal failure requiring dialysis (HR = 2.32, 95% CI: 1.25-4.32, P = 0.008).\n    \n\n\n          Conclusions and relevance:\n        \n      \n      Long-term survival rates after diabetic vitrectomy in Australia are similar to rates reported in other populations. Indigenous ethnicity and chronic renal failure were the most significant factors associated with long-term mortality. This information can guide allocation of future resources to improve the prognosis of these high risk groups."
        },
        {
            "title": "The current and emerging role of cardiovascular magnetic resonance imaging in hypertrophic cardiomyopathy.",
            "abstract": "Hypertrophic cardiomyopathy (HCM) is the most common genetic cardiomyopathy with substantial heterogeneity in phenotypic expression and clinical course. Traditionally, two-dimensional echocardiography has been the easiest and most reliable technique for establishing a diagnosis of HCM. However, cardiovascular magnetic resonance (CMR) has emerged as a novel, three-dimensional tomographic imaging technique, which provides high spatial and temporal resolution images of the heart in any plane and without ionizing radiation. As a result, CMR is particularly well suited to provide detailed characterization of the HCM phenotype, including precise assessment of the location and distribution of left ventricular (LV) wall thickening. In this regard, CMR can identify hypertrophy (particularly in the anterolateral free wall and apex), not well appreciated (or underestimated) by two-dimensional echocardiography, with important implications for diagnosis. CMR can also provide detailed characterization of other myocardial structures such as the papillary muscles, which may impact on preoperative management strategies for patients who are candidates for surgical myectomy. Furthermore, CMR enables an accurate assessment of total LV mass, a robust marker of the overall extent of hypertrophy, which may have implications for risk stratification. In addition, a subgroup of HCM patients have normal LV mass (with focal hypertrophy), suggesting that a limited extent of hypertrophy is consistent with a diagnosis of HCM. Finally, following the intravenous administration of gadolinium, first-pass perfusion sequences can identify myocardial perfusion abnormalities, while late gadolinium enhancement (LGE) sequences can characterize areas of myocardial fibrosis/scarring. LGE is associated with systolic dysfunction and likelihood for ventricular tachyarrhythmias on ambulatory Holter monitoring in patients with HCM. However, the precise clinical implications of myocardial perfusion abnormalities and LGE in HCM are still uncertain; this information may have important implications with regard to identifying HCM patients at risk of sudden death and adverse LV remodeling associated with systolic dysfunction. Therefore, at present, CMR provides important information impacting on diagnosis and clinical management strategies in patients with HCM and will likely have an expanding role in the evaluation of patients with this complex disease."
        },
        {
            "title": "Methods for identifying long-term adverse effects of treatment in patients with eye diseases: the Systemic Immunosuppressive Therapy for Eye Diseases (SITE) Cohort Study.",
            "abstract": "Purpose:\n        \n      \n      To evaluate potential epidemiologic methods for studying long-term effects of immunosuppression on the risk of mortality and fatal malignancy, and present the methodological details of the Systemic Immunosuppressive Therapy for Eye Diseases (SITE) Cohort Study.\n    \n\n\n          Methods:\n        \n      \n      Advantages and disadvantages of potential study designs for evaluating rare, late-occurring events are reviewed, and the SITE Cohort Study approach is presented.\n    \n\n\n          Results:\n        \n      \n      The randomized, controlled trial is the most robust method for evaluating treatment effects, but long study duration, high costs, and ethical concerns when studying toxicity limit its use in this setting. Retrospective cohort studies are potentially more cost-effective and timely, if records exist providing the desired information over sufficient follow-up time in the past. Case-control methods require extremely large sample sizes to evaluate risk associated with rare exposures, and recall bias is problematic when studying mortality. The SITE Cohort Study is a retrospective cohort study. Past use of antimetabolites, T-cell inhibitors, alkylating agents, and other immunosuppressives is ascertained from medical records of approximately 9,250 ocular inflammation patients at five tertiary centers over up to 30 years. Mortality and cause-specific mortality outcomes over approximately 100,000 person-years are ascertained using the National Death Index. Immunosuppressed and non-immunosuppressed groups of patients are compared with each other and general population mortality rates from US vital statistics. Calculated detectable differences for mortality/fatal malignancy with respect to the general population are 22%/49% for antimetabolites, 28%/62% for T-cell inhibitors, and 36%/81% for alkylating agents.\n    \n\n\n          Conclusions:\n        \n      \n      Information from the SITE Cohort Study should clarify whether use of these immunosuppressive drugs for ocular inflammation increases the risk of mortality and fatal cancer. This epidemiologic approach may be useful for evaluating long-term risks of systemic therapies for other ocular diseases."
        },
        {
            "title": "Prospective evaluation of carotid artery stenosis: elliptic centric contrast-enhanced MR angiography and spiral CT angiography compared with digital subtraction angiography.",
            "abstract": "Background and purpose:\n        \n      \n      Although digital subtraction angiography (DSA) is the reference standard for assessing carotid arteries, it is uncomfortable for patients and has a small risk of disabling stroke and death. These problems have fueled the use of spiral CT angiography and MR angiography. We prospectively compared elliptic centric contrast-enhanced MR angiography and spiral CT angiography with conventional DSA for detecting carotid artery stenosis.\n    \n\n\n          Methods:\n        \n      \n      Eighty carotid arteries (in 40 symptomatic patients) were assessed. Elliptic centric MR and spiral CT angiographic data were reconstructed with maximum intensity projection and multiplanar reconstruction techniques. All patients had been referred for DSA evaluation on the basis of findings at Doppler sonography, which served as a screening method (degree of stenosis > or = 70% or inconclusive results). Degree of carotid stenosis estimated by using the three modalities was compared.\n    \n\n\n          Results:\n        \n      \n      Significant correlation with DSA was found for stenosis degree for both elliptic centric MR and spiral CT angiography; however, the correlation coefficient was higher for MR than for CT angiography (r = 0.98 vs r = 0.86). Underestimation of stenoses of 70-99% occurred in one case with elliptic centric MR angiography (a 70% stenosis was underestimated as 65%) and in nine cases with spiral CT angiography, in comparison to DSA findings. Overestimation occurred in two cases with MR angiography (stenoses of 65-67% were overestimated as 70-75%). With CT, overestimation occurred in seven cases; a stenosis of 60% in one case was overestimated as 70%. Both techniques confirmed the three cases of carotid occlusion. With elliptic centric MR angiography, carotid stenoses of 70% or greater were detected with high sensitivity, 97.1%; specificity, 95.2%; likelihood ratio (LR) for a positive test result, 20.4; and ratio of LR(+) to LR(-), -0.3. With spiral CT angiography, sensitivity, specificity, LR(+), and LR(+):LR(-) were 74.3%, 97.6%, 31.2, and 0.3, respectively.\n    \n\n\n          Conclusion:\n        \n      \n      Elliptic centric contrast-enhanced MR angiography is more accurate than spiral CT angiography to adequately evaluate carotid stenosis. Furthermore, elliptic centric contrast-enhanced MR angiography appears to be adequate to replace conventional DSA in most patients examined."
        },
        {
            "title": "The risks of screening: data from the Nottingham randomised controlled trial of faecal occult blood screening for colorectal cancer.",
            "abstract": "Aims:\n        \n      \n      To determine the harm that ensues from faecal occult blood (FOB) screening for colorectal cancer.\n    \n\n\n          Methods:\n        \n      \n      150 251 people were randomly allocated either to receive biennial Haemoccult FOB tests (n =75 253) or not to be contacted (n=74 998). Study group patients returning positive tests were offered colonic investigation; 1774 underwent complete investigation of the colon.\n    \n\n\n          Results:\n        \n      \n      There was no significant difference in the stage at presentation of interval versus control group cancers. Survival in the interval cancer group was significantly prolonged compared with the control group. Sensitivity for colonoscopy or flexible sigmoidoscopy and double contrast barium enema (DCBE) was 96.7%. There were no complications of DCBE but seven (0.5%) complications of colonoscopy, of which six required surgical intervention. There were no colonoscopy related deaths. No patients without colorectal cancer died within 30 days of colonic investigation. Five patients died within 30 days of surgery for screen detected colorectal neoplasia and a further two died without having surgery. Six patients died after 30 days but within two years of surgery for screen detected benign adenomas or stage A cancers; in all cases the cause of death was not related to colorectal cancer.\n    \n\n\n          Conclusions:\n        \n      \n      There was investigation related morbidity but no mortality and little to support overdiagnosis bias. The group returning falsely negative tests had a better outcome compared with the whole control group. There is a negative side to any screening programme but mortality reduction in this and other trials suggests that a national programme of colorectal cancer screening should be given consideration."
        },
        {
            "title": "High mass (>18g) of late gadolinium enhancement on CMR imaging is associated with major cardiac events on long-term outcome in patients with biopsy-proven extracardiac sarcoidosis.",
            "abstract": "Background:\n        \n      \n      Cardiac involvement is the most important cause of mortality in patients with systemic sarcoidosis. Late gadolinium enhancement (LGE) on cardiovascular magnetic resonance imaging (CMR) has been shown to be a predictor of major cardiovascular adverse events (MACE) in the setting of systemic sarcoidosis. We sought to evaluate the relationship between LGE mass and adverse long-term outcome in patients with biopsy-proven extracardiac sarcoidosis.\n    \n\n\n          Methods:\n        \n      \n      Between 2001 and 2013, 197 consecutive patients with suspected cardiac sarcoidosis were identified in our institution database. Of them, 56 patients have had biopsy-proven extracardiac sarcoidosis and represented our studied population. Patients were divided into two groups based on LGE mass by a median value (mild LGE<18g, high LGE>18g) for comparison of MACE.\n    \n\n\n          Results:\n        \n      \n      Twenty-eight patients had a high mass of LGE. Of them, 15 (54%) experienced MACE (OR=31.15, 95% CI 3.7-262). Except for 1 patient, no patient with mild LGE presented with any MACE during follow-up (median of 32months). Patients with high LGE had lower CMR-derived left (53.6±14.9 vs. 62.2±6.7, p<0.01) and right (49.1±11.5 vs. 56.4±9.2, p<0.05) ventricular ejection fractions. LGE mass of 18g discriminated patients with and without MACE (93% sensitivity, 88% specificity, AUC=0.972). LGE mass was the only independent predictor of MACE on multivariate Cox analysis adjusted (OR=1.7, 95% CI 1.06 to 2.72, p=0.03).\n    \n\n\n          Conclusion:\n        \n      \n      In biopsy-proven extracardiac sarcoidosis patients, a high mass of LGE >18g was associated with MACE."
        },
        {
            "title": "Diagnostic performance of contrast-enhanced and unenhanced combined pulmonary artery MRI and magnetic resonance venography techniques in the diagnosis of venous thromboembolism.",
            "abstract": "Objective::\n        \n      \n      We aimed to determine the diagnostic performance of the contrast-enhanced and unenhanced combined pulmonary arterial MRI and magnetic resonance venography techniques in the diagnosis of venous thromboembolism (VTE).\n    \n\n\n          Methods::\n        \n      \n      44 patients who underwent CT pulmonary angiography (CTPA) for suspected PE constituted the study population. Patients underwent combined pulmonary and lower extremity MRI, and Doppler ultrasonography within 72 h after CTPA. Combined MRI included two sequences: unenhanced steady-state free precession (SSFP) and contrast-enhanced three-dimensional (3D) gradient echo (GRE). The presence of emboli in pulmonary arteries and thrombi in lower extremity veins on 3D-GRE and SSFP sequences was recorded.\n    \n\n\n          Results::\n        \n      \n      CTPA showed a total of 244 emboli in 33 (75%) patients whereas contrast-enhanced 3D-GRE MRI showed deep vein thrombosis (DVT) in 34 (77%) subjects. Sensitivities for SSFP vs 3D-GRE MRI respectively in PE detection were 87.9 vs 100% on a per-patient basis, and 53.7 vs 73% on a per-embolus basis. Of 34 patients with established DVT, 31 (91%) were detected by Doppler ultrasound and 29 (85%) were detected by SSFP technique respectively.\n    \n\n\n          Conclusion::\n        \n      \n      Both contrast-enhanced and unenhanced combined MRI of acute PE and DVT are feasible one-stop-shopping techniques in patients with suspected thromboembolism.\n    \n\n\n          Advances in knowledge::\n        \n      \n      Pulmonary VTE is a common disease with high mortality. Non-invasive techniques withhigh accuracy are required for the assessment of VTE. CT-related radiation and contrast material risks cause concerns. MRI is a radiation-free technique evaluating the vessels with and without contrast. Combined contrast enhancedor unenhanced pulmonary and lower extremity MRI is feasible in patients with suspected thromboembolism."
        },
        {
            "title": "Longitudinal incidence and prevalence of adverse outcomes of diabetes mellitus in elderly patients.",
            "abstract": "Background:\n        \n      \n      The natural history of type 2 diabetes mellitus (DM) in the elderly has not been previously described in a national longitudinal sample.\n    \n\n\n          Methods:\n        \n      \n      This national longitudinal analysis (January 1, 1991, to December 31, 2004) examines mortality and morbidity rates in a representative sample of elderly patients newly diagnosed as having DM. Medicare beneficiaries diagnosed as having DM in 1994 (n=33,772) were compared with a control group (n=25,563) regarding death, lower extremity complications, nephropathy, retinopathy, cardiovascular complications, and cerebrovascular complications.\n    \n\n\n          Results:\n        \n      \n      The DM group had excess mortality of 9.2% by year 11 compared with the control group. By 2004, 91.8% of the DM group experienced an adverse complication compared with 72.0% of the control group. The DM group had a higher prevalence and incidence of microvascular and macrovascular complications at all time points compared with controls. Patients with DM were at increased risk for all lower extremity complications, particularly those requiring surgical intervention (gangrene, debridement, and amputation). Cardiovascular complications were a leading cause of morbidity, with 57.6% of the DM group diagnosed as having heart failure compared with 34.1% of the controls.\n    \n\n\n          Conclusion:\n        \n      \n      Elderly persons newly diagnosed as having DM experienced high rates of complications during 10-year follow-up, far in excess of elderly persons without this diagnosis, implying a substantial burden on the individual and on the health care system."
        },
        {
            "title": "A national evaluation of the nighttime and passenger restriction components of graduated driver licensing.",
            "abstract": "Introduction:\n        \n      \n      The high crash rate of youthful novice drivers has been recognized for half a century. Over the last decade, graduated driver licensing (GDL) systems, which extend the period of supervised driving and limit the novice's exposure to higher-risk conditions (such as nighttime driving), have effectively reduced crash involvements of novice drivers.\n    \n\n\n          Method:\n        \n      \n      This study used data from the Fatality Analysis Reporting System (FARS) and the implementation dates of GDL laws in a state-by-year panel study to evaluate the effectiveness of two key elements of GDL laws: nighttime restrictions and passenger limitations.\n    \n\n\n          Results:\n        \n      \n      Nighttime restrictions were found to reduce 16- and 17-year-old driver involvements in nighttime fatal crashes by an estimated 10% and 16- and 17-year-old drinking drivers in nighttime fatal crashes by 13%. Passenger restrictions were found to reduce 16- and 17-year-old driver involvements in fatal crashes with teen passengers by an estimated 9%.\n    \n\n\n          Conclusions:\n        \n      \n      These results confirm the effectiveness of these provisions in GDL systems. Impact on Public Health. States without the nighttime or passenger restrictions in their GDL law should strongly consider adopting them.\n    \n\n\n          Impact on industry:\n        \n      \n      The results of this study indicate that nighttime restrictions and passenger limitations are very important components of any GDL law."
        },
        {
            "title": "A comparison of comorbidities obtained from hospital administrative data and medical charts in older patients with pneumonia.",
            "abstract": "Background:\n        \n      \n      The use of comorbidities in risk adjustment for health outcomes research is frequently necessary to explain some of the observed variations. Medical charts reviews to obtain information on comorbidities is laborious. Increasingly, electronic health care databases have provided an alternative for health services researchers to obtain comorbidity information. However, the rates obtained from databases may be either over- or under-reported. This study aims to (a) quantify the agreement between administrative data and medical charts review across a set of comorbidities; and (b) examine the factors associated with under- or over-reporting of comorbidities by administrative data.\n    \n\n\n          Methods:\n        \n      \n      This is a retrospective cross-sectional study of patients aged 55 years and above, hospitalized for pneumonia at 3 acute care hospitals. Information on comorbidities were obtained from an electronic administrative database and compared with information from medical charts review. Logistic regression was performed to identify factors that were associated with under- or over-reporting of comorbidities by administrative data.\n    \n\n\n          Results:\n        \n      \n      The prevalence of almost all comorbidities obtained from administrative data was lower than that obtained from medical charts review. Agreement between comorbidities obtained from medical charts and administrative data ranged from poor to very strong (kappa 0.01 to 0.78). Factors associated with over-reporting of comorbidities were increased length of hospital stay, disease severity, and death in hospital. In contrast, those associated with under-reporting were number of comorbidities, age, and hospital admission in the previous 90 days.\n    \n\n\n          Conclusions:\n        \n      \n      The validity of using secondary diagnoses from administrative data as an alternative to medical charts for identification of comorbidities varies with the specific condition in question, and is influenced by factors such as age, number of comorbidities, hospital admission in the previous 90 days, severity of illness, length of hospitalization, and whether inhospital death occurred. These factors need to be taken into account when relying on administrative data for comorbidity information."
        },
        {
            "title": "[Influence of visual impairment on mortality in the elderly aged 65 years and older in 8 longevity areas in China].",
            "abstract": "Objective: To understand the relationship between visual impairment and risk of all-cause mortality in the elderly aged 65 years and older in 8 longevity areas in China. Methods: The data of the elderly aged 65 years and older in the project in 2012 were obtained from Healthy Aging and Biomarkers Cohort Study, a sub-cohort of the Chinese Longitudinal Healthy Longevity Survey, including physical measurement and survival status, and a follow-up for survival outcomes were conducted in 2014 and 2017 respectively. Cox proportional hazard regression model was used to analyze the influence of visual impairment on mortality. Gender and age specific analysis was conducted. Results: A total of 1 736 elderly adults were included. A total of 943 deaths occurred during the 5-year follow-up period with a 5-year mortality rate of 54.3%. The 5-year mortality rate was 76.7% in the group with visual impairment, and 47.6% in the group without visual impairment (P<0.001). After adjusting for demographic information, life style and some disease factors, the risk of 5-year mortality in the group with visual impairment group was 1.30 times higher than that in the group without visual impairment (HR=1.30, 95%CI: 1.09-1.55). In the females, the risk for mortality in the group with visual impairment was 1.48 times higher than that in the group without visual impairment (HR=1.48, 95%CI:1.20-1.84). However, vision status was not associated with the risk for mortality in males (HR=1.02, 95%CI: 0.72-1.43). The risk for mortality in the group with visual impairment was 1.39 times higher than that in the group without visual impairment in the elderly aged over 90 years (HR=1.39, 95%CI: 1.13-1.70). Vision status was not associated with mortality risk in the elderly aged 65-79 years and 80-89 years (HR=1.37, 95%CI: 0.61-3.07; HR=0.95, 95%CI: 0.61-1.48). Conclusion: In the elderly people in China, visual impairment is a risk factor for mortality."
        },
        {
            "title": "Trajectories of Serum Albumin Predict Survival of Peritoneal Dialysis Patients: A 15-year Follow-Up Study.",
            "abstract": "Although initial serum albumin level is highly associated with overall and cardiovascular mortality in peritoneal dialysis (PD) patients, we consider that the dynamic change and trend of albumin after initiation of PD are also essential.We enrolled patients who received PD for more than 3 months from January 1999 to March 2014. We categorized these patients into 2 groups by the difference in serum albumin level (Δalbumin = difference between peak with initial albumin level = peak albumin level - initial albumin level) after PD. The patients with Δalbumin < 0.2 g/dL (median level) were considered as group A (n, number = 238) and those with Δalbumin ≥ 0.2 g/dL were considered as group B (n = 278). Further, we stratified these patients into quartiles: Q1 Δalbumin < -0.2 g/dL; Q2, -0.2 ≦∼ <0.2 g/dL; Q3, 0.2 ≦∼ <0.6 g/dL; and Q4, ≥0.6 g/dL. Regression analysis was performed to determine the correlation of initial albumin and Δalbumin.Group A patients presented with higher levels of serum albumin (3.71 ± 0.54 vs 3.04 ± 0.55 g/dL; P < 0.001) and hematocrit as well as better initial residual renal function. However, those in group A had lower serum albumin increment and downward-sloped trends after dialysis. In contrast, the albumin trend was upward sloped and the increment of albumin was remarkable in group B, despite the high prevalence of cardiovascular diseases and diabetes. Overtime, group A patients had poorer survival and experienced more frequent and longer hospitalizations. Group Q1 patients with least albumin increment had worst survival. Group Q4 patients with lowest initial albumin also had poor survival. Age, diabetes, cardiovascular diseases, BMI, initial albumin, and Δalbumin could affect patient outcomes independently. Regression analysis showed a better outcome can be obtained if the initial albumin level is at least above 3.15 g/dL. (Initial albumin level = -0.61 × Δalbumin + 3.50.)The increment and trend of albumin especially during early period of PD may be a more crucial determinant for survival."
        },
        {
            "title": "Tree-Based Models for Predicting Mortality in Gram-Negative Bacteremia: Avoid Putting the CART before the Horse.",
            "abstract": "Increasingly, infectious disease studies employ tree-based approaches, e.g., classification and regression tree modeling, to identify clinical thresholds. We present tree-based-model-derived thresholds along with their measures of uncertainty. We explored individual and pooled clinical cohorts of bacteremic patients to identify modified acute physiology and chronic health evaluation (II) (m-APACHE-II) score mortality thresholds using a tree-based approach. Predictive performance measures for each candidate threshold were calculated. Candidate thresholds were examined according to binary logistic regression probabilities of the primary outcome, correct classification predictive matrices, and receiver operating characteristic curves. Three individual cohorts comprising a total of 235 patients were studied. Within the pooled cohort, the mean (± standard deviation) m-APACHE-II score was 13.6 ± 5.3, with an in-hospital mortality of 16.6%. The probability of death was greater at higher m-APACHE II scores in only one of three cohorts (odds ratio for cohort 1 [OR1] = 1.15, 95% confidence interval [CI] = 0.99 to 1.34; OR2 = 1.04, 95% CI = 0.94 to 1.16; OR3 = 1.18, 95% CI = 1.02 to 1.38) and was greater at higher scores within the pooled cohort (OR4 = 1.11, 95% CI = 1.04 to 1.19). In contrast, tree-based models overcame power constraints and identified m-APACHE-II thresholds for mortality in two of three cohorts (P = 0.02, 0.1, and 0.008) and the pooled cohort (P = 0.001). Predictive performance at each threshold was highly variable among cohorts. The selection of any one predictive threshold value resulted in fixed sensitivity and specificity. Tree-based models increased power and identified threshold values from continuous predictor variables; however, sample size and data distributions influenced the identified thresholds. The provision of predictive matrices or graphical displays of predicted probabilities within infectious disease studies can improve the interpretation of tree-based model-derived thresholds."
        },
        {
            "title": "Necrotizing pancreatitis: contemporary analysis of 99 consecutive cases.",
            "abstract": "Objective:\n        \n      \n      To analyze the impact of a conservative strategy of management in patients with necrotizing pancreatitis, reserving intervention for patients with documented infection or the late complications of organized necrosis.\n    \n\n\n          Summary background data:\n        \n      \n      The role of surgery in patients with sterile pancreatic necrosis remains controversial. Although a conservative approach is being increasingly used, few studies have evaluated this strategy when applied to the entire spectrum of patients with necrotizing pancreatitis.\n    \n\n\n          Methods:\n        \n      \n      The authors reviewed 1,110 consecutive patients with acute pancreatitis managed at Brigham and Women's Hospital between January 1, 1995, and January 1, 2000, focusing on those with pancreatic necrosis documented by contrast-enhanced computed tomography. Fine-needle aspiration, the presence of extraintestinal gas on computed tomography, or both were used to identify infection.\n    \n\n\n          Results:\n        \n      \n      There were 99 (9%) patients with necrotizing pancreatitis treated, with an overall death rate of 14%. In three patients with underlying medical problems, the decision was made initially not to intervene. Of the other 62 patients without documented infection, all but 3 were managed conservatively; this group's death rate was 11%. Of these seven deaths, all were related to multiorgan failure. Five patients in this group eventually required surgery for organized necrosis, with no deaths. Of the 34 patients with infected necrosis, 31 underwent surgery and 3 underwent percutaneous drainage. Only four (12%) of these patients died, all of multiorgan failure. Of the total 11 patients who died, few if any would have been candidates for earlier surgical intervention.\n    \n\n\n          Conclusions:\n        \n      \n      These results suggest that conservative strategies can be applied successfully to manage most patients with necrotizing pancreatitis, although some will eventually require surgery for symptomatic organized necrosis. Few if any patients seem likely to benefit from a more aggressive strategy."
        },
        {
            "title": "Diagnostic performance of late gadolinium enhancement in the assessment of acute cellular rejection after heart transplantation.",
            "abstract": "Objective:\n        \n      \n      Allograft rejection is still an important cause of morbidity and mortality after heart transplantation (HTx). Many techniques in cardiac magnetic resonance imaging (CMR) were investigated to diagnose acute cellular rejection (ACR). However, there is not enough information about late gadolinium enhancement (LGE) in the myocardium and ACR.\n    \n\n\n          Methods:\n        \n      \n      We prospectively analyzed our consecutive 41 heart transplant recipients who were admitted for routine endomyocardial biopsies. CMR was performed maximum 6 h before the scheduled endomyocardial biopsy. Correlation between LGE in the myocardium and ACR was investigated.\n    \n\n\n          Results:\n        \n      \n      Twenty-seven patients showed no rejection, and nine of them had LGE in the myocardium. Fourteen patients had LGE in the left ventricle (LV), and two patients had LGE also in the right ventricle (RV). There was no correlation between LGE and ACR (p=0.879). There was no difference in the left ventricular ejection fraction (LVEF), right ventricular fractional area change (RVFAC), and cardiac ischemic time between the groups (p=0.825, p=0.370, and p=0.419, respectively). LGE in the myocardium could be due to previous rejection episodes; therefore, all patients were retrospectively searched for previous rejection grades and number of episodes. Thirty-eight of the 41 patients had a history of one ACR episode, but none of them had a statistically significant correlation with LGE (for grade 1R, p=0.964 and grade 3R, p=1) There was also no correlation between number of rejection episodes history and LGE.\n    \n\n\n          Conclusion:\n        \n      \n      LGE is not suitable to detect ACR in heart transplant patients. LGE and the history of ACR have no correlation."
        },
        {
            "title": "Awareness of necessity to call 9-1-1 for stroke symptoms, upstate New York.",
            "abstract": "Introduction:\n        \n      \n      Stroke is the third leading cause of death and a leading cause of disability in New York State. A New York study determined that only 19.9% of patients arrived at a designated stroke center within 3 hours of symptom onset. Yet, receiving treatment within 90 minutes of stroke symptom onset is optimal for improved outcomes. Delay in recognition of stroke symptoms and their severity contributes to treatment delay.\n    \n\n\n          Methods:\n        \n      \n      A random-digit-dialed, list-assisted telephone survey about stroke knowledge was administered to 1789 adults aged 30 years or older in upstate New York in 2006. Bivariate and regression analysis were used to examine factors associated with intent to call 9-1-1 for symptoms of stroke.\n    \n\n\n          Results:\n        \n      \n      The largest proportion of respondents (72.4%; 95% confidence interval [CI], 69.9%-74.8%) reported they would call 9-1-1 if they noticed they or someone else had difficulty speaking, and the fewest (33.3%; 95% CI, 30.7%-36.0%) respondents reported they would call 9-1-1 for trouble seeing or double vision. Multivariate analysis found that those who had a history of delay in getting medical care in the past 6 months had decreased odds of intending to call 9-1-1 for stroke symptoms (difficulty speaking: adjusted odds ratio [AOR], 0.76; 95% CI, 0.58-1.00; trouble seeing: AOR, 0.69; 95% CI, 0.53-0.91; facial droop: AOR, 0.85; 95% CI, 0.65-1.11; arm weakness: AOR, 0.80; 95% CI, 0.63-1.03). Age, education, and history of a stroke or heart event were not consistently associated with intent to call 9-1-1.\n    \n\n\n          Conclusion:\n        \n      \n      Survey respondents do not interpret some stroke symptoms as urgent enough to activate the emergency medical system. History of delaying care is a behavioral pattern that influenced intent to call 9-1-1."
        },
        {
            "title": "Utility of the Logistic Clinical Syntax Score in the Prediction of Contrast-Induced Nephropathy After Primary Percutaneous Coronary Intervention.",
            "abstract": "Background:\n        \n      \n      The Logistic Clinical Syntax Score (log CSS) is a combined risk scoring system that includes clinical and anatomic parameters; it has been found to be effective for the prediction of mortality in patients with ST-elevation myocardial infarction (STEMI). The aim of the present study was to assess whether the log CSS was associated with the development of contrast-induced nephropathy (CIN) in patients who underwent primary percutaneous coronary intervention (pPCI).\n    \n\n\n          Methods:\n        \n      \n      A total of 930 patients with STEMI undergoing pPCI between January 2012 and August 2013 were included prospectively. The patients were grouped according to the development of CIN. Either an absolute serum creatinine level ≥ 0.5 mg/dL or a 25% increase in the serum creatinine level compared with the baseline level within 48 hours after the administration of contrast medium was defined as CIN.\n    \n\n\n          Results:\n        \n      \n      The Synergy Between Percutaneous Coronary Interventions With Taxus and Cardiac Surgery score (SYNTAX [SS]) and log CSS were higher in patients with CIN than in those without. In the multivariate analysis, log CSS (odds ratio, 1.405, 95% confidence interval, 1.318-1.497; P < 0.001), hemoglobin, and contrast volume were found to be independent predictors of CIN. In the receiver operating characteristic analysis, a log CSS > 9.5 had a 74.5% sensitivity and a 90.5% specificity for predicting CIN, with an area under the curve (AUC) of 0.892, whereas an SS > 18.5 had a 64% sensitivity, a 58.1% specificity, and an AUC of 0.625 (0.892 vs 0.625; P < 0.001). A log CSS > 9.5 was associated with in-hospital and long-term mortality, reinfarction, revascularization, and in-hospital hemodialysis (P < 0.001 for each).\n    \n\n\n          Conclusions:\n        \n      \n      The log CSS may improve the accuracy of risk stratification for the development of CIN in patients undergoing pPCI."
        },
        {
            "title": "Myocardial fibrosis severity on cardiac magnetic resonance imaging predicts sustained arrhythmic events in hypertrophic cardiomyopathy.",
            "abstract": "Background:\n        \n      \n      The purpose of our study was to correlate the incidence of adequate implantable cardioverter-defibrillator (ICD) interventions in hypertrophic cardiomyopathy (HCM) patients with risk markers (RMs) for sudden cardiac death (SCD) plus myocardial fibrosis as detected by late gadolinium-enhanced cardiac magnetic resonance (LGE-CMR) imaging.\n    \n\n\n          Methods:\n        \n      \n      In all, 87 patients with HCM underwent LGE-CMR imaging prior to ICD implantation, performed for secondary (n = 2; 2%) or primary SCD prophylaxis (n = 85; 98%). Fibrosis was graded with a 17-segment left ventricular model (0 = absent, 1 = point-shaped, 2 = limited to 1 left ventricular segment, 3 = involving ≥ 2 segments). During follow-up, ICD memories were read out by a physician blinded to the individual patient data.\n    \n\n\n          Results:\n        \n      \n      The number of RMs per patient was 1.9 ± 0.8. Myocardial fibrosis was present in 78 patients (90%); 26 (30%) had a fibrosis score of 3. During follow-up (3.5 ± 2.6 [range, 0.2-11.4 years]), 15 patients had 50 appropriate ICD interventions. Episodes of atrial fibrillation were found in 28 patients. Fibrosis severity correlated with occurrence of ventricular tachycardia (Cramér's V, or φc = 0.4, P < 0.001) and atrial fibrillation (φc = 0.6, P < 0.001). On multivariate regression analysis, an independent association between myocardial fibrosis (ß = 0.6, P < 0.01) and sustained ventricular tachycardia was found.\n    \n\n\n          Conclusions:\n        \n      \n      In HCM patients treated with ICD implantation because of a high SCD risk by traditional RM assessment, a high rate of arrhythmic events was observed during long-term follow-up. In a cohort of patients with clinical markers for high risk of SCD, severity of myocardial fibrosis as detected by an easy LGE-CMR scoring system was associated with future arrhythmic events and appropriate ICD therapies."
        },
        {
            "title": "Extent of left ventricular scar predicts outcomes in ischemic cardiomyopathy patients with significantly reduced systolic function: a delayed hyperenhancement cardiac magnetic resonance study.",
            "abstract": "Objectives:\n        \n      \n      The objective of the study was to determine whether the extent of left ventricular scar, measured with delayed hyperenhancement cardiac magnetic resonance (DHE-CMR), predicts survival in patients with ischemic cardiomyopathy (ICM) and severely reduced left ventricular ejection fraction (LVEF).\n    \n\n\n          Background:\n        \n      \n      Patients with ICM and reduced LVEF have poor survival. Such patients have a high myocardial scar burden. CMR is highly accurate in delineation of myocardial scar.\n    \n\n\n          Methods:\n        \n      \n      We studied 349 patients (76% men) with severe ICM (>or=70% disease in >or=1 epicardial coronary, and mean LVEF of 24%) that underwent DHE-CMR (Siemens 1.5-T scanner, Erlangen, Germany), between 2003 and 2006. Scar (quantified as percentage of myocardium) was defined on DHE-MR images as an intensity >2 standard deviations above the viable myocardium. Transmurality score was semiquantitatively recorded in a 17-segment model as: 0 = no scar, 1 = 1% to 25% scar, 2 = 26% to 50%, 3 = 51% to 75%, and 4 = >75%. The LVEF, demographic data, risk factors, need for cardiac transplantation (CTx), and all-cause mortality were recorded.\n    \n\n\n          Results:\n        \n      \n      The mean age and follow-up were 65 +/- 11 years and 2.6 +/- 1.2 years (median 2.4 years [1.1, 3.5]), respectively. There were 56 events (51 deaths and 5 CTx). Mean scar percentage and transmurality score were higher in patients with events versus those without (39 +/- 22 vs. 30 +/- 20, p = 0.003, and 9.7 +/- 5 vs. 7.8 +/- 5, p = 0.004). On Cox proportional hazard survival analysis, quantified scar was greater than the median (30% of total myocardium), and female gender predicted events (relative risk 1.75 [95% Confidence Interval: 1.02 to 3.03] and relative risk 1.83 [95% Confidence Interval: 1.06 to 3.16], respectively, both p = 0.03).\n    \n\n\n          Conclusions:\n        \n      \n      In patients with ICM and severely reduced LVEF, a greater extent of myocardial scar, delineated by DHE-CMR is associated with increased mortality or the need for cardiac transplantation, potentially aiding further risk-stratification."
        },
        {
            "title": "Difference between ophthalmologists' and patients' perceptions of quality of life associated with age-related macular degeneration.",
            "abstract": "Background:\n        \n      \n      There may be a wide disparity between the perceptions of patients and those of their treating physicians concerning the quality of life associated with a given state of health. Because of this potential for difference of opinion, we performed a study to evaluate patients' and ophthalmologists' perceptions of quality of life, as measured by utility analysis, associated with visual loss secondary to age-related macular degeneration (AMD).\n    \n\n\n          Methods:\n        \n      \n      Cross-sectional study. Utilities were assessed, by means of both the time trade-off method and the standard gamble method, for various degrees of theoretical visual loss secondary to AMD for ophthalmologists-in-training and graduate ophthalmologists. These were compared to utilities for a known population of patients with actual visual loss due to AMD. A utility of 1.0 is associated with perfect health, whereas a utility of 0.0 is associated with death.\n    \n\n\n          Results:\n        \n      \n      With both the time trade-off and standard gamble methods, the patients had lower mean utilities than did the ophthalmologists for the same degrees of visual loss secondary to AMD. The ophthalmologists were significantly less willing than the patients to trade years of remaining life for perfect vision with the time tradeoff method (p < or = 0.01), and with the standard gamble method they were less willing than the patients to take the risk of dying in return for perfect vision. Given the scenario of counting fingers or worse vision in both eyes, the ophthalmologists were willing to trade 3.3 of every 10 years of remaining life for perfect vision in both eyes, whereas the patients with actual vision of counting fingers or worse in both eyes were willing to trade 6.0 of every 10 years of remaining life for this result.\n    \n\n\n          Interpretation:\n        \n      \n      When presented with the scenario of visual loss secondary to AMD, ophthalmologists substantially underestimated its effect on patients' quality of life."
        },
        {
            "title": "[Relationship between flash visual evoked potential and severity and prognosis in critically ill patients].",
            "abstract": "Objective:\n        \n      \n      To explore the relationship between flash visual evoked potential (fVEP) and severity and prognosis in critically ill patients in intensive care unit (ICU).\n    \n\n\n          Methods:\n        \n      \n      Sixty-nine critically ill patients were divided into two groups according to survival (35 cases) or death (34 cases) in 28 days. fVEP, Glasgow coma scale (GCS) score, acute physiology and chronic health evaluation II (APACHE II) score and sepsis-related organ failure assessment (SOFA) score of survivors were compared with those of nonsurvivors. Also, according to primary disease, the patients were divided into a group of patients with primary intracranial disease and patients with mental disturbance but without primary intracranial lesion. Above mentioned indexes were compared, and clinical outcome was predicted with their correlation with fVEP in each patient.\n    \n\n\n          Results:\n        \n      \n      The latent period of fVEP peak appeared later in nonsurvivors than those in survivors [(228.6+/-41.7) ms vs. (190.5+/-49.2) ms, P<0.01]. APACHE II score (25.9+/-6.4 vs. 22.5+/-6.7) and SOFA score (6.7+/-2.0 vs. 5.4+/-2.5) were higher in nonsurvivors than those in survivors (both P<0.05 ), while the changes in GCS score was in contrary (6.3+/-2.4 vs. 7.0+/-3.0, P<0.05). fVEP peak latency showed a negative correlation with GCS score (r=-0.332, P<0.01). The death rate of the group of patients with primary intracranial lesion was similar to that of the total. fVEP peak latency of the group with no primary intracranial lesion but with mental impairment in nonsurvivors was significantly longer than that of survivors [(226.0+/-46.7) ms vs. (168.8+/-54.1) ms, P<0.05], fVEP peak latency was positively correlated with SOFA score (r=0.526, P<0.05). Area under receiver operator characteristic (ROC) curve of fVEP peak latency was 0.800+/-0.104 (P<0.05) for predicting outcome of patients, while that of SOFA score was 0.650+/-0.131 (P>0.05). The former could be used for predicting death.\n    \n\n\n          Conclusion:\n        \n      \n      fVEP reflects the prognosis and severity of critically ill patients in ICU. Especially, it maybe used as a tool for predicting death and multiple organ dysfunction syndrome (MODS) in the patients with no primary intracranial lesion but with mental impairment."
        },
        {
            "title": "Validation of a 4-item score predicting hip fracture and mortality risk among elderly women.",
            "abstract": "Purpose:\n        \n      \n      One in 4 Swedish women experiences a hip fracture, an event that has high concomitant morbidity and mortality. We developed and validated a clinical predictor of fracture and mortality risk, the Fracture and Mortality (FRAMO) Index.\n    \n\n\n          Methods:\n        \n      \n      This was a population-based prospective cohort study with a baseline questionnaire and 2-year outcomes of hip fracture, fragility fracture, and death. The questionnaire was sent to 1,498 women aged 70 years or older in 3 rural populations, asking them about their age, weight, height, mobility, previous fractures, smoking, medication use, and housing. Some women were also asked about previous vertebral radiographs. We defined 2 risk models before outcome data collection and subsequently renamed 1 model (age =80 years, weight <60 kg, previous fragility fracture, and the need to use arms to rise from the sitting position) the FRAMO Index. We used logistic regression analysis to study the association between the FRAMO Index and outcomes in all participants.\n    \n\n\n          Results:\n        \n      \n      The participation rate was 83% in this elderly female population (N = 1,248). The 63% of women with 0 to 1 risk factor had a 2-year hip fracture risk of 0.8% and mortality risk of 3.2%. In contrast, women with 2 to 4 risk factors had a 2-year hip fracture risk of 5.4% (odds ratio = 7.5; 95% confidence interval, 3.0-18.4) and mortality risk of 23.7% (odds ratio = 9.5; 95% confidence interval, 6.0-14.9). These differences remained significant after adjustment for age as a continuous variable. Mortality increased with the number of risk factors. The proportion of women reporting previous vertebral fractures was higher among the group specifically questioned about vertebral radiographs (P <.001).\n    \n\n\n          Conclusions:\n        \n      \n      The FRAMO Index identified the majority of women who experienced hip fractures during a 2-year follow-up, who might have been candidates for intensified preventive measures. The FRAMO Index, based on 4 binary risk factors, would be practical for routine use in primary care."
        },
        {
            "title": "Quantitative myocardial contrast echocardiography: a new method for the non-invasive detection of chronic heart transplant rejection.",
            "abstract": "Background:\n        \n      \n      Chronic heart transplant rejection, i.e. cardiac allograft vasculopathy (CAV) is a major adverse prognostic factor after heart transplantation (HTx). This study tested the hypothesis that the relative myocardial blood volume (rBV) as quantified by myocardial contrast echocardiography accurately detects severe CAV as defined by coronary intravascular ultrasound (IVUS).\n    \n\n\n          Methods and results:\n        \n      \n      Forty-five HTx patients underwent a total of 50 quantitative IVUS measurements for intima thickness assessment (>1 mm = severe CAV; the reference method). Simultaneously, the two factors constituting myocardial perfusion (mL/min/g) were obtained by transthoracic contrast echocardiography at rest: rBV (the test method), a measure of microvascular density (mL/mL), and its exchange rate β (1/s; a measure of coronary conductance) after mechanical contrast bubble disruption.Sixty-nine per cent (31 of 45) of the HTx patients showed severe CAV. rBV at rest was equal to 0.17 ± 0.05 in the group without severe CAV, and it was equal to 0.12 ± 0.07 in the group with severe CAV (P = 0.0157). Conversely, β amounted to 6.4 ± 4.5 in the former and to 10.3 ± 6.2 in the latter group (P = 0.0410), thus, maintaining normal resting myocardial perfusion at 1 mL/min/g. IVUS determined intima thickness correlated significantly and inversely with rBV at rest. An rBV value at rest <0.14 accurately detected severe CAV (intima thickness >1 mm): area under the receiver operating characteristics curve = 0.844, P = 0.004, sensitivity = 0.90, specificity = 0.75.\n    \n\n\n          Conclusion:\n        \n      \n      Severe CAV can be detected using the non-invasive method of quantitative myocardial contrast echocardiography. rBV at rest amounting to <14% of the surrounding tissue accurately detects coronary intima thickness >1 mm as determined invasively by IVUS.\n    \n\n\n          Clinical trial number:\n        \nNCT00414895."
        },
        {
            "title": "[Combined posterior chamber intraocular lens implantation and trabeculectomy--a life-table analysis of postoperative clinical course].",
            "abstract": "Postoperative courses of 68 eyes of 58 glaucoma patients who underwent posterior chamber intraocular lens implantation combined with trabeculectomy were studied retrospectively. The visual acuity was 0.5 or better in 66% of the eyes at 3 months postoperatively. The average intraocular pressure (IOP) was below 15 mmHg till 16 months postoperatively. An analysis of the postoperative course using the life-table method of Kaplan-Meier revealed that the probability of successful IOP control with medication was 83% at 18 months and 47% without medication. The subjects aged 75 years or older showed significantly higher success probability than those younger than 75 years, while types of glaucoma, the preoperative IOP control, location of conjunctival or sclerocorneal incision, postoperative 5-fluorouracil (5-FU) injections or surgical intervention on the iris had no significant effects on the probability of successful IOP control. The probability of subsistence of functional filtering bleb was 24% at 18-month follow-up, on which postoperative administration of 5-FU of 35 mg or more had a favorable effect."
        },
        {
            "title": "Failure-to-rescue rate as a measure of quality of care in a cardiac surgery recovery unit: a five-year study.",
            "abstract": "Background:\n        \n      \n      Failure to rescue, which is defined as the probability of death after a complication that was not present on admission, was introduced as a quality measure in the 1990s, to complement mortality and morbidity outcomes. The objective of this study was to evaluate possible incremental benefits of measuring failure to rescue after cardiac surgery, to facilitate quality improvement efforts.\n    \n\n\n          Methods:\n        \n      \n      Data were collected prospectively on 4,978 consecutive patients who underwent cardiac operations during a 5-year period. Institutional logistic regression models were used to generate predicted rates of mortality and major complications. Frequency distributions of morbidities were determined, and failure to rescue was calculated. The annual failure-to-rescue rates were contrasted using χ(2) tests and compared with morbidity and mortality measures.\n    \n\n\n          Results:\n        \n      \n      The overall mortality rate was 3.6%, the total complication rate was 16.8%, and the failure-to-rescue rate was 19.8% (95% confidence interval, 17.1% to 22.7%). The predicted risk of mortality and of major complications increased during the last 2 years of the study, whereas the observed complication rate decreased. Failure to rescue for new renal failure was the highest of all complications (48.4%), followed by septicemia (42.6%). Despite the decreased complication rate toward the end of the study, the failure-to-rescue rate did not change significantly (p = 0.28).\n    \n\n\n          Conclusions:\n        \n      \n      Failure to rescue should be monitored as a quality-of-care metric, in addition to mortality and complication rates. Postoperative renal failure and septicemia still have a high failure-to-rescue rate and should be targeted by quality improvement efforts."
        },
        {
            "title": "Predicting survival and early clinical response to primary chemotherapy for patients with locally advanced breast cancer using DCE-MRI.",
            "abstract": "Purpose:\n        \n      \n      To evaluate dynamic contrast-enhanced magnetic resonance imaging (DCE-MRI) as a tool for early prediction of response to neoadjuvant chemotherapy (NAC) and 5-year survival in patients with locally advanced breast cancer.\n    \n\n\n          Materials and methods:\n        \n      \n      DCE-MRI was performed in patients scheduled for NAC (n = 24) before and after the first treatment cycle. Clinical response was evaluated after completed NAC. Relative signal intensity (RSI) and area under the curve (AUC) were calculated from the DCE-curves and compared to clinical treatment response. Kohonen and probabilistic neural network (KNN and PNN) analysis were used to predict 5-year survival.\n    \n\n\n          Results:\n        \n      \n      RSI and AUC were reduced after only one cycle of NAC in patients with clinical treatment response (P = 0.02 and P = 0.08). The mean and 10th percentile RSI values before NAC were significantly lower in patients surviving more than 5 years compared to nonsurvivors (P = 0.05 and 0.02). This relationship was confirmed using KNN, which demonstrated that patients who remained alive clustered in separate regions from those that died. Calibration of contrast enhancement curves by PNN for patient survival at 5 years yielded sensitivity and specificity for training and testing ranging from 80%-92%.\n    \n\n\n          Conclusion:\n        \n      \n      DCE-MRI in locally advanced breast cancer has the potential to predict 5-year survival in a small patient cohort. In addition, changes in tumor vascularization after one cycle of NAC can be assessed."
        },
        {
            "title": "Dental health and functional ageing. A study of 70-year-old people.",
            "abstract": "Functional ageing, including dental health, was studied in the gerontological population study in Gothenburg. This study was based on three 70-yr-old cohorts, born in 1901-2, 1906-7, and 1911-12 (n = 1380). The Eichner index was used as a measurement of deterioration in dental state and dental functional impairment. This impairment was significantly associated with a lower capacity in cognition, visual ability, hearing ability, lung volume, heart volume, muscle strength, and bone mineral content as well as a lower self-assessment of health. A multiple regression model showed that an index of seven functional capacities was the most predictive factor for dental status independent of confounding factors such as socioeconomic factors, tobacco smoking, and the most incapacitating diseases. These associations were more marked in men and the survival rate between 70 and 79 years of age was higher among men with a well preserved dental state of the age of 70. The co-variation between impairment in dental function and other functional variables and survival indicate a common functional ageing."
        },
        {
            "title": "From guaiac to immune fecal occult blood tests: the emergence of technology in colorectal cancer screening.",
            "abstract": "Colorectal cancer is the second leading cause of cancer deaths in the United States for both men and women. Colorectal cancer screening is an important means for reducing morbidity and mortality. The American Cancer Society recommends five different screening regimens for adults of average risk, age 50 years and older. The optimal effectiveness of a screening program is dependent on the accuracy of the screening test used. An accurate screening test would have high sensitivity (positive) when an adenomatous polyp or cancer is present and high specificity (negative) in their absence. In April 2002, the American Cancer Society Colorectal Cancer Advisory Group concluded that the immunochemical fecal occult blood test has some advantages that merit revision of their guideline statement for fecal occult blood testing, to include the immunochemical fecal occult blood test. The advantages cited were the possibility of improved sensitivity and specificity and the lack of required dietary restrictions, which make it a more patient-friendly test. Several types of immunochemical fecal occult blood tests are discussed in this article, including their advantages and disadvantages compared with those of the traditional guaiac fecal occult blood testing."
        },
        {
            "title": "A randomized, placebo-controlled, clinical trial of high-dose supplementation with vitamins C and E and beta carotene for age-related cataract and vision loss: AREDS report no. 9.",
            "abstract": "Background:\n        \n      \n      Experimental and observational data suggest that micronutrients with antioxidant capabilities may retard the development of age-related cataract.\n    \n\n\n          Objective:\n        \n      \n      To evaluate the effect of a high-dose antioxidant formulation on the development and progression of age-related lens opacities and visual acuity loss.\n    \n\n\n          Design:\n        \n      \n      The 11-center Age-Related Eye Disease Study (AREDS) was a double-masked clinical trial. Participants were randomly assigned to receive daily oral tablets containing either antioxidants (vitamin C, 500 mg; vitamin E, 400 IU; and beta carotene, 15 mg) or no antioxidants. Participants with more than a few small drusen were also randomly assigned to receive tablets with or without zinc (80 mg of zinc as zinc oxide) and copper (2 mg of copper as cupric oxide) as part of the age-related macular degeneration trial. Baseline and annual (starting at year 2) lens photographs were graded at a reading center for the severity of lens opacities using the AREDS cataract grading scale.\n    \n\n\n          Main outcome measures:\n        \n      \n      Primary outcomes were (1) an increase from baseline in nuclear, cortical, or posterior subcapsular opacity grades or cataract surgery, and (2) at least moderate visual acuity loss from baseline (>/=15 letters). Primary analyses used repeated-measures logistic regression with a statistical significance level of P =.01. Serum level measurements, medical histories, and mortality rates were used for safety monitoring.\n    \n\n\n          Results:\n        \n      \n      Of 4757 participants enrolled, 4629 who were aged from 55 to 80 years had at least 1 natural lens present and were followed up for an average of 6.3 years. No statistically significant effect of the antioxidant formulation was seen on the development or progression of age-related lens opacities (odds ratio = 0.97, P =.55). There was also no statistically significant effect of treatment in reducing the risk of progression for any of the 3 lens opacity types or for cataract surgery. For the 1117 participants with no age-related macular degeneration at baseline, no statistically significant difference was noted between treatment groups for at least moderate visual acuity loss. No statistically significant serious adverse effect was associated with treatment.\n    \n\n\n          Conclusion:\n        \n      \n      Use of a high-dose formulation of vitamin C, vitamin E, and beta carotene in a relatively well-nourished older adult cohort had no apparent effect on the 7-year risk of development or progression of age-related lens opacities or visual acuity loss."
        },
        {
            "title": "Current cervical cancer screening knowledge, awareness, and practices among U.S. affiliated pacific island providers: opportunities and challenges.",
            "abstract": "Background:\n        \n      \n      Cervical cancer is a leading cause of cancer mortality in nearly all U.S. Affiliated Pacific Island Jurisdictions (USAPIJ); however, most jurisdictions are financially and geographically limited in their capacity to deliver routine screening.\n    \n\n\n          Methods:\n        \n      \n      We conducted a cross-sectional survey of 72 health care providers from five of the six USAPIJ in 2011 to assess knowledge, beliefs, practices, and perceived barriers regarding routine cervical cancer screening. We compared the responses of providers from jurisdictions that were funded by the Centers for Disease Control and Prevention's National Breast and Cervical Cancer Early Detection Program (NBCCEDP) with those that were not funded.\n    \n\n\n          Results:\n        \n      \n      Most providers reported cervical cancer prevention as a priority in their clinical practices (90.3%) and use the Papanicolaou test for screening (86.1%). Many providers reported knowledge of screening guidelines (76.4%); however, more than half reported that annual screening is most effective (56.9%). Providers in non-NBCCEDP-funded jurisdictions reported greater acceptance of visual inspection with acetic acid (93.9%) and self-sampling for human papillomavirus testing (48.5%) compared with NBCCEDP-funded jurisdictions (15.4% and 30.8% respectively). Providers from non-NBCCEDP-funded jurisdictions reported inadequate technological resources for screening women (42.4%), and approximately 25% of providers in both groups believed that screening was cost-prohibitive.\n    \n\n\n          Conclusion:\n        \n      \n      Although cervical cancer screening is a priority in clinical practice, beliefs about annual screening, costs associated with screening, and varying levels of support for alternative screening tests pose barriers to providers throughout the USAPIJ. Further exploration of using evidence-based, lower cost, and sustainable screening technologies is warranted in addition to emphasizing timely follow-up of all positive cases."
        },
        {
            "title": "Atlas of the Global Burden of Stroke (1990-2013): The GBD 2013 Study.",
            "abstract": "Background:\n        \n      \n      World mapping is an important tool to visualize stroke burden and its trends in various regions and countries.\n    \n\n\n          Objectives:\n        \n      \n      To show geographic patterns of incidence, prevalence, mortality, disability-adjusted life years (DALYs) and years lived with disability (YLDs) and their trends for ischemic stroke and hemorrhagic stroke in the world for 1990-2013.\n    \n\n\n          Methodology:\n        \n      \n      Stroke incidence, prevalence, mortality, DALYs and YLDs were estimated following the general approach of the Global Burden of Disease (GBD) 2010 with several important improvements in methods. Data were updated for mortality (through April 2014) and stroke incidence, prevalence, case fatality and severity through 2013. Death was estimated using an ensemble modeling approach. A new software package, DisMod-MR 2.0, was used as part of a custom modeling process to estimate YLDs. All rates were age-standardized to new GBD estimates of global population. All estimates have been computed with 95% uncertainty intervals.\n    \n\n\n          Results:\n        \n      \n      Age-standardized incidence, mortality, prevalence and DALYs/YLDs declined over the period from 1990 to 2013. However, the absolute number of people affected by stroke has substantially increased across all countries in the world over the same time period, suggesting that the global stroke burden continues to increase. There were significant geographical (country and regional) differences in stroke burden in the world, with the majority of the burden borne by low- and middle-income countries.\n    \n\n\n          Conclusions:\n        \n      \n      Global burden of stroke has continued to increase in spite of dramatic declines in age-standardized incidence, prevalence, mortality rates and disability. Population growth and aging have played an important role in the observed increase in stroke burden."
        },
        {
            "title": "Long-term results of penetrating keratoplasty. A 10-year-plus retrospective study.",
            "abstract": "Background:\n        \n      \n      The aim of this study was to retrospectively analyse the outcome of a series of grafted patients over a period of more than 10 years and to determine their long-term survival probability.\n    \n\n\n          Methods:\n        \n      \n      The records of 89 patients who had 103 grafts performed in 97 eyes were analysed. Mean follow-up was 12.8 years (range 10-17 years). Life table analysis (Kaplan-Meier) was used to evaluate the graft survival of the total population and of different groups.\n    \n\n\n          Results:\n        \n      \n      Eighteen out of 89 patients (20.2%) had died. At the last visit before their death, 10 of the 21 grafts in those patients were still clear. Graft survival rates after 1, 2, 5 and 10 years were 79%, 73%, 59% and 50%; the rate at the end of follow-up was 47%. Survival rate at the end of the study was 94.7% for keratoconus, 57.1% herpes keratitis, 33.3% for pseudophakic keratopathies, 28.5% for post-traumatic keratopathies and 11.1% for re-grafts. In the group of patients grafted for aphakic or pseudophakic keratopathy, 40% died during the study. In 45% of cases their grafts were clear at the time of death. Endothelial decompensation and definitive graft rejection were the main causes of failure.\n    \n\n\n          Conclusions:\n        \n      \n      The outcome of keratoplasty is progressively getting worse with time in pseudophakic or traumatic keratopathies whereas survival rates are still stable from 10 to 17 years in grafts performed after keratoconus or herpetic keratitis."
        },
        {
            "title": "Modeling mortality in the intensive care unit: comparing the performance of a back-propagation, associative-learning neural network with multivariate logistic regression.",
            "abstract": "The objective of this study was to compare and contrast two techniques of modeling mortality in a 30 bed multi-disciplinary ICU; neural networks and logistic regression. Fifteen physiological variables were recorded on day 3 for 422 consecutive patients whose duration of stay was over 72 hours. Two separate models were built using each technique. First, logistic and neural network models were constructed on the complete 422 patient dataset and discrimination was compared. Second, the database was randomly divided into a 284 patient developmental dataset and a 138 patient validation dataset. The developmental dataset was used to construct logistic and neural net models and the predictive power of these models was verified on the validation dataset. On the complete dataset, the neural network clearly outperformed the logistic model (sensitivity and specificity of 1 and .997 vs. .525 and .966, area under ROC curve .9993 vs. .9259), while both performed equally well on the validation dataset (area under ROC of .82). The excellent performance of the neural net on the complete dataset reveals that the problem is classifiable. Since our dataset only contained 40 mortality events, it is highly likely that the validation dataset was not representative of the developmental dataset, which led to a decreased predictive performance by both the neural net and the logistic regression models. Theoretically, given an extensive dataset, the neural network should be able to perform mortality prediction with a sensitivity and a specificity approaching 95%. Clinically, this would be an extremely important achievement.(ABSTRACT TRUNCATED AT 250 WORDS)"
        },
        {
            "title": "Colorectal cancer screening: new opportunities.",
            "abstract": "Colorectal cancer is the third most common cancer among men and women in the United States, and the third leading cause of cancer death. Strategies currently available to screen for colorectal cancer include fecal occult blood tests, sigmoidoscopy, or both tests used in combination. Colonoscopy and double contrast barium enema are potentially preferable options because they offer improved sensitivity over currently available tests, but the feasibility of these tests for population screening remains in doubt. Future opportunities for screening include focusing special efforts to deliver screening to higher risk individuals based on family history or age and the use of molecular or computer-aided radiographic techniques as alternatives to colonoscopy."
        },
        {
            "title": "Tuberculum and diaphragma sella meningioma--surgical technique and visual outcome in a series of 20 cases operated over a 2.5-year period.",
            "abstract": "Background:\n        \n      \n      A retrospective analysis of 20 cases of tuberculum sella meningioma with emphasis on the surgical technique and visual outcome.\n    \n\n\n          Methods:\n        \n      \n      Between 2003 and 2006 twenty patients with tuberculum and diaphragma sella meningioma were treated at the Tel Aviv medical center. There were 17 females and 3 males. The age range was 28-83. Most patients presented with visual deterioration. Surgery was performed using the subfrontal approach. The visual function before and after surgery was evaluated as the main outcome parameter of the surgical treatment of these tumours.\n    \n\n\n          Findings:\n        \n      \n      In 16 patients complete tumour resection was achieved and in 4 subtotal removal was performed. Visual acuity improved in 32% of the eyes and deterioration was observed in two eyes (5%). Visual field improved in 28% of the eyes and deteriorated in 14%. There was no complete vision loss as a result of surgery. There was no mortality in our series.\n    \n\n\n          Conclusions:\n        \n      \n      Tuberculum and diaphragma sella meningioma can be safely resected using the subfrontal approach with preservation and even improvement of visual function after surgery. Early surgery with better pre-operation visual function and smaller tumour size were associated with a better outcome."
        },
        {
            "title": "Impulsivity is an independent predictor of 15-year mortality risk among individuals seeking help for alcohol-related problems.",
            "abstract": "Background:\n        \n      \n      Although past research has found impulsivity to be a significant predictor of mortality, no studies have tested this association in samples of individuals with alcohol-related problems or examined moderation of this effect via socio-contextual processes. The current study addressed these issues in a mixed-gender sample of individuals seeking help for alcohol-related problems.\n    \n\n\n          Methods:\n        \n      \n      Using Cox proportional hazard models, variables measured at baseline and Year 1 of a 16-year prospective study were used to predict the probability of death from Years 1 to 16 (i.e., 15-year mortality risk). There were 628 participants at baseline (47.1% women); 515 and 405 participated in the follow-up assessments at Years 1 and 16, respectively. Among Year 1 participants, 93 individuals were known to have died between Years 1 and 16.\n    \n\n\n          Results:\n        \n      \n      After controlling for age, gender, and marital status, higher impulsivity at baseline was associated with an increased risk of mortality from Years 1 to 16; however, this association was accounted for by the severity of alcohol use at baseline. In contrast, higher impulsivity at Year 1 was associated with an increased risk of mortality from Years 1 to 16, and remained significant when accounting for the severity of alcohol use, as well as physical health problems, emotional discharge coping, and interpersonal stress and support at Year 1. In addition, the association between Year 1 impulsivity and 15-year mortality risk was moderated by interpersonal support at Year 1, such that individuals high on impulsivity had a lower mortality risk when peer/friend support was high than when it was low.\n    \n\n\n          Conclusions:\n        \n      \n      The findings highlight impulsivity as a robust and independent predictor of mortality and suggest the need to consider interactions between personality traits and socio-contextual processes in the prediction of health-related outcomes for individuals with alcohol use disorders."
        },
        {
            "title": "[Prevalence of and risk factors for functional dependence in the non-institutionalised elderly population of 11 Italian Regions: results of the Argento Study, 2002.].",
            "abstract": "Introduction:\n        \n      \n      Elderly people who are not capable of performing the basic activities of daily living (ADL) represent a fragile population at greater risk for morbidity and mortality. In order to better describe the size and characteristics of the non self-sufficient population in Italy, we evaluated data from the Argento Study, a survey conducted in 2002 in 11 Italian regions.\n    \n\n\n          Materials and methods:\n        \n      \n      A sample of 210 non-institutionalised elderly individuals aged >65 years was selected in each region (310 in the Campania region) by the cluster sampling technique. Home interviews were performed using a standardised questionnaire which included 6 questions on ADL. Participants were considered to have a severe level of dependence if unable to perform any of the 6 activities of daily living independently, partially dependent if able to perform only 1-5 activities independently, and self-sufficient if able to perform all of the activities. A multivariate analysis was performed to evaluate risk factors associated with functional dependence.\n    \n\n\n          Results:\n        \n      \n      Complete information regarding ADLs was available for 2,355 (99%) of the interviewed subjects. Of these, 78% (95% CI 76-80%) were found to be self-sufficient, 19% (95% CI 18-22%) partially dependent and 3% (95% CI 1.9-3.2%) severely dependent. Twenty percent of self sufficient subjects and 18% of partially dependent subjects lived alone. Multivariate analysis showed a statistically significant association between being either partially or severely dependent and the following factors: age >75 years (OR 2.8), female sex (OR 1.5), having >2 chronic disorders, (OR 2.8), history of ictus (OR 2.8), having a cognitive disorder (OR 2.6), vision problems (OR 2.3) and hearing problems (OR 1.9).\n    \n\n\n          Discussion:\n        \n      \n      These results highlight the presence of a substantial number of partially dependent elderly people that live in the community and that have numerous medical problems and a high frequency of cognitive disorders. It is essential that these fragile elderly subjects be identified, through the active involvement of general practitioners, so that the necessary measures may be undertaken to improve quality of life and of emergency interventions (for example, during heat waves)."
        },
        {
            "title": "Predictors of contrast-induced nephropathy in chronic total occlusion percutaneous coronary intervention.",
            "abstract": "Aims:\n        \n      \n      Contrast-induced nephropathy (CIN) is a leading cause of morbidity and mortality in patients undergoing percutaneous coronary intervention (PCI). Limited data, however, are available on predictors of CIN in PCI for chronic total occlusion (CTO) lesions. The aim of the study was to determine the risk of developing CIN in patients undergoing CTO PCI by studying the effects of clinical variables, interventional techniques, and CTO lesion characteristics on renal function.\n    \n\n\n          Methods and results:\n        \n      \n      This retrospective analysis included consecutive patients referred for CTO PCI between January 2002 and December 2009. CIN was defined as an elevated serum creatinine level ≥25% of baseline serum creatinine level at 48-72 hours after procedure. Patient characteristics, Mehran score, lesion characteristics, interventional procedure, and devices used were compared between CIN and non-CIN groups. For the 516 patients eligible for analysis, the incidence of CIN was 5.4% (28/516). Two patients needed transient haemodialysis (0.4%, 2/516). Analysis of risk using Mehran scoring found that the incidence of CIN was 0.5% (1/207) among low-risk patients, 3.4% (7/205) among moderate-risk patients, 15.9% (14/88) among high-risk patients and 37.5% (6/16) among very high-risk patients. The Mehran score high-risk group (11-15) and the very high-risk group (≥16) were definitely predictors of CIN after CTO PCI (OR: 27.022 [95% CI: 2.787-262.028, p=0.004]; OR: 32.512 [95% CI: 2.149-491.978, p=0.012]). Severe tortuosity was the only predictor of CIN after CTO PCI in angiographic and procedural findings (OR: 6.621 [95% CI: 1.090-40.227, p=0.040]).\n    \n\n\n          Conclusions:\n        \n      \n      Being in the Mehran score high-risk group (11-15) or the very high-risk group (≥16) and severe tortuosity were predictors of CIN after CTO PCI."
        },
        {
            "title": "Estimates of incidence rates with longitudinal claims data.",
            "abstract": "Objective:\n        \n      \n      To estimate incidence rates of the 3 major chronic eye diseases--diabetic retinopathy (DR), glaucoma, and age-related macular degeneration (ARMD)--by using longitudinal claims data from Medicare.\n    \n\n\n          Methods:\n        \n      \n      Longitudinal cases were ascertained by using a national probability sample of Medicare beneficiaries aged 65 years and older in 1991 who initially had none of the eye diseases documented. After adjusting for death and enrollment in a health maintenance organization, claims filed by optometrists or ophthalmologists with an International Classification of Diseases, Ninth Revision, Clinical Modification code for all forms of DR, glaucoma, and ARMD were used to indicate diagnosis.\n    \n\n\n          Results:\n        \n      \n      Annual incidence rates for the 3 conditions after the first year of observation ranged from 14.3% to 17.7% (higher earlier) across an 8-year longitudinal follow-up. Incidence rates among those with diabetes mellitus for any form of DR varied between 3.8% and 6.5%, while those for glaucoma varied between 4.6% and 7.8% and those for ARMD varied between 7.5% and 9.3%.\n    \n\n\n          Conclusions:\n        \n      \n      Longitudinal claims data after the first year provide relatively stable estimates of incidence rates on an annual basis. These estimates are comparable with those of the few population-based studies available."
        },
        {
            "title": "Review of imaging techniques in the diagnosis of hepatocellular carcinoma in patients who require a liver transplant.",
            "abstract": "Objectives:\n        \n      \n      The aim of the study was to retrospectively compare the diagnostic performance of ultrasound (US), contrast-enhanced multidetector computed tomography (MDCT) and contrast-enhanced MRI in cirrhotic patients who were candidates for liver transplantation.\n    \n\n\n          Materials and methods:\n        \n      \n      A total of 273 consecutive patients with 218 hepatocellular carcinoma (HCC) nodules, who underwent imaging and subsequent transplantation, were examined. Diagnosis of HCC was based on explant correlation of the whole liver. Three different imaging data sets were evaluated: US, MDCT and MRI unenhanced and dynamic phases. Diagnostic accuracy, sensitivity, specificity, positive predictive value and negative predictive value, with corresponding 95% confidence intervals, were determined. Statistical analysis was performed for all lesions and for two lesion subgroups (≤2 and >2 cm). Preoperative tumour staging was analysed.\n    \n\n\n          Results:\n        \n      \n      Patient sensitivity to US, MDCT and MRI was 80.4, 81.1 and 90.5%, respectively. Specificity was 96.3, 96.2 and 82.1%. Combined US and MDCT improved sensitivity (88%) without significant loss in specificity (95.7%). Imaging tests resulted in accurate tumour staging in 83.4% of the patients. In per-nodule analysis, technique sensitivity was 55.6, 52.4 and 65.9%, respectively. Sensitivity figures improved when the nodule was larger than 2 cm.\n    \n\n\n          Conclusion:\n        \n      \n      Combining imaging techniques is a good strategy for pretransplant HCC diagnosis and provides more accurate cancer staging in patients, which is necessary to decide the correct therapeutic approach."
        },
        {
            "title": "Fuzzy and crisp set-theoretic-based classification of health and disease. A qualitative and quantitative comparison.",
            "abstract": "Conventional cluster analyses of patient populations are intended to assist in the identification and characterization of groups that may represent etiological or pathological subtypes within a particular disease class. These methods have been criticized as being insensitive to subtle patient differences, which may be masked as a result of the all-or-nothing concept of cluster membership intrinsic to crisp set-theoretic-based grouping algorithms. As an alternative to conventional clustering procedures, several investigators have studied the use of fuzzy classification methods. In general, these measure a patient's clinical status in terms of a real number defined on the closed unit interval, reflecting the extent or degree to which a particular grouping entity characterizes the patient. This paper compares and contrasts the applications of crisp and fuzzy set-theoretic-based clustering procedures to a set of data describing the cognitive and intellectual functioning of a group of subjects participating in a longitudinal study of aging. Emphasis is placed on both qualitative and quantitative aspects corresponding, respectively, to the clinical interpretation of cluster definitions, and the robustness or sensitivity of the classification procedures to changes in patient profiles over time. The fuzzy set-theoretic-based model was found to be more sensitive to changes in subject level of functioning over time, to provide superior quantitative protrayals of patterns of aging, and to reflect properties of the aging process derived from other research."
        },
        {
            "title": "Distinguishing Tumor From Bland Portal Vein Thrombus in Liver Transplant Candidates With Hepatocellular Carcinoma: the A-VENA Criteria.",
            "abstract": "Differentiating tumor versus bland portal vein thrombosis (PVT) is essential in determining liver transplantation (LT) candidacy for patients with hepatocellular carcinoma (HCC). We aimed to evaluate radiographic and clinical features that could noninvasively distinguish tumor PVT from bland PVT in HCC patients. Of 467 patients with HCC listed for LT from 2004 to 2011, 59 (12.6%) had PVT and 12 of 59 (20.3%) were deemed malignant. When comparing tumor versus bland PVT, thrombus enhancement was seen in 100% versus 8.5%; venous expansion was seen in 91.7% versus 10.6%; neovascularity was seen in 58.3% versus 2.1%; and being adjacent to HCC or prior treatment site was seen in 100% versus 21.3% (all P < 0.001). Combining these 4 imaging characteristics with alpha-fetoprotein (AFP) >1000 ng/dL, the presence of ≥3 criteria best characterized tumor PVT with 100% sensitivity, 93.6% specificity, 80% positive predictive value, and 100% negative predictive value. No LT recipients with presumed bland PVT had macrovascular invasion on explant. There were no differences in post-LT survival or HCC recurrence with bland PVT versus no PVT. In conclusion, we proposed noninvasive criteria that could accurately differentiate tumor PVT from bland PVT called A-VENA, which is based on the presence of ≥3 of the following: AFP >1000 ng/dL; venous expansion; thrombus enhancement; neovascularity; and adjacent to HCC. Use of the A-VENA criteria can assist in standardizing the evaluation of PVT in patients with HCC being considered for LT."
        },
        {
            "title": "[Chronic diseases in persons of 60-69 years of age].",
            "abstract": "Objective:\n        \n      \n      The results of the National Survey of Chronic Diseases (Mexico, 1993) regarding the prevalence of hypertension, diabetes mellitus and obesity in the 60 to 69 years old group are presented on a national and regional level.\n    \n\n\n          Material and methods:\n        \n      \n      Measurements taken included weight, height, blood pressure, visual acuity, glycosylated hemoglobin, cholesterol, lipoproteins, insulin, triglycerides and albumin. Data analysis were performed using the statistical package SPSS to carry out Mantel-Haenszel and chi square tests.\n    \n\n\n          Results:\n        \n      \n      Analysis of data for 1239 individuals showed that 38% of the aging population have hypertension. 25% are obese and 21% have diabetes. Findings showed that 28% of the individuals with hypertension and 18% of those with diabetes were detected through the survey. Obesity was strongly associated with hypertension and 33% of cases were not under treatment. Differences on the regional level are presented and results are discussed by sex and risk factors. The risk of diabetes was higher in those with other family members with diabetes, while microalbuminuria and hypercholesterolemia were associated with diabetes in this population.\n    \n\n\n          Conclusions:\n        \n      \n      The results of the study support the need to improve early detection programs and intensify those interventions that prevent early mortality due to these particular diseases."
        },
        {
            "title": "Ocular diseases and 10-year mortality: the Beijing Eye Study 2001/2011.",
            "abstract": "## PURPOSE\nTo examine the relationship between major ocular diseases and mortality.\n## METHODS\nThe population-based longitudinal study Beijing Eye Study was performed in 2001 and repeated in 2011. The participants underwent a detailed ophthalmic examination at baseline in 2001.\n## RESULTS\nOf 4439 subjects examined in 2001, 2695 (60.7%) subjects returned for the follow-up examination in 2011, while 379 (8.5%) subjects were dead and 1365 (30.8%) subjects were alive, however, did not agree to be re-examined. In multivariate regression analysis, mortality was significantly associated with the systemic parameters of older age (p < 0.001; Odds ratio (OR): 1.07; 95% confidence interval (CI): 1.05, 1.09), male gender (p < 0.001; OR: 0.56; 95% CI: 0.40, 0.78), lower level of education (p < 0.001; OR: 0.66; 95% CI: 0.59, 0.74) and smoking (p < 0.001; OR: 1.84; 95% CI: 1.36, 2.49) and with the ocular parameters of presence of diabetic retinopathy (p = 0.002; OR: 2.26; 95% CI: 1.34, 3.81), non-glaucomatous optic nerve damage (p = 0.001; OR: 4.90; 95% CI: 1.90, 12.7) and higher degree of nuclear cataract (p = 0.002; OR: 1.29; 95% CI: 1.10, 1.52). In that model, mortality was not significantly (all p > 0.05) associated with refractive error, cortical or subcapsular posterior cataract, intraocular pressure, best corrected visual acuity, visual field defects, prevalence of age-related macular degeneration, retinal vein occlusions, open-angle glaucoma and angle-closure glaucoma.\n## CONCLUSIONS\nAfter adjustment for age, gender, level of education and smoking, mortality was significantly higher in subjects with diabetic retinopathy, non-glaucomatous optic nerve damage and nuclear cataract. Other major ophthalmic parameters and disorders such as hyperopia, myopia, high myopia, pterygium, age-related macular degeneration, retinal vein occlusion, glaucoma and cortical or nuclear cataract were not significantly associated with mortality in the multivariate analysis.\n"
        },
        {
            "title": "Advances in oral cancer detection.",
            "abstract": "High incidence of oral carcinoma and its late-stage presentation are the major global healthcare issues. The World Health Organization (WHO) has set early diagnosis and prevention of oral cancer as their primary objective. It is important to consider the time of oral screening, as it plays a pivotal role in understanding the disease prognosis. Critical signs and symptoms that can be identified during initial oral screening can improve the chances of patient's survival. Reports suggest that socio-economic factors, lack of public awareness and delays from primary health care centers are few of the major parameters that contribute to patient's mortality and morbidity. Conventional technique of visual examination of the oral lesion can effectively monitor patient mortality when exposed to risk factors. However, several disadvantages limit the clinical utility of this technique. Thus, screening aids that efficiently differentiate between a benign and malignant lesion as well as deliver information about early OSCC can ameliorate the complications associated with oral cancer diagnosis. Recent advances in optical imaging systems, such as tissue-fluorescence imaging and optical coherence tomography have been proved to be considerably efficient. Additionally, extensive research has been directed towards nanoparticle-based immunosensors, DNA analysis, and salivary proteomics. However, lack of proper clinical trials and correlation with biopsy result hinder the usage of these screening techniques in clinics. In this review, we highlight the importance of early diagnosis of oral cancer as well as discuss about the effectiveness and limitations of the recent diagnostic aids. It can be stated that public awareness regarding routine oral examination and employing screening methods that are non-invasive, robust, and economic, would enhance early stage diagnosis of oral cancer and have a positive impact on patient's survival."
        },
        {
            "title": "Natural history and effects on 2-year outcomes of urinary incontinence after stroke.",
            "abstract": "Background and purpose:\n        \n      \n      We sought to describe the natural history of poststroke incontinence and estimate its effect on survival and 2-year outcomes in stroke survivors.\n    \n\n\n          Methods:\n        \n      \n      Two hundred thirty-five incident cases of stroke in 1995 were classified by continence status at 10 days after stroke. Age, sex, ethnicity, diabetes, hypertension, atrial fibrillation, premorbid disability, and Oxfordshire Community Stroke Project classification were recorded. Outcome data collected at 3 months and at 1 and 2 years included disability, case-fatality rates, and institutionalization rates. Disability was classified as severe, moderate, mild, or independent using the Barthel Index (without its \"continence\" component: 0-9, 10-14, 15-17, and 18, respectively) and Frenchay Activity Index (0-15, 16-30, and 31-45).\n    \n\n\n          Results:\n        \n      \n      Of 235 cases, 95 were initially incontinent (group 1); 140 were continent (group 2). At the initial, 3-month, and 1- and 2-year assessments, incontinence was recorded in 95 patients (40%), 34 (19%), 23 (15%), and 12 (10%), respectively. In univariate analyses, the 2 groups were not different in terms of demographic factors and risk factors. Compared with group 2, group 1 patients were more likely to have atrial fibrillation (28% versus 16%; P:=0.02). Multivariate analyses showed that age >75 years (OR 15.9; CI 2.2 to 116.2), dysphagia (OR 4.03; CI 1.85 to 8.73), motor weakness (OR 5.41; CI 1.38 to 21.1) and visual field defects (OR 4.78; CI 1.78 to 12.9) were all significantly associated with incontinence. Incontinence was less common in lacunar infarctions (OR 0.12; CI 0.02 to 0.62). At 2 years, compared with group 2, group 1 had higher case-fatality rates (67% versus 20%; P:<0.001), higher institutionalization rates (39% versus 16%; P:=0.007), and greater disability (Barthel [0-9]: 39% versus 5%; P:<0.001; Frenchay [0-15]: 75% versus 37%; P:=0.001). Death or disability at 2 years was worse in subjects with initial incontinence(OR 4.43; CI 1.76 to 11.2).\n    \n\n\n          Conclusions:\n        \n      \n      Incontinence remains a prevalent condition 2 years after stroke. Initial incontinence was associated with age >75 years, dysphagia, visual field defect, and motor weakness. Poststroke incontinence adversely affected 2-year stroke survival, disability, and institutionalization rates."
        },
        {
            "title": "Glaucoma and mortality.",
            "abstract": "Purpose:\n        \n      \n      To compare mortality rates in glaucoma patients and matched controls from a large population screening as well as glaucoma patients diagnosed through routine clinical examination (self-selected patients).\n    \n\n\n          Methods:\n        \n      \n      A population-based screening of 32,918 elderly citizens of Malmö was conducted between 1992 and 1997. Individuals with newly detected, previously untreated open-angle glaucoma were identified. Two controls of the same age and gender were chosen among the screening negative participants for each patient. From the same birth cohorts, glaucoma patients seen in routine clinical practice (self-selected patients) were identified through retrospective examination of patient records from the Eye Department at Malmö University Hospital. The number and time of deaths for each group were determined based on centrally administered registers.\n    \n\n\n          Results:\n        \n      \n      Mean follow-up time was 7.75 years. Five-year mortality did not differ significantly between the groups, and was 9.2% among glaucoma patients from the screening (n=402), and 11.9% among the controls (n=804; p=0.7406). Self-selected glaucoma patients had a 5-year mortality of 8.5% (n=354), not significantly different from the screening-detected glaucoma patients (p=0.1361). Among glaucoma patients, neither IOP (p=0.1781) nor pseudoexfoliation (p=0.8882) was related to significantly increased mortality.\n    \n\n\n          Conclusions:\n        \n      \n      The results of this study strongly suggest that the life expectancy of glaucoma patients does not differ from the population at large."
        },
        {
            "title": "CMR imaging for the evaluation of myocardial stunning after acute myocardial infarction: a meta-analysis of prospective trials.",
            "abstract": "Background:\n        \n      \n      Myocardial stunning is an important sequela of acute coronary syndromes and its determination might affect decisions on defibrillator implantation and assist devices after myocardial infarction (AMI). The aim of the study was to evaluate and compare the sensitivity, specificity, negative predictive value (NPV), and positive predictive value (PPV) of cardiac magnetic resonance imaging (CMR) assessing myocardial stunning after acute myocardial infarction using low-dose dobutamine (LDD), end-diastolic wall thickness, and contrast delayed enhancement (DE).\n    \n\n\n          Methods and results:\n        \n      \n      A systematic review of Medline, Embase, and Cochrane for all prospective trials assessing myocardial stunning by CMR following AMI was performed using a standard approach for meta-analysis for diagnostic test and a bivariate analysis. Search results revealed 9384 studies, out of which 17 met criteria. A total of 634 patients (mean age 59 years, 85% male, mean left ventricular ejection fraction: 52%) were included. DE-CMR had a weighted sensitivity of 87% and specificity of 68% to detect myocardial stunning using 50% transmurality as a cut-off, with a PPV and NPV of 83 and 72%, respectively. With an overall diagnostic accuracy of 82%, LDD-CMR had a sensitivity of 67% and a specificity of 81%, with a PPV and NPV of 82 and 63%, respectively. LDD showed an overall accuracy of 74%.\n    \n\n\n          Conclusion:\n        \n      \n      DE-CMR has a higher sensitivity, whereas LDD-CMR has a higher specificity for the detection of viable stunned myocardium following myocardial infarction. Whether the combination of DE and LDD may improve the prediction of myocardial recovery remains to be determined."
        },
        {
            "title": "Latanoprost could exacerbate the progression of presbyopia.",
            "abstract": "Purpose:\n        \n      \n      Prostaglandin analogues (PG) reduce intra-ocular pressure by enhancing uveoscleral flow at the ciliary body, which controls accommodation via the ciliary muscle. We investigated the effect of PG on accommodation and presbyopia progression in glaucoma patients.\n    \n\n\n          Methods:\n        \n      \n      We conducted a clinic-based, retrospective, cross-sectional study. Inclusion criteria were bilateral phakic patients aged 40-69 years with best corrected visual acuity better than 20/30. Exclusion criteria were any disease affecting vision other than glaucoma and history of ocular surgery. Subjects with no prescription or vision-affecting disease served as controls (n = 260). The glaucoma patients were prescribed eye drops containing 0.005% latanoprost for more than six months (n = 23). We measured the binocular near add power at a distance of 30 cm in both groups and compared the results using Kaplan-Meier analysis.\n    \n\n\n          Results:\n        \n      \n      The mean age (± SD) of the control subjects was 51.5 ± 5.2 years and 39% were male. Similarly, the glaucoma patients had a mean age of 51.0 ± 7.2 years and 39% were male. There were no significant differences in age, gender, intra-ocular pressure, spherical equivalent, astigmatism, or anisometropia between groups. Survival analysis indicated that the glaucoma patients in this study reached the endpoint (near add power of +3.00 D) significantly earlier than control patients (P = 0.0001; generalized Wilcoxon test).\n    \n\n\n          Conclusions:\n        \n      \n      Exacerbation of presbyopia progression in glaucoma patients is a potential side effect of latanoprost eyedrops."
        },
        {
            "title": "Lifetime visual prognosis for patients with primary open-angle glaucoma.",
            "abstract": "Aim:\n        \n      \n      To investigate final visual outcome in primary open angle glaucoma (POAG) including low-tension glaucoma (LTG).\n    \n\n\n          Methods:\n        \n      \n      Retrospective review of case notes for patients who died between 1999 and 2002. All were booked for a follow-up appointment in glaucoma clinic at time of death.\n    \n\n\n          Results:\n        \n      \n      A total of 121 case notes were reviewed. In all, 113 patients had POAG and eight had LTG. All were White Caucasians. Mean ages at presentation and death were 74.6 (SD 9.6, range 49-94) and 81.9 (SD 8.3, range 51-98) years, respectively. Mean follow-up duration was 7.4 (SD 6.8, range up to 29) years. Average number of clinic visits was 18 (SD 17, range 1-95). At final visit, 50.4% had cataract operations, and 45.5% had glaucoma operations. At final visit, vision was inadequate for driving in the UK in 47.1%. In 18.2%, this was due to glaucoma alone, while in 28.9%, other ocular pathologies contributed to poor vision. In all, 14% were eligible for partial sight certification, with 6.6% due to glaucoma alone. A total of 3.3% were eligible for blind certification, none due to glaucoma alone.\n    \n\n\n          Conclusion:\n        \n      \n      This study shows that POAG does affect the quality of life, with regards to glaucoma clinic visits, eye drops, and surgical procedures. Most patients with treated POAG in Norfolk will retain useful vision for their whole life. A significant proportion of patients with POAG do lose vision resulting in driving ineligibility and certification as visually impaired, although actual blindness is uncommon."
        },
        {
            "title": "Current status of vulnerable plaque detection.",
            "abstract": "Critical coronary stenoses have been shown to contribute to only a minority of acute coronary syndromes (ACS) and sudden cardiac death. Autopsy studies have identified a subgroup of high-risk patients with disrupted vulnerable plaque and modest stenosis. Consequently, a clinical need exists to develop methods to identify these plaques prospectively before disruption and clinical expression of disease. Recent advances in invasive and noninvasive imaging techniques have shown the potential to identify these high-risk plaques. The anatomical characteristics of the vulnerable plaque such as thin cap fibroatheroma and lipid pool can be identified with angioscopy, high frequency intravascular ultrasound, intravascular MRI, and optical coherence tomography. Efforts have also been made to recognize active inflammation in high-risk plaques using intravascular thermography. Plaque chemical composition by measuring electromagnetic radiation using spectroscopy is also an emerging technology to detect vulnerable plaques. Noninvasive imaging with MRI, CT, and PET also holds the potential to differentiate between low and high-risk plaques. However, at present none of these imaging modalities are able to detect vulnerable plaque neither has been shown to definitively predict outcome. Nevertheless in contrast, there has been a parallel development in the physiological assessment of advanced atherosclerotic coronary artery disease. Thus recent trials using fractional flow reserve in patients with modest non flow-limiting stenoses have shown that deferral of PCI with optimal medical therapy in these patients is superior to coronary intervention. Further trials are needed to provide more information regarding the natural history of high-risk but non flow-limiting plaque to establish patient-specific targeted therapy and to refine plaque stabilizing strategies in the future."
        },
        {
            "title": "Predicting death from coronary heart disease using a questionnaire.",
            "abstract": "The ten-year coronary heart disease (CHD) mortality is reported for 18,322 male civil servants aged 40 to 64 according to questionnaire responses at entry into the Whitehall study. In all 1714 died, 723 from CHD. The predictive power of the questionnaire was examined with a view to its use as a screening tool in population studies. In predicting death from coronary heart disease the greatest specificity (true negative rate) was achieved with men reporting both angina (A) and a history of severe chest pain (possible myocardial infarction, PMI). This strategy (A plus PMI) achieved a specificity of 99% but a sensitivity (true positive rate) of only 7%. In contrast, in men reporting angina and/or PMI, specificity was 90% and sensitivity 29%. If this 'and/or' algorithm was extended to include the report of dyspnoea, diabetes, and/or attending a primary care physician with heart disease or hypertension, then specificity was still 85%, but sensitivity increased to 44%. This combination (11 questions in all) is therefore recommended for screening purposes. Identifying and excluding those who favour positive answers ('yes-set' responders), using questions such as the effect of weather on breathing, led to small increases in specificity but relatively large falls in sensitivity. Among subjects reporting chest pain, those who also complained of non-specific symptoms experienced only half the mortality of those with none of these additional complaints."
        },
        {
            "title": "Estimating the Years Lived With and Without Age-Related Sensory Impairment.",
            "abstract": "Background:\n        \n      \n      The aim of this study was to estimate the expected years lived with hearing impairment, vision impairment, and dual sensory impairment among older adults.\n    \n\n\n          Methods:\n        \n      \n      A total of 4,160 adults (45.1% men) from two Australian community based studies were followed for up to 16 years (average 8.9 years). Hearing impairment was defined by a pure-tone average (500-4000 Hz) greater than 25 dB in the better ear. Vision impairment was defined by presenting distance visual acuity worse than 6/12 (20/40). Postliminary analyses were also conducted for moderate levels of sensory impairment. Dual sensory impairment was defined by concurrent hearing and vision impairment. Multistate Markov models were used to calculate sensory life expectancies based on transition probabilities between health states (no sensory impairment, sensory impairment, and death).\n    \n\n\n          Results:\n        \n      \n      Based on thresholds for mild impairment, men aged 65 had a total life expectancy of 19.4 years, and were estimated to live for 10.4 years (95% confidence interval [CI]: 9.1, 11.7) with hearing impairment, 2.8 years (95% CI: 2.4, 3.2) with vision impairment, and 2.2 years (95% CI: 1.8, 2.6) with dual sensory impairment. Women aged 65 had a total life expectancy of 23.2 years, and were estimated to live for 12.9 years (95% CI: 11.9, 13.9) with hearing impairment, 3.9 years (95% CI: 3.4, 4.4) with vision impairment, and 3.2 years (95% CI: 2.7, 3.7) with dual sensory impairment.\n    \n\n\n          Conclusions:\n        \n      \n      In addition to being highly prevalent, hearing and vision impairment affect older adults for substantial periods of their remaining life. Given their broad ranging impacts on health and well-being, sensory impairments are ideal targets for strategies to compress morbidity in late life."
        },
        {
            "title": "Vision and driving self-restriction in older adults.",
            "abstract": "Objectives:\n        \n      \n      To assess driving self-restriction (vision related and nonvision related) in relation to vision test performance of older adults.\n    \n\n\n          Design:\n        \n      \n      Cross-sectional study.\n    \n\n\n          Setting:\n        \n      \n      Population-based cohort of community-dwelling older adults.\n    \n\n\n          Participants:\n        \n      \n      Six hundred twenty-nine current drivers aged 55 and older had driving behavior, health, and physical function assessed and vision function tested in 1993-95.\n    \n\n\n          Measurements:\n        \n      \n      Self-report of driving restriction as vision or non-vision related and performance on a comprehensive battery of vision tests (visual acuity; contrast sensitivity; effects of illumination level, contrast, and glare on acuity; visual fields with and without attentional load; color vision; temporal sensitivity; and the effect of dim light on walking ability).\n    \n\n\n          Results:\n        \n      \n      Demographic, health, and functional characteristics differed significantly between restrictors and nonrestrictors but not between vision- and nonvision-related restrictors. Controlling for potential confounding, only vision-related driving self-restriction was significantly associated with reduced performance on nonstandard measures of acuity. Poor depth perception was significantly associated with restriction for both vision- and nonvision-related reasons. Poor performance on attentional visual field tests, analyzed individually and in combination with standard field tests, was not associated with driving self-restriction.\n    \n\n\n          Conclusion:\n        \n      \n      Older adults with early changes in spatial vision function and depth perception appear to recognize their limitations and restrict their driving even if they do not acknowledge the visual impairment as the cause for restriction. Poor visual attention, a risk factor for crashes, may not be recognized. Additional studies of driving self-restriction in relation to risk factors for crashes in older adults may help refine this strategy of reducing driving-related injury and death."
        },
        {
            "title": "Impact of bifurcation lesions on angiographic characteristics and procedural success in primary percutaneous coronary intervention for ST-segment elevation myocardial infarction.",
            "abstract": "Background:\n        \n      \n      Bifurcation lesions (BFLs) remain a challenging lesion subset, often associated with lower success rates than less complex lesions. There are few data regarding the impact of BFLs in the setting of ST-segment elevation myocardial infarction (STEMI).\n    \n\n\n          Aims:\n        \n      \n      To assess the impact of BFLs on angiographic characteristics and procedural success in primary percutaneous coronary interventions (PCIs).\n    \n\n\n          Methods:\n        \n      \n      Out of 1070 primary PCIs performed between November 2006 and December 2008, 114 patients (10.7%) with a BFL (side branch ≥2.0mm) were identified and matched with 114 patients without a BFL, according to age, sex and infarct-related artery.\n    \n\n\n          Results:\n        \n      \n      Baseline characteristics were similar in both groups. Using the Medina classification, true BFLs ([1,1,1]; [1,0,1]; [0,1,1]) were found in 46.5% of cases. Mean contrast volume (265±91 and 207±68mL), procedural time (51.0±26.6 vs 35.3±11.5min) and fluoroscopy time (16.2±11.2 vs 9.8±5.1min) were significantly higher in the BFL group than the non-BFL group (p<0.0001). However, time to reperfusion and angiographic success rates (residual stenosis ≤ 30% and Thrombolysis in Myocardial Infarction flow grade 3 in main branch) were similar in BFL and non-BFL patients (13.7±7.9 vs 12.1±5.7min, respectively, p=0.087; 96.5 vs 99.1%, respectively, p=0.18), with no periprocedural events (in-hospital death, emergent coronary artery bypass graft or repeat PCI<24h).\n    \n\n\n          Conclusion:\n        \n      \n      Despite being challenging lesions, BFLs in STEMI were associated with similar time to reperfusion and procedural success but led to significantly greater contrast use and prolonged procedural time compared with non-BFLs."
        },
        {
            "title": "Clinical Outcomes of Proton Radiotherapy for Uveal Melanoma.",
            "abstract": "Aims:\n        \n      \n      Although clinical experience with proton beam radiotherapy (PBT) for most tumours is limited, there is relatively longstanding experience for uveal melanomas. Because of potential to reduce ocular toxicities, PBT is an attractive option for these tumours. However, summative data remain scarce. We systematically reviewed clinical outcomes of uveal melanoma patients treated with PBT, to comprehensively assess outcomes such as tumour control, survival, enucleation rates, toxicity and visual acuity preservation.\n    \n\n\n          Materials and methods:\n        \n      \n      A systematic search of PubMed, EMBASE, abstracts from meetings of the American Societies for Radiation Oncology and Clinical Oncology, and the Particle Therapy Co-Operative Group was conducted from 2000 to 2015. Fourteen original investigations from 10 different institutions were analysed.\n    \n\n\n          Results:\n        \n      \n      Most tumours were choroidal and medium-/large-sized, and received 50-70 Cobalt Gray equivalent dose; more recent data reported lower doses. Five year local control rates exceed 90%, which persisted at 10 and 15 years. Five-year overall survival rates ranged from 70 to 85%, 5 year metastasis-free survival and disease-specific survival rates from 75 to 90%, with more recent series reporting higher values. With the removal of smaller studies, 5 year enucleation rates were consistently between 7 and 10%. Many patients (60-70%) showed a post-PBT visual acuity decrease, but still retained purposeful vision (>20/200); more recent, higher-volume series reported superior numbers. Complication rates were quite variable but showed improvements on historical plaque brachytherapy data. Only one randomised trial directly compared particle therapy (helium) with plaque brachytherapy, showing the former to be superior; this is addressed separately.\n    \n\n\n          Conclusions:\n        \n      \n      PBT is an excellent modality to treat uveal melanomas, with high survival outcomes and visual acuity preservation. Although there are low toxicity and enucleation rates, the recent development of supportive therapies for radiation toxicities can further decrease clinical adverse effects."
        },
        {
            "title": "Quantitative assessment of structural damage in eyes with localized visual field abnormalities.",
            "abstract": "Purpose:\n        \n      \n      To evaluate the pattern of structural damage in the macula and peripapillary retinal nerve fiber layer (RNFL) using optical coherence tomography (OCT) and scanning laser polarimetry (SLP-VCC) in glaucomatous eyes with localized visual field defects.\n    \n\n\n          Design:\n        \n      \n      Prospective, cross-sectional analysis.\n    \n\n\n          Methods:\n        \n      \n      Complete examination, automated achromatic perimetry (AAP), Stratus OCT imaging (512 A-scans) of the peripapillary retina and macula, and SLP-VCC imaging of the peripapillary RNFL were performed. Thickness values in the retinal segments associated with the visual field defect (glaucomatous segments) were compared with corresponding segments across the horizontal raphe (nonglaucomatous segments) and age-matched normal controls.\n    \n\n\n          Results:\n        \n      \n      Forty eyes of 40 patients (20 normal, 20 glaucomatous) were enrolled (mean age, 71 +/- 10 years; range, 50 to 89). Mean RNFL thickness using SLP-VCC and OCT in the nonglaucomatous segments of glaucomatous eyes (54.0 +/- 9.7 microm, 64.7 +/- 19.0 microm) were significantly (P =.009, <0.0001) reduced compared with the thickness measurements in the corresponding segments of age-matched normal subjects (62.5 +/- 9.2 microm, 105.6 +/- 19.0 microm) respectively. No significant (P =.4) differences in the macular thickness measurements were observed between nonglaucomatous (239.0 +/- 19.4 microm) and normal segments (243.5 +/- 15.0 microm). Compared with age-matched controls, RNFL thickness in the nonglaucomatous segment was abnormal in 15 of 20 patients (75%) with SLP-VCC and in 18 of 20 patients (90%) with OCT. Macular thickness in the nonglaucomatous segment was abnormal in 11 of 20 patients (55%).\n    \n\n\n          Conclusions:\n        \n      \n      Diffuse RNFL and retinal ganglion cell loss is present in eyes with localized visual field abnormalities. Detection of localized changes in macular thickness is limited by measurement overlap among normal and glaucomatous eyes."
        },
        {
            "title": "An analysis of excess mortality rates for persons with non-insulin-dependent diabetes mellitus in Western Australia using the Cox proportional hazards regression model.",
            "abstract": "A cohort of 888 rural, nonaboriginal persons with non-insulin-dependent diabetes mellitus identified in Western Australia through surveys in 1978-1982 were followed for death until the end of 1986. A total of 257 deaths were observed. Excess mortality in this cohort as compared with the general Australian population was investigated by calculating standardized mortality ratios and using the Cox proportional hazards regression model with hazard rates for the general population as the baseline. The overall standardized mortality ratio was 1.83 (95% confidence interval 1.51-2.16) for women and 1.43 (95% confidence interval 1.18-1.67) for men. Cause-specific comparisons with the general population showed that the majority of excess deaths could be attributed to diseases of the circulatory system. Factors assessed at the baseline survey that were independently prognostic of shorter survival were early onset of diabetes (for females only), high plasma glucose level, retinopathy, macrovascular disease, albuminuria (for females only), and elevated plasma creatinine level. Reductions in life expectancy at 60 years of age as compared with the general population averaged about 5 years but could be as much as 16 years for female diabetics with early onset of diabetes, high plasma glucose levels, and several complications."
        },
        {
            "title": "[Clinical results of fornix-based trabeculectomy with a scleral tunnel].",
            "abstract": "Purpose:\n        \n      \n      To evaluate retrospectivery the efficacy and safety of fornix-based trabeculectomy with a scleral tunnel.\n    \n\n\n          Patients and methods:\n        \n      \n      We studied the records of 204 eyes of 156 patients who underwent fornix-based trabeculectomy with mitomycin C as their primary surgery between 2000 and 2002 and had a follow-up period of 6 months or more. A 3.5 or 4 mm rectangular double scleral flap incision was made and a scleral tunnel was fashioned by removing the second flap to allow the aqueous to flow into the fornix side.\n    \n\n\n          Results:\n        \n      \n      The mean intraocular pressure was significantly decreased from 22.2 +/- 7.8 (mean +/- standard deviation) mmHg to 12.4 +/- 3.9 mmHg 2 years after surgery (p < 0.0001). When the target pressure was defined as 15 mmHg, the 2-year survival rate using the Kaplan-Meier survival analysis was 69.1 %. Early wound leakages occurred in 16 eyes (7.8%) and additional sutures were needed on 13 eyes. The visual acuity of 22 eyes (11.0%) decreased by at least 2 lines.\n    \n\n\n          Conclusion:\n        \n      \n      Although there are some complications specific to trabeculectomy, fornix-based trabeculectomy with a scleral tunnel appears to be an effective method of decreasing intraocular pressure."
        },
        {
            "title": "Risk scoring system to predict contrast induced nephropathy following percutaneous coronary intervention.",
            "abstract": "Background:\n        \n      \n      Contrast induced nephropathy (CIN) is associated with significant morbidity and mortality after percutaneous coronary intervention (PCI). The aim of this study is to evaluate the collective probability of CIN in Indian population by developing a scoring system of several identified risk factors in patients undergoing PCI.\n    \n\n\n          Methods:\n        \n      \n      This is a prospective single center study of 1200 consecutive patients who underwent PCI from 2008 to 2011. Patients were randomized in 3:1 ratio into development (n = 900) and validation (n = 300) groups. CIN was defined as an increase of ≥25% and/or ≥0.5 mg/dl in serum creatinine at 48 hours after PCI when compared to baseline value. Seven independent predictors of CIN were identified using logistic regression analysis - amount of contrast, diabetes with microangiopathy, hypotension, peripheral vascular disease, albuminuria, glomerular filtration rate (GFR) and anemia. A formula was then developed to identify the probability of CIN using the logistic regression equation.\n    \n\n\n          Results:\n        \n      \n      The mean (±SD) age was 57.3 (±10.2) years. 83.6% were males. The total incidence of CIN was 9.7% in the development group. The total risk of renal replacement therapy in the study group is 1.1%. Mortality is 0.5%. The risk scoring model correlated well in the validation group (incidence of CIN was 8.7%, sensitivity 92.3%, specificity 82.1%, c statistic 0.95).\n    \n\n\n          Conclusion:\n        \n      \n      A simple risk scoring equation can be employed to predict the probability of CIN following PCI, applying it to each individual. More vigilant preventive measures can be applied to the high risk candidates."
        },
        {
            "title": "Colorectal cancer screening: clinical applications.",
            "abstract": "Screening for colorectal cancer reduces mortality in individuals aged 50 years or older. A number of screening tests, including fecal occult blood tests, sigmoidoscopy, double-contrast barium enema, and colonoscopy, are recommended by professional organizations for colorectal cancer screening, yet the rates of colorectal cancer screening remain low. Questions regarding the quality of evidence for each screening test, whether screening for individuals at higher risk should be modified, the availability of the tests, and cost-effectiveness are addressed. Many potential barriers to colorectal cancer screening exist for the patient and the physician. Strategies to increase compliance for colorectal cancer screening are proposed."
        },
        {
            "title": "Evaluation of Anastomotic Leak after Esophagectomy for Esophageal Cancer: Typical Time Point of Occurrence, Mode of Diagnosis, Value of Routine Radiocontrast Agent Studies and Therapeutic Options.",
            "abstract": "Background:\n        \n      \n      Data on the typical time point of occurrence of anastomotic leak (AL) after esophagectomy for esophageal cancer are currently scarce. Therefore, the usefulness of routine radiocontrast agent studies (RRCS) for testing proper healing of the anastomosis after esophagectomy remains unclear. Furthermore, preferred available tools to diagnose postoperative AL and therapeutic options are still under debate.\n    \n\n\n          Methods:\n        \n      \n      We present a retrospective analysis of 328 consecutive patients who underwent esophagectomy for esophageal cancer between 2005 and 2015. A RRCS has been performed to date in our center on the fifth postoperative day (POD), before returning to normal oral intake.\n    \n\n\n          Results:\n        \n      \n      In total, 49 of 328 patients developed AL after esophagectomy (15%). A total of 11 patients (23%) developed AL before the RRCS and 34 patients (69%) after an unremarkable RRCS; and 4 patients (8%) with AL were diagnosed by RRCS, resulting in overall sensitivity of 16%. The median time point of occurrence of AL was POD 9, the majority of AL (84%) occurred between POD 1 and 19. Computed tomography led to the diagnosis of AL in 41% of patients. The most frequent therapy of AL was stenting in 47% of patients. Endoscopic vacuum therapy was used in 4 patients.\n    \n\n\n          Conclusions:\n        \n      \n      The majority of AL occurred within the first 3 weeks after esophagectomy without a typical time point. In our series, RRCS on the fifth POD had a low sensitivity of 16%. Therefore, standardized RRCS and fasting till the examination cannot be generally recommended. In case of clinical suspicion of AL, computed tomography of the chest and abdomen with oral contrast agent should be performed, followed by endoscopy. Endoscopic stent placement remains the standard therapy of AL in our center. Endoscopic vacuum therapy evolves as it is an interesting alternative therapeutic option and can be combined with stenting in selected cases."
        },
        {
            "title": "Coronary flow velocity pattern and coronary flow reserve by contrast-enhanced transthoracic echocardiography predict long-term outcome in heart transplantation.",
            "abstract": "Background:\n        \n      \n      We assessed coronary flow velocity pattern and coronary flow reserve (CFR) by contrast-enhanced transthoracic echocardiography (CE-TTE) as markers of major adverse cardiac events (MACE) related to cardiac allograft vasculopathy (CAV) after heart transplantation (HT).\n    \n\n\n          Methods and results:\n        \n      \n      Deceleration time of diastolic flow velocity (DDT) and CFR were measured in the left anterior descending coronary artery (LAD) by CE-TTE in 66 consecutive HT patients (follow-up 19+/-5 months). CFR was calculated as the ratio of hyperemic to basal diastolic flow velocity. Angiographies were analyzed by a qualitative grading system; CAV was defined as changes grade II or higher. MACE were cardiac death, stent implantation, and heart failure. Patients with MACE had higher CAV incidence (P=0.004) and grade (P=0.008), shorter DDT (P=0.006), and lower CFR (P=0.008). A receiver-operating characteristic-derived DDT cutpoint < or = 840 ms (area under the curve 0.793; P=0.01) was 75% specific and 86% sensitive for predicting MACE, with positive predictive value (PPV) and negative predictive value (NPV) of 33% and 97%, respectively (P=0.002). A CFR cutpoint of < or =2.6 (area under the curve 0.746; P=0.01) was 62% specific and 91% sensitive for predicting MACE (PPV =32%, NPV =97%) (P=0.001). Patients with CFR < or = 2.6 and patients with DDT < or = 840 ms had a lower survival free from MACE (P=0.006 and P=0.009, respectively). By Cox regression, only a lower CFR predicted the risk of MACE (relative risk 3.1; 95% CI, 1.26 to 7.9; P=0.01).\n    \n\n\n          Conclusions:\n        \n      \n      In HT patients, shorter DDT and lower CFR by CE-TTE are reliable markers for CAV-related MACE. CFR is the main independent predictor of MACE."
        },
        {
            "title": "Complement Factor H polymorphism Y402H associates with inflammation, visual acuity, and cardiovascular mortality in the elderly population at large.",
            "abstract": "The haplotype tagging Y402H polymorphism in the Complement Factor H gene (CFH) has consistently been associated with age-related macular degeneration, whereas conflicting results have been reported on its relationship with cardiovascular disease. CFH plays a role in inflammation, which is causal to both diseases and both are highly prevalent in old age. Therefore, we investigated whether or not Y402H associated with inflammation, visual acuity, and cardiovascular disease in old age. Within the Leiden 85-plus Study, a prospective population-based study of participants aged 85 years and older, we found that carriers of the CFH 402HH variant had a higher production of IL-6 in whole blood samples compared to those carrying the 402YY variant (P=0.029). Carriers of the 402HH genotype also had a steeper increase in circulating C-reactive protein levels during follow-up (P=0.009), lower visual acuity (P=0.020), and an increased risk of cardiovascular mortality (P=0.004). Subjects in the lowest tertile of visual acuity had a twofold increased risk of cardiovascular mortality compared to those in the highest tertile (P=0.001). We conclude that the CFH Y402H polymorphism associates with inflammation, visual impairment, and cardiovascular mortality in the elderly population at large. Visual impairment identifies elderly with an increased risk of cardiovascular disease."
        },
        {
            "title": "Results following episcleral ruthenium plaque radiotherapy for posterior uveal melanoma. The Swedish experience.",
            "abstract": "The Swedish experience of ruthenium 106 plaque radiotherapy for posterior uveal melanoma includes 266 patients treated between 1979 and 1995. The median dose delivered at the tumour apex was 100 Gy and the median follow-up after radiotherapy was 3.6 years (range = 0.5 to 12.5 years) with no patient being lost to follow-up. Visual acuity deteriorated moderately following treatment but appeared to stabilize after 5 to 6 years. Treatment failure defined as enucleation following plaque treatment occurred in 46 of the 266 (17%) studied patients. The cumulative 5-year probability of retaining the eye after radiotherapy was 82% and by univariate analysis tumour height, tumour diameter and tumour stage each predicted subsequent treatment failure, whereas in multivariate analysis no single covariate retained a predictive value. Forty-five of the 266 patients died of any cause during follow-up; 27 of these deaths were melanoma-related. The cumulative 5-year survival proportion (based on melanoma-related deaths only) was 86%. Death in metastatic disease appeared to be more common among patients that failed ruthenium plaque radiotherapy, however these patients also tended to have large tumours."
        },
        {
            "title": "Myocardial characterization in pre-dialysis chronic kidney disease: a study of prevalence, patterns and outcomes.",
            "abstract": "Background:\n        \n      \n      Late gadolinium enhancement (LGE) using cardiac magnetic resonance (CMR) characterizes myocardial disease and predicts an adverse cardiovascular (CV) prognosis. Myocardial abnormalities, are present in early chronic kidney disease (CKD). To date there are no data defining prevalence, pattern and clinical implications of LGE-CMR in CKD.\n    \n\n\n          Methods:\n        \n      \n      Patients with pre-dialysis CKD (stage 2-5) attending specialist renal clinics at University Hospital Birmingham (UK) who underwent gadolinium enhanced CMR (1.5 T) between 2005 and 2017 were included. The patterns and presence (LGEpos) / absence (LGEneg) of LGE were assessed by two blinded observers. Association between LGE and CV outcomes were assessed.\n    \n\n\n          Results:\n        \n      \n      In total, 159 patients received gadolinium (male 61%, mean age 55 years, mean left ventricular ejection fraction 69%, left ventricular hypertrophy 5%) with a median follow up period of 3.8 years [1.04-11.59]. LGEpos was present in 55 (34%) subjects; the patterns were: right ventricular insertion point n = 28 (51%), mid wall n = 18 (33%), sub-endocardial n = 5 (9%) and sub-epicardial n = 4 (7%). There were no differences in left ventricular structural or functional parameters with LGEpos. There were 12 adverse CV outcomes over follow up; 7 of 55 with LGEpos and 5 of 104 LGEneg. LGEpos was not predicted by age, gender, glomerular filtration rate or electrocardiographic abnormalities.\n    \n\n\n          Conclusions:\n        \n      \n      In a selected cohort of subjects with moderate CKD but low CV risk, LGE was present in approximately a third of patients. LGE was not associated with adverse CV outcomes. Further studies in high risk CKD cohorts are required to assess the role of LGE with multiplicative risk factors."
        },
        {
            "title": "Determining diabetic retinopathy screening interval based on time from no retinopathy to laser therapy.",
            "abstract": "Objectives To determine the necessary screening interval for retinopathy in diabetic patients with no retinopathy based on time to laser therapy and to assess long-term visual outcome following screening. Methods In a population-based community screening programme in North Wales, 2917 patients were followed until death or for approximately 12 years. At screening, 2493 had no retinopathy; 424 had mostly minor degrees of non-proliferative retinopathy. Data on timing of first laser therapy and visual outcome following screening were obtained from local hospitals and ophthalmology units. Results Survival analysis showed that very few of the no retinopathy at screening group required laser therapy in the early years compared with the non-proliferative retinopathy group ( p < 0.001). After two years, <0.1% of the no retinopathy at screening group required laser therapy, and at three years 0.2% (cumulative), lower rates of treatment than have been suggested by analyses of sight-threatening retinopathy determined photographically. At follow-up (mean 7.8 ± 4.6 years), mild to moderate visual impairment in one or both eyes due to diabetic retinopathy was more common in those with retinopathy at screening (26% vs. 5%, p < 0.001), but blindness due to diabetes occurred in only 1 in 1000. Conclusions Optimum screening intervals should be determined from time to active treatment. Based on requirement for laser therapy, the screening interval for diabetic patients with no retinopathy can be extended to two to three years. Patients who attend for retinal screening and treatment who have no or non-proliferative retinopathy now have a very low risk of eventual blindness from diabetes."
        },
        {
            "title": "MR Imaging in Hypertrophic Cardiomyopathy: From Magnet to Bedside.",
            "abstract": "Hypertrophic cardiomyopathy ( HCM hypertrophic cardiomyopathy ), the most common genetically transmitted cardiac disorder, has been the focus of extensive research over the past 50 years. HCM hypertrophic cardiomyopathy is a multifaceted disease with highly heterogeneous genetic background, phenotypic expression, clinical presentation, and long-term outcome. Though most patients have an indolent course with a life expectancy comparable to that of the general population, early diagnosis and accurate risk profiling are essential to identify the sizeable subset at increased risk of sudden cardiac death or disease progression and heart failure-related complications, requiring aggressive management options. Imaging has a central role in the diagnosis and prognostic assessment of HCM hypertrophic cardiomyopathy patients, as well as screening of potentially affected family members. In this context, magnetic resonance (MR) imaging has recently emerged as an ideal complement to transthoracic echocardiography. Its multiparametric approach, fusing spatial, contrast, and temporal resolution, provides the clinician with detailed characterization of the HCM hypertrophic cardiomyopathy phenotype and assessment of its functional consequences including causes and site of dynamic obstruction, presence and extent of myocardial perfusion abnormalities, and fibrosis. Moreover, MR is key in differentiating HCM hypertrophic cardiomyopathy from \"phenocopies\"-that is, hearts with similar morphology but profoundly different etiology, such as amyloid or Anderson-Fabry disease. Long term, the incremental information provided by MR is relevant to planning of septal reduction therapies, identification of the early stages of end-stage progression, and stratification of arrhythmic risk. The aim of this review is to depict the increasingly important role of MR imaging in relation to the complexity of HCM hypertrophic cardiomyopathy , highlighting its role in clinical decision making."
        },
        {
            "title": "Infarct tissue heterogeneity assessed with contrast-enhanced MRI predicts spontaneous ventricular arrhythmia in patients with ischemic cardiomyopathy and implantable cardioverter-defibrillator.",
            "abstract": "Background:\n        \n      \n      The relation between infarct tissue heterogeneity on contrast-enhanced MRI and the occurrence of spontaneous ventricular arrhythmia (or sudden cardiac death) is unknown. Therefore, the study purpose was to evaluate the predictive value of infarct tissue heterogeneity assessed with contrast-enhanced MRI on the occurrence of spontaneous ventricular arrhythmia with subsequent implantable cardioverter-defibrillator (ICD) therapy (as surrogate of sudden cardiac death) in patients with previous myocardial infarction.\n    \n\n\n          Methods and results:\n        \n      \n      Ninety-one patients (age, 65+/-11 years) with previous myocardial infarction scheduled for ICD implantation underwent cine MRI to evaluate left ventricular function and volumes and contrast-enhanced MRI for characterization of scar tissue (infarct gray zone as measure of infarct tissue heterogeneity, infarct core, and total infarct size). Appropriate ICD therapy was documented in 18 patients (20%) during a median follow-up of 8.5 months (interquartile range, 2.1 to 20.3). Multivariable Cox proportional hazards analysis revealed that infarct gray zone was the strongest predictor of the occurrence of spontaneous ventricular arrhythmia with subsequent ICD therapy (hazard ratio, 1.49/10 g; CI, 1.01 to 2.20; chi(2)=4.0; P=0.04).\n    \n\n\n          Conclusions:\n        \n      \n      Infarct tissue heterogeneity on contrast-enhanced MRI is the strongest predictor of spontaneous ventricular arrhythmia with subsequent ICD therapy (as surrogate of sudden cardiac death) among other clinical and MRI variables, that is, total infarct size and left ventricular function and volumes, in patients with previous myocardial infarction."
        },
        {
            "title": "Association of myocardial fibrosis, electrocardiography and ventricular tachyarrhythmia in hypertrophic cardiomyopathy: a delayed contrast enhanced MRI study.",
            "abstract": "Background:\n        \n      \n      Patients with hypertrophic cardiomyopathy (HCM) are predisposed to ventricular tachyarrhythmia (VT); likely due to myocardial fibrosis or disarray. Delayed hyperenhancement magnetic resonance imaging (DHE-MRI) accurately detects myocardial fibrosis (scar). We sought to determine the association between septal thickness, myocardial scar and VT.\n    \n\n\n          Methods:\n        \n      \n      Sixty-eight patients (mean age 44 years, 69% males) with documented HCM underwent cine MRI (Siemens Sonata or Avanto 1.5 T scanner, Erlangen, Germany) in short axis, 2, 3 and 4-chamber views and maximal interventricular septal thickness was recorded at end-diastole. Corresponding DHE-MR images (8-10 mm thick) were obtained, approximately 20 min after injection of 0.2 mmol/kg of Gadolinium. Scar was determined semi-automatically (as % of total myocardium) using VPT software (Siemens) and defined as intensity >2 SD above viable myocardium in a 12 segment short-axis model at apex, mid LV and base. Presence of VT (documented on ambulatory ECG monitoring) and history of sudden death were recorded.\n    \n\n\n          Results:\n        \n      \n      One patient had a history of sudden death and 9 (13%) had VT on ambulatory ECG monitoring. On DHE-MRI, 39 (57%) patients had myocardial scar. Patients with VT had significantly higher scar %, compared to those without: 14% [6, 19] vs. 6% [0, 10], P = 0.01. On logistic regression, only the size of the scar was a significant predictor of VT (P = 0.03).\n    \n\n\n          Conclusions:\n        \n      \n      HCM subjects with VT have a higher % of myocardial scarring noted on DHE-MRI, independent of septal thickness or beta-blocker use."
        },
        {
            "title": "Primary transpupillary thermotherapy for choroidal indeterminate melanocytic lesions.",
            "abstract": "Objective:\n        \n      \n      This study aimed to assess the ocular and metastatic outcomes of patients with choroidal indeterminate melanocytic lesions treated by primary transpupillary thermotherapy (TTT).\n    \n\n\n          Design:\n        \n      \n      Retrospective case series.\n    \n\n\n          Participants:\n        \n      \n      Eight patients presenting choroidal indeterminate melanocytic lesions treated by primary TTT.\n    \n\n\n          Methods:\n        \n      \n      A retrospective chart review was conducted for patients with a newly diagnosed choroidal indeterminate melanocytic lesion treated by at least 3 TTT sessions from 2002 to 2011. Best-corrected visual acuity and lesion dimensions were measured at baseline and during follow-up. Complications were recorded including lesion growth, metastasis, melanoma-related mortality, and treatment-related complications.\n    \n\n\n          Results:\n        \n      \n      Mean initial thickness was 2.0 ± 0.8 mm. Patients had an average of 3.0 ± 0.9 risk factors for lesion growing. Three patients (38%) had lesion growth. Two patients (25%) had severe visual loss (>1.0 logMAR) directly related to TTT treatment. There were no fatalities due to metastasis.\n    \n\n\n          Conclusions:\n        \n      \n      Despite careful patient selection and systematic treatment with at least 3 TTT sessions, the use of primary TTT to treat patients with choroidal indeterminate melanocytic lesions with ≥ 1 risk factor for lesion growth yielded poor local lesion control and the possibility for severe ocular complications."
        },
        {
            "title": "Cataract and mortality. The Beijing eye study.",
            "abstract": "Background:\n        \n      \n      To assess an association between cataract and mortality in a population-based setting.\n    \n\n\n          Methods:\n        \n      \n      At baseline in 2001, the Beijing Eye Study examined 4255 subjects for cataract using standardized lens photographs which were examined according to the Age-Related Eye Disease Study scheme. In 2006, all study participants were re-invited for a follow-up examination.\n    \n\n\n          Results:\n        \n      \n      Out of the 4255 subjects, 3142 subjects (73.8%) returned for follow-up examination, while 135 subjects (3.2%) were dead and 978 subjects (23.0%) did not agree to be re-examined or had moved away. In multivariate analysis, the amount subcapsular cataract was significantly associated with increased mortality (P = 0.029; OR: 2.14; 95%CI: 1.08, 4.25), particularly for the subjects with an age of 65+ years and 70+ years (P = 0.016 and P = 0.003 respectively). Correspondingly, product of age times amount of subcapsular cataract was significantly associated with mortality (P = 0.001). Degree of nuclear cataract and amount of cortical cataract were not associated with mortality in multivariate analysis (P = 0.910 and P = 0.938 respectively).\n    \n\n\n          Conclusions:\n        \n      \n      As also found in previous epidemiological studies on Western populations, cataract, namely subcapsular cataract in elderly subjects, is associated with an increased mortality risk."
        },
        {
            "title": "[Tuberculosis in Salvador, Brazil: costs to health system and families].",
            "abstract": "Objective:\n        \n      \n      Tuberculosis is one the greatest causes of mortality worldwide, but its economic effects are not well known. This study had the objective of estimating the costs to the public and private healthcare systems and to families of tuberculosis treatment and prevention.\n    \n\n\n          Methods:\n        \n      \n      This study was made in the municipality of Salvador, State of Bahia, Brazil, in 1999. Data for estimating the costs to the healthcare system were collected from the Department of Health, healthcare facilities and a philanthropic institution. The public and private costs were analyzed using cost accounting methodology. Cost data relating to families were collected by means of questionnaires, and included data on transportation, food and other expenses, and also income losses associated with this disease.\n    \n\n\n          Results:\n        \n      \n      The average cost of treating one new case of tuberculosis was approximately US$103. The cost of treating one multiresistant patient was 27 higher than this. The cost to the public services consisted of 65% on hospitalization, 32% on treatment, and only 3% on prevention. The families committed around 33% of their income on expenses related to tuberculosis.\n    \n\n\n          Conclusions:\n        \n      \n      Despite the fact that the families did not have to pay for medications and treatment, given that this service is offered by the State, the costs to families related to loss of income due to the disease were very high. The proportion of public service funds utilized for prevention is small. Greater investment in prevention campaigns not only might diminish the numbers of cases but also might lead to earlier diagnosis, thus reducing the costs associated with hospitalization. The lack of an integrated cost accounting system makes it impossible to visualize costs across the various sectors."
        },
        {
            "title": "Mortality during 6 years of follow-up in relation to visual impairment and eye disease: results from a population-based cohort study of people aged 50 years and above in Nakuru, Kenya.",
            "abstract": "Objective:\n        \n      \n      To estimate the association between (1) visual impairment (VI) and (2) eye disease and 6-year mortality risk within a cohort of elderly Kenyan people.\n    \n\n\n          Design, setting and participants:\n        \n      \n      The baseline of the Nakuru Posterior Segment Eye Disease Study was formed from a population-based survey of 4318 participants aged ≥50 years, enrolled in 2007-2008. Ophthalmic and anthropometric examinations were undertaken on all participants at baseline, and a questionnaire was administered, including medical and ophthalmic history. Participants were retraced in 2013-2014 for a second examination. Vital status was recorded for all participants through information from community members. Cumulative incidence of mortality, and its relationship with baseline VI and types of eye disease was estimated. Inverse probability weighting was used to adjust for non-participation.\n    \n\n\n          Primary outcome measures:\n        \n      \n      Cumulative incidence of mortality in relation to VI level at baseline.\n    \n\n\n          Results:\n        \n      \n      Of the baseline sample, 2170 (50%) were re-examined at follow-up and 407 (10%) were known to have died (adjusted risk of 11.9% over 6 years). Compared to those with normal vision (visual acuity (VA) ≥6/12, risk=9.7%), the 6-year mortality risk was higher among people with VI (<6/18 to ≥6/60; risk=28.3%; risk ratio (RR) 1.75, 95% CI 1.28 to 2.40) or severe VI (SVI)/blindness (<6/60; risk=34.9%; RR 1.98, 95% CI 1.04 to 3.80). These associations remained after adjustment for non-communicable disease (NCD) risk factors (mortality: RR 1.56, 95% CI 1.14 to 2.15; SVI/blind: RR 1.46, 95% CI 0.80 to 2.68). Mortality risk was also associated with presence of diabetic retinopathy at baseline (RR 3.18, 95% CI 1.98 to 5.09), cataract (RR 1.26, 95% CI 0.95 to 1.66) and presence of both cataract and VI (RR 1.57, 95% CI 1.24 to 1.98). Mortality risk was higher among people with age-related macular degeneration at baseline (with or without VI), compared with those without (RR 1.42, 95% CI 0.91 to 2.22 and RR 1.34, 95% CI 0.99 to 1.81, respectively).\n    \n\n\n          Conclusions:\n        \n      \n      Visual acuity was related to 6-year mortality risk in this cohort of elderly Kenyan people, potentially because both VI and mortality are related to ageing and risk factors for NCD."
        },
        {
            "title": "Giant cell arteritis and mortality.",
            "abstract": "Background:\n        \n      \n      Giant cell arteritis (GCA) is a systemic vasculitis of elderly individuals associated with significant morbidity, including blindness, stroke, and myocardial infarction. Previous studies have investigated whether GCA is associated with increased mortality, with conflicting results. The objective of this study is to determine whether GCA, is associated with increased mortality.\n    \n\n\n          Methods:\n        \n      \n      Forty-four cases with GCA were identified from the University of Utah Health Sciences Center, the major tertiary care center for the Intermountain West. The Utah Population Database, a unique biomedical information resource, selected cases and age- and gender-matched controls. Cases were defined as patients with a temporal artery biopsy-proven diagnosis of GCA (international classification of diseases [ICD]-9 code 446.5) between 1991 and 2005. Exclusion criteria included a negative biopsy, alternative diagnoses, or insufficient clinical data. For each of the 44 cases, 100 controls were identified; thus, 4,400 controls were included in the data analysis. Median survival time and 5-year cumulative survival were measured for cases and controls.\n    \n\n\n          Results:\n        \n      \n      The median survival time for the 44 GCA cases was 1,357 days (3.71 years) after diagnosis compared with 3,044 days (8.34 years) for the 4,400 controls (p = 0.04). Five-year cumulative survival was 67% for the control group versus 35% for the cases (p < .001). Survival rates for cases and controls converged at approximately 11.12 years.\n    \n\n\n          Conclusions:\n        \n      \n      Patients with GCA were more likely than age- and gender-matched controls to die within the first 5 years following diagnosis."
        },
        {
            "title": "A population-based study of the incidence of complications associated with type 2 diabetes in the elderly.",
            "abstract": "One hundred and eighty-eight known Type 2 diabetic patients aged over 60 years identified by a geographically based survey of a population of 40,076 were followed for a median of 6 years to determine the incidence of various complications. There were 63 deaths and two patients were lost to follow-up. The presence of complications was determined using a structured questionnaire and clinical examination. Incidence rates of ischaemic heart disease, stroke, and peripheral vascular disease (PVD) were 56 (95% CI 41-75), 22 (13-35), and 146 (117-174) 1000-person-years-1 of follow-up, respectively. Rates of stroke and PVD rose significantly with age. Retinopathy occurred at a rate of 60 (42-83) 1000-person-years-1 and cataract at 29 (17-46) 1000-person-years-1 although visual acuity in survivors did not deteriorate overall, probably reflecting the high mortality associated with cataract. The rate of proteinuria (albumin concentration greater than 300 mg l-1) was 19 (9-34) 1000-person-years-1. Incidence rates were unrelated to sex or duration of diabetes. Diabetes is associated with a continuing incidence of complications into old age. Adequate facilities are required to assess and treat the resulting morbidity in a population with an increasing proportion of elderly people."
        },
        {
            "title": "Thermal Imaging Techniques for Breast Screening - A Survey.",
            "abstract": "Breast cancer is the second leading cause of cancer death among women preceded by cervix cancer. It has been reported that at the early stage of detection there is 85% chance of getting cured, whereas only 10% chance at later stage diagnosis. The current screening modalities are expensive, they have intricate imaging measures and they are unhealthy due to radiation exposure. Therefore, a screening tool that is non-invasive, has no connection with the body, free from radiation, such as Medical Thermography is necessary. It is reported that the sensitivity and specificity of medical thermography are high largely in dense breast tissues. The clinical interpretation primarily depends on the asymmetrical analysis of these thermograms subjectively. The appearance of an asymmetric thermal image may indicate the pathological conditions. For earlier detection of breast cancer, it is essential to identify the advanced methods in image processing techniques which enhance the significance of diagnostics. In that analysis, the required breast region is unglued from the background image. The segmented image is separated into symmetrical left and right breast tissues. The statistical and histogram features extracted from both regions are used to identify the abnormal thermograms using machine learning algorithms. From literature, it is reported that the thermal images are inherently low contrast images and have low single to noise ratio. Moreover, they are amorphous in nature and no clear edges are seen. The difficulty lies in the detection of lower breast boundaries and inframammary folds. So, in general, the first attempt is made in improving the signal to noise ratio and contrast of the image which helps to extract the true regions of breast tissues. Then, asymmetry analysis of the normal and abnormal breast tissues is performed using different techniques. This work demonstrates the review of a few image processing methods or the development which are elaborated in the detection of breast cancer from thermal images."
        },
        {
            "title": "Influential factors on cognitive performance in middle-aged cohort: Third National Health and Nutrition Examination Survey-based study.",
            "abstract": "Aging-associated cognitive decline is closely linked to illness, dementia, increased mortality, and is a major health and social issue. The purpose of this study was to determine modifiable factors associated with cognitive performance.We analyzed data from a random sample of participants of the Third National Health and Nutrition Examination Survey, which is a cross-sectional survey, of the US population, aged 20 to 59 years, who underwent computer-based neurocognitive testing. There were 5 outcome measures in 3 neurocognitive tests: the mean of simple reaction time test, the mean total latency of the symbol digit substitution test (SDST), the average number of errors of the SDST, the average trials to criterion of the serial digit learning test (SDLT), and the average total score of the SDLT.Socioeconomic status, including older age, black ethnicity, lower income ratio, and lower education level, were associated with poorer neurocognitive function in all analyzed tests. In addition, participants with poor health, nonsmokers, and nondrinkers performed worse in all administered tests compared with individuals with good health, smokers, and participants consuming alcoholic beverages. Dietary and biochemical characteristics of the blood were not consistently associated with neurocognitive performance.Our results indicate that socioeconomic factors, health-related and dietary habits, biochemical parameters of the blood, and job category were associated with neurocognitive performance in visual attention, learning, and concentration in a large, nationally representative sample of healthy, ethnically diverse 20 to 59-year-olds. Future studies are needed to understand the mechanisms of cognitive aging and the factors that contribute to its individual differences."
        },
        {
            "title": "Screening for conjunctival melanoma metastasis: literature review.",
            "abstract": "Local tumour control in conjunctival melanoma has improved in recent years. However the tumour-related death rate of these patients is still 14% at 5 years up to 33% at 15 years. With the introduction of sentinel node biopsies for conjunctival melanomas with a poor prognosis and screening for locoregional and distant metasases prognosis might be improved."
        },
        {
            "title": "Multi-detector row computed tomography angiography: an alternative imaging method for surgical strategy in lower extremity arterial occlusive disease.",
            "abstract": "In order to show the value of CT angiography in the pretherapeutic assessment of lower leg ischemia, we studied 93 CT angiographies in 85 patients. Two groups were defined according to the level of revascularization: 52 angioscanner were made prior to suprainguinal revascularization and 41 prior to infrainguinal reconstruction. Two decision attitudes were chosen by two different physicians, a radiologist and vascular surgeon, members of the same team. The attitudes where then compared in order to evaluate the value of CT angiography. The first attitude was a pragmatic strategy based on the images as interpreted by the first physician and on the intraoperative information including surgical treatment and, if necessary, angiography. This indicates that the results of this attitude cover the performed revascularizations. The second attitude determined a virtual strategy and was chosen by the second physician a posteriori, based solely on the medical file with the same CT angiography images. These two strategies were compared in order to assess the agreement on the level of the lesion and the choice of revascularization. In 84 CT angiographies (90.3%), the analysis of the lesions and the choice of lesions to be treated were identical. In 9.6% of scans the strategies were not comparable because the lesions were interpreted differently or the scans were difficult to read. The sensitivity of CT angiography in detecting lesions and guiding the therapeutic strategy was 96% and its positive predictive value was 93%. Follow-up was reported according to the life-table method to assess the overall outcome and the results in both groups. The overall survival rate at 12 months for 85 patients was 90%. Secondary patency rates at 12 months in the group of patients who underwent a suprainguinal and infrainguinal revascularization were 98% and 71% respectively. Overall limb salvage at 12 months was 94%. In this setting, CT angiography allowed us to select adequate treatment in the majority of cases. These results obtained after a strategy based on CT angiography images are comparable with the results as published in the literature after the strategy based on conventional angiography."
        },
        {
            "title": "Utility values and age-related macular degeneration.",
            "abstract": "Objective:\n        \n      \n      To ascertain the utility values associated with age-related macular degeneration and varying degrees of visual loss.\n    \n\n\n          Design:\n        \n      \n      A cross-sectional study.\n    \n\n\n          Participants:\n        \n      \n      Eighty white patients with unilateral or bilateral age-related macular degeneration in 1 or both eyes, and visual loss to a minimum of the 20/40 level in at least 1 eye.\n    \n\n\n          Main outcome measures:\n        \n      \n      Utility values were measured in 5 groups according to the visual acuity in the better-seeing eye, 1 (20/20 to 20/25), 2 (20/30 to 20/ 50), 3 (20/60 to 20/100), 4 (20/200 to 20/400), and 5 (counting fingers to light perception), using the time trade-off and the standard gamble methods. Conventionally assigned anchor utility values were 1.0 for perfect health and 0.0 for death.\n    \n\n\n          Results:\n        \n      \n      The mean utility value for the total group with age-related macular degeneration was 0.72 (95% confidence interval [CI], 0.66-0.78) using the time trade-off method and 0.81 (95% CI, 0.76-0.86) using the standard gamble method. Using the time trade-off method correlated with the visual acuity in the better-seeing eye, the results were as follow: group 1, 0.89 (95% CI, 0.82-0.96), group 2, 0.81 (95% CI, 0.73-0.89), group 3, 0.57 (95% CI, 0.47-0.67), group 4, 0.52 (95% CI, 0.38-0.66), and group 5, 0.40 (95% CI, 0.29-0.50). Thus, those patients in group 1 were willing to trade 11% of their remaining lifetime in return for perfect vision in each eye, whereas those in group 5 were willing to trade 60% of their remaining lifetime in return for perfect vision in each eye.\n    \n\n\n          Conclusion:\n        \n      \n      Age-related macular degeneration causes a substantial decrease in patient utility values and is highly dependent on the degree of visual loss in the better-seeing eye."
        },
        {
            "title": "Costs of mobile screening for diabetic retinopathy: a practical framework for rural populations.",
            "abstract": "Australia's rural and remote residents experience considerably higher hospitalisation and death rates due to diabetes than their metropolitan counterparts. There is clearly a need for improved diabetes care services in these areas and interventions that target conditions associated with diabetes will yield beneficial results for the community. All people with diabetes are at risk for diabetic retinopathy, which can cause vision loss and blindness. Although vision loss and blindness due to diabetes is nearly 100% preventable through regular eye examinations, 35% of Victoria's rural population with diabetes do not have their eyes examined on a regular basis. A pilot, mobile screening program for the early detection of diabetic eye disease was conducted in rural Victoria and proved to be a successful model of adjunct eye care for people with diabetes. Actual costs from the pilot screening were applied to a permanent model for rural eye care. At A$41 per participant, costs for mobile screening were competitive with Medicare rebate costs for eye examinations. The model addresses barriers of accessibility and availability, targets a portion of the rural population with diabetes that is not otherwise having eye examinations, and is cost-saving to the Government."
        },
        {
            "title": "Percutaneous coronary intervention of unprotected left main stenoses - Procedural data and outcome depending on SYNTAX I Score.",
            "abstract": "Background:\n        \n      \n      We hypothesized that SYNTAX I score is a predictor for procedure complexity in left main PCI. Procedure complexity, duration and contrast load may contribute to adverse outcome of complex left main percutaneous coronary intervention (PCI).\n    \n\n\n          Methods:\n        \n      \n      In 105 consecutive patients who underwent PCI of unprotected left main coronary artery stenoses between 2014 and 2016, clinical parameters as well as PCI characteristics and follow-up data were analyzed.\n    \n\n\n          Results:\n        \n      \n      The mean SYNTAX I score was 29 ± 12, with 66 patients having a SYNTAX I score ≤ 32 and 39 patients a SYNTAX I score > 32. In patients with high SYNTAX I score vs. low-to-intermediate SYNTAX I score, single stent techniques were performed significantly less frequently (18% vs. 68%; p < 0.001), while Crush (44% vs. 5%; p < 0.001) and Culotte techniques (20% vs. 5%; p = 0.003) were performed significantly more frequently. Procedural success was achieved in all 105 cases without periprocedural mortality. During follow up, repeat PCI was necessary significantly more frequently in patients with high compared to patients with low-to-intermediate SYNTAX I score (34% vs.13%; p = 0.003). Nevertheless, overall mortality did not differ between patients with high vs. low-to-intermediate SYNTAX I score (20% vs. 18%).\n    \n\n\n          Conclusions:\n        \n      \n      PCI strategies for the treatment of left main coronary artery stenoses get significantly more complex with increasing SYNTAX I scores. While this translates into a significantly longer procedure duration and contrast load, short-term outcome seems not to be influenced by the SYNTAX I score."
        },
        {
            "title": "The epidemiology of retinal reticular drusen.",
            "abstract": "Purpose:\n        \n      \n      To describe the prevalence and 15-year cumulative incidence of and risk factors for reticular drusen.\n    \n\n\n          Design:\n        \n      \n      Population-based prospective study.\n    \n\n\n          Methods:\n        \n      \n      Four thousand nine hundred and twenty-six persons, 43 to 86 years of age, were included between 1988 and 1990, of whom 3,684, 2,764, and 2,119 participated in five-, 10-, and 15-year follow-up examinations, respectively, in Beaver Dam, Wisconsin. Main outcome measures included prevalence and 15-year incidence of reticular drusen determined by grading stereoscopic color fundus photographs.\n    \n\n\n          Results:\n        \n      \n      The prevalence at baseline and the 15-year cumulative incidence in either eye of reticular drusen was 0.7% and 3.0%, respectively. The 15-year incidence of reticular drusen varied with age from 0.4% in those 43 to 54 years of age to 6.6% in those 75 years or older at baseline (P < .001). In a multivariate model, while controlling for age, risk factors statistically significantly associated with increased risk of incident reticular drusen included: being female (odds ratio [OR], 2.8), current smoking (OR vs never, 1.9), less education (OR per category, 1.7), B-vitamin complex use (OR vs none, 2.5), single vitamin B (OR vs none, 2.9), history of steroid eye drops use (OR, 5.9), glaucoma (OR, 2.8), and more severe drusen type (e.g., soft indistinct drusen; OR, 1.4), whereas diabetes (OR, 0.1) at baseline was associated with decreased risk. Right eyes with reticular drusen at baseline had higher cumulative incidence of geographic atrophy (21% vs 9%) and exudative age-related macular degeneration [AMD] (20% vs 10%) compared with eyes with soft indistinct drusen.\n    \n\n\n          Conclusions:\n        \n      \n      This population-based study documents the long-term cumulative incidence of reticular drusen and its risk factors and shows its association with a high risk of incident late AMD."
        },
        {
            "title": "CMR sensitivity varies with clinical presentation and extent of cell necrosis in biopsy-proven acute myocarditis.",
            "abstract": "Objectives:\n        \n      \n      The aim of this study was to determine whether clinical presentation and type of cell death in acute myocarditis might contribute to cardiac magnetic resonance (CMR) sensitivity.\n    \n\n\n          Background:\n        \n      \n      Growing evidence indicates CMR is the reference noninvasive tool for the diagnosis of acute myocarditis. However, factors affecting CMR sensitivity are still unclear.\n    \n\n\n          Methods:\n        \n      \n      We retrospectively evaluated 57 consecutive patients with a diagnosis of acute myocarditis made on the basis of clinical history (≤3 months) and endomyocardial biopsy evidence of lymphocytic infiltrates (≥14 infiltrating leukocytes/mm(2) at immunohistochemistry) in association with damage of the adjacent myocytes and absence or minimal evidence of myocardial fibrosis. CMR acquisition protocol included T2-weighted (edema), early (hyperemia), and late (fibrosis/necrosis) gadolinium enhancement sequences. Presence of ≥2 CMR criteria denoted myocarditis. Type of cell death was evaluated by using in situ ligation with hairpin probes.\n    \n\n\n          Results:\n        \n      \n      Three clinical myocarditis patterns were recognized: infarct-like (pattern 1, n = 21), cardiomyopathic (pattern 2, n = 21), and arrhythmic (pattern 3, n = 15). Tissue edema was observed in 81% of pattern 1, 28% of pattern 2, and 27% of pattern 3. Early enhancement was evident in 71% of pattern 1, 67% of pattern 2, and 40% of pattern 3. Late gadolinium enhancement was documented in 71% of pattern 1, 57% of pattern 2, and 47% of pattern 3. CMR sensitivity was significantly higher in pattern 1 (80%) compared with pattern 2 (57%) and pattern 3 (40%) (p < 0.05). Cell necrosis was the prevalent mechanism of death in pattern 1 compared with pattern 2 (p < 0.001) and pattern 3 (p < 0.05), whereas apoptosis prevailed in pattern 2 (p < 0.001 vs. pattern 1 and p < 0.05 vs. pattern 3).\n    \n\n\n          Conclusions:\n        \n      \n      In acute myocarditis, CMR sensitivity is high for infarct-like, low for cardiomyopathic, and very low for arrhythmic clinical presentation; it correlates with the extent of cell necrosis-promoting expansion of interstitial space."
        },
        {
            "title": "Angiographic versus functional severity of coronary artery stenoses in the FAME study fractional flow reserve versus angiography in multivessel evaluation.",
            "abstract": "Objectives:\n        \n      \n      The purpose of this study was to investigate the relationship between angiographic and functional severity of coronary artery stenoses in the FAME (Fractional Flow Reserve Versus Angiography in Multivessel Evaluation) study.\n    \n\n\n          Background:\n        \n      \n      It can be difficult to determine on the coronary angiogram which lesions cause ischemia. Revascularization of coronary stenoses that induce ischemia improves a patient's functional status and outcome. For stenoses that do not induce ischemia, however, the benefit of revascularization is less clear.\n    \n\n\n          Methods:\n        \n      \n      In the FAME study, routine measurement of the fractional flow reserve (FFR) was compared with angiography for guiding percutaneous coronary intervention in patients with multivessel coronary artery disease. The use of the FFR in addition to angiography significantly reduced the rate of all major adverse cardiac events at 1 year. Of the 1,414 lesions (509 patients) in the FFR-guided arm of the FAME study, 1,329 were successfully assessed by the FFR and are included in this analysis.\n    \n\n\n          Results:\n        \n      \n      Before FFR measurement, these lesions were categorized into 50% to 70% (47% of all lesions), 71% to 90% (39% of all lesions), and 91% to 99% (15% of all lesions) diameter stenosis by visual assessment. In the category 50% to 70% stenosis, 35% were functionally significant (FFR <or=0.80) and 65% were not (FFR >0.80). In the category 71% to 90% stenosis, 80% were functionally significant and 20% were not. In the category of subtotal stenoses, 96% were functionally significant. Of all 509 patients with angiographically defined multivessel disease, only 235 (46%) had functional multivessel disease (>or=2 coronary arteries with an FFR <or=0.80).\n    \n\n\n          Conclusions:\n        \n      \n      Angiography is inaccurate in assessing the functional significance of a coronary stenosis when compared with the FFR, not only in the 50% to 70% category but also in the 70% to 90% angiographic severity category."
        },
        {
            "title": "Thai falls risk assessment test (Thai-FRAT) developed for community-dwelling Thai elderly.",
            "abstract": "Objective:\n        \n      \n      To develop falls risk assessment test that is appropriate for community-dwelling Thai elderly, and to verify this test with the second set of population.\n    \n\n\n          Material and method:\n        \n      \n      A cross-sectional study was performed in 270 elderly living in Bansrang subdistrict, Ayuttaya province to identify a combination of variables that effectively predicted fall status in order to develop the Thai-FRAT The Thai-FRAT was validated with a second set of population whose cohort data had been collected during 1997-2002 in the study named \"CERB project\". One hundred fifty six elderly subjects were recruited in the analysis.\n    \n\n\n          Results:\n        \n      \n      The newly developed Thai-FRAT was composed of six factors including \"History of falls\", \"Impaired body balance\", \"Female\", \"Specific medication use\", \"Impaired visual acuity\" and \"Thai style house\". Possible score of the Thai-FRAT ranged from 0-11. The best cutoff score identified by the receiver operating curve analysis was 4. Sensitivity and specificity were 0.92 and 0.83 respectively. The Thai-FRAT could predict recurrent fall after two years among the elderly subjects who had had a history of fall during the past six months in the second set of population. Association between the Thai-FRAT score and mortality was also shown.\n    \n\n\n          Conclusion:\n        \n      \n      The Thai-FRAT is the first fall risk assessment test developed for Thai community-dwelling elderly. It is a valid and reliable measure of fall risk. An effect of environment on falls among Thai elderly was clarified in the present study."
        },
        {
            "title": "The All-Cause Mortality and a Screening Tool to Determine High-Risk Patients among Prevalent Type 2 Diabetes Mellitus Patients.",
            "abstract": "Aims:\n        \n      \n      This study aims to determine the all-cause mortality and the associated risk factors for all-cause mortality among the prevalent type 2 diabetes mellitus (T2DM) patients within five years' period and to develop a screening tool to determine high-risk patients.\n    \n\n\n          Methods:\n        \n      \n      This is a cohort study of T2DM patients in the national diabetes registry, Malaysia. Patients' particulars were derived from the database between 1st January 2009 and 31st December 2009. Their records were matched with the national death record at the end of year 2013 to determine the status after five years. The factors associated with mortality were investigated, and a prognostic model was developed based on logistic regression model.\n    \n\n\n          Results:\n        \n      \n      There were 69,555 records analyzed. The mortality rate was 1.4 persons per 100 person-years. The major cause of death were diseases of the circulatory system (28.4%), infectious and parasitic diseases (19.7%), and respiratory system (16.0%). The risk factors of mortality within five years were age group (p < 0.001), body mass index category (p < 0.001), duration of diabetes (p < 0.001), retinopathy (p = 0.001), ischaemic heart disease (p < 0.001), cerebrovascular (p = 0.007), nephropathy (p = 0.001), and foot problem (p = 0.001). The sensitivity and specificity of the proposed model was fairly strong with 70.2% and 61.3%, respectively.\n    \n\n\n          Conclusions:\n        \n      \n      The elderly and underweight T2DM patients with complications have higher risk for mortality within five years. The model has moderate accuracy; the prognostic model can be used as a screening tool to classify T2DM patients who are at higher risk for mortality within five years."
        },
        {
            "title": "[Ophthalmological complications associated with clinically significant carotid stenosis].",
            "abstract": "The aim of the study was to show ocular manifestations in carotid artery occlusive disease, with pathogenesis, diagnostic and therapeutic abilities of this changes. Carotid arteries are the main route by which the blood is supplied to the cerebrum and eyes. Clinical significant carotid artery stenosis is mainly caused by atherosclerosis. Most frequent neurological symptoms are transient ischemic attacks (TIA) and temporary visual loss (amaurosos fugax) are most common ocular symptoms. Other ocular pathologies in fundus examination are retinal embolies, retinal vein occlusion, anterior ischemic optic neuropathy, ocular ischemic syndrome or glaucoma. Most dangerous complications are stroke, blindness, or even patients death. Besides clinical examination the diagnosis is usually confirmed by carotid artery color Doppler ultrasound, magnetic resonance angiography and retinal fluorescein angiography. It is important to refer a patient with suspected or confirmed significant carotid artery stenosis for appropriate evaluation and treatment to a endovascular surgeon."
        },
        {
            "title": "Health-related Quality of Life and Pain in a Real-world Castration-resistant Prostate Cancer Population: Results From the PRO-CAPRI Study in the Netherlands.",
            "abstract": "Background:\n        \n      \n      The purpose of this study was to determine generic, cancer-specific, and prostate cancer-specific health-related quality of life (HRQoL), pain and changes over time in patients with metastatic castration-resistant prostate cancer (mCRPC) in daily practice.\n    \n\n\n          Patients and methods:\n        \n      \n      PRO-CAPRI is an observational, prospective study in 10 hospitals in the Netherlands. Patients with mCRPC completed the EQ-5D, European Organization for the Research and Treatment of Cancer Quality of Life Questionnaire (EORTC QLQ-C30), and Brief Pain Inventory-Short Form (BPI-SF) every 3 months and European Organization for the Research and Treatment of Cancer Quality of Life Questionnaire-Prostate Cancer Module (EORTC QLQ-PR25) every 6 months for a maximum of 2 years. Subgroups were identified based on chemotherapy pretreatment. Outcomes were generic, cancer-specific, and prostate cancer-specific HRQoL and self-reported pain. Descriptive statistics were performed including changes over time and minimal important differences (MID) between subgroups.\n    \n\n\n          Results:\n        \n      \n      In total, 151 included patients answered 873 questionnaires. The median follow-up from the start of the study was 19.5 months, and 84% were treated with at least 1 life-prolonging agent. Overall, patients were in good clinical condition (Eatern Cooperative Oncology Group performance status 0-1 in 78%) with normal baseline hemoglobin, lactate dehydrogenase, and alkaline phosphatase. At inclusion, generic HRQoL was high with a mean EQ visual analog score of 73.2 out of 100. The lowest scores were reported on role and physical functioning (mean scores of 69 and 76 of 100, respectively), and fatigue, pain, and insomnia were the most impaired domains. These domains deteriorated in > 50% of patients.\n    \n\n\n          Conclusion:\n        \n      \n      Although most patients were treated with new treatments during follow-up, mCRPC has a negative impact on HRQoL with deterioration in all domains over time, especially role and physical functioning. These domains need specific attention during follow-up to maintain HRQoL as long as possible by timely start of adequate supportive care management."
        },
        {
            "title": "Knowledge, attitudes and practices of the Advance Medical Directive in a residential estate in Singapore.",
            "abstract": "Introduction:\n        \n      \n      This study investigates the knowledge, attitudes and practices of residents in a residential estate in Singapore on the Advance Medical Directive (AMD).\n    \n\n\n          Materials and methods:\n        \n      \n      A community-based cross-sectional study was conducted with residents in the residential estate of Toa Payoh Lorong 6, Singapore. A stratifi ed random sampling was conducted to obtain a representative sample of the estate. Only residents aged 21 years and older were included. An interviewer-administered questionnaire was conducted, and only those who understood the AMD suffi ciently were further evaluated on their knowledge, attitudes and practices.\n    \n\n\n          Results:\n        \n      \n      A total of 414 residents were enrolled (50.1% response rate). Only 37.9% of the participants knew about the AMD prior to this study. Participants who had a higher knowledge level of AMD, did not wish \"for an artifi cially prolonged life\", \"to be kept alive indefi nitely on a life-support machine\", wished to \"lessen the fi nancial burden of loved ones\", \"avoid prolonged suffering\" and accepted the \"imminence of death\" were more willing to sign an AMD. However, \"religious beliefs\", \"personal ethical views\", \"dissuasion by family members\" and \"unclear terminology in the AMD\" discouraged the participants from signing an AMD. After adjusting for signifi cant factors, participants who did not wish \"to be kept alive indefi nitely on a life-support machine\" and accepted the \"imminence of death\" were found to correlate signifi cantly with the willingness to sign an AMD [Prevalence Rate Ratio (PRR) = 2.050 [1.140-3.685], P = 0.016; PRR = 2.669 [1.449-4.917], P = 0.02, respectively].\n    \n\n\n          Conclusions:\n        \n      \n      There is a need to increase awareness on the AMD. Public education methods can be improved to inform residents on the implications of the AMD."
        },
        {
            "title": "Communicating infectious disease prevalence through graphics: Results from an international survey.",
            "abstract": "Background:\n        \n      \n      Graphics are increasingly used to represent the spread of infectious diseases (e.g., influenza, Zika, Ebola); however, the impact of using graphics to adequately inform the general population is unknown.\n    \n\n\n          Objective:\n        \n      \n      To examine whether three ways of visually presenting data (heat map, dot map, or picto-trendline)-all depicting the same information regarding the spread of a hypothetical outbreak of influenza-influence intent to vaccinate, risk perception, and knowledge.\n    \n\n\n          Design:\n        \n      \n      Survey with participants randomized to receive a simulated news article accompanied by one of the three graphics that communicated prevalence of influenza and number of influenza-related deaths.\n    \n\n\n          Setting:\n        \n      \n      International online survey.\n    \n\n\n          Participants:\n        \n      \n      16,510 adults living in 11 countries selected using stratified random sampling based on age and gender.\n    \n\n\n          Measurements:\n        \n      \n      After reading the article and viewing the presented graphic, participants completed a survey that measured interest in vaccination, perceived risk of contracting disease, knowledge gained, interest in additional information about the disease, and perception of the graphic.\n    \n\n\n          Results:\n        \n      \n      Heat maps and picto-trendlines were evaluated more positively than dot maps. Heat maps were more effective than picto-trendlines and no different from dot maps at increasing interest in vaccination, perceived risk of contracting disease, and interest in additional information about the disease. Heat maps and picto-trendlines were more successful at conveying knowledge than dot maps. Overall, heat maps were the only graphic to be superior in every outcome.\n    \n\n\n          Limitations:\n        \n      \n      Results are based on a hypothetical scenario.\n    \n\n\n          Conclusion:\n        \n      \n      Heat maps are a viable option to promote interest in and concern about infectious diseases."
        },
        {
            "title": "Survival of glaucoma patients.",
            "abstract": "Purpose:\n        \n      \n      To investigate the survival of patients with capsular or simple glaucoma compared with that of the common population, with particular attention to the impact of sex and use of acetazolamide (Diamox).\n    \n\n\n          Methods:\n        \n      \n      The 30 year survival of 1147 patients with capsular or simple glaucoma who were finally hospitalized at the Eye Department, Rikshospitalet, Oslo, from 1961 to 1970, are analysed, using log rank tests. The time varying impacts of sex and acetazolamide on survival are also studied using a regression model.\n    \n\n\n          Results:\n        \n      \n      There was a significant increased mortality for patients with acetazolamide, and for men also those not using it. The observed mortality for men was initially lower than the average Norwegian population, but later the mortality increased more rapidly in the glaucoma group. This may be explained by a selection of the healthiest patients to Rikshospitalet, and actually indicates that the excess mortality is even higher than calculated here.\n    \n\n\n          Conclusion:\n        \n      \n      The analysis of data indicated increased mortality for glaucoma patients when the disease had lasted for some time. This was especially pronounced for men using acetazolamide. A similar study from a period when acetazolamide was not in common use and an analysis of causes of death is also asked for."
        },
        {
            "title": "Interferon alfa-2b, colchicine, and benzathine penicillin versus colchicine and benzathine penicillin in Behçet's disease: a randomised trial.",
            "abstract": "Background:\n        \n      \n      Sight-threatening eye involvement is a serious complication of Behçet's disease. Extraocular complications such as arthritis, vascular occlusive disorders, mucocutaneous lesions, and central-nervous-system disease may lead to morbidity and even death. We designed a prospective study in newly diagnosed patients without previous eye disease to assess whether prevention of eye involvement and extraocular manifestations, and preservation of visual acuity are possible with combination treatments with and without interferon alfa-2b.\n    \n\n\n          Methods:\n        \n      \n      Patients were randomly assigned 3 million units interferon alfa-2b subcutaneously every other day for the first 6 months plus 1.5 mg colchicine orally daily and 1.2 million units benzathine penicillin intramuscularly every 3 weeks (n=67), or colchicine and benzathine penicillin alone (n=68). The primary endpoint was visual-acuity loss. Analysis was by intention to treat.\n    \n\n\n          Findings:\n        \n      \n      Significantly fewer patients who were treated with interferon had eye involvement than did patients who did not receive interferon (eight vs 27, relative risk 0.21 [95% CI 0.09-0.50], p<0.001). Ocular attack rate was 0.2 (SD 0.62) per year with interferon therapy and 1.02 (1.13) without interferon therapy (p=0.0001). Visual-acuity loss was significantly lower among patients treated with interferon than in those without interferon (two vs 13, relative risk 0.13 [95% CI 0.03-0.60], p=0.003). Arthritis episodes, vascular events, and mucocutaneous lesions were also less frequent in patients treated with interferon than in those not receiving interferon. No serious side-effects were reported.\n    \n\n\n          Interpretation:\n        \n      \n      Therapy with interferon alfa-2b, colchicine, and benzathine penicillin seems to be an effective regimen in Behçet's disease for the prevention of recurrent eye attacks and extraocular complications, and for the protection of vision."
        },
        {
            "title": "Clear cell renal cell carcinoma: discrimination from other renal cell carcinoma subtypes and oncocytoma at multiphasic multidetector CT.",
            "abstract": "Purpose:\n        \n      \n      To determine whether enhancement at multiphasic multidetector computed tomography (CT) can help differentiate clear cell renal cell carcinoma (RCC) from oncocytoma, papillary RCC, and chromophobe RCC.\n    \n\n\n          Materials and methods:\n        \n      \n      With institutional review board approval for this HIPAA-compliant retrospective study, the pathology database was queried to derive a cohort of 298 cases of RCC and oncocytoma with preoperative multiphasic multidetector CT with as many as four phases (unenhanced, corticomedullary, nephrographic, and excretory). A total of 170 clear cell RCCs, 57 papillary RCCs, 49 oncocytomas, and 22 chromophobe RCCs were evaluated for multiphasic enhancement and compared by using t tests. Cutoff analysis was performed to determine optimal threshold levels to discriminate among the four groups.\n    \n\n\n          Results:\n        \n      \n      Mean enhancement of clear cell RCCs and oncocytomas peaked in the corticomedullary phase; mean enhancement of papillary and chromophobe RCCs peaked in the nephrographic phase. Enhancement of clear cell RCCs was greater than that of oncocytomas in the corticomedullary (125 HU vs 106 HU, P = .045) and excretory (80 HU vs 67 HU, P = .034) phases. Enhancement of clear cell RCCs was greater than that of papillary RCCs in the corticomedullary (125 HU vs 54 HU, P < .001), nephrographic (103 HU vs 64 HU, P < .001), and excretory (80 HU vs 54 HU, P < .001) phases. Enhancement of clear cell RCCs was greater than that of chromophobe RCCs in the corticomedullary (125 HU vs 74 HU, P < .001) and excretory (80 HU vs 60 HU, P = .008) phases. Thresholding of enhancement helped to discriminate clear cell RCC from oncocytoma, papillary RCC, and chromophobe RCC with accuracies of 77% (83 of 108 cases), 85% (101 of 119 cases), and 84% (81 of 97 cases).\n    \n\n\n          Conclusion:\n        \n      \n      Enhancement at multiphasic multidetector CT, if prospectively validated, may assist in the discrimination of clear cell RCC from oncocytoma, papillary RCC, and chromophobe RCC."
        },
        {
            "title": "Black-blood thrombus imaging (BTI): a contrast-free cardiovascular magnetic resonance approach for the diagnosis of non-acute deep vein thrombosis.",
            "abstract": "Background:\n        \n      \n      ﻿Deep vein thrombosis (DVT) is a common but elusive illness that can result in long-term disability or death. Accurate detection of thrombosis and assessment of its size and distribution are critical for treatment decision-making. In the present study, we sought to develop and evaluate a cardiovascular magnetic resonance (CMR) black-blood thrombus imaging (BTI) technique, based on delay alternating with nutation for tailored excitation black-blood preparation and variable flip angle turbo-spin-echo readout, for the diagnosis of non-acute DVT.﻿ METHODS: This prospective study was approved by institutional review board and informed consent obtained from all subjects. BTI was first conducted in 11 healthy subjects for parameter optimization and then conducted in 18 non-acute DVT patients to evaluate its diagnostic performance. Two clinically used CMR techniques, contrast-enhanced CMR venography (CE-MRV) and three dimensional magnetization prepared rapid acquisition gradient echo (MPRAGE), were also conducted in all patients for comparison. All images obtained from patients were analyzed on a per-segment basis. Using the consensus diagnosis of CE-MRV as the reference, the sensitivity (SE), specificity (SP), positive and negative predictive values (PPV and NPV), and accuracy (ACC) of BTI and MPRAGE as well as their diagnostic agreement with CE-MRV were calculated. Besides, diagnostic confidence and interreader diagnostic agreement were evaluated for all three techniques.\n    \n\n\n          Results:\n        \n      \n      BTI with optimized parameters effectively nulled the venous blood flow signal and allowed directly visualizing the thrombus within the black-blood lumen. Higher SE (90.4% vs 67.6%), SP (99.0% vs. 97.4%), PPV (95.4% vs. 85.6%), NPV (97.8% vs 92.9%) and ACC (97.4% vs. 91.8%) were obtained by BTI in comparison with MPRAGE. Good diagnostic confidence and excellent diagnostic and interreader agreements were achieved by BTI, which were superior to MPRAGE on detecting the chronic thrombus.\n    \n\n\n          Conclusion:\n        \n      \n      BTI allows direct visualization of non-acute DVT within the dark venous lumen and has the potential to be a reliable diagnostic tool without the use of contrast medium."
        },
        {
            "title": "Comparison of Diagnostic Accuracy of MRI with and Without Contrast in Diagnosis of Traumatic Spinal Cord Injuries.",
            "abstract": "Acute spinal cord injury (SCI) is one of the most common causes of severe disability and mortality after trauma. Magnetic resonance imaging (MRI) can identify different levels of SCI, but sometimes unable to detect the associated soft tissue injuries. The role of MRI with contrast in patients with SCI has not been studied. This is the first study in human to compare the efficacy of MRI with and without contrast in diagnosis and prognosis evaluation of SCIs.In this cross-sectional diagnostic study, MRI with and without contrast was performed on 40 patients with acute spinal injury. In these patients, 3 different types of MRI signal patterns were detected and compared.The most common cases of spinal injuries were accident (72.5%) and the after fall (27.5%). The prevalence of lesions detected includes spine fracture (70%), spinal stenosis (32.5%), soft tissue injuries (30%), and tearing of the spinal cord (2.5%). A classification was developed using 3 patterns of SCIs. Type I, seen in 2 (5.0%) of the patients, demonstrated a decreased signal intensity consistent with acute intraspinal hemorrhage. Type II, seen in 8 (20.0%) of the patients, demonstrated a bright signal intensity consistent with acute cord edema. Type III, seen in 1 (2.5%) of the patients, demonstrated a mixed signal of hypointensity centrally and hyperintensity peripherally consistent with contusion. In the diagnosis of all injuries, MRI with contrast efficacy comparable to noncontrast MRI, except in the diagnosis of soft tissue, which was significantly higher sensitivity (P < 0.05).So given that is not significant differences between noncontrast and contrast-enhanced MRI in the diagnosis of major injuries (hematoma, edema, etc.) and contrast-enhanced MRI just better in soft tissues. We recommend to the MRI with contrast only used in cases of suspected severe soft tissue injury, which have been ignored by detection MRI without contrast."
        },
        {
            "title": "Safe contrast volumes for preventing contrast-induced nephropathy in elderly patients with relatively normal renal function during percutaneous coronary intervention.",
            "abstract": "The aim of this study was to evaluate contrast media volume to creatinine clearance (V/CrCl) ratio for predicting contrast-induced nephropathy (CIN) and to determine a safe V/CrCl cut off value to avoid CIN in elderly patients with relatively normal renal function during percutaneous coronary intervention (PCI). We prospectively enrolled 1020 consecutive elderly patients (age ≥65 years) with relative normal renal function (baseline serum creatinine <1.5 mg/dL) undergoing PCI. Receiver operating characteristic (ROC) curves were used to identify the optimal cut off value of V/CrCl for detecting CIN. The predictive value of V/CrCl for CIN was assessed with a multivariate logistic regression. Thirty-nine patients (3.8%) developed CIN. There was a significant association between a higher V/CrCl ratio and CIN risk (P < 0.001). ROC curve analysis indicated that a V/CrCl ratio of 2.74 was a fair discriminator for CIN (C statistic = 0.68). After adjusting for other known CIN risk factors, V/CrCl ratios >2.74 remained significantly associated with CIN (odds ratio = 3.21, 95% confidence interval [CI] 1.45-7.09, P = 0.004) and worse long-term mortality (hazard ratio = 1.96, 95% CI 1.14-3.38, P = 0.016). A V/CrCl ratio >2.74 was a significant independent predictor of CIN and was independently associated with long-term mortality in elderly patients with relatively normal renal function."
        },
        {
            "title": "Determinants of disease-specific health-related quality of life in Turkish stroke survivors.",
            "abstract": "Stroke is a worldwide cause of morbidity and mortality that affects health-related quality of life. In this study, our objective was to identify determinants of disease-specific health-related quality of life in Turkish stroke survivors. A total of 114 consecutive patients who experienced a stroke at least 6 months earlier were studied. Health-related quality of life was measured using Stroke-specific Quality of Life (SS-QoL) consisting of 12 domains. Demographic and clinical data were collected, including age, sex, marital status, years of education, time since stroke, whether the patient received rehabilitation before enrollment, stroke etiology, whether the dominant hand was affected or not, presence of vision defect, neglect, aphasia, and dysarthria. The patients were assessed by the functional independence measure (FIM) and the Mini-Mental State Examination. A multiple linear regression analysis was carried out using a stepwise method to determine the predictors of 12 domains and the total score of the SS-QoL. The domains of work, social roles, mobility, and self-care had the lowest SS-QoL scores, whereas the highest scores were for the domains of personality, thinking, language, and vision. The total SS-QoL score was explained by the total FIM and Mini-Mental State Examination. Among the 12 domains, the mobility domain was explained the best (R=0.50) by motor FIM, previously received rehabilitation, and age, followed by the language domain (R=0.37) explained by the presence of aphasia and dysarthria, and previously received rehabilitation. The domains of mood (R=0.13) and upper extremity (R=0.19) were explained the worst. The results indicated that functional independence, age, cognitive status, and receiving a rehabilitation program were the primary determinants of the SS-QoL."
        },
        {
            "title": "COMPARISON OF VISUAL PROGNOSIS AND CLINICAL FEATURES OF CYTOMEGALOVIRUS RETINITIS IN HIV AND NON-HIV PATIENTS.",
            "abstract": "Purpose:\n        \n      \n      To compare the visual prognosis and clinical features of cytomegalovirus (CMV) retinitis between HIV and non-HIV patients.\n    \n\n\n          Methods:\n        \n      \n      Retrospective cross-sectional study on patients diagnosed with CMV retinitis. Depending on the presence of HIV infection, best-corrected visual acuity (VA) and clinical feature of CMV retinitis were analyzed. The clinical characteristics associated with poor visual prognosis after antiviral treatment were also identified.\n    \n\n\n          Results:\n        \n      \n      A total of 78 eyes (58 patients) with CMV retinitis were included in this study: 21 eyes and 57 eyes in HIV and non-HIV patients, respectively. Best-corrected VA was not significantly different between HIV and non-HIV patients. The rate of foveal involvement, retinal detachment, involved zone, and mortality did not significantly differ between the two groups. Visual acuity after antiviral treatment was significantly worse (pretreatment logarithm of the minimal angle of resolution best-corrected VA, 0.54 ± 0.67 [Snellen VA, 20/63]; posttreatment logarithm of the minimal angle of resolution best-corrected VA, 0.77 ± 0.94 [Snellen VA, 20/125]; P = 0.014). Poor visual prognosis was significantly associated with Zone 1 involvement, retinal detachment, and a poor general condition.\n    \n\n\n          Conclusion:\n        \n      \n      The overall visual prognosis and the clinical features of CMV retinitis do not differ between HIV and non-HIV patients. The visual prognosis of CMV retinitis still remains quite poor despite advancements in antiviral treatment. This poor prognosis after antiviral treatment is associated with retinal detachment during follow-up, Zone 1 involvement, and the poor general condition of the patient."
        },
        {
            "title": "Longitudinal profiling of plasma cytokines in melioidosis and their association with mortality: a prospective cohort study.",
            "abstract": "Objectives:\n        \n      \n      To characterize plasma cytokine responses in melioidosis and analyse their association with mortality.\n    \n\n\n          Methods:\n        \n      \n      A prospective longitudinal study was conducted in two hospitals in Northeast Thailand to enrol 161 individuals with melioidosis, plus 13 uninfected healthy individuals and 11 uninfected individuals with diabetes to act as controls. Blood was obtained from all individuals at enrolment (day 0), and at days 5, 12 and 28 from surviving melioidosis patients. Interferon-γ (IFN-γ), interleukin-1β (IL-1β), IL-2, IL-4, IL-6, IL-8, IL-10, IL-12p70, IL-13, IL-17A, IL-23, and tumour necrosis factor-α (TNF-α) were assayed in plasma. The association of each cytokine and its dynamics with 28-day mortality was determined.\n    \n\n\n          Results:\n        \n      \n      Of the individuals with melioidosis, 131/161 (81%) were bacteraemic, and 68/161 (42%) died. On enrolment, median levels of IFN-γ, IL-6, IL-8, IL-10, IL-23 and TNF-α were higher in individuals with melioidosis compared with uninfected healthy individuals and all but IFN-γ were positively associated with 28-day mortality. Interleukin-8 provided the best discrimination of mortality (area under the receiver operating characteristic curve 0.78, 95% CI 0.71-0.85). Over time, non-survivors had increasing IL-6, IL-8 and IL-17A levels, in contrast to survivors. In joint modelling, temporal trajectories of IFN-γ, IL-6, IL-8, IL-10 and TNF-α predicted survival.\n    \n\n\n          Conclusions:\n        \n      \n      In a severely ill cohort of individuals with melioidosis, specific pro- and anti-inflammatory and T helper type 17 cytokines were associated with survival from melioidosis, at enrolment and over time. Persistent inflammation preceded death. These findings support further evaluation of these mediators as prognostic biomarkers and to guide targeted immunotherapeutic development for severe melioidosis."
        },
        {
            "title": "Predictive value of the RIFLE urine output criteria on contrast-induced nephropathy in critically ill patients.",
            "abstract": "Background:\n        \n      \n      To investigate the predictive value of decreased urine output based on the Risk of renal dysfunction, Injury to the kidney, Failure of kidney function, Loss of kidney function and End-stage renal disease (RIFLE) classification on contrast- induced acute kidney injury (CA-AKI) in intensive care (ICU) patients.\n    \n\n\n          Methods:\n        \n      \n      All patients who received contrast media (CM) injection for CT scan or coronary angiography during a 3-year period in a 24 bed medico-surgical ICU were reviewed.\n    \n\n\n          Results:\n        \n      \n      Daily serum creatinine concentrations and diuresis were measured for 3 days after CM injection. We identified 23 cases of CA-AKI in the 149 patients included (15.4 %). Patients who developed CA-AKI were more likely to require renal replacement therapy and had higher ICU mortality rates. At least one RIFLE urine output criteria was observed in 45 patients (30.2 %) and 14 of these 45 patients (31.1 %) developed CA-AKI based on creatinine concentrations. In 30 % of these cases, urine output decreased or didn't change after the increase in creatinine concentrations. The RIFLE urine output criteria had low sensitivity (39.1 %) and specificity (67.9 %) for prediction of CA-AKI, a low positive predictive value of 50 % and a negative predictive value of 87.2 %. The maximal dose of vasopressors before CM was the only independent predictive factor for CA-AKI.\n    \n\n\n          Conclusions:\n        \n      \n      CA-AKI is a frequent pathology observed in ICU patients and is associated with increased need for renal replacement therapy and increased mortality. The predictive value of RIFLE urine output criteria for the development of CA-AKI based on creatinine concentrations was low, which limits its use for assessing the effects of therapeutic interventions on the development and progression of AKI."
        },
        {
            "title": "Prevalence and outcomes of frailty in Korean elderly population: comparisons of a multidimensional frailty index with two phenotype models.",
            "abstract": "Background:\n        \n      \n      Frailty is related to adverse outcomes in the elderly. However, current status and clinical significance of frailty have not been evaluated for the Korean elderly population. We aimed to investigate the usefulness of established frailty criteria for community-dwelling Korean elderly. We also tried to develop and validate a new frailty index based on a multidimensional model.\n    \n\n\n          Methods:\n        \n      \n      We studied 693 participants of the Korean Longitudinal Study on Health and Aging (KLoSHA). We developed a new frailty index (KLoSHA Frailty Index, KFI) and compared predictability of it with the established frailty indexes from the Cardiovascular Health Study (CHS) and Study of Osteoporotic Fracture (SOF). Mortality, hospitalization, and functional decline were evaluated.\n    \n\n\n          Results:\n        \n      \n      The prevalence of frailty was 9.2% (SOF index), 13.2% (CHS index), and 15.6% (KFI). The criteria from CHS and KFI correlated with each other, but SOF did not correlate with KFI. During the follow-up period (5.6 ± 0.9 years), 97 participants (14.0%) died. Frailty defined by KFI predicted mortality better than CHS index (c-index: 0.713 and 0.596, respectively; p<0.001, better for KFI). In contrast, frailty by SOF index was not related to mortality. The KFI showed better predictability for following functional decline than CHS index (area under the receiver-operating characteristic curve was 0.937 for KFI and 0.704 for CHS index, p = 0.001). However, the SOF index could not predict subsequent functional decline. Frailty by the KFI (OR = 2.13, 95% CI 1.04-4.35) and CHS index (OR = 2.24, 95% CI 1.05-4.76) were associated with hospitalization. In contrast, frailty by the SOF index was not correlated with hospitalization (OR = 1.43, 95% CI 0.68-3.01).\n    \n\n\n          Conclusion:\n        \n      \n      Prevalence of frailty was higher in Korea compared to previous studies in other countries. A novel frailty index (KFI), which includes domains of comprehensive geriatric assessment, is a valid criterion for the evaluation and prediction of frailty in the Korean elderly population."
        },
        {
            "title": "Nonarteritic anterior ischaemic optic neuropathy and its association with obstructive sleep apnoea: a health insurance database study.",
            "abstract": "Background:\n        \n      \n      Nonarteritic anterior ischaemic optic neuropathy (NAION) is the most common acute optic neuropathy in old age. Although there are several known risk factors, the influence of obstructive sleep apnoea (OSA) has not been completely elucidated. The aim of this study was to evaluate the association between NAION and OSA.\n    \n\n\n          Methods:\n        \n      \n      This retrospective, longitudinal cohort study used the national health insurance database of Taiwan covering the period 1996-2013. Patients without NAION at the diagnosis of OSA or who developed NAION 1 year after the diagnosis of OSA were enrolled. The patients were followed until death or the last day of the study. Cox proportional hazard regression was used to compute hazard ratios (HRs) and 95% confidence intervals (CIs) to investigate the association between OSA and NAION.\n    \n\n\n          Results:\n        \n      \n      There were 8488 patients in the OSA group and 33 952 in the control group (without OSA), for a ratio of approximately 1:4. The percentages of NAION were 0.36% and 0.2% in the OSA and control groups, respectively, with a statistically significant difference (p < 0.01; chi-square test), and this significant difference remained in multivariate analysis (p = 0.019) with a significantly higher HR (1.66; 95% CI: 1.08-2.55). There was significant difference in the 30-39 years age group in multivariate analysis (p < 0.01, HR: 6.30; 95% CI: 2.28-17.40).\n    \n\n\n          Conclusion:\n        \n      \n      There was a strong association between NAION and OSA, and the patients with OSA had a higher risk of NAION. Further large-scale, prospective studies are warranted to evaluate the effect of OSA on developing NAION."
        },
        {
            "title": "Stereotactic radiotherapy in the treatment of juxtapapillary choroidal melanoma: preliminary results.",
            "abstract": "Purpose:\n        \n      \n      To evaluate the preliminary results of stereotactic radiotherapy in the management of patients with juxtapapillary choroidal melanoma.\n    \n\n\n          Methods & materials:\n        \n      \n      A retrospective, consecutive case series of 28 patients with choroidal melanoma located within 2 mm of the optic nerve who were treated with stereotactic radiotherapy at Princess Margaret Hospital, Toronto, between October 1998 and May 2001.\n    \n\n\n          Results:\n        \n      \n      Median age was 62 years. Median tumor height was 4.6 mm and median maximum tumor diameter was 9.4 mm. The prescribed radiation dose was 70 Gy in five fractions over 10 days and median follow-up was 18.5 months. Posttreatment, 2 patients developed local tumor regrowth and 3 patients developed liver metastases. Actuarial rates of local tumor control, metastases, and survival at 18 months were 96%, 10%, and 94%, respectively. Actuarial rates of radiation-induced neovascular glaucoma, cataract, retinopathy, and optic neuropathy at 18 months were 20%, 29%, 30%, and 37%, respectively. A higher radiation dose to the lens was associated with an increased risk of cataract (p = 0.02).\n    \n\n\n          Conclusions:\n        \n      \n      Stereotactic radiotherapy offers a noninvasive alternative to enucleation and brachytherapy in the management of juxtapapillary choroidal melanoma. However, further efforts are needed to optimize local tumor control and minimize radiation-induced complications."
        },
        {
            "title": "A questionnaire-based (UM-PDHQ) study of hallucinations in Parkinson's disease.",
            "abstract": "Background:\n        \n      \n      Hallucinations occur in 20-40% of PD patients and have been associated with unfavorable clinical outcomes (i.e., nursing home placement, increased mortality). Hallucinations, like other non-motor features of PD, are not well recognized in routine primary/secondary clinical practice. So far, there has been no instrument for uniform characterization of hallucinations in PD. To this end, we developed the University of Miami Parkinson's disease Hallucinations Questionnaire (UM-PDHQ) that allows comprehensive assessment of hallucinations in clinical or research settings.\n    \n\n\n          Methods:\n        \n      \n      The UM-PDHQ is composed of 6 quantitative and 14 qualitative items. For our study PD patients of all ages and in all stages of the disease were recruited over an 18-month period. The UPDRS, MMSE, and Beck Depression and Anxiety Inventories were used for comparisons.\n    \n\n\n          Results and discussion:\n        \n      \n      Seventy consecutive PD patients were included in the analyses. Thirty-one (44.3%) were classified as hallucinators and 39 as non-hallucinators. No significant group differences were observed in terms of demographics, disease characteristics, stage, education, depressive/anxiety scores or cognitive functioning (MMSE) between hallucinators and non-hallucinators. Single mode hallucinations were reported in 20/31 (visual/14, auditory/4, olfactory/2) whereas multiple modalities were reported in 11/31 patients. The most common hallucinatory experience was a whole person followed by small animals, insects and reptiles.\n    \n\n\n          Conclusion:\n        \n      \n      Using the UM-PDHQ, we were able to define the key characteristics of hallucinations in PD in our cohort. Future directions include the validation of the quantitative part of the questionnaire than will serve as a rating scale for severity of hallucinations."
        },
        {
            "title": "Long-term suprapubic catheterisation: clinical outcome and satisfaction survey.",
            "abstract": "We report on the clinical outcome and satisfaction survey of long-term suprapubic catheterisation in patients with neuropathic bladder dysfunction. Between early 1988 and later 1995, 185 suprapubic catheters were inserted under direct cystoscopic vision. Anti-cholinergic therapy was given to all patients with significant detrusor hyper-reflexia; the catheters clamped daily for two hours and changed every six weeks. Ultrasonography and assessment of the serum creatinine were used to assess the upper renal tracts, and the results of the pre- and post-catheter video-cystometrography was used to evaluate bladder morphology, cystometric capacity, maximum detrusor pressure and the presence of vesico-ureteric reflux. There were equivalent numbers of males and females. The follow-up ranges from 3-68 months. Following catheterisation, there was a 50% reduction in the average maximum detrusor pressure, bladder morphology improved in 85% of the cases; the bladder capacity and upper renal tracts remained unchanged. Vesico-ureteric reflux was abolished in 33% of the cases. Complaints were common consisting of recurrent catheter blockage, persistent urinary leakage and recurrent urinary tract infections. There was a 2.7% incidence of small bowel injury with one fatality. However, the general level of satisfaction was high. It is concluded that suprapubic catheterisation is an effective and well tolerated method of management in selected patients with neuropathic bladder dysfunction for whom only major surgery would otherwise provide a solution to incontinence. We are encouraged to find preservation of renal function with maintained bladder volumes and reduced maximum detrusor pressures thus justifying the policy of catheter clamping and anti-cholinergic therapy in the presence of significant detrusor hyper-reflexia. However, even in expert hands this procedure is not without hazards."
        },
        {
            "title": "Transition of early-phase treatment for acute pancreatitis: An analysis of nationwide epidemiological survey.",
            "abstract": "Treatment of acute pancreatitis (AP) is one of the critical challenges to the field of gastroenterology because of its high mortality rate and high medical costs associated with the treatment of severe cases. Early-phase treatments for AP have been optimized in Japan, and clinical guidelines have been provided. However, changes in early-phase treatments and the relationship between treatment strategy and clinical outcome remain unclear. Retrospective analysis of nationwide epidemiological data shows that time for AP diagnosis has shortened, and the amount of initial fluid resuscitation has increased over time, indicating the compliance with guidelines. In contrast, prophylactic use of broad-spectrum antibiotics has emerged. Despite the potential benefits of early enteral nutrition, its use is still limited. The roles of continuous regional arterial infusion in the improvement of prognosis and the prevention of late complications are uncertain. Furthermore, early-phase treatments have had little impact on late-phase complications, such as walled-off necrosis, surgery requirements and late (> 4 w) AP-related death. Based on these observations, early-phase treatments for AP in Japan have approached the optimal level, but late-phase complications have become concerning issues. Early-phase treatments and the therapeutic strategy for late-phase complications both need to be optimized based on firm clinical evidence and cost-effectiveness."
        },
        {
            "title": "Computed tomographic pulmonary angiography in the assessment of severity of acute pulmonary embolism and right ventricular dysfunction.",
            "abstract": "Background:\n        \n      \n      The distinction between severe pulmonary embolism (PE) and right heart dysfunction is important for predicting patient mortality.\n    \n\n\n          Purpose:\n        \n      \n      To identify the role of computed tomographic pulmonary angiography (CTPA) in the assessment of the severity of acute PE and right ventricular dysfunction.\n    \n\n\n          Material and methods:\n        \n      \n      Eighty-five patients suspected of having PE, as diagnosed by CTPA and scintigraphy, were divided into three groups: hemodynamically unstable PE (HUPE) (n = 20), hemodynamically stable PE (HSPE) (n = 33), and no PE (n = 32). For each patient, obstruction scores, including short-axis diameters of the right ventricle (RV) and left ventricle (LV), main pulmonary artery, and superior vena cava (SVC), were measured. The RV/LV short-axis ratios were calculated. The shapes of the interventricular septum and the reflux of the contrast medium into the inferior vena cava (IVC) were evaluated. The mortality due to PE within a 1-month follow-up period was recorded.\n    \n\n\n          Results:\n        \n      \n      The median CTPA obstruction score (HUPE 64%, HSPE 28%, P < 0.001), median RV/LV short-axis ratio (HUPE 1.4, HSPE 1.0, P < 0.01), median RV diameter (HUPE 55 mm, HSPE 42 mm, P < 0.001), median SVC diameter (HUPE 23 mm, HSPE 19 mm, P < 0.01), interventricular septum convex toward the LV (HUPE 70%, HSPE 18%, P < 0.001), and reflux of the contrast medium into the IVC (HUPE 65%, HSPE 33%, p < 0.05) were significantly different between the HUPE and HSPE groups. With ROC analysis, the CTPA obstruction score and RV/LV short-axis ratio threshold values for the HUPE patients were calculated to be 48% (95% sensitivity, 76% specificity) and 1.1 (85% sensitivity, 76% specificity), respectively. Three patients in the HUPE group died within the first 24 hours. Logistic regression methods revealed only the RV diameter as a significant predictor of death (odds ratio 1.24; 95% CI 1.04-1.48; P = 0.01).\n    \n\n\n          Conclusion:\n        \n      \n      This study found that the parameters useful for distinguishing HUPE and HSPE included CTPA obstruction score, RV and SVC diameters, RV/LV short-axis ratio, interventricular septum shape, and reflux into the IVC. RV dilatation may be a significant predictor for mortality."
        },
        {
            "title": "Pancreatic stone protein predicts positive sputum bacteriology in exacerbations of COPD.",
            "abstract": "Background:\n        \n      \n      Pancreatic stone protein/regenerating protein (PSP/reg) serum levels are supposed to be increased in bacterial inflammation. PSP/reg levels also might be useful, therefore, as a predictor of bacterial infection in COPD.\n    \n\n\n          Methods:\n        \n      \n      Two hundred consecutive patients presenting to the ED due to acute exacerbation of COPD were prospectively assessed. Patients were evaluated based on clinical, laboratory, and lung functional parameters at admission (exacerbation) and after short-term follow-up (14-21 days). PSP/reg serum values were measured by a newly developed enzyme-linked immunosorbent assay.\n    \n\n\n          Results:\n        \n      \n      PSP/reg levels were elevated in subjects with COPD exacerbation (23.8 ng/mL; 95% CI, 17.1-32.7) when compared with those with stable disease (19.1 ng/mL; 95% CI, 14.1-30.4; P 5 .03) and healthy control subjects (14.0 ng/mL; 95% CI , 12.0-19.0; P , .01). Higher PSP/reg values were observed in exacerbations with positive sputum bacteriology compared with those with negative sputum bacteriology (26.1 ng/mL [95% CI, 19.2-38.1] vs 20.8 ng/mL [95% CI , 15.6-27.2]; P , .01). Multivariate regression analysis revealed PSP/reg level as an independent predictor of positive sputum bacteriology. A combination of a PSP/reg cutoff value of . 33.9 ng/mL and presence of discolored sputum had a specificity of 97% to identify patients with pathogenic bacteria on sputum culture. In contrast, PSP/reg levels , 18.4 ng/mL and nonpurulent sputum ruled out positive bacterial sputum culture (sensitivity, 92%). In survival analysis, high PSP/reg levels at hospital admission were associated with increased 2-year mortality.\n    \n\n\n          Conclusions:\n        \n      \n      Serum PSP/reg level might represent a promising new biomarker to identify bacterial etiology of COPD exacerbation."
        },
        {
            "title": "Lens opacities and mortality : the Barbados Eye Studies.",
            "abstract": "Objective:\n        \n      \n      To evaluate the association between cataract and mortality in a black population by type of opacity, which has not been documented previously.\n    \n\n\n          Design:\n        \n      \n      Population-based cohort study.\n    \n\n\n          Participants:\n        \n      \n      The Barbados Incidence Study of Eye Diseases reexamined the Barbados Eye Study cohort, identified through a simple random sample of predominantly black Barbadian-born citizens, aged 40 to 84 years. Of those eligible, 85% (3427 participants) had a 4-year follow-up visit.\n    \n\n\n          Methods:\n        \n      \n      Baseline and follow-up visits included an interview, blood pressure and other measurements, and a detailed ophthalmologic examination with slit-lamp lens gradings (Lens Opacities Classification System [LOCS] II protocol). Mortality at follow-up was verified from Ministry of Health records.\n    \n\n\n          Main outcome measures:\n        \n      \n      Lens opacities were defined by a LOCS II score of 2 or more. Opacity types were classified in two ways: (1) single (cortical-only, nuclear-only, and posterior subcapsular-only) and mixed opacities; and (2) any cortical, any nuclear, or any posterior subcapsular opacities. Information on dates and causes of death was obtained from death certificates.\n    \n\n\n          Results:\n        \n      \n      Cardiovascular disease was the principal cause of death in black participants (3.6%), followed by malignant neoplasms (1.4%). The cumulative 4-year mortality varied with lens types, increasing from 3.2% for those without cataract to 6.0% for cortical-only, 8.8% for nuclear-only, and 20.9% for mixed opacities. Persons with mixed opacities had a 1.6-fold increase in mortality, while controlling for other factors (age, male gender, diabetes, hypertension, obesity, cigarette smoking, cardiovascular disease, and family history of diabetes) in Cox proportional-hazards regression analyses. Persons with any nuclear opacities also had increased mortality (death rate ratio, 1.5). The death rate ratios increased with age, but peaked at age 60 to 69 years. Coexisting diabetes further increased mortality: people with mixed opacities and diabetes had a 2.7-fold increased risk of death. A trend toward increased mortality from neoplasms was observed for individuals with mixed opacities or with any nuclear opacities.\n    \n\n\n          Conclusions:\n        \n      \n      Participants with mixed opacities or any nuclear opacities had increased 4-year mortality rates, with diabetes acting as an effect modifier. This study is the first to identify a relationship between type of cataract and mortality in an African-descent population."
        },
        {
            "title": "Clinical and prognostic relevance of echocardiographic evaluation of right ventricular geometry in patients with idiopathic pulmonary arterial hypertension.",
            "abstract": "The aim of the present study was to assess the clinical and prognostic significance of right ventricular (RV) dilation and RV hypertrophy at echocardiography in patients with idiopathic pulmonary arterial hypertension. Echocardiography and right heart catheterization were performed in 72 consecutive patients with idiopathic pulmonary arterial hypertension admitted to our institution. The median follow-up period was 38 months. The patients were grouped according to the median value of RV wall thickness (6.6 mm) and the median value of the RV diameter (36.5 mm). On multivariate analysis, the mean pulmonary artery pressure (p = 0.018) was the only independent predictor of RV wall thickness, and age (p = 0.011) and moderate to severe tricuspid regurgitation (p = 0.027) were the independent predictors of RV diameter. During follow-up, 22 patients died. The death rate was greater in the patients with a RV diameter >36.5 mm than in patients with a RV diameter ≤36.5 mm: 15.9 (95% confidence interval 9.4 to 26.8) vs 6.6 (95% confidence interval 3.3 to 13.2) events per 100-person years (p = 0.0442). In contrast, the death rate was similar in patients with RV wall thickness above or below the median value. However, among the patients with a RV wall thickness >6.6 mm, a RV diameter >36 mm was not associated with a poorer prognosis (p = 0.6837). In conclusion, in patients with idiopathic pulmonary arterial hypertension, a larger RV diameter is a marker of a poor prognosis but a greater RV wall thickness reduces the risk of death associated with a dilated right ventricle."
        },
        {
            "title": "The preventable burden of breast cancers for premenopausal and postmenopausal women in Australia: A pooled cohort study.",
            "abstract": "Estimates of the future breast cancer burden preventable through modifications to current behaviours are lacking. We assessed the effect of individual and joint behaviour modifications on breast cancer burden for premenopausal and postmenopausal Australian women, and whether effects differed between population subgroups. We linked pooled data from six Australian cohort studies (n = 214,536) to national cancer and death registries, and estimated the strength of the associations between behaviours causally related to cancer incidence and death using adjusted proportional hazards models. We estimated exposure prevalence from representative health surveys. We combined these estimates to calculate Population Attributable Fractions (PAFs) with 95% confidence intervals (CIs), and compared PAFs for population subgroups. During the first 10 years follow-up, there were 640 incident breast cancers for premenopausal women, 2,632 for postmenopausal women, and 8,761 deaths from any cause. Of future breast cancers for premenopausal women, any regular alcohol consumption explains 12.6% (CI = 4.3-20.2%), current use of oral contraceptives for ≥5 years 7.1% (CI = 0.3-13.5%), and these factors combined 18.8% (CI = 9.1-27.4%). Of future breast cancers for postmenopausal women, overweight or obesity (BMI ≥25 kg/m2 ) explains 12.8% (CI = 7.8-17.5%), current use of menopausal hormone therapy (MHT) 6.9% (CI = 4.8-8.9%), any regular alcohol consumption 6.6% (CI = 1.5-11.4%), and these factors combined 24.2% (CI = 17.6-30.3%). The MHT-related postmenopausal breast cancer burden varied by body fatness, alcohol consumption and socio-economic status, the body fatness-related postmenopausal breast cancer burden by alcohol consumption and educational attainment, and the alcohol-related postmenopausal breast cancer burden by breast feeding history. Our results provide evidence to support targeted and population-level cancer control activities."
        },
        {
            "title": "Mortality after a cerebrovascular event in age-related macular degeneration patients treated with bevacizumab ocular injections.",
            "abstract": "Purpose:\n        \n      \n      To analyse the mortality associated with intravitreal injections of bevacizumab for age-related macular degeneration (AMD) in patients previously diagnosed with stroke or transient ischaemic attack (TIA).\n    \n\n\n          Methods:\n        \n      \n      We reviewed bevacizumab-treated AMD patients with a diagnosis of stroke or TIA prior to their first bevacizumab injection (n = 948). Those patients, naïve to any anti-vascular endothelial growth factor (anti-VEGF) at the time of stroke/TIA, were then compared to age- and gender-matched patients who had a stroke/TIA at the same time and had never been exposed to anti-VEGF. Survival analysis was performed using adjusted Cox regression. The main outcome measure was survival. Adjusted variables were age, smoking, alcohol abuse, hypertension, diabetes mellitus, obesity, ischaemic heart disease, congestive heart failure and liver cancer.\n    \n\n\n          Results:\n        \n      \n      Age and gender distribution of bevacizumab-treated patients and controls were similar (mean age: 83.4 versus 83.7 years, p = 0.3; 51.7% males versus 52.5% males, p = 0.7). The adjusted mortality in patients who received bevacizumab within 3 months after stroke/TIA was significantly different than in patients non-exposed to bevacizumab (OR = 6.92, 95%, CI 1.88-25.43, p < 0.01). Within 6 months after stroke/TIA, the difference in adjusted mortality showed a strong trend (OR = 2.00, 95%, CI 0.96-4.16, p = 0.064). Within 12 months, it was insignificant (OR = 1.30, 95%, CI 0.75-2.26, p = 0.348).\n    \n\n\n          Conclusion:\n        \n      \n      We found increased mortality within three months after a cerebrovascular event in patients treated with bevacizumab for AMD compared to patients for whom there was no record of a prescription to any anti-VEGF agent."
        },
        {
            "title": "High-sensitivity C-reactive protein predicts contrast-induced nephropathy after primary percutaneous coronary intervention.",
            "abstract": "Background:\n        \n      \n      Few studies have investigated hs-CRP as a risk factor for contrast-induced nephropathy (CIN). The aim of this study was to evaluate the predictive value of high-sensitivity C-reactive protein (hs-CRP) for risk of CIN in patients with acute ST-segment elevation myocardial infarction (STEMI) who were undergoing primary percutaneous coronary intervention (PCI).\n    \n\n\n          Methods:\n        \n      \n      We prospectively observed 165 consenting patients with STEMI undergoing primary PCI. An increase in serum creatinine of more than 0.5 mg/dL from baseline within 48-72 hours of contrast media exposure was defined as CIN. Demographics, traditional risk factors, CIN incidence and other in-hospital clinical outcomes were compared among hs-CRP quartiles. Receiver operator characteristic curves were used to identify the optimal sensitivity for the observed range of hs-CRP. The predictive value of hs-CRP for the risk of CIN was assessed using multivariate logistic regression.\n    \n\n\n          Results:\n        \n      \n      CIN occurred in 17 patients (10%). Univariate analysis revealed CIN incidence was significantly associated with hs-CRP, with 2.4% for quartile Q1 (<6.00 mg/L), 2.3% for Q2 (6.00-13.90), 12.5% for Q3 (13.91-32.75) and 24.4% for Q4 (>32.75) (P-trend <0.001), as was in-hospital death (0% for Q1, 2.3% for Q2, 5% for Q3 and 12.2% for Q4; P-trend = 0.009). Receiver operator characteristic curve analysis showed that an hs-CRP of 16.10 mg/L was a fair discriminator for the early creatinine increase (C statistic 0.78). After adjusting for potential confounding predictors, hs-CRP >16.10 mg/L remained significantly associated with CIN (odds ratio = 6.51; 95% confidence interval, 1.26-33.61).\n    \n\n\n          Conclusion:\n        \n      \n      An hs-CRP >16.10 was a significant and independent predictor of CIN after primary PCI in patients with STEMI."
        },
        {
            "title": "The evaluation of creatinine clearance, estimated glomerular filtration rate and serum creatinine in predicting contrast-induced acute kidney injury among patients undergoing percutaneous coronary intervention.",
            "abstract": "Purpose:\n        \n      \n      The purpose of the study was to compare creatinine clearance (CrCl), estimated glomerular filtration rate (eGFR) and serum creatinine (SCr) in predicting contrast-induced acute kidney injury (CI-AKI), dialysis and death following percutaneous coronary intervention (PCI).\n    \n\n\n          Methods and materials:\n        \n      \n      Data were prospectively collected on 7759 consecutive patients within the Dartmouth Dynamic Registry undergoing PCI between January 1, 2000, and December 31, 2006. Renal function was measured at baseline and within 48 h after PCI using three methods: CrCl using the Cockcroft-Gault equation, eGFR using the abbreviated Modification of Diet in Renal Disease equation and SCr. We compared CrCl, eGFR and SCr in predicting CI-AKI, post-PCI dialysis-dependent renal failure and in-hospital mortality. Areas under the receiver operating characteristic curve (ROC) were calculated using logistic regression and tested for equality.\n    \n\n\n          Results:\n        \n      \n      On univariable analysis, CrCl [ROC: 0.69; 95% confidence interval (CI): 0.67-0.72] predicted CI-AKI better than eGFR (ROC: 0.67; 95% CI: 0.64-0.70) (P=.013) and SCr (ROC: 0.64; 95% CI: 0.61-0.67) (P<.001). Creatinine clearance (ROC: 0.73; 95% CI: 0.69-0.77) and eGFR (ROC: 0.70; 95% CI: 0.65-0.74) outperformed SCr for predicting in-hospital mortality. On multivariable analysis, CrCl (ROC: 0.77; 95% CI: 0.75-0.80), SCr (ROC: 0.78; 95% CI: 0.76-0.80) and eGFR (ROC: 0.77; 95% CI: 0.75-0.80) predicted CI-AKI well. Creatinine clearance (ROC: 0.88; 95% CI: 0.85-0.90) and eGFR (ROC: 0.87; 95% CI: 0.85-0.90) were strong independent predictors of in-hospital mortality.\n    \n\n\n          Conclusions:\n        \n      \n      Creatinine clearance, eGFR and SCr predict CI-AKI equally well. Creatinine clearance and eGFR are strong independent predictors of in-hospital mortality."
        },
        {
            "title": "Self-reported use of eye care among Latinos: the Los Angeles Latino Eye Study.",
            "abstract": "Purpose:\n        \n      \n      To identify the prevalence and determinants of self-reported eye care use in Latinos.\n    \n\n\n          Design:\n        \n      \n      Population-based ocular epidemiologic study in Latinos aged 40+ years living in La Puente, California.\n    \n\n\n          Participants:\n        \n      \n      A total of 5455 participants.\n    \n\n\n          Methods:\n        \n      \n      Univariate, multivariable, and stepwise logistic regression analyses were conducted to identify predisposing, enabling, and need variables associated with self-reported eye care use.\n    \n\n\n          Main outcome measures:\n        \n      \n      Prevalence of self-reported use: eye care visit, having had a dilated examination in the past 12 months, ever having had a dilated examination, and odds ratios for factors associated with self-reported use.\n    \n\n\n          Results:\n        \n      \n      Overall, 36% of participants reported an eye care visit and 19% reported having a dilated examination in the past year. Fifty-seven percent reported ever having had a dilated eye examination. Greater eye care use was associated with older age, female gender, bilingual language proficiency (English and Spanish), more education, having health insurance, having a usual place for care, having a regular provider of care, a greater number of comorbidities, visual impairment, and lower vision-specific quality of life scores.\n    \n\n\n          Conclusions:\n        \n      \n      Multiple modifiable factors are associated with greater use and access to eye care for Latinos. Modification of these factors should be a priority because visual impairment has significant impacts on well-being and mortality."
        },
        {
            "title": "Prognostic significance of tumor-enhancement and angiogenesis in oligodendroglioma.",
            "abstract": "Objective:\n        \n      \n      To study the prognostic significance of angiogenesis and enhancement on contrast-enhanced computerized tomography (CT) in oligodendrogliomas.\n    \n\n\n          Material and methods:\n        \n      \n      CD34 immunostaining was employed in samples of 26 low-grade oligodendrogliomas from patients treated by extensive resection and radiotherapy to determine the tumor angiogenesis index (TAI), calculated by measuring the immunostained endothelial surface area, in microm(2), per 1000 tumor cells. Preoperative CT scan was evaluated in each case, and the absence or presence of tumor enhancement after contrast administration was recorded. Survival was analyzed and statistically compared for subgroups of patients with lesions in which the TAI was less than or greater than 15, and for subgroups of patients having tumors showing presence or absence of enhancement on contrast-enhanced CT.\n    \n\n\n          Results:\n        \n      \n      Survival of patients with tumors showing a TAI of less than 15 was 100% and 71% at 5 and 10 years, respectively, vs a survival of 50% and 0% for patients showing a TAI of more than 15 (P < 0.05). The 14 patients whose tumors showed enhancement in preoperative contrast-enhanced CT had 5- and 10-year survival rates of 57% and 14%, respectively, vs 100% and 83% for the 12 patients whose tumors presented no enhancement (P < 0.05). Moreover, 79% of the tumors showing contrast enhancement had a TAI greater than 15, while 92% of those exhibiting no enhancement had a TAI of less than 15.\n    \n\n\n          Conclusion:\n        \n      \n      These findings indicate a relationship between enhancement on preoperative CT scan and endothelial surface area in oligodendrogliomas, and suggest that this enhancement and the TAI may be considered angiogenesis-related factors with similar prognostic significance in terms of survival."
        },
        {
            "title": "Diabetic retinopathy is associated with the presence and burden of subclinical carotid atherosclerosis in type 1 diabetes.",
            "abstract": "Background:\n        \n      \n      Cardiovascular (CV) disease due to atherosclerosis is a major cause of morbidity and mortality in adult patients with diabetes, either type 1 or type 2 diabetes. The aim of the study was to assess the association of the frequency and the burden of subclinical carotid atherosclerotic disease in patients with type 1 diabetes according to the presence and severity of diabetic retinopathy (DR).\n    \n\n\n          Methods:\n        \n      \n      A cross-sectional study was conducted in 340 patients with type 1 diabetes (41.5% with DR), and in 304 non-diabetic individuals. All participants were free from previous CV disease and chronic kidney disease (CKD). B-mode carotid ultrasound imaging was performed in all the study subjects. Patients with type 1 diabetes underwent a full eye examination, and DR patients were divided into two groups: mild disease and advanced disease.\n    \n\n\n          Results:\n        \n      \n      In the group of patients with type 1 diabetes, the percentage of patients with carotid plaques was higher in those with DR compared with those without DR (44.7% vs. 24.1%, p < 0.001). Patients with DR also presented a higher incidence of ≥ 2 carotid plaques (25.5% vs. 11.1%, p < 0.001). Apart from other traditional cardiovascular risk factors, the presence of advanced stages of DR was independently associated with the presence (p = 0.044) and the burden (≥ 2 carotid plaques; p = 0.009) of subclinical carotid atherosclerosis.\n    \n\n\n          Conclusions:\n        \n      \n      In patients with type 1 diabetes without previous CV disease or established CKD, the presence of advanced stages of DR is associated with a higher atherosclerotic burden in the carotid arteries. The presence of DR identifies patients at risk for carotid atherosclerotic disease."
        },
        {
            "title": "[Introduction of a mammography screening program in Germany. Consideration of benefits and risks].",
            "abstract": "For women between 50 and 70 years of age, X-ray mammography presently represents the most effective method for early breast cancer detection. It is commonly accepted that quality assured mammography examinations conducted at regular intervals can reduce mortality from breast cancer. In the year 2002, the German Bundestag agreed to the implementation of a mammography screening program for Germany based on the European guidelines. The effectiveness of a mammography screening program is controversially discussed and two of the most commonly cited hazards are the occurrence of false-positive results and the so-called overdiagnosis. Another issue of criticism is the radiation risk due to the mammography examinations. However, in women aged 50-70 years the radiation risk has no substantial importance. In contrast to the present situation in Germany in which opportunistic screening is widespread, standardized quality assured screening will guarantee that false-positive rates are kept as low as possible and that further assessment diagnostics are effective and minimally invasive."
        },
        {
            "title": "Deep Learning Enables Automatic Classification of Emphysema Pattern at CT.",
            "abstract": "BackgroundPattern of emphysema at chest CT, scored visually by using the Fleischner Society system, is associated with physiologic impairment and mortality risk.PurposeTo determine whether participant-level emphysema pattern could predict impairment and mortality when classified by using a deep learning method.Materials and MethodsThis retrospective analysis of Genetic Epidemiology of COPD (COPDGene) study participants enrolled between 2007 and 2011 included those with baseline CT, visual emphysema scores, and survival data through 2018. Participants were partitioned into nonoverlapping sets of 2407 for algorithm training, 100 for validation and parameter tuning, and 7143 for testing. A deep learning algorithm using convolutional neural network and long short-term memory architectures was trained to classify pattern of emphysema according to Fleischner criteria. Deep learning scores were compared with visual scores and clinical parameters including pulmonary function tests. Cox proportional hazard models were used to evaluate relationships between emphysema scores and survival. The algorithm was also tested by using CT and clinical data in 1962 participants enrolled in the Evaluation of COPD Longitudinally to Identify Predictive Surrogate End-points (ECLIPSE) study.ResultsA total of 7143 COPDGene participants (mean age ± standard deviation, 59.8 years ± 8.9; 3734 men and 3409 women) were evaluated. Deep learning emphysema classifications were associated with impaired pulmonary function tests, 6-minute walk distance, and St George's Respiratory Questionnaire at univariate analysis (P < .001 for each). Testing in the ECLIPSE cohort showed similar associations (P < .001). In the COPDGene test cohort, deep learning emphysema classification improved the fit of linear mixed models in the prediction of these clinical parameters compared with visual scoring (P < .001). Compared with participants without emphysema, mortality was greater in participants classified by the deep learning algorithm as having any grade of emphysema (adjusted hazard ratios were 1.5, 1.7, 2.9, 5.3, and 9.7, respectively, for trace, mild, moderate, confluent, and advanced destructive emphysema; P < .05).ConclusionDeep learning automation of the Fleischner grade of emphysema at chest CT is associated with clinical measures of pulmonary insufficiency and the risk of mortality.© RSNA, 2019Online supplemental material is available for this article."
        },
        {
            "title": "Age-related maculopathy: a risk indicator for poorer survival in women: the Copenhagen City Eye Study.",
            "abstract": "Purpose:\n        \n      \n      To examine patient survival in age-related maculopathy in a 14-year follow-up study.\n    \n\n\n          Design:\n        \n      \n      Population-based 14-year cohort study.\n    \n\n\n          Participants:\n        \n      \n      Nine hundred forty-six residents, aged 60 to 80 years, living in the Osterbro district of Copenhagen, Denmark, participated in the first examination conducted from 1986 to 1988. These participants were followed until death or until May 1, 2002, whichever came first.\n    \n\n\n          Methods:\n        \n      \n      Participants underwent an extensive ophthalmologic examination at Rigshospitalet, the National University Hospital of Copenhagen. Standardized protocols for physical examination, blood samples, and data from the National Central Person Register, the National Death Register, and the National Patient Register were used.\n    \n\n\n          Main outcome measures:\n        \n      \n      Mortality and age-related maculopathy.\n    \n\n\n          Results:\n        \n      \n      By May 1, 2002, 60.9% (577 of 946) of the participants of the baseline study cohort had died. The adjusted 14-year cumulative mortality hazard ratio for subjects with early and late age-related maculopathy at baseline was 1.26 (95% confidence interval [CI], 1.06-1.51). We identified a strong correlation between mortality and age-related maculopathy among women (relative risk, 1.59; 95% CI, 1.23-2.07) but not among men.\n    \n\n\n          Conclusions:\n        \n      \n      When adjusting for survival-related factors, age-related maculopathy is a significant risk indicator for poorer survival in women and may be a marker of underlying serious systemic factors or aging processes specific to women."
        },
        {
            "title": "Clinical outcomes of glaucoma treatments over a patient lifetime: a Markov model.",
            "abstract": "Purpose:\n        \n      \n      To quantify the clinical outcome of glaucoma treatment over a patient's lifetime to identify treatment parameters that would preserve vision across a population.\n    \n\n\n          Methods:\n        \n      \n      A Markov model was used to reproduce glaucoma treatments and clinical responses over a patient's lifetime. Markov states comprised first-line to fourth-line pharmacological treatments, no treatment, laser therapy, surgery, blindness, and death. All patients began with first-line treatment and passed to the next treatment in line following a failure. After each failure, and always after a fourth-line failure, patients could receive laser therapy or surgery, followed by no treatment or a new first-line treatment. Transitional probabilities came from a cross-sectional study and national statistics. Sensitivity analyses were based on second order Monte-Carlo simulation.\n    \n\n\n          Results:\n        \n      \n      Cohort demographics were average age 59.7 years, life expectancy 22.5 years, females 54.0%. Treatment durations (months) for a patient with ocular hypertension were first-line (89), second-line (49), third-line (41), and no treatment (44); laser interventions numbered 1.251 and surgical 0.882. For a patient with glaucoma the corresponding values were 70, 44, 40, 18, 42, 1.197, and 0.842. First-line or second-line drugs best at controlling intraocular pressure (IOP) reduced laser therapy, surgery, and the prevalence of blindness. The most effective drug should be prescribed first-line. According to our model drugs specifically able to arrest disease progression would be more beneficial than intraocular pressure control.\n    \n\n\n          Conclusion:\n        \n      \n      More powerful glaucoma medication (first-line or second-line) would contribute to the preservation of long-term vision."
        },
        {
            "title": "Value of contrast-enhanced computerized tomography in the early diagnosis and prognosis of acute pancreatitis. A prospective study of 202 patients.",
            "abstract": "Two hundred two patients admitted with the clinical suspicion of acute pancreatitis underwent computerized tomography scanning within 36 hours of admission. The diagnostic value of the computerized tomography findings was excellent, with a sensitivity of 92 percent and a specificity of 100 percent. One hundred seventy-six patients with acute pancreatitis defined according to the overall clinical course were included in the prognostic study. The pancreatitis was fatal in 21 patients, severe in 47 patients, and mild in 108 patients. The computerized tomography findings were classified into the following three groups on the basis of the extent of phlegmonous extrapancreatic spread: Group I, no phlegmonous extrapancreatic spread (100 patients, none died); Group II, phlegmonous extrapancreatic spread in one or two areas (28 patients, mortality rate 4 percent); and Group III, phlegmonous extrapancreatic spread in three or more areas (48 patients, mortality rate 42 percent) (p less than 0.0001). The following three scores from prognostic clinical and laboratory data were also obtained: Score 1, zero or one positive sign (82 patients, none died); Score 2, two to four positive signs (54 patients, mortality rate 13 percent); Score 3, five or more positive signs (40 patients, mortality rate 35 percent) (p less than 0.001). The combination of computerized tomography findings and prognostic signs had the best predictive value. Patients in Group III, Score 3 (24 patients) or Group III, Score 2 (19 patients) had mortality rates of 58 percent and 32 percent, respectively, and complications developed in all of the survivors. In addition, all except two acute pancreatitis patients in whom pancreatic abscess developed were found in Group III (p less than 0.0001). Furthermore, for Group III patients, the prediction of death associated with abscesses was enhanced by the number of prognostic signs. The mortality rate increased from 17 percent for Score 2 patients to 81 percent for Score 3 patients (p = 0.0078). As a result of this study, we recommend early computerized tomography for all Score 2 and Score 3 patients, since it allows prompt recognition of patients at high risk for systemic and local complications. Adequate therapy can then be directed to the group of patients to whom it is best suited. Serial computerized tomographies should be reserved for those patients presenting with phlegmonous extrapancreatic spread."
        },
        {
            "title": "Usefulness of C-Reactive Protein as a Predictor of Contrast-Induced Nephropathy After Percutaneous Coronary Interventions in Patients With Acute Myocardial Infarction and Presentation of a New Risk Score (Athens CIN Score).",
            "abstract": "Contrast-induced nephropathy (CIN) after percutaneous coronary interventions (PCI) in patients with acute myocardial infarction (AMI) is associated with high morbidity and mortality, whereas there are no reliable predictive tools easy to use. We evaluated the association of pre-procedural high-sensitivity C-reactive protein (hsCRP) with the development of CIN and integrated this variable in a new risk CIN prediction model. Consecutive patients (348 AMI subjects) who underwent PCI were recruited. Creatinine levels were detected on admission, at 24, 48, and 72 hours after PCI. CIN was defined using the Kidney Disease: Improving Global Outcomes criteria. In our study population (348 subjects), CIN developed in 54 patients (15.5%). Patients with CIN were older and had higher hsCRP at admission, whereas their ejection fraction (EF) and glomerular filtration rate (GFR) were lower. In multivariate analysis after incorporating potential confounders, hsCRP at admission was an independent predictor of CIN (OR for logCRP 2.00, p = 0.01). In receiver-operating characteristic curve analysis, a model incorporating hsCRP, age, GFR, and EF showed good accuracy in predicting the development of CIN (c statistic 0.84, 95% confidence interval 0.793 to 0.879). A total risk score derived from the proposed model yielded significant positive and negative predictive values and classified 85.8% of our patients correctly for CIN. In conclusion, measuring hsCRP levels at admission in patients who underwent PCI for AMI may offer additional assistance in predicting the development of CIN. A model incorporating age and admission hsCRP, EF, and GFR emerged as an accurate tool for predicting CIN in this context."
        },
        {
            "title": "Application of a new pulmonary artery obstruction score in the prognostic evaluation of acute pulmonary embolism: comparison with clinical and haemodynamic parameters.",
            "abstract": "Purpose:\n        \n      \n      Evaluation of computed tomography (CT) pulmonary angiography parameters revealing pulmonary embolism (PE) severity with particular attention to pulmonary obstruction indexes. Comparison with clinical and hemodynamic data and determination of predictive role in the development of chronic pulmonary heart disease.\n    \n\n\n          Materials and methods:\n        \n      \n      This retrospective study analyzes 45 not consecutive patients from November 2007 to December 2008 with CT angiography diagnosis of acute PE. Included in the study are patients at the first documented episode of acute PE, with 6 month follow-up. Patients with severe pre-existent cardiopulmonary pathology or neoplastic diseases were excluded from the study. CT angiography evaluated right ventricular (RV)/left ventricular (LV) ratio, obstruction index according to Qanadli and Total Clot Burden (Ghanima score). PE indexes were compared with Troponin I measurement and echocardiography result; at last hospitalization and intensive care time were reported.\n    \n\n\n          Results:\n        \n      \n      A significant association was found between Ghanima and Qanadli score: the two indexes are equivalent in quantification of pulmonary arterial obstruction (p<0.001). Among others CT parameters, the new Ghanima score evidenced the best accuracy to detect patients evolving to chronic pulmonary heart disease (76%). This value is higher than that of echocardiography (71%). Troponins showed highest accuracy (82%).\n    \n\n\n          Conclusions:\n        \n      \n      Ghanima score can be used in emergency CT angiography diagnosis as prognostic marker for a quickly risk stratification of pulmonary heart disease or death in patients with acute PE. This approach allows to obtain, with just one test, both the diagnosis and a rather accurate acute PE risk stratification."
        },
        {
            "title": "Predictors of five-year mortality in older Canadians: the Canadian Study of Health and Aging.",
            "abstract": "Objective:\n        \n      \n      Based on the Canadian Study of Health and Aging (CSHA), to determine the importance of cognitive status, sociodemographic factors, functional status, and other health related factors as predictors of 5-year overall mortality in older Canadians.\n    \n\n\n          Design, setting and participants:\n        \n      \n      Two partially overlapping groups from the Canadian Study of Health and Aging (1991) were identified: (1) older people living in the community (n = 8949) who had a screening interview (larger sample, fewer variables) and (2) older people who underwent an extensive clinical examination (smaller sample, more objective variables; n = 2914). Deaths in the subsequent 5 years were determined from death certificates and interviews with the caregivers. Multivariate logistic regression models, with death within 5 years as the outcome, were developed separately for men and women. Predictor variables were introduced in the following groups: sociodemographic factors, physical and cognitive status, and physical illnesses and life style factors. Parallel models were developed for the screening sample and for the clinical sample.\n    \n\n\n          Results and discussion:\n        \n      \n      Five-year mortality ranged from 10.0% (women aged 65-74 living in the community) to 88.1% (men aged 85 and older living in institutions). Multivariate models showed that the odds of death within 5 years increased with age. This effect remained after adjustment for all other variables. Odds of death increased with institutionalization and with increasing cognitive and physical impairment. Although vision and hearing problems and the presence of heart disease, stroke, and diabetes were all strongly related to 5-year mortality in univariate, unadjusted analyses, their contributions were minimal in the multivariate analyses. Increased Body Mass Index was associated with lower mortality in both univariate and multivariate analyses.\n    \n\n\n          Conclusions:\n        \n      \n      This population-based study supported the importance of gender, age, functional status, cognition, and health status in predicting 5-year mortality, and after accounting for cognitive status, physical status, and specific disease variables, the difference in mortality between older people in the community and in institutions was reduced. Knowledge about survival and prognosis is important not only for the planning of long-term facilities and home care, but it can also be helpful for clinical decision-making and for family and caregivers."
        },
        {
            "title": "Cost-effectiveness of ranibizumab compared with pegaptanib in neovascular age-related macular degeneration.",
            "abstract": "Objective:\n        \n      \n      To assess the cost-effectiveness of ranibizumab compared with pegaptanib in the treatment of patients with minimally classic/occult neovascular age-related macular degeneration (AMD), from a societal perspective in Spain.\n    \n\n\n          Methods:\n        \n      \n      We constructed a Markov model with five states defined by visual acuity (VA) in the better-seeing eye (Snellen scale): VA >20/40, < or =20/40 to >20/80, < or =20/80 to >20/200, < or =20/200 to >20/400, < or =20/400, and an additional death state. Two cohorts of patients were distributed along the VA states, and treated with either ranibizumab or pegaptanib. Transition probabilities assigned for movement between these states with both drugs were obtained from published randomized clinical trials. Medical costs related to AMD treatment and follow-up, medical costs related to AMD comorbidities, and non-medical-related costs were taken into account. Costs (2008 Euro), health outcomes (Quality-adjusted life years--QALYs), both discounted at a 3.5% annual rate, and incremental cost-effectiveness ratios (ICER: euro/QALY), were determined for a lifetime horizon in the base case analysis. Sensitivity analyses were conducted to explore different scenarios and assumptions in the model.\n    \n\n\n          Results:\n        \n      \n      Treating patients with varying degrees of visual impairment with monthly ranibizumab instead of pegaptanib was 71,206 euro more costly and provided 2.437 additional QALYs (29,224 euro/QALY). When administered on an as-needed basis, as in the Prospective Optical Coherence Tomography Imaging of Patients with Neovascular AMD Treated with Intraocular Ranibizumab (PrONTO) trial, the cost per QALY gained with ranibizumab was reduced to 4,623 euro.\n    \n\n\n          Conclusions:\n        \n      \n      The cost per QALY gained with monthly ranibizumab compared with pegaptanib in the minimally classic/occult neovascular AMD population is just below the 30,000 euro threshold below which new drugs are sometimes regarded as cost-effective strategies in Spain. In this model, the key variables with greater impact on the cost-effectiveness results were the selected time horizon and the chosen extrapolation method, the source for data on pegaptanib efficacy and the number of ranibizumab injections. When administered on an as-needed basis, ranibizumab was a cost-effective strategy compared to pegaptanib in this population."
        },
        {
            "title": "Complications of anterior clinoidectomy through lateral supraorbital approach.",
            "abstract": "Objective:\n        \n      \n      We reviewed the surgical complications from our recent experience in vascular and tumor patients who underwent anterior clinoidectomy through the lateral supraorbital (LSO) approach.\n    \n\n\n          Methods:\n        \n      \n      Between June 2007 and January 2011, a total of 82 patients with neoplastic and vascular lesions underwent anterior clinoidectomy by the senior author (J.H.) through the LSO approach. We analyzed the operative videos paying particular attention to the surgical technique used for removal of the anterior clinoid process (ACP) and compared the microsurgical nuances to postoperative complications related to anterior clinoidectomy.\n    \n\n\n          Results:\n        \n      \n      Forty-five patients were treated for aneurysms; 35 patients for intraorbital, parasellar, and suprasellar tumors; and 2 patients for carotid-cavernous fistulas. Intradural anterior clinoidectomy was performed in 67 (82%) cases; in 15 (18%) cases an extradural approach was used. In 51 (62%) cases, ACP was removed completely, whereas in the remaining 31 (38%) a tailored anterior clinoidectomy was performed. Four (5%) patients had new postoperative visual deficits and 3 (4%) experienced a worsening of preoperative visual deficits. Twelve (15%) patients improved their preoperative visual deficits after intradural anterior clinoidectomy. Ultrasonic bone device is a useful tool but may damage the optic nerve when performing anterior clinoidectomy. There was no mortality in our series.\n    \n\n\n          Conclusion:\n        \n      \n      Anterior clinoidectomy can be performed through an LSO approach with a safety profile that is comparable to other approaches. Ultrasonic bone dissector is a useful tool but may lead to injury of the optic nerve and should be used very carefully in its vicinity."
        },
        {
            "title": "Prognostic value of adenosine cardiac magnetic resonance imaging in patients presenting with chest pain.",
            "abstract": "Adenosine cardiac magnetic resonance imaging (AS-CMR) has emerged as an alternative to other stress tests for identifying coronary artery disease. From January 1, 2002 to January 1, 2009, 564 consecutive patients underwent AS-CMR for evaluation of chest pain. The clinical characteristics, AS-CMR findings, and outcomes were evaluated by retrospective chart review and telephone interview. The median follow-up was 51 months. Major adverse cardiac events (MACE) were defined as cardiac death, nonfatal myocardial infarction, and revascularization with percutaneous coronary intervention or bypass surgery. The AS-CMR findings were normal in 264, ischemic in 201, and scar in 240 patients. No cardiac death occurred in the normal AS-CMR group. Among the ischemic and scar groups, 7.2% and 8.3% experienced an event, respectively. On univariate analysis, ischemia (hazard ratio 5.3, 95% confidence interval 2.5 to 11.5, p <0.001) and the presence of scar (hazard ratio 5.7, 95% confidence interval 2.6 to 12.4, p <0.001) were independent predictors of all cardiac events. Multivariate Cox regression analysis for MACE identified the presence of ischemia (hazard ratio 2.8, 95% confidence interval 1.2 to 6.2, p = 0.01) and scarring (hazard ratio 2.9, 95% confidence interval 1.3 to 6.6, p = 0.01) as the strongest independent factors. The annual event rate for hard events was 0% in the normal, 1.7% in the scar, and 1.5% in the ischemia group. For the MACE end points, the rate was 0.5% in the normal, 2.4% in the scar, and 2.6% in the ischemia group. In conclusion, in the present, single-center cohort with chest pain, normal AS-CMR findings conferred very low risk (<1% annually) of MACE. However, the findings of ischemia or scar were a significant and independent predictor of hard events and MACE."
        },
        {
            "title": "CMV retinitis in China and SE Asia: the way forward.",
            "abstract": "AIDS-related CMV retinitis is a common clinical problem in patients with advanced HIV/AIDS in China and Southeast Asia. The disease is causing blindness, and current clinical management, commonly characterized by delayed diagnosis and inadequate treatment, results in poor clinical outcomes: 21%-36% of eyes with CMV retinitis are already blind at the time the diagnosis is first established by an ophthalmologist. CMV retinitis also identifies a group of patients at extraordinary risk of mortality, and the direct or indirect contribution of extra-ocular CMV disease to AIDS-related morbidity and mortality is currently unmeasured and clinically often overlooked. The obvious way to improve clinical management of CMV retinitis is to screen all patients with CD4 counts < 100 cells/μL with indirect ophthalmoscopy at the time they first present for care, and to provide systemic treatment with oral valganciclovir when active CMV retinitis is detected. Treatment of opportunistic infections is an integral part of HIV management, and, with appropriate training and support, CMV retinitis screening and treatment can be managed by the HIV clinicians, like all other opportunistic infections. Access to ophthalmologist has been problematic for HIV patients in China, and although non-ophthalmologists can perform screening, sophisticated ophthalmological skills are required for the management of retinal detachment and immune recovery uveitis, the major complications of CMV retinitis. CMV retinitis has been clinically ignored, in part, because of the perceived complexity and expense of treatment, and this obstacle can be removed by making valganciclovir affordable and widely available. Valganciclovir is an essential drug for developing successful programs for management of CMV retinitis in China and throughout SE Asia."
        },
        {
            "title": "Forecasting life satisfaction across adulthood: benefits of seeing a dark future?",
            "abstract": "Anticipating one's future self is a unique human capacity that contributes importantly to adaptation and health throughout adulthood and old age. Using the adult life span sample of the national German Socio-Economic Panel (SOEP; N > 10,000, age range 18 to 96 years), we investigated age-differential stability, correlates, and outcomes of accuracy in anticipation of future life satisfaction across 6 subsequent 5-year time intervals. As expected, we observed few age differences in current life satisfaction but stronger age differences in future expectations: Younger adults anticipated improved future life satisfaction, overestimating their actual life satisfaction 5 years later. By contrast, older adults were more pessimistic about the future, generally underestimating their actual life satisfaction after 5 years. Such age differences persisted above and beyond the effects of self-rated health and income. Survival analyses revealed that, in later adulthood, underestimating one's life satisfaction 5 years later was related to lower hazard ratios for disability (n = 735 became disabled) and mortality (n = 879 died) across 10 or more years, even after controlling for age, sex, education, income, and self-rated health. Findings suggest that older adults are more likely to underestimate their life satisfaction in the future and that such underestimation was associated with positive health outcomes."
        },
        {
            "title": "Mortality and causes of mortality among cataract-extracted patients. A 10-year follow-up.",
            "abstract": "Purpose:\n        \n      \n      The purpose of this study was to compare the mortality among patients undergoing intracapsular cataract extraction to the mortality in a gender-and age-identical Danish reference population, and to compare the patients' primary causes of death to those in the general population.\n    \n\n\n          Materials and methods:\n        \n      \n      We reviewed medical records of patients undergoing ICCE from January 1st 1984 to December 31st 1986 at the Department of Ophthalmology, Aalborg Hospital, Denmark. Information on the deaths of these patients was obtained from the Danish National Population Register. Information on mortality in Denmark was obtained from published statistics.\n    \n\n\n          Results:\n        \n      \n      We found an increased mortality among the patients with cataract with an SMR (standard mortality rate) of 1.12 (95% confidence interval 1.02-1.23). The slightly increased mortality was observed for both men and women and for all examined causes of death.\n    \n\n\n          Conclusion:\n        \n      \n      The slightly increased mortality among patients with cataract may indicate a general deterioration of health for these patients."
        },
        {
            "title": "Inaccuracies of creatinine and creatinine-based equations in candidates for liver transplantation with low creatinine: impact on the model for end-stage liver disease score.",
            "abstract": "Renal function has a significant impact on early mortality in patients with cirrhosis. However, creatinine and creatinine-based equations are inaccurate markers of renal function in cirrhosis. The aim of this study was to reassess correlations between creatinine-based equations and measured glomerular filtration rate (GFR) and to investigate the impact of inaccuracies on the Model for End-Stage Liver Disease (MELD) score. GFR was measured using iohexol clearance and calculated with creatinine-based equations in 157 patients with cirrhosis during pretransplant evaluation. We compared the accuracy of creatinine to that of true GFR in a prognostic score also including bilirubin and the international normalized ratio. In patients with creatinine below 1 mg/dL, true GFR ranged from 34-163 mL/minute/1.73 m(2). Cockcroft and Modification of Diet in Renal Disease (MDRD) significantly overestimated true GFR. On multivariate analysis, younger age and ascites were significantly correlated with the overestimation of true GFR by 20% or more. Body mass index was an independent risk factor of overestimation of GFR with Cockcroft but not with MDRD. The accuracy of a prognostic score combining bilirubin, international normalized ratio, and true GFR was superior to that of MELD, whether creatinine was rounded to 1 mg/dL when lower than 1 mg/dL or not (c-statistic of 0.8 versus 0.75 and 0.73, respectively). Creatinine-based formulas overestimate true GFR, especially in patients younger than 50 years or with ascites. In patients with serum creatinine below 1 mg/dL, the spectrum of true GFR is large. True GFR seems to have a better prognostic value than creatinine and creatinine-based equations. Specific equations are needed in patients with cirrhosis to improve prognostic scores."
        },
        {
            "title": "Mechanotechnical faults and particular issues of anastomotic complications following robot-assisted anterior resection in 968 rectal cancer patients.",
            "abstract": "Background:\n        \n      \n      As most risk factors for anastomotic complications (AC) in rectal cancer patients appear to be noncorrectable, it is needed to find the correctable causes. Additionally, the outcomes of indocyanine-green fluorescence imaging (IFI) and robot-stapled anastomosis have yet been undetermined.\n    \n\n\n          Methods:\n        \n      \n      This study retrospectively analyzed 968 consecutive patients with rectal cancer, who underwent curative robot-assisted anterior resections between 2010 and 2018. IFI parameters and stapling features in the surgical records were reviewed, and reconfirmed.\n    \n\n\n          Results:\n        \n      \n      AC occurred in 54 patients (5.6%), 34 (3.5%) with anastomotic leakage (AL) and 24 (2.5%) with anastomotic stenosis (AS). Mechanotechnical faults including defective stapling configurations, including angles lesser than or equal to 150° and outer deviation (more than half from the center of the circle) of linear staples, between the two linear staples were independently associated with AL (P < .001 each). IFI significantly reduced AL rate (2.5% vs 5.3%, P = .029) and AS rate (2% vs 18.8%, P = .006), respectively. Robot linear stapling enabled to maintain the obtuse angle during consecutive staplings and reduced console time. AL and AS were independent risk factors for disease-free survival (P = .02) and local recurrence (P = .03), respectively.\n    \n\n\n          Conclusions:\n        \n      \n      AC were associated with some correctable causes, namely, mechanotechnical errors and lack of use of IFI."
        },
        {
            "title": "[Giant cell arteritis. A study of 191 patients].",
            "abstract": "Background:\n        \n      \n      The aim of the present was to study the clinical features of a wide series of patients with giant cell arteritis (GCA) diagnosed with accurate criteria and to evaluate the sensitivity of the criteria proposed by the ACR for classification of GCA.\n    \n\n\n          Methods:\n        \n      \n      A retrospective analysis of 191 patients with GCA, 184 of whom were diagnosed by biopsy and 7 due to their clinical manifestations was carried out.\n    \n\n\n          Results:\n        \n      \n      The age was 73 +/- 7 years with the most frequent symptoms being headache (87%), abnormalities in the temporal arteries (75%), general malaise (60%), rheumatic polymyalgia (49%) and mandibular claudication (40%). The frequency of GCA was equal in both genders although the most complex syndrome was observed in women with a greater frequency of polymyalgia (p < 0.005), jaw claudication (p < 0.01) and anemia (p < 0.01). The patients with polymyalgia were characterized by a predominance of the polymyalgic syndrome in the initial phases and a higher frequency of amaurosis. Out of 47 patients with amaurosis, 23 remained with permanent unit or bilateral blindness. Unilateral biopsy of the temporal artery was diagnosed in 91% of the cases (CI 95%; 86 to 95%) increasing to 96.3% (CI 95%; 92 to 98%) on biopsy of both arteries. Ninety-eight percent of the patients (CI 95%; 95 to 99%) had 3 or more GCA criteria for classification as GCA.\n    \n\n\n          Conclusions:\n        \n      \n      The clinical manifestations of giant cell arteritis in Spain, with the exception of an equal frequency in both sexes, are similar to that described in other series of patients selected with strict criteria. The present data confirm the sensitivity of the criteria proposed by the ACR for the classification of giant cell arteritis although its application does not avoid the need for temporal artery biopsy for diagnosis. Unilateral biopsy is usually suffice in most of the cases."
        },
        {
            "title": "Long-term relative survival following surgery for abdominal aortic aneurysm: a review.",
            "abstract": "Background:\n        \n      \n      The literature reporting the long-term survival following surgery for abdominal aortic aneurysm (AAA) tends to be confusing. As a result, many clinicians looking after patients with AAA may be uncertain about the five-year survival of a given patient. This is in marked contrast to the situation for patients with malignant disease. With the current interest in population screening and endoluminal stenting for AAA, an understanding of long-term survival is increasingly important.\n    \n\n\n          Methods:\n        \n      \n      Thirty two publications in the English language over the last 20 years, containing data pertaining to five-year survival following routine elective surgery for AAA in unselected patients, were identified using Medline searches.\n    \n\n\n          Results and conclusions:\n        \n      \n      A range of important methodological differences were noted. The mean five-year crude survival was about 70% while the expected survival of a matched population was close to 80%. Survival was further reduced by about 10% in cases with significant coronary heart disease. Age alone is not a predictor of long-term relative survival with octogenarians who survive beyond 30 days surviving longer than an age-matched population."
        },
        {
            "title": "Electrocardiographic deep terminal negativity of the P wave in V1 and risk of mortality: the National Health and Nutrition Examination Survey III.",
            "abstract": "Introduction:\n        \n      \n      Deep terminal negativity of P wave in V1 (DTNPV1), defined as negative P prime larger than one small box (1 mm, or 0.1 mV), could be easily detected by simple visual inspection of the resting 12-lead ECG. The objective of this study was to determine the relationship between DTNPV1 and all-cause-, cardiovascular disease (CVD), and ischemic heart disease (IHD) mortality in the National Health and Nutrition Examination Survey III (NHANES III).\n    \n\n\n          Methods and results:\n        \n      \n      After exclusion of participants with atrial fibrillation and missing data, DTNPV1 was automatically measured from standard 12-lead ECG in 8,146 participants. Minnesota and Novacode algorithms were used for the determination of major and minor ECG abnormalities. National Death Index was used to identify the date and cause of death. During a median follow-up of 13.8 years, a total of 2,975 deaths (1,303 CVD and 742 IHD deaths) occurred. After adjustment for age, gender, race/ethnicity, IHD, heart failure, chronic obstructive pulmonary disease, cancer, diabetes, body mass index, smoking, dyslipidemia, hypertension, use of antihypertensive and lipid-lowering medications, and ECG abnormalities, DTNPV1 was associated with significantly increased risk of all-cause death (HR [95% CI]: 1.30 [1.10, 1.53]; P = 0.002), CVD death (HR [95% CI]: 1.36 [1.08, 1.72]; P = 0.010), and IHD death (HR [95% CI]: 1.36 [1.00, 1.85]; P = 0.047).\n    \n\n\n          Conclusion:\n        \n      \n      In a large sample of the adult United States population, DTNPV1 is independently associated with increased risk of death due to all-cause, CVD, and IHD, findings suggesting its potential usefulness as a simple marker to identify individuals at risk of poor outcomes."
        },
        {
            "title": "Comparison of BISAP, Ranson's, APACHE-II, and CTSI scores in predicting organ failure, complications, and mortality in acute pancreatitis.",
            "abstract": "Objectives:\n        \n      \n      Identification of patients at risk for severe disease early in the course of acute pancreatitis (AP) is an important step to guiding management and improving outcomes. A new prognostic scoring system, the bedside index for severity in AP (BISAP), has been proposed as an accurate method for early identification of patients at risk for in-hospital mortality. The aim of this study was to compare BISAP (blood urea nitrogen >25 mg/dl, impaired mental status, systemic inflammatory response syndrome (SIRS), age>60 years, and pleural effusions) with the \"traditional\" multifactorial scoring systems: Ranson's, Acute Physiology and Chronic Health Examination (APACHE)-II, and computed tomography severity index (CTSI) in predicting severity, pancreatic necrosis (PNec), and mortality in a prospective cohort of patients with AP.\n    \n\n\n          Methods:\n        \n      \n      Extensive demographic, radiographic, and laboratory data from consecutive patients with AP admitted or transferred to our institution was collected between June 2003 and September 2007. The BISAP and APACHE-II scores were calculated using data from the first 24 h from admission. Predictive accuracy of the scoring systems was measured by the area under the receiver-operating curve (AUC).\n    \n\n\n          Results:\n        \n      \n      There were 185 patients with AP (mean age 51.7, 51% males), of which 73% underwent contrast-enhanced CT scan. Forty patients developed organ failure and were classified as severe AP (SAP; 22%). Thirty-six developed PNec (19%), and 7 died (mortality 3.8%). The number of patients with a BISAP score of > or =3 was 26; Ranson's > or =3 was 47, APACHE-II > or =8 was 66, and CTSI > or =3 was 59. Of the seven patients that died, one had a BISAP score of 1, two had a score of 2, and four had a score of 3. AUCs for BISAP, Ranson's, APACHE-II, and CTSI in predicting SAP are 0.81 (confidence interval (CI) 0.74-0.87), 0.94 (CI 0.89-0.97), 0.78 (CI 0.71-0.84), and 0.84 (CI 0.76-0.89), respectively.\n    \n\n\n          Conclusions:\n        \n      \n      We confirmed that the BISAP score is an accurate means for risk stratification in patients with AP. Its components are clinically relevant and easy to obtain. The prognostic accuracy of BISAP is similar to those of the other scoring systems. We conclude that simple scoring systems may have reached their maximal utility and novel models are needed to further improve predictive accuracy."
        },
        {
            "title": "Heterogeneity of longitudinal and circumferential contraction in relation to late gadolinium enhancement in hypertrophic cardiomyopathy patients with preserved left ventricular ejection fraction.",
            "abstract": "Purpose:\n        \n      \n      To evaluate heterogeneity of myocardial contraction in relation to extensive late gadolinium enhancement (LGE) in hypertrophic cardiomyopathy (HCM) patients with preserved left ventricular ejection fraction, using fast strain-encoded magnetic resonance imaging.\n    \n\n\n          Materials and methods:\n        \n      \n      Twenty-two HCM patients and 24 age-matched control subjects were included in this retrospective study. The regional and global peak values of longitudinal and circumferential strain (LSregional, LSglobal, CSregional, CSglobal), and their regional heterogeneities were evaluated using coefficients of variation (LSCoV, CSCoV) in relation to LGE. Receiver operating characteristic curve analysis was performed to identify patients with a total left ventricular myocardial LGE ≥ 15%.\n    \n\n\n          Results:\n        \n      \n      LSglobal in HCM patients was significantly decreased compared to that in controls (- 14.4 ± 2.4% vs - 17.2 ± 2.0%; p = 0.0004), while CSglobal was not (p = 1.0). Negative LGE segments demonstrated decreased LSregional in HCM patients compared to in controls (p < 0.0001), while CSregional was not decreased. CSCoV demonstrated the largest area under the curve (AUC) (0.91), with high sensitivity (83%) and specificity (94%) for detection of HCM patients with extensive LGE, while the AUC of LSCoV was low (0.49).\n    \n\n\n          Conclusion:\n        \n      \n      The heterogeneity in CSregional has a high diagnostic value for detection of HCM patients with extensive LGE."
        },
        {
            "title": "Long-term retinal, renal and cardiovascular outcomes in diabetic chronic kidney disease without proteinuria.",
            "abstract": "Background:\n        \n      \n      Patients with diabetes mellitus (DM) with chronic kidney disease (CKD) often have no proteinuria.\n    \n\n\n          Methods:\n        \n      \n      To compare the characteristics that differ between DM + CKD patients with and without proteinuria, we conducted a cross-sectional study followed by surveillance over a decade for 'hard' cardiovascular, renal and retinal outcomes. Groups were stratified by presence (n = 129) and absence (n = 284) of DM. Each stratum had three groups: no CKD, CKD without proteinuria and CKD with proteinuria.\n    \n\n\n          Results:\n        \n      \n      Compared to DM + CKD + proteinuria patients, those with DM + CKD but without proteinuria had similar clinical characteristics including estimated glomerular filtration rate. However, they had lower 24-h ambulatory systolic and diastolic blood pressure. Crude all-cause mortality rates per 1000 patient-years in the nondiabetic group with no CKD, CKD with no proteinuria and CKD with overt proteinuria were 29.3, 68.5 and 111.1, respectively. Respective rates in the diabetic group were 50.1, 105.7 and 136.8. Diabetes increased the risk of coronary (P = 0.01) and end-stage renal disease (ESRD) events (P = 0.05) even after multivariate adjustments. Proteinuria aggravated the risk of cardiovascular events, ESRD, death and time to first of these events similarly among diabetics with CKD compared to nondiabetics with CKD. Diabetic patients with CKD but no overt proteinuria were much more likely than nondiabetics to progress to overt proteinuria [adjusted hazard ratio 5.28 (95% confidence interval 1.64-17.02), P < 0.01). CKD was a risk factor for prevalent retinopathy and proteinuria was a risk factor for incident diabetic retinopathy.\n    \n\n\n          Conclusions:\n        \n      \n      To protect sight, those with proteinuria and DM need regular retinal examinations. Since diabetic CKD patients without proteinuria are more likely to develop overt proteinuria, close follow-up and risk factor management among these patients appear to be more important than among nondiabetic patients with CKD and no proteinuria."
        },
        {
            "title": "A reliable measure of frailty for a community dwelling older population.",
            "abstract": "Background:\n        \n      \n      Frailty remains an elusive concept despite many efforts to define and measure it. The difficulty in translating the clinical profile of frail elderly people into a quantifiable assessment tool is due to the complex and heterogeneous nature of their health problems. Viewing frailty as a 'latent vulnerability' in older people this study aims to derive a model based measurement of frailty and examines its internal reliability in community dwelling elderly.\n    \n\n\n          Method:\n        \n      \n      The British Women's Heart and Health Study (BWHHS) cohort of 4286 women aged 60-79 years from 23 towns in Britain provided 35 frailty indicators expressed as binary categorical variables. These indicators were corrected for measurement error and assigned relative weights in its association with frailty. Exploratory factor analysis (EFA) reduced the data to a smaller number of factors and was subjected to confirmatory factor analysis (CFA) which restricted the model by fitting the EFA-driven structure to observed data. Cox regression analysis compared the hazard ratios for adverse outcomes of the newly developed British frailty index (FI) with a widely known FI. This process was replicated in the MRC Assessment study of older people, a larger cohort drawn from 106 general practices in Britain.\n    \n\n\n          Results:\n        \n      \n      Seven factors explained the association between frailty indicators: physical ability, cardiac symptoms/disease, respiratory symptoms/disease, physiological measures, psychological problems, co-morbidities and visual impairment. Based on existing concepts and statistical indices of fit, frailty was best described using a General Specific Model. The British FI would serve as a better population metric than the FI as it enables people with varying degrees of frailty to be better distinguished over a wider range of scores. The British FI was a better independent predictor of all-cause mortality, hospitalization and institutionalization than the FI in both cohorts.\n    \n\n\n          Conclusions:\n        \n      \n      Frailty is a multidimensional concept represented by a wide range of latent (not directly observed) attributes. This new measure provides more precise information than is currently recognized, of which cluster of frailty indicators are important in older people. This study could potentially improve quality of life among older people through targeted efforts in early prevention and treatment of frailty."
        },
        {
            "title": "Dual-Energy Contrast-Enhanced Spectral Mammography: Enhancement Analysis on BI-RADS 4 Non-Mass Microcalcifications in Screened Women.",
            "abstract": "Background:\n        \n      \n      Mammography screening is a cost-efficient modality with high sensitivity for detecting impalpable cancer with microcalcifications, and results in reduced mortality rates. However, the probability of finding microcalcifications without associated cancerous masses varies. We retrospectively evaluated the diagnosis and cancer probability of the non-mass screened microcalcifications by dual-energy contrast-enhanced spectral mammography (DE-CESM).\n    \n\n\n          Patients and methods:\n        \n      \n      With ethical approval from our hospital, we enrolled the cases of DE-CESM for analysis under the following inclusion criteria: (1) referrals due to screened BI-RADS 4 microcalcifications; (2) having DE-CESM prior to stereotactic biopsy; (3) no associated mass found by sonography and physical examination; and (4) pathology-based diagnosis using stereotactic vacuum-assisted breast biopsy. We analyzed the added value of post-contrast enhancement on DE-CESM.\n    \n\n\n          Results:\n        \n      \n      A total of 94 biopsed lesions were available for analysis in our 87 women, yielding 27 cancers [19 ductal carcinoma in situ (DCIS), and 8 invasive ductal carcinoma (IDC)], 32 pre-malignant and 35 benign lesions. Of these 94 lesions, 33 showed associated enhancement in DE-CESM while the other 61 did not. All 8 IDC (100%) and 16 of 19 DCIS (84.21%) showed enhancement, but the other 3 DCIS (15.79%) did not. Overall sensitivity, specificity, positive predictive value, negative predictive value and accuracy were 88.89%, 86.56%, 72.72%, 95.08% and 87.24%, respectively. The performances of DE-CESM on both amorphous and pleomorphic microcalcifications were satisfactory (AUC 0.8 and 0.92, respectively). The pleomorphous microcalcifications with enhancement showed higher positive predictive value (90.00% vs 46.15%, p = 0.013) and higher cancer probability than the amorphous microcalcifications (46.3% VS 15.1%). The Odds Ratio was 4.85 (95% CI: 1.84-12.82).\n    \n\n\n          Conclusion:\n        \n      \n      DE-CESM might provide added value in assessing the non-mass screened breast microcalcification, with enhancement favorable to the diagnosis of cancers or lack of enhancement virtually diagnostic for non-malignant lesions or noninvasive subgroup cancers."
        },
        {
            "title": "Adverse outcomes and correlates of change in the Short Physical Performance Battery over 36 months in the African American health project.",
            "abstract": "Background:\n        \n      \n      The Short Physical Performance Battery (SPPB) is a well-established measure of lower body physical functioning in older persons but has not been adequately examined in African Americans or younger persons. Moreover, factors associated with changes in SPPB over time have not been reported.\n    \n\n\n          Methods:\n        \n      \n      A representative sample of 998 African Americans (49-65 years old at baseline) living in St. Louis, Missouri were followed for 36 months to examine the predictive validity of SPPB in this population and identify factors associated with changes in SPPB. SPPB was calibrated to this population, ranged from 0 (worst) to 12 (best), and required imputation for about 50% of scores. Adverse outcomes of baseline SPPB included death, nursing home placement, hospitalization, physician visits, incident basic and instrumental activity of daily living disabilities, and functional limitations. Changes in SPPB over 36 months were modeled.\n    \n\n\n          Results:\n        \n      \n      Adjusted for appropriate covariates, weighted appropriately, and using propensity scores to address potential selection bias, baseline SPPB scores were associated with all adverse outcomes except physician visits, and were marginally associated with hospitalization. Declines in SPPB scores were associated with low falls efficacy (b = -1.311), perceived income adequacy (-0.121), older age (-0.073 per year), poor vision (-0.754), diabetes mellitus (-0.565), refusal to report household income (1.48), ever had Medicaid insurance (-0.610), obesity (-0.437), hospitalization in the prior year (-0.521), and kidney disease (-.956).\n    \n\n\n          Conclusions:\n        \n      \n      The effect of baseline SPPB on adverse outcomes in this late middle-age African American population confirms reports involving older, primarily white participants. Alleviating deterioration in lower body physical functioning guided by the associated covariates may avoid or delay multiple age-associated adverse outcomes."
        },
        {
            "title": "Choroidal Thickness Profiles in Myopic Eyes of Young Adults in the Correction of Myopia Evaluation Trial Cohort.",
            "abstract": "Purpose:\n        \n      \n      To examine the relationship of choroidal thickness with axial length (AL) and myopia in young adult eyes in the ethnically diverse Correction of Myopia Evaluation Trial (COMET) cohort.\n    \n\n\n          Design:\n        \n      \n      Cross-sectional, multicenter study.\n    \n\n\n          Methods:\n        \n      \n      In addition to measures of myopia by cycloplegic autorefraction and AL by A-scan ultrasonography, participants underwent optical coherence tomography imaging of the choroid in both eyes at their last visit (14 years after baseline). Using digital calipers, 2 independent readers measured choroidal thickness in the right eye (left eye if poor quality; n = 37) at 7 locations: fovea and 750, 1500, and 2250 μm nasal (N) and temporal (T) to the fovea.\n    \n\n\n          Results:\n        \n      \n      Choroidal thickness measurements were available from 294 of 346 (85%) imaged participants (mean age: 24.3 ± 1.4 years; 44.9% male) with mean myopia of -5.3 ± 2.0 diopters and mean AL of 25.5 ± 1.0 mm. Overall, choroidal thickness varied by location (P < .0001) and was thickest at the fovea (273.8 ± 70.9 μm) and thinnest nasally (N2250, 191.5 ± 69.3 μm). Multivariable analyses showed significantly thinner choroids in eyes with more myopia and longer AL at all locations except T2250 (P ≤ .001) and presence of peripapillary crescent at all locations except T1500 and T2250 (P ≤ .0001). Choroidal thickness varied by ethnicity at N2250 (P < .0001), with Asians having the thinnest and African Americans the thickest choroids.\n    \n\n\n          Conclusion:\n        \n      \n      Choroids are thinner in longer, more myopic young adult eyes. The thinning was most prominent nasally and in eyes with a crescent. In the furthest nasal location, ethnicity was associated with choroidal thickness. The findings suggest that choroidal thickness should be evaluated, especially in the nasal regions where myopic degenerations are most commonly seen clinically."
        },
        {
            "title": "Risk factors for fractures in older men and women: The Leisure World Cohort Study.",
            "abstract": "Background:\n        \n      \n      Osteoporosis results in >1.5 million fractures in the United States each year, leading to substantial health care costs and loss of quality of life. One major gap in our knowledge is how to effectively identify individuals at risk of developing a fracture.\n    \n\n\n          Objective:\n        \n      \n      We examined a population-based cohort for risk factors for fractures of the hip, wrist, and spine in men and women.\n    \n\n\n          Methods:\n        \n      \n      The Leisure World Cohort Study was established between 1981 and 1985 when residents of a southern California retirement community completed a postal health survey. Multiple lifestyle, medical, attitudinal, and anthropomorphic factors were self-reported. Fractures were identified from 4 follow-up surveys, hospital discharge records, and death certificates. Fracture rates were determined separately for men and women. Cox proportional hazards regression was used to identify predictors of fracture.\n    \n\n\n          Results:\n        \n      \n      Incident fractures of the hip (n = 1,227), wrist (n = 445), and spine (n = 729) incurred over the course of 2 decades were identified in the 13,978 residents surveyed. Mean (SD) age at entry was 74.9 (7.2) years for men and 73.7 (7.4) years for women. The most important risk factors for fracture were the same in men and women: age increased risk of hip and spine fractures (hazard ratio [HR] = 2.3-3.2 per 10 years) and history of fracture increased fracture risk at all 3 sites (HR = 1.4-3.2). In both men and women, glaucoma was a significant risk factor for hip fracture (HR = 1.9 and 1.3, respectively), and smoking was a risk factor for hip and spine fractures. Men and women with a positive mental attitude had fewer hip and spine fractures (HR = 0.7-0.9). High body mass index was protective at all 3 fracture sites in women (HR = 0.7-0.8), but those who used vitamin A supplements had increased rates of hip and wrist fracture (HR = 1.1 per 10,000 IU per day).\n    \n\n\n          Conclusions:\n        \n      \n      Attitude, lifestyle choices, and the presence of medical conditions may influence the rate of osteoporotic fracture in older women and men and may help identify individuals at high risk."
        },
        {
            "title": "Cardiovascular magnetic resonance and prognosis in cardiac amyloidosis.",
            "abstract": "Background:\n        \n      \n      Cardiac involvement is common in amyloidosis and associated with a variably adverse outcome. We have previously shown that cardiovascular magnetic resonance (CMR) can assess deposition of amyloid protein in the myocardial interstitium. In this study we assessed the prognostic value of late gadolinium enhancement (LGE) and gadolinium kinetics in cardiac amyloidosis in a prospective longitudinal study.\n    \n\n\n          Materials and methods:\n        \n      \n      The pre-defined study end point was all-cause mortality. We prospectively followed a cohort of 29 patients with proven cardiac amyloidosis. All patients underwent biopsy, 2D-echocardiography and Doppler studies, 123I-SAP scintigraphy, serum NT pro BNP assay, and CMR with a T1 mapping method and late gadolinium enhancement (LGE).\n    \n\n\n          Results:\n        \n      \n      Patients with were followed for a median of 623 days (IQ range 221, 1436), during which 17 (58%) patients died. The presence of myocardial LGE by itself was not a significant predictor of mortality. However, death was predicted by gadolinium kinetics, with the 2 minute post-gadolinium intramyocardial T1 difference between subepicardium and subendocardium predicting mortality with 85% accuracy at a threshold value of 23 ms (the lower the difference the worse the prognosis). Intramyocardial T1 gradient was a better predictor of survival than FLC response to chemotherapy (Kaplan Meier analysis P = 0.049) or diastolic function (Kaplan-Meier analysis P = 0.205).\n    \n\n\n          Conclusion:\n        \n      \n      In cardiac amyloidosis, CMR provides unique information relating to risk of mortality based on gadolinium kinetics which reflects the severity of the cardiac amyloid burden."
        },
        {
            "title": "Hypertensive Retinopathy: A Prognostic Factor for Morbidity and Mortality after Acute ST Elevation Myocardial Infarction.",
            "abstract": "Objective:\n        \n      \n      To determine the association between hypertensive retinopathy (HR) and post ST elevation myocardial infarction (STEMI) complications among successfully thrombolysed patients.\n    \n\n\n          Study design:\n        \n      \n      A cohort study.\n    \n\n\n          Place and duration of study:\n        \n      \n      Cardiology Unit, Lady Reading Hospital, Peshawar, from June 2016 to December 2017.\n    \n\n\n          Methodology:\n        \n      \n      Patients with history of hypertension for at least 5 years who presented with STEMI and were successfully thrombolysed, were included. On the basis of fundoscopy, patients were grouped into no, mild, moderate, and severe hypertensive retinopathy. Primary and secondary endpoints included a composite of death, re-MI, stroke, re-hospitalisation secondary to left ventricular failure, cardiogenic shock, arrhythmia, heart block, and ventricular septal rupture at 30 days and 4 months, respectively. Association between hypertensive retinopathy and post STEMI complications was determined by Chi-square test. Regression model was used to calculate relative risk of complications with hypertensive retinopathy. P <0.05 was taken as significant.\n    \n\n\n          Results:\n        \n      \n      A total of 118 patients with a mean age of 54.83 ±8.6 years were included in the study. Of these, 49.2% (n=58) were males. Moreover, 38.1% (n=45) of patients were grouped under no HR, 22.8% (n=27) under mild HR, 21.1% (n=25) and 17.7% (n=21) under moderate and severe HR, respectively. Primary endpoints achieved were 0% in no HR group and 19% in severe HR group x² = 18.1, p <0.001). Secondary endpoints were achieved in 2.2% in no HR group and 40.7%, 56% and 100% in mild, moderate and severe HR group, respectively, ( x² = 81.1, p <0.001). HR also increased the relative risk of complications by 3.17 times (p <0.001) and death by 1.75 times (p <0.001).\n    \n\n\n          Conclusion:\n        \n      \n      Hypertensive retinopathy is an independent risk factor for post-acute STEMI complications in successfully thrombolysed patients and increased the relative risk for complications by 3.17 times."
        },
        {
            "title": "High intraocular pressure and survival: the Framingham Studies.",
            "abstract": "Purpose:\n        \n      \n      To examine whether high intraocular pressure (greater than or equal to 25 mm Hg) or a history of treatment for glaucoma is associated with decreased survival and, if so, how such ocular markers might be explained.\n    \n\n\n          Methods:\n        \n      \n      Eye examinations, including applanation tonometry, were conducted on members of the Framingham Eye Study cohort from February 1, 1973, to February 1, 1975. Participants who reported a history of treatment for glaucoma were identified. Survival data, including information on the date of death, were available from the time of the Eye Study through March 31, 1990.\n    \n\n\n          Results:\n        \n      \n      Of the 1,764 persons under the age of 70 years at the baseline eye examination, 1,421 persons had low intraocular pressure (< or =20 mm Hg), 264 persons had medium intraocular pressure levels (20 to 24 mm Hg), and 79 persons had high intraocular pressure (> or =25 mm Hg) or history of glaucoma treatment. During the follow-up period, 29%, 30%, and 47% died in the groups with low, medium, and high intraocular pressure (or history of glaucoma treatment), respectively. In an age-and-sex adjusted Cox proportional hazards analysis, the death rate ratio for the group with medium intraocular pressure relative to the group with low intraocular pressure was 1.04. The corresponding death rate ratio for the group with high intraocular pressure was 1.56 with a 95% confidence interval of 1.11 to 2.19 (P < .001). After adjustment for age, sex, hypertension, diabetes, cigarette smoking, and body mass index, a positive relationship remained, but at a borderline level of significance (P = .075).\n    \n\n\n          Conclusions:\n        \n      \n      High intraocular pressure or the presence of glaucoma is a marker for decreased life expectancy in the Framingham Eye Study cohort. The relationship is present even after adjustment for risk factors known to be associated with higher mortality such as age, sex, hypertension, diabetes, cigarette smoking, and body mass index. Special attention to the general health status of patients with high intraocular pressure or glaucoma seems warranted."
        },
        {
            "title": "Early prediction of septic complications after cardiac surgery by APACHE II score.",
            "abstract": "In 110 patients admitted to the intensive care unit after cardiac surgery, daily monitoring [clinical parameters, cardiac index (CI), left ventricular stroke work index (LVSWI) and APACHE II score] was performed in regard to its usefulness in the early prediction of septic complications, a major cause of postoperative mortality. Septic complications (defined as Elebute sepsis score of > or = 12 on > or = 2 days) occurred in 16 patients and were associated with a significantly worse prognosis (mortality 69% vs 1%, P < 0.0001) than was seen in patients without sepsis. While preoperative APACHE II score values did not differentiate between the patients with an uneventful postoperative course and those with septic complications, patients who ultimately developed septic complications had significantly (P < 0.001) higher scores as early as on the evening of the operation day (\"day 0\"). In addition, in contrast to patients without sepsis, whose scores dropped markedly (P < 0.001) between day 0 and day 1, patients with septic complications invariably had high scores. Compared to single parameters (fever, leucocyte count, CI, LVSWI), the APACHE II score proved to be superior in differentiating between patients who developed sepsis and those who did not. A score of 19 or more on the 1st postoperative day had a sensitivity of 75%, a specificity of 98%, a Youden index of 0.73, a positive predictive value of 86%, and a negative predictive value of 96% in regard to prediction of septic complications.(ABSTRACT TRUNCATED AT 250 WORDS)"
        },
        {
            "title": "Transient monocular blindness and the risk of vascular complications according to subtype: a prospective cohort study.",
            "abstract": "Patients with transient monocular blindness (TMB) can present with many different symptoms, and diagnosis is usually based on the history alone. In this study, we assessed the risk of vascular complications according to different characteristics of TMB. We prospectively studied 341 consecutive patients with TMB. All patients were interviewed by a single investigator with a standardized questionnaire; reported symptoms were classified into predefined categories. We performed Cox regression analyses with adjustment for baseline vascular risk factors. During a mean follow-up of 4.0 years, the primary outcome event of vascular death, stroke, myocardial infarction, or retinal infarction occurred in 60 patients (annual incidence 4.4 %, 95 % confidence interval (CI) 3.4-5.7). An ipsilateral ischemic stroke occurred in 14 patients; an ipsilateral retinal infarct in six. Characteristics of TMB independently associated with subsequent vascular events were: involvement of only the peripheral part of the visual field (hazard ratio (HR) 6.5, 95 % CI 3.0-14.1), constricting onset of loss of vision (HR 3.5, 95 % CI 1.0-12.1), downward onset of loss of vision (HR 1.9, 95 % CI 1.0-3.5), upward resolution of loss of vision (HR 2.0, 95 % CI 1.0-4.0), and the occurrence of more than three attacks (HR 1.7, 95 % CI 1.0-2.9). We could not identify characteristics of TMB that predicted a low risk of vascular complications. In conclusion, careful recording the features of the attack in patients with TMB can provide important information about the risk of future vascular events."
        },
        {
            "title": "Characteristics of caregiver perceptions of end-of-life caregiving experiences in cancer survivorship: in-depth interview study.",
            "abstract": "Objective:\n        \n      \n      Little is known about caregiver experiences during the end-of-life period. Our objective was to characterize caregiver perceptions of their experiences in cancer survivorship with special reference to the end-of-life stage considering depression in bereavement.\n    \n\n\n          Methods:\n        \n      \n      Qualitative research using in-depth interviews of 34 caregivers from two palliative care units in Japan. Data were analyzed inductively using framework analysis. Depression and personality traits were measured using the Center for Epidemiological Studies Depression (CES-D) and Sense of Coherence (SOC) scales, respectively.\n    \n\n\n          Results:\n        \n      \n      Caregiver perceptions were characterized along two axes. One axis involved four caregiver-cancer patient relationships: strengthening, reconstruction, intimacy-maintained, and estrangement-maintained. The core concept was transformation of relationships: caregivers reappraised aspects of caregiver-patient interactions through caregiving. The other axis involved subjective caregiving experiences divided into five concepts: spontaneity of care, discussing death, sympathy for patient emotions, impressions on first witnessing death, and introspective reflections in bereavement. Strengthening and reconstruction relationships appeared similar among the four relationship types, but only the former tended to overcompensate by sacrificing private time. Although median CES-D scores in each relationship type were under the cutoff for possible depression, four of eight caregivers suspected to have depressive tendencies belonged to the strengthening type. The mean SOC score for all caregivers was intermediate relative to scores previously reported in Japanese studies.\n    \n\n\n          Conclusions:\n        \n      \n      While caregivers' subjective experiences can be classified, their relationship to depression in bereavement needs future research. The present findings indicate that caregivers should also be considered in clinicians' views of cancer survivorship."
        },
        {
            "title": "Pancreatic head cancer: accuracy of CT in determination of resectability.",
            "abstract": "Background:\n        \n      \n      Pancreatic cancer is a devastating disease whose early detection remains difficult. There is no 100% reliable imaging test to diagnose and stage pancreatic cancer. We assessed the surgical value of contrast-enhanced spiral computed tomography (CT) in predicting the resectability and survival rates of patients who had pancreatic head cancer.\n    \n\n\n          Methods:\n        \n      \n      Eighty-nine patients who had pancreatic head cancer were investigated with spiral CT. Based on the preoperative CT results, we assigned patients to one of three CT groups based on resectability.\n    \n\n\n          Results:\n        \n      \n      A correlation between classification of CT resectability and intraoperative finding was found in 83% of patients. The sensitivity, specificity, negative predictive value, positive predictive value, and accuracy of spiral CT in identifying predictive unresectability were 79%, 82%, 91%, 62%, and 81%, respectively. Sensitivity, specificity, positive predictive value, negative predictive value, and accuracy of CT in diagnosis of vascular invasion were 94%, 84.2%, 94%, 84%, and 91.3%, respectively.\n    \n\n\n          Conclusion:\n        \n      \n      The use of CT in the evaluation of pancreatic tumors provides valuable preoperative assessment of surgical resectability and should be performed for clinical examination. Classifying patients by tumor resectability on CT helps to estimate more precisely the tumor stage and to prognosticate survival rates of these patients."
        },
        {
            "title": "Age-related cataract and 10-year mortality: the Liwan Eye Study.",
            "abstract": "Purpose:\n        \n      \n      To explore the association between age-related cataract and 10-year mortality in an adult population in urban China.\n    \n\n\n          Methods:\n        \n      \n      A total of 1405 participants aged 50 years or older were examined at baseline in the Guangzhou Liwan Eye Study. All participants were invited to attend a 10-year follow-up visit. Cataract cases were defined as either having visible lens opacity confirmed with direct ophthalmoscope under pupil dilation or previous history of cataract surgery. Visual impairment (VI) was defined as a visual acuity of 20/40 or worse in the better-seeing eye with habitual correction if worn. Body mass index (BMI) was based on anthropometric data. A brief questionnaire regarding family income, educational attainment and medical history of systemic disease was administered. Mortality rates were compared using the log-rank test and Cox proportional hazards regression models.\n    \n\n\n          Results:\n        \n      \n      Among 1405 participants examined at baseline, 957 participants (68.1%) had visible lens opacity or history of cataract surgery. After 10 years, 320 (22.8%) participants died. The 10-year mortality rate was significantly higher in participants with cataract than in those without (30.1% versus 7.14%, log-rank p < 0.05). After adjusting for age, gender, family income, educational attainment, BMI, history of diabetes and hypertension and presence of VI, presence of cataract predicted a nearly threefold increase in the risk of mortality (HR, 2.99; 95% CI, 1.89-4.71).\n    \n\n\n          Conclusions:\n        \n      \n      Our findings that age-related cataract is a predictor for poorer survival compared to those without may imply that cataract is a biomarker of ageing and frailty."
        },
        {
            "title": "The predictive performance of SAPS 2 and SAPS 3 in an intermediate care unit for internal medicine at a German university transplant center; A retrospective analysis.",
            "abstract": "Objective:\n        \n      \n      To analyze and compare the performance of the Simplified-Acute-Physiology-Score (SAPS) 2 and SAPS 3 among intermediate care patients with internal disorders.\n    \n\n\n          Materials and methods:\n        \n      \n      We conducted a retrospective single-center analysis in patients (n = 305) admitted to an intermediate-care-unit (ImCU) for internal medicine at the University Hospital Essen, Germany. We employed and compared the SAPS 2 vs. the SAPS 3 scoring system for the assessment of disease severity and prediction of mortality rates among patients admitted to the ImCU within an 18-month period. Both scores, which utilize parameters recorded at admission to the intensive-care-unit (ICU), represent the most widely applied scoring systems in European intensive care medicine. The area-under-the-receiver-operating-characteristic-curve (AUROC) was used to evaluate the SAPS 2 and SAPS 3 discrimination performance. Ultimately, standardized-mortality-ratios (SMRs) were calculated alongside their respective 95%-confidence-intervals (95% CI) in order to determine the observed-to-expected death ratio and calibration belt plots were generated to evaluate the SAPS 2 and SAPS 3 calibration performance.\n    \n\n\n          Results:\n        \n      \n      Both scores provided acceptable discrimination performance, i.e., the AUROC was 0.71 (95% CI, 0.65-0.77) for SAPS 2 and 0.77 (95% CI, 0.72-0.82) for SAPS 3. Against the observed in-hospital mortality of 30.2%, SAPS 2 showed a weak performance with a predicted mortality of 17.4% and a SMR of 1.74 (95% CI, 1.38-2.09), especially in association with liver diseases and/or sepsis. SAPS 3 performed accurately, resulting in a predicted mortality of 29.9% and a SMR of 1.01 (95% CI, 0.8-1.21). Based on Calibration belt plots, SAPS 2 showed a poor calibration-performance especially in patients with low mortality risk (P<0.001), while SAPS 3 exhibited a highly accurate calibration performance (P = 0.906) across all risk levels.\n    \n\n\n          Conclusions:\n        \n      \n      In our study, the SAPS 3 exhibited high accuracy in prediction of mortality in ImCU patients with internal disorders. In contrast, the SAPS 2 underestimated mortality particularly in patients with liver diseases and sepsis."
        },
        {
            "title": "Pegaptanib sodium for neovascular age-related macular degeneration: two-year safety results of the two prospective, multicenter, controlled clinical trials.",
            "abstract": "Objective:\n        \n      \n      To evaluate the safety of pegaptanib sodium injection, a specific vascular endothelial growth factor (VEGF) antagonist, in the treatment of neovascular age-related macular degeneration (AMD) during 2 years of therapy.\n    \n\n\n          Design:\n        \n      \n      Two concurrent, prospective, randomized, multicenter, double-masked, sham-controlled studies.\n    \n\n\n          Methods:\n        \n      \n      Patients with all angiographic choroidal neovascularization lesion compositions of AMD received either intravitreous pegaptanib sodium (0.3 mg, 1 mg, 3 mg) or sham injections every 6 weeks for 54 weeks. Those initially assigned to pegaptanib were re-randomized (1:1) to continue or discontinue therapy for 48 more weeks; sham-treated patients were re-randomized (1:1:1:1:1) to continue sham, discontinue, or receive one of the pegaptanib doses.\n    \n\n\n          Main outcome measures:\n        \n      \n      All reported adverse events, serious adverse events, and deaths.\n    \n\n\n          Participants:\n        \n      \n      In year 1, 1190 subjects received at least one study treatment (0.3 mg, n = 295; 1 mg, n = 301; 3 mg, n = 296; sham, n = 298); 7545 intravitreous injections of pegaptanib were administered. In year 2, 425 subjects (0.3 mg, n = 128; 1 mg, n = 126; 3 mg, n = 120; sham, n = 51) continued the same masked treatment as in year 1 and received at least one study treatment in year 2; 2663 intravitreous injections of pegaptanib were administered in these subjects.\n    \n\n\n          Results:\n        \n      \n      All doses of pegaptanib were well tolerated. The most common ocular adverse events were transient, mild to moderate in intensity, and attributed to the injection preparation and procedure. There was no evidence of an increase in deaths, in events associated with systemic VEGF inhibition (e.g., hypertension, thromboembolic events, serious hemorrhagic events), or in severe ocular inflammation, cataract progression, or glaucoma in pegaptanib-treated patients relative to sham-treated patients. In year 1, serious injection-related complications included endophthalmitis (12 events, 0.16%/injection), retinal detachment (RD) (6 events [4 rhegmatogenous, 2 exudative], 0.08%/injection), and traumatic cataract (5 events, 0.07%/injection). Most cases of endophthalmitis followed violations of the injection preparation protocol. In patients receiving pegaptanib for >1 year, there were no reports of endophthalmitis or traumatic cataract in year 2; RD was reported in 4 patients (all rhegmatogenous, 0.15%/injection).\n    \n\n\n          Conclusion:\n        \n      \n      The 2-year safety profile of pegaptanib sodium is favorable in patients with exudative AMD."
        },
        {
            "title": "Falls, injuries from falls, health related quality of life and mortality in older adults with vision and hearing impairment--is there a gender difference?",
            "abstract": "Background:\n        \n      \n      Vision and hearing decline with age. Loss of these senses is associated with increased risk of falls, injuries from falls, mortality and decreased health-related quality of life (HRQOL). Our objective was to determine if there are gender differences in the associations between visual and hearing impairment and these outcomes.\n    \n\n\n          Methods:\n        \n      \n      2340 men and 3014 women aged 76-81 years from the Health in Men Study and the Australian Longitudinal Study on Women's Health were followed for an average of 6.36 years. Dependent variables were self-reported vision and hearing impairment. Outcome variables were falls, injuries from falls, physical and mental components of HRQOL (SF-36 PCS and MCS) and all-cause mortality.\n    \n\n\n          Results:\n        \n      \n      Vision impairment was more common in women and hearing impairment was more common in men. Vision impairment was associated with increased falls risk (odds ratio (OR)=1.77, 95% CI=1.35-2.32 in men; OR=1.82, 95% CI=1.44-2.30 in women), injuries from falls (OR=1.69, 95% CI=1.23-2.34 in men, OR=1.79, 95% CI=1.38-2.33 in women), and mortality (hazard ratio (HR)=1.44; 95% CI=1.17-1.77 in men; HR=1.50, 95% CI=1.24-1.82 in women) and declines in SF-36 PCS and MCS. Hearing impairment was associated with increased falls risk (OR=1.38, 95% CI=1.08-1.78 in men; OR=1.45, 95% CI=1.08-1.93 in women) and declines in SF-36 PCS and MCS. Overall there were no gender differences in the association between vision and hearing impairment and the outcomes.\n    \n\n\n          Conclusion:\n        \n      \n      In men and women aged 76-81 years, there were no gender differences in the association between self-reported vision and hearing impairment and the outcomes of falls, mortality and HRQOL."
        },
        {
            "title": "Mortality rates after cataract extraction.",
            "abstract": "Senile cataract may be a marker of generalized tissue aging. We examined this hypothesis using population-based linked health data. We hypothesized that any such association would diminish with increased use of cataract surgery. Mortality rates of those 50-95 years of age undergoing cataract surgery in British Columbia during either 1985 or 1989 were compared with the provincial population of comparable age who did not undergo cataract surgery during the study period. The 1985 cohort included 8,262 patients undergoing surgery and a comparison population of 804,303, and the 1989 cohort included 11,952 patients and a comparison population of 839,393. Using Cox regression, for the 1985 cohort, the hazard ratios for dying during follow-up were 3.2 for males 50-54.9 years of age [95% confidence limits (CL) = 2.0, 5.0] and 3.3 for females (95% CL = 1.9, 5.7). Hazard ratios for older age groups decreased with age. We also fit an additive risk model that produced excess mortalities that were less age dependent. In the 1985 analysis, these ranged from +7.1 per 1,000 (95% CL = +0.44, +13.76) to +20.3 (95% CL = +13.24, +27.36) for males and -17.5 (95% CL = -28.28, -6.72) to +2.0 (95% CL = -2.12, +6.12) for females. Findings for the 1989 analyses were similar, indicating that the association between cataracts and generalized aging remained constant despite a large increase in the use of cataract surgery."
        },
        {
            "title": "Nonoperative management of patients with a diagnosis of high-grade small bowel obstruction by computed tomography.",
            "abstract": "Objective:\n        \n      \n      To determine the natural history and treatment of high-grade small bowel obstruction (HGSBO). Small bowel obstruction is a frequent complication of abdominal surgery. Complete and strangulating obstructions are managed operatively while partial obstructions receive a trial of nonoperative therapy. The management and outcome of patients with HGSBO diagnosed by computed tomography (CT) has not been examined.\n    \n\n\n          Design:\n        \n      \n      Retrospective medical record review. Outcomes for nonoperative vs operative management were analyzed using Fisher exact and log-rank tests.\n    \n\n\n          Setting:\n        \n      \n      Tertiary care referral center.\n    \n\n\n          Patients:\n        \n      \n      One thousand five hundred sixty-eight consecutive patients admitted from the emergency department with a diagnosis of small bowel obstruction between 2000 and 2005 by CT criteria.\n    \n\n\n          Main outcome measures:\n        \n      \n      Recurrence of symptoms and complications.\n    \n\n\n          Results:\n        \n      \n      One hundred forty-five patients (9%) with HGSBO were identified, with 88% follow-up (median, 332 days; range, 4-2067 days). Sixty-six (46%) were successfully managed nonoperatively while 79 (54%) required an operation. Length of stay and complications were significantly increased in the operative group (4.7 days vs 10.8 days and 3% vs 23%; P < .001). Nonoperative management was associated with a higher recurrence rate (24% vs 9%; P < .005) and shorter time to recurrence (39 days vs 105 days; P < .005) compared with operative intervention. Computed tomography signs of ischemia, admission laboratory results, and presence of cancer or inflammatory bowel disease were not predictive of an operation.\n    \n\n\n          Conclusions:\n        \n      \n      Patients with HGSBO by CT can be managed safely with nonoperative therapy; however, they have a significantly higher rate of recurrence requiring readmission or operation within 5 years."
        },
        {
            "title": "Computed tomography pulmonary embolism index for the assessment of survival in patients with pulmonary embolism.",
            "abstract": "This study was an analysis of the correlation between pulmonary embolism (PE) and patient survival. Among 694 consecutive patients referred to our institution with clinical suspicion of acute PE who underwent CT pulmonary angiography, 188 patients comprised the study group: 87 women (46.3%, median age: 60.7; age range: 19-88 years) and 101 men (53.7%, median age: 66.9; age range: 21-97 years). PE was assessed by two radiologist who were blinded to the results from the follow-up. A PE index was derived for each set of images on the basis of the embolus size and location. Results were analyzed using logistic regression, and correlation with risk factors and patient outcome (survival or death) was calculated. We observed no significant correlation between the CTPE index and patient outcome (p = 0.703). The test of logistic regression with the sum of heart and liver disease or presence of cancer was significantly (p< 0.05) correlated with PE and overall patient outcome. Interobserver agreement showed a significant correlation rate for the assessment of the PE index (0.993; p< 0.001). In our study the CT PE index did not translate into patient outcome. Prospective larger scale studies are needed to confirm the predictive value of the index and refine the index criteria."
        },
        {
            "title": "Talking about sexual health during survivorship: understanding what shapes breast cancer survivors' willingness to communicate with providers.",
            "abstract": "Purpose:\n        \n      \n      Breast cancer survivor (BCS)-provider communication about sexual health (SH) is often absent or inadequate. Patients report wanting providers to broach the topic, but providers cite barriers to initiating discussions. While the health care community works to address barriers, it is unrealistic to rely solely on provider initiation of SH conversations. This research investigates willingness to communicate about sexual health (WTCSH) to better understand what may interfere with survivors' ability to self-advocate and receive care for these concerns.\n    \n\n\n          Methods:\n        \n      \n      (N = 305) BCSs completed online surveys. Hierarchical multiple regression determined the relationship between Sexual Quality of Life-Female (SQOL-F), which measures psychological and social dimensions of SH and WTCSH. Interviews were then conducted with forty BCSs. The constant comparative method was used to thematically analyze the transcripts.\n    \n\n\n          Results:\n        \n      \n      The mean SQOL score was 53.4 out of 100. No statistically significant differences in SQOL or WTCSH were found by age or survivorship length. The positive relationship between WTCSH and SQOL was significant, F (6,266) = 4.92, p < .000, adj. R2 = .080). Five themes illustrated factors that shape WTCSH: (1) comfort discussing SH; (2) perception of demographic similarity/discordance; (3) patient-centered communication; (4) belief that SH is (un)treatable, and (5) ability to access timely/coordinated care.\n    \n\n\n          Conclusions:\n        \n      \n      Findings establish the significance of SH concerns and provide an in-depth understanding of intrapersonal, interpersonal, and organizational issues informing WTCSH.\n    \n\n\n          Implications for cancer survivors:\n        \n      \n      Age and gender dynamics, perceptions of provider SQOL messaging, and futility influence survivor openness. Addressing these areas may encourage disclosure among women who would otherwise continue to suffer in silence."
        },
        {
            "title": "Body mass index versus waist circumference as predictors of mortality in Canadian adults.",
            "abstract": "Background:\n        \n      \n      Elevated body mass index (BMI) and waist circumference (WC) are associated with increased mortality risk, but it is unclear which anthropometric measurement most highly relates to mortality. We examined single and combined associations between BMI, WC, waist-hip ratio (WHR) and all-cause, cardiovascular disease (CVD) and cancer mortality.\n    \n\n\n          Methods:\n        \n      \n      We used Cox proportional hazard regression models to estimate relative risks of all-cause, CVD and cancer mortality in 8061 adults (aged 18-74 years) in the Canadian Heart Health Follow-Up Study (1986-2004). Models controlled for age, sex, exam year, smoking, alcohol use and education.\n    \n\n\n          Results:\n        \n      \n      There were 887 deaths over a mean 13 (SD 3.1) years follow-up. Increased risk of death from all-causes, CVD and cancer were associated with elevated BMI, WC and WHR (P<0.05). Risk of death was consistently higher from elevated WC versus BMI or WHR. Ascending tertiles of each anthropometric measure predicted increased CVD mortality risk. In contrast, all-cause mortality risk was only predicted by ascending WC and WHR tertiles and cancer mortality risk by ascending WC tertiles. Higher risk of all-cause death was associated with WC in overweight and obese adults and with WHR in obese adults. Compared with non-obese adults with a low WC, adults with high WC had higher all-cause mortality risk regardless of BMI status.\n    \n\n\n          Conclusion:\n        \n      \n      [corrected] BMI and WC predicted higher all-cause and cause-specific mortality, and WC predicted the highest risk for death overall and among overweight and obese adults. Elevated WC has clinical significance in predicting mortality risk beyond BMI."
        },
        {
            "title": "Flavonoid intake and risk of chronic diseases.",
            "abstract": "Background:\n        \n      \n      Flavonoids are effective antioxidants and may protect against several chronic diseases.\n    \n\n\n          Objective:\n        \n      \n      The association between flavonoid intake and risk of several chronic diseases was studied.\n    \n\n\n          Design:\n        \n      \n      The total dietary intakes of 10 054 men and women during the year preceding the baseline examination were determined with a dietary history method. Flavonoid intakes were estimated, mainly on the basis of the flavonoid concentrations in Finnish foods. The incident cases of the diseases considered were identified from different national public health registers.\n    \n\n\n          Results:\n        \n      \n      Persons with higher quercetin intakes had lower mortality from ischemic heart disease. The relative risk (RR) between the highest and lowest quartiles was 0.79 (95% CI: 0.63, 0.99: P for trend = 0.02). The incidence of cerebrovascular disease was lower at higher kaempferol (0.70; 0.56, 0.86; P = 0.003), naringenin (0.79; 0.64, 0.98; P = 0.06), and hesperetin (0.80; 0.64, 0.99; P = 0.008) intakes. Men with higher quercetin intakes had a lower lung cancer incidence (0.42; 0.25, 0.72; P = 0.001), and men with higher myricetin intakes had a lower prostate cancer risk (0.43; 0.22, 0.86; P = 0.002). Asthma incidence was lower at higher quercetin (0.76; 0.56, 1.01; P = 0.005), naringenin (0.69; 0.50, 0.94; P = 0.06), and hesperetin (0.64; 0.46, 0.88; P = 0.03) intakes. A trend toward a reduction in risk of type 2 diabetes was associated with higher quercetin (0.81; 0.64, 1.02; P = 0.07) and myricetin (0.79; 0.62, 1.00; P = 0.07) intakes.\n    \n\n\n          Conclusion:\n        \n      \n      The risk of some chronic diseases may be lower at higher dietary flavonoid intakes."
        },
        {
            "title": "Diagnosis of anomalous origin and course of coronary arteries using non-contrast cardiac CT scan and detection features.",
            "abstract": "Background:\n        \n      \n      Anomalous origin and course of coronary arteries (AOCA) is a potential cause of sudden cardiac death. Coronary CT angiography (coronary CTA) enables detailed 3-dimensional visualization of AOCA. Data are limited about the diagnostic performance of noncontrast cardiac CT obtained during coronary calcium scan for detecting AOCA.\n    \n\n\n          Objective:\n        \n      \n      We assessed the feasibility of using noncontrast cardiac CT for detecting AOCA.\n    \n\n\n          Methods:\n        \n      \n      Participants had noncontrast cardiac CT and coronary CTA performed (2005-2010). Cases had AOCA as diagnosed with coronary CTA. Controls were without AOCA. Noncontrast cardiac CT images were independently evaluated for AOCA by a cardiologist and a radiologist blinded to prior AOCA diagnosis. Detection features to assist AOCA diagnosis on noncontrast cardiac CT were evaluated.\n    \n\n\n          Results:\n        \n      \n      The study enrolled 54 cases and 155 controls. Sensitivity and specificity for detecting AOCA were 82% (95% CI, 69%-90%) and 90% (95% CI, 85%-94%) for observer 1, respectively, and 82% (95% CI, 69%-90%) and 85% (95% CI, 79%-90%) for observer 2, respectively. Average sensitivity and specificity were 82% and 88%, respectively. Interobserver agreement (Cohen κ) was κ = 0.65 (95% CI, 0.53-0.76). Inability to visualize the right coronary artery (RCA) origin at the right sinus significantly predicted RCA anomaly. Inability to visualize the left main coronary artery branching point into the left anterior descending coronary artery and the left circumflex coronary artery significantly predicted left coronary artery anomaly.\n    \n\n\n          Conclusion:\n        \n      \n      Noncontrast cardiac CT in conjunction with detection features has the potential for use in the diagnosis of AOCA. A prospective study is needed for validation and to determine the modality's accuracy for detecting AOCA."
        },
        {
            "title": "Orbital masses in granulomatosis with polyangiitis are associated with a refractory course and a high burden of local damage.",
            "abstract": "Objectives:\n        \n      \n      To identify and characterize patients with orbital masses in a monocentric cohort of 1142 GPA patients followed up from 1990 until the end of 2010 with regard to disease stage, local orbital inflammation, course of disease and outcome and to assess the efficacy of immunosuppressive treatment.\n    \n\n\n          Methods:\n        \n      \n      All GPA patients fulfilling ACR criteria or Chapel Hill Consensus Conference definitions or who had localized GPA and who developed orbital masses were evaluated regarding the course and outcome of the orbital masses (assessed by MRI, ophthalmologist and ENT specialist), all other clinical manifestations, disease stages, ANCA status, immunosuppression and its side effects and surgical procedures.\n    \n\n\n          Results:\n        \n      \n      Of 1142 GPA patients 58 developed orbital masses during a median follow-up of 101.5 months (range 23-255 months). Forty patients fulfilled the inclusion criteria and had complete clinical assessments [44% females, median age 43 (20-74) years, 85% ANCA positive]. Seventy-five per cent (29/40) had systemic disease when orbital masses occurred; both orbits were affected in 30%. Seventy-two per cent had evidence of infiltration from paranasal sinuses. Under highly potent immunosuppression (mostly CYC and glucocorticoids), 41% were refractory, 24% had unchanged activity, 24% showed a response and 8.1% had complete remission. Forty-four per cent had relapses of orbital masses. Seventy-two per cent developed visual impairment, 19% suffered blindness. Blindness was associated with a longer time to remission and a relapsing and refractory course.\n    \n\n\n          Conclusion:\n        \n      \n      Orbital masses are a rare manifestation of GPA and are characterized by a refractory course and by a high rate of local damage. Patients with a refractory or relapsing course are at higher risk of developing blindness."
        },
        {
            "title": "Proliferative retinopathy and proteinuria predict mortality rate in type 1 diabetic patients from Fyn County, Denmark.",
            "abstract": "Aims/hypothesis:\n        \n      \n      We evaluated the effect of diabetic retinopathy on 25 year survival rate among a population-based cohort of type 1 diabetic patients from Fyn County, Denmark.\n    \n\n\n          Methods:\n        \n      \n      In 1973 all diabetic patients from Fyn County, Denmark with onset before the age of 30 years as of 1 July 1973 were identified (n=727). In 1981, only 627 patients were still alive and resident in Denmark. Of these, 573 (91%) participated in a clinical baseline examination, in which diabetic retinopathy was graded and other markers of diabetes measured. Mortality rate was examined in a 25 year follow-up and related to the baseline examination.\n    \n\n\n          Results:\n        \n      \n      Of the 573 patients examined at baseline in 1981 and 1982, 297 (51.8%) were still alive in November 2006. Of the others, 256 (44.7%) had died, three (0.5%) had left Denmark and 17 (3%) were of unknown status. Age- and sex-adjusted HRs of mortality rate were 1.01 (95% CI 0.72-1.42) and 2.04 (1.43-2.91) for patients with non-proliferative and proliferative retinopathy respectively at baseline compared with patients with no retinopathy. After adjusting for proteinuria, HR among patients with proliferative retinopathy lost statistical significance, but still remained 1.48 (95% CI 0.98-2.23). The 10 year survival rate of patients who had proliferative retinopathy as well as proteinuria at baseline was 22.2% and significantly lower (p<0.001) than in patients with proteinuria only (70.3%), proliferative retinopathy only (79.0%) or neither (86.6%).\n    \n\n\n          Conclusions/interpretation:\n        \n      \n      Proliferative retinopathy and proteinuria predict mortality rate in a population-based cohort of type 1 diabetic patients. In combination they act even more strongly. Non-proliferative diabetic retinopathy did not affect survival rate."
        },
        {
            "title": "Temporal lobe surgery around the world. Results, complications, and mortality.",
            "abstract": "This survey covers 2,282 published temporal lobe resections, performed from 1928-1973, all over the world, as treatment of invalidating, drug-resistant epilepsy. At follow-up, two-thirds of the patients were free or almost free from seizures; and over half of those patients who were mentally abnormal before the operation were normalized or had obtained a marked improvement. The operative mortality has always been very low. No operative mortality has been recorded within the last decade. The risk of severe complications such as persistent hemiparesis and/or a complete homonymous hemianopia has decreased markedly, and is now only a few per cent."
        },
        {
            "title": "External validation of the Revised Cardiac Risk Index and update of its renal variable to predict 30-day risk of major cardiac complications after non-cardiac surgery: rationale and plan for analyses of the VISION study.",
            "abstract": "Introduction:\n        \n      \n      The Revised Cardiac Risk Index (RCRI) is a popular classification system to estimate patients' risk of postoperative cardiac complications based on preoperative risk factors. Renal impairment, defined as serum creatinine >2.0 mg/dL (177 µmol/L), is a component of the RCRI. The estimated glomerular filtration rate has become accepted as a more accurate indicator of renal function. We will externally validate the RCRI in a modern cohort of patients undergoing non-cardiac surgery and update its renal component.\n    \n\n\n          Methods and analysis:\n        \n      \n      The Vascular Events in Non-cardiac Surgery Patients Cohort Evaluation (VISION) study is an international prospective cohort study. In this prespecified secondary analysis of VISION, we will test the risk estimation performance of the RCRI in ∼34 000 participants who underwent elective non-cardiac surgery between 2007 and 2013 from 29 hospitals in 15 countries. Using data from the first 20 000 eligible participants (the derivation set), we will derive an optimal threshold for dichotomising preoperative renal function quantified using the Chronic Kidney Disease Epidemiology Collaboration (CKD-Epi) glomerular filtration rate estimating equation in a manner that preserves the original structure of the RCRI. We will also develop a continuous risk estimating equation integrating age and CKD-Epi with existing RCRI risk factors. In the remaining (approximately) 14 000 participants, we will compare the risk estimation for cardiac complications of the original RCRI to this modified version. Cardiac complications will include 30-day non-fatal myocardial infarction, non-fatal cardiac arrest and death due to cardiac causes. We have examined an early sample to estimate the number of events and the distribution of predictors and missing data, but have not seen the validation data at the time of writing.\n    \n\n\n          Ethics and dissemination:\n        \n      \n      The research ethics board at each site approved the VISION protocol prior to recruitment. We will publish our results and make our models available online at http://www.perioperativerisk.com.\n    \n\n\n          Trial registration number:\n        \n      \n      ClinicalTrials.gov NCT00512109."
        },
        {
            "title": "CT perfusion for confirmation of brain death.",
            "abstract": "For pronouncing brain death, unlike CTP, the 2-phase CTA gives no functional information and is limited by inadvertent delay of the second acquisition, which may give false-negative results. The purpose of our study was to compare CTP and CTA derived from the CTP data with the Dupas and Frampas criteria for confirmation of brain death. A retrospective review of CTP in 11 consecutive patients for confirmation of brain death showed a sensitivity of 72.7% for 7- and 4-point scores, 81.8% for opacification of the ICV, and 100% for CTP scores in the brain stem. CTA obtained from the CTP data showed similar sensitivity in the diagnosis of brain death. This protocol also reduces the iodinated contrast dose and is less operator-dependent. The addition of the functional tools of CTP increased the sensitivity of CTA in the confirmation of brain death."
        },
        {
            "title": "Fifteen-year argon laser and xenon photocoagulation results of Bascom Palmer Eye Institute's patients participating in the diabetic retinopathy study.",
            "abstract": "Fifteen years after panretinal photocoagulation in the Diabetic Retinopathy Study, 86 (57%) patients had died, 14 (9%) could not be located, and 51 (34%) of 151 patients were examined to determine the long-term treatment effects. Of the eyes randomized to photocoagulation only 1 (5%) of 19 argon-treated and 1 (3%) of 32 xenon-treated eyes had received additional laser treatment, but 8 argon-treated and 7 xenon-treated eyes had had cataract removal. Eleven (58%) of the initially argon-treated and 13 (41%) of the initially xenon-treated eyes had 20/40 or better acuity, and 18 (95%) of the initially argon-treated and 26 (82%) of the initially xenon-treated eyes had 20/200 or better acuity. Of the control eyes 17 (33%) had 20/40 or better, and 30 (58%) had 20/200 or better acuity. Argon and xenon panretinal photocoagulation for diabetic retinopathy provide good results for at least 15 years."
        },
        {
            "title": "Lifetime visual disability in open-angle glaucoma and ocular hypertension.",
            "abstract": "Purpose:\n        \n      \n      To study how many of the patients with treated glaucoma or ocular hypertension go blind during their lifetime and which factors are associated with blindness.\n    \n\n\n          Patients:\n        \n      \n      The data on 106 consecutive patients who had died between 1991 and 2002 was retrospectively evaluated. At diagnosis 39 patients had primary open-angle glaucoma, 27 had exfoliation glaucoma, and 40 had ocular hypertension.\n    \n\n\n          Methods:\n        \n      \n      Clinical records and causes of death were reviewed. Visual disability at the last visit before death was evaluated. Outcome measures were visual handicap and blindness based on visual acuity and/or visual fields.\n    \n\n\n          Results:\n        \n      \n      At the last visit, 17 patients [16%, 95% confidence interval (CI) 9-23] were visually handicapped. Thirteen patients (14%) were bilaterally blind. Glaucoma was the cause of blindness in one or both eyes in 16 patients (15%, 95% CI 8-22) and in both eyes in 6 patients (6%, 95% CI 1-10). In the analysis of only 1 eye of each patient, the cumulative incidence of glaucoma-caused blindness was 6% (95% CI 2-11) at 5 years, 9% (95% CI 4-15) at 10 years, and 15% (95% CI 9-23) at 15 years. An advanced stage of glaucoma at diagnosis, fluctuation in intraocular pressure during treatment, the presence of exfoliation syndrome, and poor patient compliance increased the occurrence of blindness. Positive family history of glaucoma and vascular causes of death had no effect on visual outcome.\n    \n\n\n          Conclusions:\n        \n      \n      Glaucoma-caused blindness was associated with an advanced stage of glaucoma at diagnosis, fluctuation of intraocular pressure during treatment, the presence of exfoliation syndrome, and poor patient compliance. The risk of going blind from glaucoma in both eyes was 6%."
        },
        {
            "title": "The Quebec Association of Gastroenterology position paper on colorectal cancer screening - 2003.",
            "abstract": "Colorectal cancer is a leading cause of death and the third most common cancer in Canada. Evidence suggests that screening can reduce mortality rates and the cost effectiveness of a program compares favourably with initiatives for breast and cervical cancer. The objectives of the Association des gastro-entérologues du Québec Task Force were to determine the need for a policy on screening for colorectal cancer in Quebec, to evaluate the testing methods available and to propose one or more of these alternatives as part of a formal screening program, if indicated. Fecal occult blood testing (FOBT), endoscopy (including sigmoidoscopy and colonoscopy), barium enema and virtual colonoscopy were considered. Although most clinical efficacy data are available for FOBT and sigmoidoscopy, there are limitations to programs based on these strategies. FOBT has a high false positive rate and a low detection yield, and even a combination of these strategies will miss 24% of cancers. Colonoscopy is the best strategy to both detect and remove polyps and to diagnose colorectal cancer, with double contrast barium enema also being a sensitive detection method. The Task Force recommended the establishment, in Quebec, of a screening program with five- to 10-yearly double contrast barium enema or 10-yearly colonoscopy for individuals aged 50 years or older at low risk. The program should include outcome monitoring, public and professional education to increase awareness and promote compliance, and central coordination with other provincial programs. The program should be evaluated; specific billing codes for screening for colorectal cancer would help facilitate this. Formal feasibility, effectiveness and cost-effectiveness studies in Quebec are now warranted."
        },
        {
            "title": "Model for end-stage liver disease-based allocation system for liver transplantation in Argentina: does it work outside the United States?",
            "abstract": "Background:\n        \n      \n      In July 2005, Argentina was the first country after the United States to adopt the MELD system. The purpose of the present study was to analyse the impact of this new system on the adult liver waiting list (WL).\n    \n\n\n          Methods:\n        \n      \n      Between 2005 and 2009, 1773 adult patients were listed for liver transplantation: 150 emergencies and 1623 electives. Elective patients were categorized using the MELD system. A prospective database was used to analyse mortality and probability to be transplanted (PTBT) on the WL.\n    \n\n\n          Results:\n        \n      \n      The waiting time increased inversely with the MELD score and PTBT positively correlated with MELD score. With scores >/= 18 the PTBT remained over 50%. However, the largest MELD subgroup with <10 points (n = 433) had the lower PTBT (3%). In contrast, patients with T(2) hepatocellular carcinoma benefited excessively with the highest PTBT (84.2%) and the lowest mortality rate (5.4%). The WL mortality increased after MELD adoption (10% vs. 14.8% vs. P < 0.01). Patients with <10 MELD points had >fourfold probability of dying on the WL than PTBT (14.3% vs. 3%; P < 0.0001).\n    \n\n\n          Conclusions:\n        \n      \n      After MELD implementation, WL mortality increased and most patients who died had a low MELD score. A comprehensive revision of the MELD system must be performed to include cultural and socio-economical variables that could affect each country individually."
        },
        {
            "title": "Subtraction color map of contrast-enhanced and unenhanced CT for the prediction of pancreatic necrosis in early stage of acute pancreatitis.",
            "abstract": "Objective:\n        \n      \n      The objective of our study was to evaluate the accuracy of subtraction color-map images created from contrast-enhanced CT (CECT) and unenhanced CT for the diagnosis of pancreatic necrosis in the early stage of acute pancreatitis.\n    \n\n\n          Materials and methods:\n        \n      \n      Forty-eight patients underwent unenhanced CT and CECT within 72 hours from the onset of acute pancreatitis. Subtraction color-map images were created from unenhanced CT and CECT using a 3D nonrigid registration method. Three radiologists reviewed two image sets: CECT alone and subtraction color-map images in conjunction with CECT. Readers evaluated each image set for the presence of pancreatic necrosis. The reference standard for pancreatic necrosis was CT or MRI 1 week or more after the onset of acute pancreatitis. The performance of each image set for the prediction of pancreatic necrosis was calculated and compared using the McNemar test.\n    \n\n\n          Results:\n        \n      \n      Eleven of the 48 patients developed pancreatic necrosis. There were no technical failures creating the subtraction images. The sensitivity, specificity, and accuracy for predicting pancreatic necrosis with CECT were 64%, 97%, and 90%, respectively, for reader 1; 73%, 87%, and 83% for reader 2; and 73%, 87%, and 83% for reader 3. The sensitivity, specificity, and accuracy for predicting pancreatic necrosis with the subtraction color maps were 100%, 100%, and 100%, respectively, for reader 1; 100%, 95%, and 96% for reader 2; and 82%, 92%, and 90% for reader 3. Accuracy significantly improved with the addition of subtraction color maps compared with CECT alone for reader 1 (p = 0.03) and reader 2 (p = 0.02) but not for reader 3 (p = 0.37).\n    \n\n\n          Conclusion:\n        \n      \n      A subtraction color map is accurate in the diagnosis of pancreatic necrosis in the early stage of acute pancreatitis."
        },
        {
            "title": "Investigating the relationship between neighborhood poverty and mortality risk: a marginal structural modeling approach.",
            "abstract": "Extant observational studies generally support the existence of a link between neighborhood context and health. However, estimating the causal impact of neighborhood effects from observational data has proven to be a challenge. Omission of relevant factors may lead to overestimating the effects of neighborhoods on health while inclusion of time-varying confounders that may also be mediators (e.g., income, labor force status) may lead to underestimation. Using longitudinal data from the 1990 to 2007 years of the Panel Study of Income Dynamics, this study investigates the link between neighborhood poverty and overall mortality risk. A marginal structural modeling strategy is employed to appropriately adjust for simultaneous mediating and confounding factors. To address the issue of possible upward bias from the omission of key variables, sensitivity analysis to assess the robustness of results against unobserved confounding is conducted. We examine two continuous measures of neighborhood poverty - single-point and a running average. Both were specified as piece-wise linear splines with a knot at 20 percent. We found no evidence from the traditional naïve strategy that neighborhood context influences mortality risk. In contrast, for both the single-point and running average neighborhood poverty specifications, the marginal structural model estimates indicated a statistically significant increase in mortality risk with increasing neighborhood poverty above the 20 percent threshold. For example, below 20 percent neighborhood poverty, no association was found. However, after the 20 percent poverty threshold is reached, each 10 percentage point increase in running average neighborhood poverty was found to increase the odds for mortality by 89 percent [95% CI = 1.22, 2.91]. Sensitivity analysis indicated that estimates were moderately robust to omitted variable bias."
        },
        {
            "title": "Panorama of orbital space-occupying lesions. The 24-year experience of a referral centre.",
            "abstract": "Purpose:\n        \n      \n      The aim of this survey was to study the frequency and management of orbital lesions requiring incisional or excisional biopsy for diagnostic or therapeutic purposes.\n    \n\n\n          Methods:\n        \n      \n      A histopathological review of specimens from 300 consecutive patients with space-occupying orbital lesions managed over a period of 24 years at a tertiary referral centre.\n    \n\n\n          Results:\n        \n      \n      The lesions could be attributed to 73 different entities with low-grade, non-Hodgkin lymphoma being the most common. More than half (54.3%) of lesions were neoplastic and malignant disease was present in 29.0% of patients. The majority of lesions were biopsied using the anterior transseptal or transconjunctival approach.\n    \n\n\n          Conclusion:\n        \n      \n      Most orbital space-occupying lesions requiring biopsy are benign and easily accessible. However, the diversity of these rare lesions and complexity of management suggest that patient care is best provided by a team of experienced subspecialists at a designated orbital centre."
        },
        {
            "title": "Lifetime visual prognosis of patients with glaucoma.",
            "abstract": "Background:\n        \n      \n      To investigate the visual outcome of glaucoma patients.\n    \n\n\n          Design:\n        \n      \n      This is a retrospective study of case notes of patients who died while under follow up in a glaucoma clinic of a University Hospital in Scotland between 2006 and 2009.\n    \n\n\n          Participants:\n        \n      \n      Seventy-seven patients were identified.\n    \n\n\n          Methods:\n        \n      \n      Data collected included type of glaucoma, coexisting pathology and best-corrected visual acuity in Snellen (converted to decimal values) for the first and final clinic visit. The final visual status was evaluated based on the best-corrected visual acuity of the better seeing eye at the last glaucoma clinic visit. Patients who had best-corrected visual acuity of less than Snellen decimal 0.5 were considered not to meet the standards for driving.\n    \n\n\n          Main outcome measures:\n        \n      \n      Snellen decimal best-corrected visual acuity, fulfilment of driving standards, and eligibility for partial sight and blind registration at the last clinic visit.\n    \n\n\n          Results:\n        \n      \n      The mean ages at presentation and death were 71.8 ± 10.3 years and 82.2 ± 8.7 years respectively. The mean Snellen decimal best-corrected visual acuity of the better eye at presentation was 0.78, and at the final clinic visit was 0.61. At the final clinic visit, no patients were partial sight registrable, four (5.2%) were blind registrable, and 27 (35.1%) did not fulfil UK driving criteria. Glaucoma patients with other ocular pathologies were more likely to fail UK driving criteria at presentation (P = 0.02) and at last clinic visit (P = 0.03).\n    \n\n\n          Conclusion:\n        \n      \n      The majority of glaucoma patients maintained good visual function at the end of their lifetime."
        },
        {
            "title": "Cost-effectiveness of screening for hepatopulmonary syndrome in liver transplant candidates.",
            "abstract": "The hepatopulmonary syndrome (HPS) is present in 15-20% of patients with cirrhosis undergoing orthotopic liver transplantation (OLT) evaluation. Both preoperative and post-OLT mortality is increased in HPS patients particularly when hypoxemia is severe. Screening for HPS could enhance detection of OLT candidates with sufficient hypoxemia to merit higher priority for transplant and thereby decrease mortality. However, the cost-effectiveness of such an approach has not been assessed. Our objective was to perform a cost-effectiveness analysis from a third-party payer's perspective of screening for HPS in liver OLT candidates. The costs and outcomes of 3 different strategies were compared: (1) no screening, (2) screening patients with a validated dyspnea questionnaire, and (3) screening all patients with pulse oximetry. Arterial blood gas analyses and contrast echocardiography were performed in patients with dyspnea or a pulse oximetry (SpO(2)) < or =97% to define the presence of HPS. A Markov model was constructed simulating the natural history of cirrhosis in a cohort of patients 50 years old over a time horizon of their remaining life expectancy. Transition probabilities were obtained from published data available through Medline and U.S. vital statistics. Costs represented Medicare reimbursement data at our institution. Costs and health effects were discounted at a 3% annual rate. No screening was associated with a total cost of 291,898 dollars and a life expectancy of 11.131 years. Screening with pulse oximetry was associated with a cost of 299,719 dollars and a life expectancy of 12.27 years. Screening patients with the dyspnea-fatigue index was associated with a cost and life expectancy of 300,278 dollars and 12.28 years, respectively. The incremental cost-effectiveness ratio of screening with pulse oximetry (compared to no screening) was 6,867 dollars per life year gained, whereas that of the dyspnea-fatigue index (compared to pulse oximetry) was 55,900 dollars per life year gained. The cost-effectiveness of screening depended on the prevalence and severity of HPS, and the choice of screening strategy was dependent on the sensitivity of the screening modality. In conclusion, screening for HPS, especially with pulse oximetry, is a cost-effective strategy that improves survival in transplant candidates predominantly by targeting the transplant to the subgroup of patients most likely to benefit. The utility of screening depends on the prevalence and severity of HPS in the target population."
        },
        {
            "title": "Long-term prognostic value of dipyridamole stress myocardial contrast echocardiography.",
            "abstract": "Aims:\n        \n      \n      The aim of this prospective study was to determine long-term prognostic value of myocardial contrast echocardiography (MCE) combined with high-dose dipyridamole stress echocardiography (DSE) in patients undergoing diagnostic work-up for stable coronary artery disease (CAD).\n    \n\n\n          Methods:\n        \n      \n      A total of 202 consecutive patients (67% males, age 57±8 years) with suspected or known stable CAD scheduled for coronary angiography underwent high-dose dipyridamole/atropine stress echocardiography (dipyridamole 0.84 mg/kg, iv; atropine up to 1 mg, iv) with MCE at baseline and peak stress. In 102 patients MCE was performed using electrocardiographic-triggered end-systolic harmonic imaging and in 100 patients using real-time MCE. Contrast enhancement was obtained by repeated iv boluses of contrast and was visually scored in 18 segments by consensus of 2 experienced observers. All patients completed prospective follow-up regarding major adverse cardiovascular events (cardiac mortality, revascularization, infarction and unstable angina) for a mean period of 32±11 months (range: 1-89 months). The prognostic value of inducible wall motion abnormalities (WMA) and perfusion defects (PD) was then analysed.\n    \n\n\n          Results:\n        \n      \n      CAD defined as ≥70% stenosis was found in 152 patients (75%). During follow-up major adverse cardiovascular events (MACE) occurred in 109 (54%) patients (10 deaths, 16 infarctions, 83 revascularizations). The presence of inducible WMA in DSE was associated with high risk of MACE [hazard ratio (HR): 5.4; 95% CI: 3.64-8.05, P<0.0001]. Cardiovascular complications were best predicted by the presence of any inducible abnormality-PD or WMA (HR: 6.1; 95% CI: 4.1-9.1, P<0.0001).\n    \n\n\n          Conclusion:\n        \n      \n      Stress MCE is highly predictive of cardiovascular events in patients with suspected or known CAD in long-term follow-up."
        },
        {
            "title": "Analysis of the Interleukin-6 (-174) Locus Polymorphism and Serum IL-6 Levels with the Severity of Normal Tension Glaucoma.",
            "abstract": "Purpose:\n        \n      \n      In normal tension glaucoma (NTG), factors other than elevated intraocular pressure are likely to have a role in the pathogenesis of optic neuropathy. Recent studies of glaucoma or retinal ganglion cells (RGCs) reveal that the cytokine interleukin-6 (IL-6) is linked to the pathogenesis of glaucoma and may regulate RGC survival or death. The IL-6 (-174) G allele has also been shown to increase the IL-6 protein. We hypothesized that the IL-6 (-174) polymorphism may be a predisposing genetic factor affecting the severity of glaucoma. The aim of the present study was to evaluate the IL-6 polymorphism and serum IL-6 levels as a potential risk factor related to the severity of NTG.\n    \n\n\n          Methods:\n        \n      \n      A total of 256 subjects with NTG in the Chinese population were enrolled. The patients were genotyped for the IL-6 (-174) C/G polymorphism. Genomic DNA was amplified by a polymerase chain reaction, followed by the enzymatic restriction fragment length polymorphism technique. Serum IL-6 levels were measured by ELISA. Patient age at diagnosis, cup/disc (C/D) ratio, rim area (RA), retinal nerve fiber layer (RNFL) thickness, and visual field (VF) were analyzed. The associations between genotypes of IL-6 (-174) C/G and the clinical parameters were calculated using a logistic regression.\n    \n\n\n          Results:\n        \n      \n      The IL-6 (-174) GC genotype in NTG patients was significantly associated with a smaller C/D ratio (p = 0.04), larger RA (p = 0.04), and thicker RNFL (p = 0.05) compared with IL-6 (-174) GG patients. The allele frequency of IL-6 (-174) C was significantly higher in the NTG patients at an early-moderate stage than at an advanced stage according to the C/D ratio (OR 0.55; 95% CI 0.31-0.99). Pattern standard deviation of VF was borderline lower in IL-6 (-174) GC patients (p = 0.06), and serum IL-6 levels were borderline higher in advanced stages than in early-moderate stages (7.66 ± 3.22 vs. 4.46 ± 3.83 pg/mL; p = 0.06).\n    \n\n\n          Conclusion:\n        \n      \n      The IL-6 (-174) GC genotype is associated with a smaller C/D ratio, larger RA, and thicker RNFL compared with IL-6 (-174) GG in NTG patients. We found that the IL-6 (-174) G/C polymorphism and serum IL-6 levels may be associated with the severity of NTG."
        },
        {
            "title": "Assessing clinical impact of myocardial perfusion studies: ischemia or other prognostic indicators?",
            "abstract": "One of the major strengths of nuclear myocardial perfusion imaging (MPI) is the robust prognostic databases from observational studies demonstrating significantly different outcomes in patients with low-risk vs high-risk scans. The severity of the MPI defect can be semi-quantitated using the summed stress score (SSS) and summed difference score (SDS). SSS is more strongly associated with mortality, whereas SDS is the better predictor of subsequent coronary angiography and revascularization. The strength of MPI variables as prognostic indicators decreases when adjusted for prognostically important clinical and stress test variables. Nonetheless, most studies of general patient populations have demonstrated that MPI adds incremental prognostic value to clinical and stress test information. In contrast to these positive results from observational studies, the application of MPI ischemia as a treatment guide in several recent trials (DIAD, WOMEN, COURAGE, BARI 2D, STICH) has largely failed to identify patient subsets with improved outcome. This issue will continue to be investigated in the ongoing PROMISE and ISCHEMIA trials."
        },
        {
            "title": "Characteristics of participants' and caregivers' influence on non-response in a cross-sectional study of dementia in an older population.",
            "abstract": "Objective:\n        \n      \n      The issue of non-response in dementia epidemiological studies, which may result in the underestimation of the prevalence of dementia, has attracted little attention. We aimed to explore the causes and related factors of non-response in a dementia survey among Chinese veterans.\n    \n\n\n          Methods:\n        \n      \n      A two-phase, cross-sectional study investigated the prevalence of dementia and mild cognitive impairment in Chinese veterans aged ≥ 60 years. We collected the socio-demographic data and prior medical history, evaluated the health status of veterans and their caregivers, assessed the cognitive status of veterans, and evaluated the care burden of caregivers by Caregiver Burden Inventory (CBI).\n    \n\n\n          Results:\n        \n      \n      Of 9676 eligible participants, 525 (5.4%) veterans in phase 1 and 1706 (35.0%) veterans among 4875 veterans in phase 2 did not respond. Illness, hospitalization and death accounted for 63.0% and 75.5% non-response in phases 1 and 2, respectively. Non-participation in social activities, self-perceived poor health status, worsened health changes, self-reported need for life care, and history of hearing loss or glaucoma independently predicted non-response in phase 1 or 2. The heavy care burden, suggested by the higher CBI scores and self-reported health deterioration of the primary caregivers, predicted non-response in phase 1 or 2.\n    \n\n\n          Conclusions:\n        \n      \n      The negative factors from both the participants and their caregivers independently predicted the non-response in the dementia study in an older population. Preventative strategies from the perspectives of the participants and caregivers should be developed to improve the response rates in both phases in a cross-sectional study."
        },
        {
            "title": "Late results after carotid endarterectomy for amaurosis fugax.",
            "abstract": "Amaurosis fugax is considered an ocular transient ischemic attack with an ominous prognosis. One hundred twenty-eight patients with amaurosis fugax as the presenting symptom underwent carotid endarterectomy at the University of California, San Diego (UCSD) and Scripps Clinic between 1970 and 1985 with one death (0.8%) and one postoperative permanent stroke (0.8%). Subsequently, these patients were followed up for 6 to 160 months (mean 45.3 months). Only two subsequent late strokes were documented (at 2 and 5 years after operation). These results were significantly better (p less than 0.01) by life-table analysis than the late stroke rate after carotid endarterectomy performed to treat anterior motor transient ischemic attacks at both UCSD and Scripps Clinic, as well as the reported late follow-up for all transient ischemic attacks after carotid endarterectomy in the literature (1.8% per year, 17 publications, 1980 operations). Thus amaurosis fugax appears to be a particularly favorable indication for carotid endarterectomy. Left untreated, this event carries a high risk of stroke; after carotid endarterectomy, which has a low operative risk, there is a very low postoperative stroke rate (two strokes in 448 patient-years of follow-up)."
        },
        {
            "title": "Determining public health priorities for an ageing population: the value of a disability survey.",
            "abstract": "In order to determine which diseases and health problems were most strongly associated with long-term disability among the Thai elderly and to determine their public health priority, a national cross-sectional multistage random sampling survey was conducted in 1997. Four thousand and forty-eight Thai older persons aged 60 years and over were recruited and interviewed by trained interviewers. Overall, 769 (19%) people reported having a long-term disability. Participants with long-term disability (LD) reported having between one and 21 long-term diseases or health problems. Eighteen of these problems were independently associated with LD in logistic regression analysis. Nearly half of the cases with LD (46.4%) suffered from two or more health problems. The odds of LD increased with the number of problems suffered. The problems contributing most to the population burden of disease as assessed by population attributable risk fractions were hemiparesis, arthritis, accidents (unintentional injuries), blindness and other eye diseases, kyphosis, weakness of limbs, deafness, and hypertension. This ranking of public health priority differs from conventional approaches using mortality statistics and disability adjusted life years (DALYs). In conclusion, national disability surveys provide a valuable means of assessing the population burden of disability and determining the underlying causes of disability. These methods provide a direct assessment of disability prevalence and disease priorities for rapidly ageing transitional countries where death certification may be incomplete or inaccurate."
        },
        {
            "title": "The impact of anchor point on utilities for 5 common ophthalmic diseases.",
            "abstract": "Purpose:\n        \n      \n      To elicit utilities on a perfect health and perfect vision scale for 5 common eye diseases.\n    \n\n\n          Design:\n        \n      \n      Cross-sectional observational preference study.\n    \n\n\n          Participants:\n        \n      \n      We included 434 patients: 58 with diabetic retinopathy, 99 with glaucoma, 44 with age-related macular degeneration (AMD), 124 with cataract; 109 with refractive error.\n    \n\n\n          Testing:\n        \n      \n      Standard gamble utilities were estimated using a computer-based preference assessment interview platform.\n    \n\n\n          Main outcome measures:\n        \n      \n      Standard gamble utilities, a quality-of-life measure that examines the willingness to accept a risk of death or unilateral blindness in return for perfect health or perfect vision.\n    \n\n\n          Results:\n        \n      \n      Using the standard policy scale, where health equivalent to death is 0 and perfect health is 1, participants with asymptomatic diabetic retinopathy had a utility of 0.93. By comparison, symptomatic diabetics had a further utility loss of 0.14. Asymptomatic glaucoma participants had a utility of 0.92 with a decrease of 0.03 for early field loss and a further decrease of 0.03 with central field loss. Participants with AMD who had > or =20/100 better-eye visual acuity reported a utility of 0.89, whereas those with more severe AMD reported 0.76. However, neither clinical cataract opacity score nor refractive error correlated with utility. Adjustment for age and comorbidity did not alter these relationships. For the same participants, utilities measured with different anchor points-monocular blindness as 0 and perfect vision as 1-were lower, especially among participants with increased disease severity. The difference between utility assessed on this perfect vision-blindness scale and the perfect health-death scale ranged from 0.04 for those with severe refractive error to 0.19 for symptomatic diabetics and 0.37 for those with severe AMD.\n    \n\n\n          Conclusions:\n        \n      \n      This paper elicits utilities with different anchor points from a previously unreported sample of 434 patients. Lower utility scores normally imply greater benefit with successful treatment or prevention of disease, but switching from the conventional policy scale to the perfect vision scale also consistently results in lower scores. Because most previous ophthalmic studies have used perfect vision as the upper anchor, the resulting utilities may not have been accurate."
        },
        {
            "title": "Association between the use of glaucoma medications and mortality.",
            "abstract": "Objective:\n        \n      \n      To evaluate the relationship between glaucoma medication use and death.\n    \n\n\n          Methods:\n        \n      \n      This study uses longitudinal data from 2003 to 2007 on persons 40 years and older with glaucoma or suspected glaucoma enrolled in a large managed care network. Cox regression analysis was performed to estimate the hazard of death associated with the use of various glaucoma medication classes and combinations thereof. Multivariable models were adjusted for demographic characteristics and comorbid medical conditions.\n    \n\n\n          Results:\n        \n      \n      Of 21 506 participants with glaucoma or suspected glaucoma, 237 (1.1%) died during the study period. The use of any class of glaucoma medication was associated with a 74% reduced hazard of death (adjusted hazard ratio [HR], 0.26; 95% confidence interval [CI], 0.16-0.40) compared with no glaucoma medication use. This association was observed for use of a single agent alone, such as a topical beta-antagonist (0.44; 0.24-0.83) or a prostaglandin analogue (0.31; 0.18-0.54), and for use of different combinations of drug classes.\n    \n\n\n          Conclusions:\n        \n      \n      After adjustment for potential confounding variables, the use of glaucoma medications was associated with a reduced likelihood of death in this large sample of US adults with glaucoma. Future investigations should explore this association further because these findings may have important clinical implications."
        },
        {
            "title": "Contrast-induced acute kidney injury: the importance of diagnostic criteria for establishing prevalence and prognosis in the intensive care unit.",
            "abstract": "Objective:\n        \n      \n      To establish whether there is superiority between contrast-induced acute kidney injury and contrast-induced nephropathy criteria as predictors of unfavorable clinical outcomes.\n    \n\n\n          Methods:\n        \n      \n      Retrospective study carried out in a tertiary hospital with 157 patients undergoing radiocontrast infusion for propaedeutic purposes.\n    \n\n\n          Results:\n        \n      \n      One hundred forty patients fulfilled the inclusion criteria: patients who met the criteria for contrast-induced acute kidney injury (59) also met the criteria for contrast-induced nephropathy (76), 44.3% met the criteria for KDIGO staging, 6.4% of the patients required renal replacement therapy, and 10.7% died.\n    \n\n\n          Conclusion:\n        \n      \n      The diagnosis of contrast-induced nephropathy was the most sensitive criterion for renal replacement therapy and death, whereas KDIGO showed the highest specificity; there was no correlation between contrast volume and progression to contrast-induced acute kidney injury, contrast-induced nephropathy, support dialysis or death in the assessed population."
        },
        {
            "title": "Optimal CT protocol for the diagnosis of active bleeding in abdominal trauma patients.",
            "abstract": "Objectives:\n        \n      \n      The aim of this study is to compare the radiologic diagnostic performance of arterial phase, portal phase and combined phase computed tomography (CT) for traumatic abdominal injury. In addition, this study is attempted to decrease lifetime attributable risks (LARs) of cancer due to radiation exposure by using optimal CT protocol.\n    \n\n\n          Materials and methods:\n        \n      \n      A total of 114 consecutive patients with a traumatic abdominal injury and an abdominal hematoma on CT were enrolled at a single tertiary regional trauma center between January 2016 and March 2017. Each CT protocol set was independently reviewed by three radiologists, and the diagnostic performance of all three CT phases were compared with regard to the capability to detect active bleeding, contained vascular injuries, and organ injuries. Additionally, LARs for cancer incidence and mortality were calculated using dose-length product values, for each phase of CT.\n    \n\n\n          Results:\n        \n      \n      The pooled area under the receiver operating characteristic curves for the diagnosis of active bleeding, contained vascular injuries, and organ injuries ranged from 0.910 to 0.922, 0.643 to 0.723, and 0.948 to 0.915 for arterial, portal, and combined phase CT, respectively. There was no statistically significant difference in the diagnosis of active bleeding and organ injuries for any combination of two phase sets. The mean LARs for cancer incidence was 0.059%, 0.062% and 0.121% during arterial, portal and combined phase CT, respectively.\n    \n\n\n          Conclusion:\n        \n      \n      Single phase CT could be a potential protocol for abdominal trauma patients. Use of single phase CT could significantly decrease the incidence of radiation-associated cancer in the future."
        },
        {
            "title": "[A cohort study on deaths from SMON in Japan].",
            "abstract": "Causes and risk factors of deaths from subacute myelo-optico neuropathy (SMON) were studied in a prospective cohort of 4,329 SMON patients followed for 3 years and 7 months (Sept. 1985-March 1989) with the following findings: (1) Recent excess deaths of SMON patients was estimated as 4% from ratio of O/E (SMR = 104) and deaths due to SMON itself was 6.4%. (2) The ratio of O/E was significantly higher for deaths from cancer of colon/rectum in females, cancer of pancreas in males, hypertension in males, pneumonia/influenza in females, chronic obstructive pulmonary diseases in males, tuberculosis and intestinal obstructive disease in males and females. (3) The ratio of O/E was 1.8 times or greater for those SMON patients with complications of cerebrovascular disease, severe blindness, complete loss of ambulation, and who were bedridden, and who are unable to receive home care from family members or trained home helpers."
        },
        {
            "title": "Computed tomographic perfusion imaging for the prediction of response and survival to transarterial radioembolization of liver metastases.",
            "abstract": "Purpose:\n        \n      \n      The purpose of this study was to evaluate prospectively, in patients with liver metastases, the ability of computed tomographic (CT) perfusion to predict the morphologic response and survival after transarterial radioembolization (TARE).\n    \n\n\n          Methods:\n        \n      \n      Thirty-eight patients (22 men; mean [SD] age, 63 [12] years) with otherwise therapy-refractory liver metastases underwent dynamic, contrast-enhanced CT perfusion within 1 hour before treatment planning catheter angiography, for calculation of the arterial perfusion (AP) of liver metastases, 20 days before TARE with Yttrium-90 microspheres. Treatment response was evaluated morphologically on follow-up imaging (mean, 114 days) on the basis of the Response Evaluation Criteria in Solid Tumors criteria (version 1.1). Pretreatment CT perfusion was compared between responders and nonresponders. One-year survival was calculated including all 38 patients using the Kaplan-Meier curves; the Cox proportional hazard model was used for calculating predictors of survival.\n    \n\n\n          Results:\n        \n      \n      Follow-up imaging was not available in 11 patients because of rapidly deteriorating health or death. From the remaining 27, a total of 9 patients (33%) were classified as responders and 18 patients (67%) were classified as nonresponders. A significant difference in AP was found on pretreatment CT perfusion between the responders and the nonresponders to the TARE (P < 0.001). Change in tumor size on the follow-up imaging correlated significantly and negatively with AP before the TARE (r = -0.60; P = 0.001). Receiver operating characteristics analysis of AP in relation to treatment response revealed an area under the curve of 0.969 (95% confidence interval, 0.911-1.000; P < 0.001). A cutoff AP of 16 mL per 100 mL/min was associated with a sensitivity of 100% (9/9) (95% CI, 70%-100%) and a specificity of 89% (16/18) (95% CI, 62%-96%) for predicting therapy response. A significantly higher 1-year survival after the TARE was found in the patients with a pretreatment AP of 16 mL per 100 mL/min or greater (P = 0.028), being a significant, independent predictor of survival (hazard ratio, 0.101; P = 0.015).\n    \n\n\n          Conclusions:\n        \n      \n      Arterial perfusion of liver metastases, as determined by pretreatment CT perfusion imaging, enables prediction of short-term morphologic response and 1-year survival to TARE."
        },
        {
            "title": "Intraocular lymphoma 2000-2005: results of a retrospective multicentre trial.",
            "abstract": "Background:\n        \n      \n      The prognosis of intraocular lymphoma (IOL) is poor, and the optimal treatment has yet to be defined. This study assesses the clinical characteristics and outcome of patients with IOL diagnosed and treated in the new millennium.\n    \n\n\n          Methods:\n        \n      \n      Patient data in this retrospective multicentre study were compiled by standardised questionnaires sent to seven university ophthalmology departments. All cases diagnosed with primary and secondary IOL in the past 5 years not associated with HIV infection were included.\n    \n\n\n          Results:\n        \n      \n      Twenty-two patients, 11 men and women; median age 64 (range 38-83) years, median Karnofsky performance status 90% (range 50-100%), were included. Nineteen patients had primary IOL (PIOL): 13 a newly diagnosed disease and six an ocular relapse of primary central nervous system lymphoma (PCNSL). Three patients had secondary IOL. First-line treatment for IOL included systemic chemotherapy in 13 cases, ocular radiation in six and intraocular chemotherapy in three. Complete remission was achieved in 14/20 evaluable patients, partial remission in five and stable disease in one. All patients treated with ifosfamide (IFO) or trofosfamide (TRO) (n=8) responded. Median progression-free survival (PFS) and overall survival were 10 (range 1+ to 44.5+) and 22.5 (range 1+ to 49+) months, respectively. Patients with newly diagnosed PIOL and ocular relapse of PCNSL had a median PFS of 10 (range 1+ to 44.5+) and 6 (range 2 to 6+) months, respectively. Median PFS was 12 (range 3+ to 22.5+) months after systemic and 5.5 (range 1+ to 44.5+) months after local first-line therapy.\n    \n\n\n          Conclusions:\n        \n      \n      The prognosis of PIOL is similar to that of PCNSL without ocular involvement. Systemic therapy possibly prolongs PFS as compared with local management of (P)IOL. The high response rate to monotherapy with IFO and TRO is promising."
        },
        {
            "title": "Cyclic variation in heart rate score by holter electrocardiogram as screening for sleep-disordered breathing in subjects with heart failure.",
            "abstract": "Background:\n        \n      \n      Sleep-disordered breathing (SDB) is critically associated with cardiovascular mortality and morbidity, especially in patients with heart failure (HF). However, the majority of SDB patients remain undiagnosed. In contrast, abnormality in heart rate variability has been reported in patients with SDB. To explore an efficient electrocardiogram (ECG)-based screening tool for SDB, we examined the usefulness of cyclic variation in heart rate score (CVHRS) by Holter ECG in subjects with HF.\n    \n\n\n          Methods:\n        \n      \n      In this study, 102 subjects with HF were enrolled. We simultaneously performed Holter ECG with overnight portable sleep monitoring, and we measured the respiratory disturbance index (RDI) and CVHRS. We determined the temporal position of the individual dips comprising the CVHRS using time-domain methods. CVHRS was measured as cyclic and autocorrelated dips in smoothed interbeat interval time series.\n    \n\n\n          Results:\n        \n      \n      There were 25 subjects with severe SDB (RDI ≥ 30 events/h) and 77 subjects with none-to-moderate SDB (0 ≤ RDI < 30 events/h). There was a significant positive correlation between CVHRS and RDI (r = 0.60, P < .001). In receiver operating characteristic analysis, CVHRS (cutoff of 30 events/h) identified severe SDB with a sensitivity of 82%, a specificity of 77%, and an area under the curve of 0.83.\n    \n\n\n          Conclusions:\n        \n      \n      CVHRS determined by Holter ECG is a useful screening index for severe SDB in subjects with HF."
        },
        {
            "title": "Detection of apoptotic cell death in vitro in the presence of Gd-DTPA-BMA.",
            "abstract": "Due to variability in patient response to cancer therapy, there is a growing interest in monitoring patient progress during treatment. Apoptotic cell death is one early marker of tumor response to treatment. Using known extracellular concentrations of gadolinium diethylenetriamine pentaacetic acid bismethylamide (Gd-DTPA-BMA) to vary the exchange regime, T(1) and T(2) relaxation data for acute myeloid leukemia (AML) cell samples were obtained and then analyzed using a two-pool model of relaxation with exchange. Leukemia cells treated with cisplatin to induce apoptosis exhibited a statistically significant (P < 0.05) decrease in intracellular longitudinal relaxation time, T(1I), from 1030 ms to 940 ms, a decrease (P < 0.001) in the intracellular water fraction, M(0I), from 0.86 to 0.68 and a statistically significant increase (P < 0.01) in transmembrane water exchange rate, k(IE), from 1.4 s(-1) to 6.8 s(-1). The changes in MR parameters correlated with quantitative histology, such as cellular cross-sectional area and average nuclear area measurements. The results of this study emphasize the importance of accounting for water exchange in dynamic contrast-enhanced MRI (DCE-MRI) studies, particularly those that examine tumor response to therapies in which apoptotic changes occur."
        },
        {
            "title": "Angiographic evidence of proliferative retinopathy predicts neuropsychiatric morbidity in diabetic patients.",
            "abstract": "Introduction:\n        \n      \n      Diabetic retinopathy (DR) is a common vasculopathy categorized as either non-proliferative (NPDR) or proliferative (PDR),characterized by dysfunctional blood-retinal barrier (BRB) and diagnosed using fluorescein angiography (FA). Since the BRB is similar in structure and function to the blood-brain barrier (BBB) and BBB dysfunction plays a key role in the pathogenesis of brain disorders, we hypothesized that PDR, the severe form of DR, is likely to mirror BBB damage and to predict a worse neuropsychiatric outcome.\n    \n\n\n          Methods:\n        \n      \n      A retrospective cohort study was conducted among subjects with diabetes (N=2982) with FA-confirmed NPDR (N=2606) or PDR (N=376). Incidence and probability to develop brain pathologies and mortality were investigated in a 10-year follow-up study. We used Kaplan-Meier, Cox and logistic regression analyses to examine association between DR severity and neuropsychiatric morbidity adjusting for confounders.\n    \n\n\n          Results:\n        \n      \n      Patients with PDR had significantly higher rates of all-cause brain pathologies (P<0.001), specifically stroke (P=0.005), epilepsy (P=0.006) and psychosis (P=0.024), and a shorter time to develop any neuropsychiatric event (P<0.001) or death (P=0.014) compared to NPDR. Cox adjusted hazard ratio for developing all-cause brain impairments was higher for PDR (HR=1.37, 95% CI 1.16-1.61, P<0.001) which was an independent predictor for all-cause brain impairments (OR 1.30, 95% CI 1.04-1.64, P=0.022), epilepsy (OR 2.16, 95% CI 1.05-4.41, P=0.035) and mortality (HR=1.35, 95% CI 1.06-1.70, P=0.014).\n    \n\n\n          Conclusions:\n        \n      \n      This is the first study to confirm that angiography-proven microvasculopathy identifies patients at high risk for neuropsychiatric morbidity and mortality."
        },
        {
            "title": "Signs of post-traumatic stress disorder in caregivers following an expected death: a qualitative study.",
            "abstract": "Background:\n        \n      \n      Complications of grief are an important area of investigation with potential to improve the well-being of palliative care caregivers. There has been little study of the prevalence or significance of post-traumatic stress disorder for those bereaved after an expected death.\n    \n\n\n          Aim:\n        \n      \n      To identify evidence suggestive of post-traumatic stress disorder symptoms in a population of bereaved caregivers of patients who have died of ovarian cancer.\n    \n\n\n          Design:\n        \n      \n      Caregivers' recollections of their end-of-life experiences were coded and analysed, using qualitative data obtained from interviews 6 months after the patient's death.\n    \n\n\n          Setting/participants:\n        \n      \n      Australian Ovarian Cancer Study-Quality of Life Study is a population-based epidemiological study using mixed methods to explore caregivers' experiences following the expected death of a woman with ovarian cancer. Thirty-two caregivers from the Australian Ovarian Cancer Study-Quality of Life Study participated in semi-structured telephone interviews 6 months post-bereavement.\n    \n\n\n          Results:\n        \n      \n      When describing the patient's death at their 6-month interview, all interviewees used language consistent with some degree of shock and traumatisation. For the majority, there was also evidence suggesting resilience and resolution. However, a number of interviewees describe intrusive memories associated with physical sights and sounds that they witnessed at the deathbed.\n    \n\n\n          Conclusions:\n        \n      \n      This exploratory study demonstrates the phenomenon of the 'shocked caregiver'. If trauma symptoms are present in bereaved carers in palliative care, it has implications for palliative care provision. Given that trauma symptoms may be distinct from prolonged grief disorder, this may also have implications for provision of bereavement counselling. Further research into this phenomenon is required."
        },
        {
            "title": "Multidetector computed tomographic angiography in planning of reoperative cardiothoracic surgery.",
            "abstract": "Background:\n        \n      \n      Redo cardiothoracic surgery is associated with increased morbidity and mortality compared with primary operations. Multidetector computed tomographic angiography (MDCTA) delineates the course of previous coronary artery bypass grafts (CABG) and proximity of mediastinal structures to the chest wall. We sought to determine if high-risk preoperative MDCTA findings were associated with greater use of preventive surgical strategies during redo cardiac surgery in patients with prior CABG.\n    \n\n\n          Methods:\n        \n      \n      We studied 167 patients (mean age 69 +/- 9 years, 79% men) with prior CABG, referred for redo cardiac surgery, who underwent contrast-enhanced MDCTA to assess CABG location and mediastinal relationship to chest wall. Preoperative risk was determined. Prevalence of high-risk MDCTA findings, use of preventive surgical strategies, frequency of severe intraoperative bleeding, and postoperative mortality were recorded.\n    \n\n\n          Results:\n        \n      \n      Mean risk score was high (7.5 +/- 3). High-risk MDCTA findings included proximity (<1 cm) of right ventricle/aorta to chest wall (24%) or CABG crossing midline in close proximity (<1 cm anteroposteriorly) to sternum (38%). Preventive surgical strategies included surgery cancelled (4%), nonmidline incision (8%), deep hypothermic circulatory arrest (5%), initiation of peripheral cardiopulmonary bypass (11%) and extrathoracic vascular exposure before incision (53%). These strategies were used at a higher frequency in patients with high-risk MDCTA findings versus those without (88% versus 28%, p < 0.0001). Frequency of severe bleeding, graft injuries, and 1-month mortality were 4.4%, 5%, and 2.5%, respectively.\n    \n\n\n          Conclusions:\n        \n      \n      Routine use of preoperative MDCTA to detect high-risk findings has a strong association with adoption of preventive surgical strategies in high-risk patients undergoing redo cardiac surgery."
        },
        {
            "title": "Cataract, visual impairment and long-term mortality in a rural cohort in India: the Andhra Pradesh Eye Disease Study.",
            "abstract": "## BACKGROUND\nA large-scale prevalence survey of blindness and visual impairment (The Andhra Pradesh Eye Diseases Study [APEDS1]) was conducted between 1996-2000 on 10,293 individuals of all ages in three rural and one urban clusters in Andhra Pradesh, Southern India. More than a decade later (June 2009-March 2010), APEDS1 participants in rural clusters were traced (termed APEDS2) to determine ocular risk factors for mortality in this longitudinal cohort.\n## METHODS AND FINDINGS\nMortality hazard ratio (HR) analysis was performed for those aged >30 years at APEDS1, using Cox proportional hazard regression models to identify associations between ocular exposures and risk of mortality. Blindness and visual impairment (VI) were defined using Indian definitions. 799/4,188 (19.1%) participants had died and 308 (7.3%) had migrated. Mortality was higher in males than females (p<0.001). In multivariable analysis, after adjusting for age, gender, diabetes, hypertension, body mass index, smoking and education status the mortality HR was 1.9 (95% CI: 1.5-2.5) for blindness; 1.4 (95% CI: 1.2-1.7) for VI; 1.8 (95% CI: 1.4-2.3) for pure nuclear cataract, 1.5 (95% CI: 1.1-2.1) for pure cortical cataract; 1.96 (95% CI: 1.6-2.4) for mixed cataract, 2.0 (95% CI: 1.4-2.9) for history of cataract surgery, and 1.58 (95% CI: 1.3-1.9) for any cataract. When all these factors were included in the model, the HRs were attenuated, being 1.5 (95% CI: 1.1-2.0) for blindness and 1.2 (95% CI: 0.9-1.5) for VI. For lens type, the HRs were as follows: pure nuclear cataract, 1.6 (95% CI: 1.3-2.1); pure cortical cataract, 1.5 (95% CI: 1.1-2.1); mixed cataract, 1.8 (95% CI: 1.4-2.2), and history of previous cataract surgery, 1.8 (95% CI: 1.3-2.6).\n## CONCLUSIONS\nAll types of cataract, history of cataract surgery and VI had an increased risk of mortality that further suggests that these could be potential markers of ageing.\n"
        },
        {
            "title": "Comorbidity among older American Indians: the native elder care study.",
            "abstract": "Comorbidity is a growing challenge and the older adult population is most at risk of developing comorbid conditions. Comorbidity is associated with increased risk of mortality, increased hospitalizations, increased doctor visits, increased prescription medications, nursing home placement, poorer mental health, and physical disability. American Indians experience some of the highest rates of chronic conditions, but to date there have been only two published studies on the subject of comorbidity in this population. With a community-based sample of 505 American Indians aged 55 years or older, this study identified the most prevalent chronic conditions, described comorbidity, and identified socio-demographic, functional limitations, and psychosocial correlates of comorbidity. Results indicated that older American Indians experience higher rates of hypertension, diabetes, back pain, and vision loss compared to national statistics of older adults. Two-thirds of the sample experienced some degree of comorbidity according to the scale used. Older age, poorer physical functioning, more depressive symptomatology, and lower personal mastery were all correlates of higher comorbidity scores. Despite medical advances increasing life expectancy, morbidity and mortality statistics suggest that the health of older American Indians lags behind the majority population. These findings highlight the importance of supporting chronic care and management services for the older American Indian population."
        },
        {
            "title": "CT vs MRI in diagnosis of recurrent rectosigmoid carcinoma.",
            "abstract": "Objective:\n        \n      \n      Our goal was to compare the relative values of MRI vs. CT in diagnosing recurrent rectosigmoid cancer.\n    \n\n\n          Materials and methods:\n        \n      \n      We conducted a retrospective review of 18 patients who had surgical resection of primary rectosigmoid carcinoma and suspected recurrence. They were studied with CT and MR and followed for up to 4 years.\n    \n\n\n          Results:\n        \n      \n      At the time of the initial imaging, 10 patients had recurrent tumor and 4 of the remaining 8 patients later demonstrated local recurrence. Magnetic resonance demonstrated 91% sensitivity, 100% specificity, a positive predictive value (PPV) of 100%, and a negative predictive value (NPV) of 89% with a 95% accuracy. Computed tomography demonstrated a sensitivity of 82% and a specificity of 50% with a PPV of 69% and an NPV of 67% with an accuracy of 68%. In three cases interpreted on CT as presacral masses, all were shown on MR to represent displaced but normal pelvic structures. In four cases MR revealed tumor involving the sacrum and sacral nerves not apparent on CT.\n    \n\n\n          Conclusion:\n        \n      \n      Magnetic resonance showed superior sensitivity, specificity, and accuracy to CT and better definition of the extent of tumor."
        },
        {
            "title": "Radiomic Profiling of Glioblastoma: Identifying an Imaging Predictor of Patient Survival with Improved Performance over Established Clinical and Radiologic Risk Models.",
            "abstract": "Purpose To evaluate whether radiomic feature-based magnetic resonance (MR) imaging signatures allow prediction of survival and stratification of patients with newly diagnosed glioblastoma with improved accuracy compared with that of established clinical and radiologic risk models. Materials and Methods Retrospective evaluation of data was approved by the local ethics committee and informed consent was waived. A total of 119 patients (allocated in a 2:1 ratio to a discovery [n = 79] or validation [n = 40] set) with newly diagnosed glioblastoma were subjected to radiomic feature extraction (12 190 features extracted, including first-order, volume, shape, and texture features) from the multiparametric (contrast material-enhanced T1-weighted and fluid-attenuated inversion-recovery imaging sequences) and multiregional (contrast-enhanced and unenhanced) tumor volumes. Radiomic features of patients in the discovery set were subjected to a supervised principal component (SPC) analysis to predict progression-free survival (PFS) and overall survival (OS) and were validated in the validation set. The performance of a Cox proportional hazards model with the SPC analysis predictor was assessed with C index and integrated Brier scores (IBS, lower scores indicating higher accuracy) and compared with Cox models based on clinical (age and Karnofsky performance score) and radiologic (Gaussian normalized relative cerebral blood volume and apparent diffusion coefficient) parameters. Results SPC analysis allowed stratification based on 11 features of patients in the discovery set into a low- or high-risk group for PFS (hazard ratio [HR], 2.43; P = .002) and OS (HR, 4.33; P < .001), and the results were validated successfully in the validation set for PFS (HR, 2.28; P = .032) and OS (HR, 3.45; P = .004). The performance of the SPC analysis (OS: IBS, 0.149; C index, 0.654; PFS: IBS, 0.138; C index, 0.611) was higher compared with that of the radiologic (OS: IBS, 0.175; C index, 0.603; PFS: IBS, 0.149; C index, 0.554) and clinical risk models (OS: IBS, 0.161, C index, 0.640; PFS: IBS, 0.139; C index, 0.599). The performance of the SPC analysis model was further improved when combined with clinical data (OS: IBS, 0.142; C index, 0.696; PFS: IBS, 0.132; C index, 0.637). Conclusion An 11-feature radiomic signature that allows prediction of survival and stratification of patients with newly diagnosed glioblastoma was identified, and improved performance compared with that of established clinical and radiologic risk models was demonstrated. (©) RSNA, 2016 Online supplemental material is available for this article."
        },
        {
            "title": "Preferred Method of Education Among Patients in Ophthalmic Care in Saudi Arabia.",
            "abstract": "Purpose:\n        \n      \n      Educating patients about their diagnosis and proposed management is integral part of healthcare. Often patient noncompliance is due to a lack of knowledge that could result in irreversible ocular damage. In an era where access to information is virtually unlimited, an understanding of the preferred method of eye care education among patients is required for greater effectiveness in lowering morbidity and mortality of diseases.\n    \n\n\n          Subjects and methods:\n        \n      \n      Patients visiting the ophthalmology clinics of a tertiary hospital in Riyadh, Saudi Arabia, were interviewed. This cross-sectional study was conducted between December 2014 and March 2015. A representative sample of 200 patients was enrolled. Close-ended questionnaire covering current and client preferred health promotion methods were used to collect clients' response. Data were analyzed with descriptive statistics.\n    \n\n\n          Results:\n        \n      \n      Out of the 200 participants, 110 (55%) were males. The majority (n = 154; 77%) listed an ophthalmologist as their current primary source of information regarding their eye condition. Approximately half of the participants (n = 95; 48%) were keen to be educated regarding the causes of the eye disease. The top four educational methods preferred by patients were one-on-one session with an eye care provider (n = 116; 58%), a group session with an eye care provider (n = 30; 15%), an application on a smartphone (n = 53; 27%), video lectures on eye health and diseases (n = 8; 4%).\n    \n\n\n          Conclusion:\n        \n      \n      Majority of patients in ophthalmic care prefer a one-on-one session with an eye care provider for their eye care education."
        },
        {
            "title": "Prognostic factors in patients with methanol poisoning.",
            "abstract": "Objective:\n        \n      \n      To identify prognostic factors in methanol poisoning and determine the effect of medical interventions on clinical outcome.\n    \n\n\n          Methods:\n        \n      \n      Retrospective review of all patients treated for methanol poisoning from 1982 through 1992 at The Toronto Hospital. Presenting history, physical examination, results of laboratory tests, medical interventions, and final outcomes after hemodialysis were abstracted.\n    \n\n\n          Results:\n        \n      \n      Of 50 patients treated for methanol poisoning, 18 (36%) died, 32 (64%) survived. Seven of the 32 survivors sustained visual sequelae (22%), the remaining 25 (78%) recovered completely. Patients presenting with coma or seizure had 84% (16/19) mortality compared to 6% (2/31) in those without (p < 0.001). Initial arterial pH < 7 was also associated with significantly higher mortality (17/19, 89% vs 1/31, 3%, p < 0.001). There were no differences in time from presentation to dialysis between survivors and fatalities (8.4 +/- 3.6 vs 7.6 +/- 3.5 hours, p = 0.47). The deceased patients had higher mean methanol concentration than the survivors (83 +/- 53 vs 41 +/- 25 mmol/L, p = 0.004). Subgroup analysis of 19 patients presenting with visual symptoms who survived showed prolonged acidosis (5.4 +/- 2.3 vs 3.0 +/- 2.1 hours, p = 0.06) in those with persistent visual sequelae.\n    \n\n\n          Conclusions:\n        \n      \n      Coma or seizure on presentation and severe metabolic acidosis, in particular initial arterial pH < 7, are poor prognostic indicators in methanol poisoning. Survivors presented with lower methanol concentrations. Patients with residual visual sequelae had more prolonged acidosis than those with complete recovery. Future studies will be needed to confirm the effect of correction of acidosis on final clinical outcome."
        },
        {
            "title": "Sensitivity of death certificate data for monitoring diabetes mortality--diabetic eye disease follow-up study, 1985-1990.",
            "abstract": "Although death certificates are a primary source of data for characterizing mortality patterns in the United States, the underreporting of diabetes as a cause of death limit the use of death certificates for monitoring diabetes mortality. To determine whether diabetes was underreported on the death certificates of patients with known diabetic eye disease, CDC analyzed data from death certificates for persons identified as deceased by the Diabetic Eye Disease Follow-Up Study (DEDFUS)."
        },
        {
            "title": "The toe-brachial index in the diagnosis of peripheral arterial disease.",
            "abstract": "Background:\n        \n      \n      Peripheral arterial disease (PAD) can be diagnosed noninvasively by segmental blood pressure measurement and calculating an ankle-brachial index (ABI) or toe-brachial index (TBI). The ABI is known to be unreliable in patients with vascular stiffness and fails to detect the early phase of arteriosclerotic development. The toe vessels are less susceptible to vessel stiffness, which makes the TBI useful. However, the diagnostic limits used in guidelines, clinical settings, and experimental studies vary substantially. This review provides an overview of the evidence supporting the clinical use of the TBI.\n    \n\n\n          Methods:\n        \n      \n      A review of the literature identified studies reporting the use of the TBI regarding guideline recommendations, normal populations, correlations to angiographic findings, and prognostic implications.\n    \n\n\n          Results:\n        \n      \n      Eight studies conducted in a normal population were identified, of which only one study used imaging techniques to rule out arterial stenosis. A reference value of 0.71 was estimated as the lowest limit of normal based on the weighted average in studies with preheating of the limbs. A further seven studies showed correlations of the TBI with angiographic findings. The TBI had a sensitivity of 90% to 100% and a specificity of 65% to 100% for the detection of vessel stenosis. Few studies investigated the value of the TBI as a prognostic marker for cardiovascular mortality and morbidity, and no firm conclusions could be made. Studies have, however, shown correlation between the TBI and comorbidities such as kidney disease, diabetes, and microvasculature disease.\n    \n\n\n          Conclusions:\n        \n      \n      In contrast to the well-defined and evidence-based limits of the ABI, the diagnostic criteria for a pathologic TBI remain ambiguous. Although several guidelines and reviews of PAD diagnostics recommend a TBI <0.70 as cutoff, it is not strictly evidence-based. The current literature is not sufficient to conclude a specific cutoff as diagnostic for PAD. The current studies in normal populations and the correlation with angiography are sparse, and additional trials are needed to further validate the limits. Large-scale trials are needed to establish the risk of morbidity and mortality for the various diagnostic limits of the TBI."
        },
        {
            "title": "Results of revascularization in patients with severe left ventricular dysfunction.",
            "abstract": "Objective:\n        \n      \n      In patients with coronary disease and poor left ventricular function, bypass grafting remains a surgical challenge. This study evaluates experience in 125 consecutive patients with ejection fraction less than 20% (study group).\n    \n\n\n          Methods:\n        \n      \n      Preoperative viability studies were not used for patient selection. Clinical data were prospectively collected. The average age of the study subjects was 59 +/- 9 years, and 112 (90%) were male. Most patients (108 [86%]) were in symptom class III or IV. Main indications for surgery included angina in 62 (50%), heart failure and angina in 36 (29%), heart failure in 9 (7%), ventricular arrhythmia in 2 (2%), and critical anatomy in 16 (13%). Significant mitral regurgitation was present in 48 (38%), and distal vessels were poorly visualized in 67 (54%). At surgery, temperature mapping guided an integrated approach to cold cardioplegia. Results in this group were compared with those obtained in case-matched control subjects receiving cardioplegia without temperature mapping (matched for age, sex, functional class, and urgency of operation).\n    \n\n\n          Results:\n        \n      \n      Hospital morbidity (intra-aortic balloon pump support) and mortality rates were significantly lower in the study group versus those of control subjects (15% vs 30%, P =. 004; and 4% vs 11%, P =.03, respectively). In study patients the 5-year actuarial survival was 72%. Among survivors, both anginal class and heart failure class improved significantly. By means of multivariate analysis, survival was adversely affected by older age, class IV symptoms, and poorly visualized distal vessels.\n    \n\n\n          Conclusions:\n        \n      \n      These results support the use of coronary artery bypass grafting in patients with severe left ventricular dysfunction without case selection on the basis of viability studies or visibility of distal vessels. Low hospital morbidity and mortality rates have been achieved when temperature mapping guides cardioplegia. Symptoms are improved in most patients, and long-term survival is encouraging."
        },
        {
            "title": "Prospective randomised trial of computer-aided diagnosis and contrast radiography in acute small bowel obstruction.",
            "abstract": "Objective:\n        \n      \n      to compare the ability of computer-aided diagnosis and contrast radiography for the diagnosis of acute mechanical small bowel obstruction.\n    \n\n\n          Design:\n        \n      \n      Prospective randomised trial.\n    \n\n\n          Setting:\n        \n      \n      Kaunas University of Medicine, Lithuania.\n    \n\n\n          Subjects:\n        \n      \n      80 patients with small bowel obstruction with no clinical evidence of strangulation who were randomised into two groups (n = 40 in each) to be investigated by computer-aided diagnosis and contrast radiography. INTENVENTIONS: 37 patients required operation (46%).\n    \n\n\n          Main outcome measures:\n        \n      \n      specificity, sensitivity, false positive and negative predictive values of the 2 methods; time necessary to make the diagnosis; and morbidity and mortality.\n    \n\n\n          Results:\n        \n      \n      The specificity, sensitivity, positive and negative predictive values in the diagnosis of complete acute small bowel obstruction for the computer-aided group were 100%, 87.5%, 100% and 92.3%, and for the contrast radiography group 100%, 76.9%, 100% and 90%, respectively. The mean time period for making the diagnosis was 1 hour in the computer-aided group and 16 hours in the radiography group (p < 0.001). The overall mortality was 3% and morbidity 9%.\n    \n\n\n          Conclusion:\n        \n      \n      Computer-aided diagnosis had no significant advantage over contrast radiography in the accuracy of diagnosis of the character of small bowel obstruction. However, significantly less time was needed to make the diagnosis in the computer-aided group."
        },
        {
            "title": "Rate, risk factors and causes of mortality in patients with Sjogren's syndrome: a systematic review and meta-analysis of cohort studies.",
            "abstract": "Objectives:\n        \n      \n      There is conflicting evidence regarding prognosis in patients with primary SS (pSS). The aim of this study was to estimate the rate, risk factors and causes of mortality in patients with pSS through a systematic review and meta-analysis.\n    \n\n\n          Methods:\n        \n      \n      Through a systematic review of multiple databases through October 2014, we identified cohort studies reporting relative risk (compared with standardized population), risk factors and causes of mortality in patients with pSS. We estimated summary risk ratios (RRs) with 95% CIs using random effects model.\n    \n\n\n          Results:\n        \n      \n      We identified 10 studies with 7888 patients (91% females) with pSS, of whom 682 patients died over a median average follow-up of 9 years. The pooled standardized mortality ratio in patients with pSS was 1.38 (95% CI 0.94, 2.01). Leading causes of mortality were cardiovascular diseases, solid-organ and lymphoid malignancies and infections; however, it is unclear whether these observed causes were overrepresented in patients with pSS as compared with the general population. Risk factors associated with increased mortality were advanced age at diagnosis [RR 1.09 (95% CI 1.07, 1.12)], male sex [RR 2.18 (95% CI 1.45, 3.27)], parotid enlargement [RR 1.81 (95% CI 1.02, 3.21)], abnormal parotid scintigraphy [RR 2.96 (95% CI 1.36, 6.45)], extraglandular involvement [RR 1.77 (95% CI 1.06, 2.95)], vasculitis [RR 7.27 (95% CI 2.70, 19.57)], anti-SSB positivity [RR 1.45 (95% CI 1.03, 2.04)], low C3 [RR 2.14 (95% CI 1.38, 3.32)] and C4 [RR 3.08 (95% CI 2.14, 4.42)] and cryoglobulinaemia [RR 2.62 (95% CI 1.77, 3.90)].\n    \n\n\n          Conclusion:\n        \n      \n      pSS is not associated with an increase in all-cause mortality as compared with the general population. However, a subset of patients with extraglandular involvement, vasculitis, hypocomplementaemia and cryoglobulinaemia may be at increased risk of mortality and require close follow-up."
        },
        {
            "title": "Lifetime risk of blindness in open-angle glaucoma.",
            "abstract": "Purpose:\n        \n      \n      To determine the lifetime risk and duration of blindness in patients with manifest open-angle glaucoma (OAG).\n    \n\n\n          Design:\n        \n      \n      Retrospective chart review.\n    \n\n\n          Methods:\n        \n      \n      We studied glaucoma patients who died between January 2006 and June 2010. Most glaucoma patients living in the catchment area (city of Malmö; n = 305 000) are managed at the Department of Ophthalmology at Skåne University Hospital in Malmö. From the patient records we extracted visual field status, visual acuity, and low vision or blindness as defined by the World Health Organization (WHO) criteria and caused by glaucoma at the time of diagnosis and during follow-up. We also noted age at diagnosis and death and when low vision or blindness occurred.\n    \n\n\n          Results:\n        \n      \n      Five hundred and ninety-two patients were included. At the time of the last visit 250 patients (42.2%) had at least 1 blind eye because of glaucoma, while 97 patients (16.4%) were bilaterally blind, and 12 patients (0.5%) had low vision. Median time with a glaucoma diagnosis was 12 years (<1-29), median age when developing bilateral blindness was 86 years, and median duration of bilateral blindness was 2 years (<1-13). The cumulative incidences of blindness in at least 1 eye and bilateral blindness from glaucoma were 26.5% and 5.5%, respectively, after 10 years, and 38.1% and 13.5% at 20 years.\n    \n\n\n          Conclusions:\n        \n      \n      Approximately 1 out of 6 glaucoma patients was bilaterally blind from glaucoma at the last visit. Median duration of bilateral blindness was 2 years."
        },
        {
            "title": "Effectiveness of early detection on breast cancer mortality reduction in Catalonia (Spain).",
            "abstract": "Background:\n        \n      \n      At present, it is complicated to use screening trials to determine the optimal age intervals and periodicities of breast cancer early detection. Mathematical models are an alternative that has been widely used. The aim of this study was to estimate the effect of different breast cancer early detection strategies in Catalonia (Spain), in terms of breast cancer mortality reduction (MR) and years of life gained (YLG), using the stochastic models developed by Lee and Zelen (LZ).\n    \n\n\n          Methods:\n        \n      \n      We used the LZ model to estimate the cumulative probability of death for a cohort exposed to different screening strategies after T years of follow-up. We also obtained the cumulative probability of death for a cohort with no screening. These probabilities were used to estimate the possible breast cancer MR and YLG by age, period and cohort of birth. The inputs of the model were: incidence of, mortality from and survival after breast cancer, mortality from other causes, distribution of breast cancer stages at diagnosis and sensitivity of mammography. The outputs were relative breast cancer MR and YLG.\n    \n\n\n          Results:\n        \n      \n      Relative breast cancer MR varied from 20% for biennial exams in the 50 to 69 age interval to 30% for annual exams in the 40 to 74 age interval. When strategies differ in periodicity but not in the age interval of exams, biennial screening achieved almost 80% of the annual screening MR. In contrast to MR, the effect on YLG of extending screening from 69 to 74 years of age was smaller than the effect of extending the screening from 50 to 45 or 40 years.\n    \n\n\n          Conclusion:\n        \n      \n      In this study we have obtained a measure of the effect of breast cancer screening in terms of mortality and years of life gained. The Lee and Zelen mathematical models have been very useful for assessing the impact of different modalities of early detection on MR and YLG in Catalonia (Spain)."
        },
        {
            "title": "Genetic, behavioral, and sociodemographic risk factors for second eye progression in age-related macular degeneration.",
            "abstract": "Purpose:\n        \n      \n      This study was conducted to investigate the correlation of genetic, sociodemographic, and behavioral risk factors with second eye progression to end-stage AMD.\n    \n\n\n          Methods:\n        \n      \n      One hundred and eight patients with end-stage AMD in one or both eyes were included in a retrospective time-to-event analysis of the onset of end-stage AMD in the second eye. Multivariate Cox regression survival analysis was performed for sex, age, smoking, body mass index (BMI), education, and 16 single nucleotide polymorphisms (SNPs) associated with AMD.\n    \n\n\n          Results:\n        \n      \n      Except for education, all sociodemographic and behavioral risk factors analyzed were significantly associated with a more rapid progression toward second eye involvement. Hazard ratios (HRs) were 2.6 (95% confidence interval [CI], 1.4-5.0) for female sex; 5.0 (95% CI, 2.0-12.5) for age >80; 2.2 (95% CI, 1.1-4.1) for BMI >30; and 4.4 (95% CI, 1.4-14.3) for >40 pack years, compared with the referent groups. Carriers of the lipoprotein lipase (LPL; rs12678919) risk alleles were at risk for more rapid progression to end-stage AMD in the second eye compared with the referent wild-type genotype (HR 2.0; 95% CI, 1.0-3.6). For complement factor I (CFI; rs10033900), homozygous carriers of the risk allele progressed faster than wild-type individuals (HR 2.2; 95% CI, 1.1-4.3).\n    \n\n\n          Conclusions:\n        \n      \n      Sociodemographic, behavioral, and genetic risk factors are associated with the rate of second eye progression toward end-stage AMD. The findings of this study underline the importance of lifestyle factors and the complement pathway in AMD progression and suggest a role of the high-density-lipoprotein metabolism in second eye progression."
        },
        {
            "title": "The impact of associated diabetic retinopathy on stroke and severe bleeding risk in diabetic patients with atrial fibrillation: the loire valley atrial fibrillation project.",
            "abstract": "Background:\n        \n      \n      Diabetes mellitus is recognized as a stroke risk factor in atrial fibrillation (AF). Patients with diabetes with retinopathy have an increased risk for systemic cardiovascular complications, and severe diabetic retinopathy predisposes to ocular bleeding. We hypothesized that patients with diabetes, retinopathy, and AF have increased stroke/thromboembolism (TE) and severe bleeding risks when compared with patients with diabetes and AF who do not have retinopathy or to patients with AF and without diabetes.\n    \n\n\n          Methods:\n        \n      \n      We tested our hypothesis in a large \"real-world\" cohort of individuals with AF from the Loire Valley Atrial Fibrillation project.\n    \n\n\n          Results:\n        \n      \n      Of 8,962 patients with AF in our dataset, 1,409 (16%) had documented diabetes mellitus. Of these, 163 (1.8% of the whole cohort) were patients with diabetic retinopathy. After a follow-up of 31 ± 36 months, when compared with patients without diabetes, the risk of stroke/TE in patients with diabetes with no retinopathy increased 1.3-fold (relative risk [RR], 1.30; 95% CI, 1.07-1.59; P = .01); in patients with diabetes with retinopathy, the risk of stroke/TE was increased 1.58-fold (RR, 1.58; 95% CI, 1.07-2.32; P = .02). There was no significant difference when patients with diabetes with no retinopathy were compared with patients with diabetes with retinopathy (RR, 1.21; 95% CI, 0.80-1.84; P = .37). A similar pattern was seen for mortality and severe bleeding. On multivariate analysis, the presence of diabetic retinopathy did not emerge as an independent predictor for stroke/TE or severe bleeding.\n    \n\n\n          Conclusions:\n        \n      \n      Crude rates of stroke/TE increased in a stepwise fashion when patients without diabetes and with AF were compared with patients with diabetes with no retinopathy and patients with diabetes with retinopathy. However, we have shown for the first time, to our knowledge, that the presence of diabetic retinopathy did not emerge as an independent predictor for stroke/TE or severe bleeding on multivariate analysis."
        },
        {
            "title": "Magnetic Resonance Imaging of Pulmonary Embolism: Diagnostic Accuracy of Unenhanced MR and Influence in Mortality Rates.",
            "abstract": "Objectives:\n        \n      \n      We evaluated the diagnostic value for pulmonary embolism (PE) of the True fast imaging with steady-state precession (TrueFISP) MRI, a method that allows the visualization of pulmonary vasculature without breath holding or intravenous contrast.\n    \n\n\n          Methods:\n        \n      \n      This is a prospective investigation including 93 patients with suspected PE. All patients underwent TrueFISP MRI after undergoing CT pulmonary angiography (CTPA). Two independent readers evaluated each MR study, and consensus was obtained. CTPA results were analysed by a third independent reviewer and these results served as the reference standard. A fourth radiologist was responsible for evaluating if lesions found on MRI for both analysis were the same and if these were the correspondent lesions on the CTPA. Sensitivity, specificity, predictive values and accuracy were calculated. Evidence for death from PE within the 1-year follow-up was also assessed.\n    \n\n\n          Results:\n        \n      \n      Two patients could not undergo the real-time MRI and were excluded from the study. PE prevalence was 22%. During the 1-year follow-up period, eight patients died, whereas PE was responsible for 12.5% of cases. Between patients who developed PE, only 5% died due to this condition. There were no differences between MR and CT embolism detection in these subjects. MR sequences had a sensitivity of 85%, specificity was 98.6% and accuracy was 95.6%. Agreement between readers was high (κ= 0.87).\n    \n\n\n          Conclusions:\n        \n      \n      Compared with contrast-enhanced CT, unenhanced MR sequences demonstrate good accuracy and no differences in the mortality rates in 1 year were detected."
        },
        {
            "title": "Colonoscopy or barium enema for population colorectal cancer screening?",
            "abstract": "There is good evidence from faecal occult blood testing trials that detection and removal of non-advanced colorectal neoplasms can achieve a reduction in colorectal cancer mortality. Both colonoscopy and barium enema have potential advantages and disadvantages if implemented for population screening. The relative merits of each are discussed."
        },
        {
            "title": "Albinism and Hermansky-Pudlak syndrome in Puerto Rico.",
            "abstract": "Five types of oculocutaneous albinism and two types of ocular albinism were found among 349 Puerto Rican albinos. The most prevalent type of albinism was the Hermansky-Pudlak syndrome (HPS). HPS was observed in five of every six albinos in Puerto Rico. The prevalence of HPS was highest in the northwestern quarter of the island, affecting approximately one in 1,800 persons, and approximately one in 22 are carriers of the gene. HPS is an autosomal recessively inherited triad of a tyrosinase-positive type of albinism, a hemorrhagic diathesis due to storage pool deficient platelets and accumulation of ceroid in tissues. The pigmentary phenotype of HPS albinos resembled that of any other type of oculocutaneous or ocular albinism. The most reliable method of diagnosing HPS is by a deficiency of platelet dense bodies observed by electron microscopy. The accumulation of ceroid in the tissues is associated with fibrotic restrictive lung disease and granulomatous enteropathic disease. The enteropathic disorder resembles Crohn's disease and with few exceptions, had its onset after 13 years of age. The major causes of death were fibrotic restrictive pulmonary disease, hemorrhagic episodes and sequelae of granulomatous enteropathic disease. Menometrorrhagia was common in women with HPS. No immune deficiency was found in HPS patients. The majority of patients with HPS had visual acuities of 20/200 or worse and consequently were legally blind. Albinos of all types, including HPS, lacked binocular vision due to nearly complete crossing of the optic tracts."
        },
        {
            "title": "The 100-plus Study of cognitively healthy centenarians: rationale, design and cohort description.",
            "abstract": "Although the incidence of dementia increases exponentially with age, some individuals reach more than 100 years with fully retained cognitive abilities. To identify the characteristics associated with the escape or delay of cognitive decline, we initiated the 100-plus Study ( www.100plus.nl ). The 100-plus Study is an on-going prospective cohort study of Dutch centenarians who self-reported to be cognitively healthy, their first-degree family members and their respective partners. We collect demographics, life history, medical history, genealogy, neuropsychological data and blood samples. Centenarians are followed annually until death. PET-MRI scans and feces donation are optional. Almost 30% of the centenarians agreed to post-mortem brain donation. To date (September 2018), 332 centenarians were included in the study. We analyzed demographic statistics of the first 300 centenarians (25% males) included in the cohort. Centenarians came from higher socio-economic classes and had higher levels of education compared to their birth cohort; alcohol consumption of centenarians was similar, and most males smoked during their lifetime. At baseline, the centenarians had a median MMSE score of 25 points (IQR 22.0-27.5); most centenarians lived independently, retained hearing and vision abilities and were independently mobile. Mortality was associated with cognitive functioning: centenarians with a baseline MMSE score ≥ 26 points had a mortality percentage of 17% per annual year in the second year after baseline, while centenarians with a baseline MMSE score < 26 points had a mortality of 42% per annual year (p = 0.003). The cohort was 2.1-fold enriched with the neuroprotective APOE-ε2 allele relative to 60-80 year-old population controls (p = 4.8 × 10-7), APOE-ε3 was unchanged and the APOE-ε4 allele was 2.3-fold depleted (p = 6.3 × 10-7). Comprehensive characterization of the 100-plus cohort of cognitively healthy centenarians might reveal protective factors that explain the physiology of long-term preserved cognitive health."
        },
        {
            "title": "Increased mortality risk among the visually impaired: the roles of mental well-being and preventive care practices.",
            "abstract": "Purpose:\n        \n      \n      Mechanisms by which visual impairment (VI) increases mortality risk are poorly understood. We estimated the direct and indirect effects of self-rated VI on risk of mortality through mental well-being and preventive care practice mechanisms.\n    \n\n\n          Methods:\n        \n      \n      Using complete data from 12,987 adult participants of the 2000 Medical Expenditure Panel Survey with mortality linkage through 2006, we undertook structural equation modeling using two latent variables representing mental well-being and poor preventive care to examine multiple effect pathways of self-rated VI on all-cause mortality. Generalized linear structural equation modeling was used to simultaneously estimate pathways including the latent variables and Cox regression model, with adjustment for controls and the complex sample survey design.\n    \n\n\n          Results:\n        \n      \n      VI increased the risk of mortality directly after adjusting for mental well-being and other covariates (hazard ratio [HR] = 1.25 [95% confidence interval: 1.01, 1.55]). Poor preventive care practices were unrelated to VI and to mortality. Mental well-being decreased mortality risk (HR = 0.68 [0.64, 0.74], P < 0.001). VI adversely affected mental well-being (β = -0.54 [-0.65, -0.43]; P < 0.001). VI also increased mortality risk indirectly through mental well-being (HR = 1.23 [1.16, 1.30]). The total effect of VI on mortality including its influence through mental well-being was HR 1.53 [1.24, 1.90]. Similar but slightly stronger patterns of association were found when examining cardiovascular disease-related mortality, but not cancer-related mortality.\n    \n\n\n          Conclusions:\n        \n      \n      VI increases the risk of mortality directly and indirectly through its adverse impact on mental well-being. Prevention of disabling ocular conditions remains a public health priority along with more aggressive diagnosis and treatment of depression and other mental health conditions in those living with VI."
        },
        {
            "title": "Monitoring visual status: why patients do or do not comply with practice guidelines.",
            "abstract": "Objective:\n        \n      \n      To determine factors affecting compliance with guidelines for annual eye examinations for persons diagnosed with diabetes mellitus (DM) or age-related macular degeneration (ARMD).\n    \n\n\n          Data sources/study setting:\n        \n      \n      Nationally representative, longitudinal sample of individuals 65+ drawn from the National Long-Term Care Survey (NLTCS) with linked Medicare claims records from 1991 to 1999.\n    \n\n\n          Study design:\n        \n      \n      Medicare beneficiaries were followed from 1991 to 1999, unless mortality intervened. All claims data were analyzed for presence of ICD-9 codes indicating diagnosis of DM or ARMD and the performance of eye exams. The dependent variable was a binary indicator for whether a person had an eye exam or not during a 15-month period. Independent variables for demographics, living conditions, supplemental insurance, income, and other factors affecting the marginal cost and benefit of an eye exam were assessed to determine reasons for noncompliance.\n    \n\n\n          Data collection/extraction methods:\n        \n      \n      Panel data were created from claims files, 1991-1999, merged with data from the NLTCS.\n    \n\n\n          Principal findings:\n        \n      \n      The probability of having an exam reflected perceived benefits, which vary by patient characteristics (e.g., education, no dementia), and factors associated with the ease of visit. African Americans were much less likely to be examined than were whites.\n    \n\n\n          Conclusions:\n        \n      \n      Having an exam reflects multiple factors. However, much of the variation in the probability of an exam remained unexplained as were reasons for the racial differences in use."
        },
        {
            "title": "Visual acuity and mortality in a chinese population. The Tanjong Pagar Study.",
            "abstract": "Objective:\n        \n      \n      To examine the relationship between visual acuity and mortality in a Chinese population.\n    \n\n\n          Design:\n        \n      \n      Population-based cohort study.\n    \n\n\n          Participants:\n        \n      \n      Chinese persons in Singapore ages 40 to 79 years at baseline examination.\n    \n\n\n          Methods:\n        \n      \n      The Tanjong Pagar Study in Singapore examined 1232 persons (response rate, 71.8%) at the baseline examination in 1997 and 1998. Participants had measurements of presenting and best-corrected visual acuity (VA) using standardized protocols. Mortality data were obtained from the National Death Registry, which linked subjects who had died since the baseline examination. Cause of death was determined from the International Classification of Diseases 9 codes. Analysis was performed on 1225 (99.4%) participants with VA data.\n    \n\n\n          Main outcome measure:\n        \n      \n      All-cause mortality.\n    \n\n\n          Results:\n        \n      \n      By December 31, 2004 (median follow-up, 6.8 years), 126 persons had died. Participants with presenting VA in the better eye worse than 20/40 (logarithm of the minimum angle of resolution [logMAR] score, 0.3) had a significantly higher mortality rate (hazard ratio [HR], 2.9; 95% confidence interval [CI], 1.4-6.3, adjusting for age, gender, hypertension, diabetes, smoking, heart attack, stroke, and income) as compared with participants with VA of 20/20 (logMAR, 0.0). Associations were similar for best-corrected VA in the better eye (HR, 2.7; 95% CI, 1.4-5.5). Among clinic participants with logMAR VA measurements, each 1-line difference in presenting VA (logMAR gain, 0.10) was associated with a 4-fold increased risk of mortality (HR, 4.4; 95% CI, 1.9-10.2).\n    \n\n\n          Conclusions:\n        \n      \n      In this Chinese population in Singapore, visual impairment was associated independently with an increased risk of mortality."
        },
        {
            "title": "The ratio of contrast volume to glomerular filtration rate predicts in-hospital and six-month mortality in patients undergoing primary angioplasty for ST-elevation myocardial infarction.",
            "abstract": "Background:\n        \n      \n      The aim of this study is to determine the impact of ratio of contrast volume to glomerular filtration rate (V/GFR) on development of contrast-induced nephropathy (CIN) and long-term mortality in patients with ST-segment elevation acute myocardial infarction (STEMI) undergoing primary percutaneous coronary intervention (PCI).\n    \n\n\n          Methods:\n        \n      \n      A total of 645 patients with STEMI undergoing primary PCI was prospectively enrolled. CIN was defined as an absolute increase in serum creatinine > 0.5 mg/dL or a relative increase > 25% within 48 h after PCI. The study population was divided into tertiles based on V/GFR. A high V/GFR was defined as a value in the third tertile (> 3.7).\n    \n\n\n          Results:\n        \n      \n      Patients in tertile 3 were older, had higher rate of smoking, diabetes mellitus and CIN, lower left ventricular ejection fraction, hemoglobin, and systolic and diastolic blood pressure compared to tertiles 1 and 2 (p < 0.05). V/GFR was found an independent predictor of in-hospital and 6-month mortality. We found 2 separate values of V/GFR for 2 different end points. While the ratio of 3.6 predicted in-hospital mortality with 78% sensitivity and 82% specificity, the ratio of 3.3 predicted 6-month mortality with 71% sensitivity and 76% specificity. Survival rate decreases as V/GFR increases both for in-hospital and during 6-month follow-up. Diabetes mellitus and multivessel disease were other predictors of in-hospital mortality.\n    \n\n\n          Conclusions:\n        \n      \n      High V/GFR level is associated with increased in-hospital and long-term mortality in patients with STEMI undergoing primary PCI."
        },
        {
            "title": "Olfactory identification deficits and increased mortality in the community.",
            "abstract": "Objective:\n        \n      \n      To examine the association between odor identification deficits and future mortality in a multiethnic community cohort of older adults.\n    \n\n\n          Methods:\n        \n      \n      Participants were evaluated with the 40-item University of Pennsylvania Smell Identification Test (UPSIT). Follow-up occurred at 2-year intervals with information on death obtained from informant interviews and the National Death Index.\n    \n\n\n          Results:\n        \n      \n      During follow-up (mean = 4.1 years, standard deviation = 2.6), 349 of 1,169 (29.9%) participants died. Participants who died were more likely to be older (p < 0.001), be male (p < 0.001), have lower UPSIT scores (p < 0.001), and have a diagnosis of dementia (p < 0.001). In a Cox model, the association between lower UPSIT score and mortality (hazard ratio [HR] = 1.07 per point interval, 95% confidence interval [CI] = 1.05-1.08, p < 0.001) persisted after controlling for age, gender, education, ethnicity, language, modified Charlson medical comorbidity index, dementia, depression, alcohol abuse, head injury, smoking, body mass index, and vision and hearing impairment (HR = 1.05, 95% CI = 1.03-1.07, p < 0.001). Compared to the fourth quartile with the highest UPSIT scores, HRs for mortality for the first, second, and third quartiles of UPSIT scores were 3.81 (95% CI = 2.71-5.34), 1.75 (95% CI = 1.23-2.50), and 1.58 (95% CI = 1.09-2.30), respectively. Participant mortality rate was 45% in the lowest quartile of UPSIT scores (anosmia) and 18% in the highest quartile of UPSIT scores.\n    \n\n\n          Interpretation:\n        \n      \n      Impaired odor identification, particularly in the anosmic range, is associated with increased mortality in older adults even after controlling for dementia and medical comorbidity."
        },
        {
            "title": "Age, gender, neck circumference, and Epworth sleepiness scale do not predict obstructive sleep apnea (OSA) in moderate to severe chronic obstructive pulmonary disease (COPD): The challenge to predict OSA in advanced COPD.",
            "abstract": "The combination of chronic obstructive pulmonary disease (COPD) and obstructive sleep apnea (OSA) is associated with substantial morbidity and mortality. We hypothesized that predictors of OSA among patients with COPD may be distinct from OSA in the general population. Therefore, we investigated associations between traditional OSA risk factors (e.g. age), and sleep questionnaires [e.g. Epworth Sleepiness Scale] in 44 patients with advanced COPD. As a second aim we proposed a pilot, simplified screening test for OSA in patients with COPD. In a prospective, observational study of patients enrolled in the UCSD Pulmonary Rehabilitation Program we collected baseline characteristics, cardiovascular events (e.g. atrial fibrillation), and sleep questionnaires [e.g. Pittsburgh Sleep Quality Index (PSQI)]. For the pilot questionnaire, a BMI ≥25 kg/m2 and the presence of cardiovascular disease were used to construct the pilot screening test. Male: 59%; OSA 66%. FEV1 (mean ± SD) = 41.0±18.2% pred., FEV1/FVC = 41.5±12.7%]. Male gender, older age, and large neck circumference were not associated with OSA. Also, Epworth Sleepiness Scale and the STOP-Bang questionnaire were not associated with OSA in univariate logistic regression. In contrast, BMI ≥25 kg/m2 (OR = 3.94, p = 0.04) and diagnosis of cardiovascular disease (OR = 5.06, p = 0.03) were significantly associated with OSA [area under curve (AUC) = 0.74]. The pilot COPD-OSA test (OR = 5.28, p = 0.05) and STOP-Bang questionnaire (OR = 5.13, p = 0.03) were both associated with OSA in Receiver Operating Characteristics (ROC) analysis. The COPD-OSA test had the best AUC (0.74), sensitivity (92%), and specificity (83%). A ten-fold cross-validation validated our results. We found that traditional OSA predictors (e.g. gender, Epworth score) did not perform well in patients with more advanced COPD. Our pilot test may be an easy to implement instrument to screen for OSA. However, a larger validation study is necessary before further clinical implementation is warranted."
        },
        {
            "title": "[Long-term results of cobalt 60 curietherapy for uveal melanoma].",
            "abstract": "Purpose:\n        \n      \n      To analyze 65 patients with uveal melanomas treated with cobalt plaque therapy with regards to mortality, visual results and complications.\n    \n\n\n          Patients and methods:\n        \n      \n      Most of the melanomas were large (T3: 52.5%), with a mean largest dimension of the base of 11 mm, and a mean thickness of 6 mm. Most of the tumors were located in the choroid (95%), with an anterior margin behind the equator (65%), and a posterior margin at less than 3 mm of the disc and/or of the macula (69%). The plaque radiotherapy delivered a mean dose of 95 Gy to the tumor apex, either with a cobalt plaque alone (51 cases), or in association with a ruthenium plaque (14 cases). The mean follow up period was over 8 years.\n    \n\n\n          Results:\n        \n      \n      The local control was achieved initially in 86% of the eyes. The estimated melanoma specific survival rate was 83% after 5 years and 74% after 10 years. The main parameter associated with the metastases was the largest dimension of the base (p < 0.01). The eye was retained in 83% of the cases. The probability of keeping a vision better than or equal to 0,1 was 39% after 5 years and 27% after 10 years. The main parameter associated with the visual loss was the tumor size (p < 0.01). The complications included cataract (39%), radiation retinopathy (34%), with maculopathy (19%) and/or papillopathy (13.5%), vitreous hemorrhages (22%), neovascular glaucoma (15%) and retinal detachment (12%).\n    \n\n\n          Conclusion:\n        \n      \n      These results supported the value of cobalt plaque radiotherapy in the management of uveal melanomas."
        },
        {
            "title": "Open-angle glaucoma and mortality: The Barbados Eye Studies.",
            "abstract": "Objective:\n        \n      \n      To evaluate the relationship between open-angle glaucoma (OAG) and mortality in a black population at 9-years' follow-up.\n    \n\n\n          Design:\n        \n      \n      Population-based cohort study of 4092 black participants (aged 40-84 years at baseline) in the Barbados Eye Studies. Open-angle glaucoma was defined by visual field defects and optic disc damage, based on standardized examinations and photograph gradings. Ocular hypertension was defined by an intraocular pressure greater than 21 mm Hg or treatment, without OAG damage. Mortality was ascertained from death certificates. Cox proportional hazards regression analyses determined associations with mortality.\n    \n\n\n          Results:\n        \n      \n      After 9 years, 764 (19%) participants were deceased. Mortality was unrelated to overall OAG at baseline (n = 300) after adjustment for confounders. However, cardiovascular mortality tended to increase in persons with previously diagnosed/treated OAG (n = 141; relative risk [RR], 1.38, P = .07) and was significantly higher with treatment involving timolol maleate (RR, 1.91, P = .04). Cardiovascular deaths also tended to increase in persons with ocular hypertension at baseline (n = 498; RR, 1.28, P = .06).\n    \n\n\n          Conclusions:\n        \n      \n      In this black population, cardiovascular mortality tended to increase in persons with previously diagnosed/treated OAG and ocular hypertension. The excess mortality associated with timolol maleate treatment of OAG, also found in a white population, warrants further investigation."
        },
        {
            "title": "A cohort study of diabetic patients and diabetic foot ulceration patients in China.",
            "abstract": "To determine the annual incidence and clinically relevant risk factors for foot ulceration in a large cohort study of diabetic foot ulcer (DFU) patients and diabetes mellitus (DM) patients in China. To investigate a cohort of 1,333 patients comprising 452 DFU patients and 881 DM patients, who underwent foot screening, physical examination, and laboratory tests in eight hospitals. The patients were assessed at baseline in terms of their demographic information, medical and social history, peripheral neuropathy disease (PND) screening, periphery artery disease (PAD) screening, assessment of nutritional status, and diabetic control. One year later, the patients were followed up to determine the incidence of new foot ulcers, amputation, and mortality. By univariate analysis, statistically significant differences were found in age, location, gender, living alone (yes/no), occupation, smoking, hypertension, PND, PAD, nephropathy, retinopathy, cataracts, duration of diabetes, Glycosylated hemoglobin A (HbA1c), fasting plasma glucose level, postprandial blood glucose level, insulin level, blood urea nitrogen, creatinine, cholesterol, triglyeride, high density lipoprotein (HDL), serum albumin, white blood cell, and body mass index. A binary logistic regression model was used to examine which of these risk factors were independent risk factors for foot ulceration. A total of 687 (51.5%) of the 1,333 patients were followed up for an average of 12 months; there were 458 DM patients and 229 DFU patients. A total of 46 patients died during the follow-up period; 13 were DM patients, and 33 were DFU patients. Of the 641 patients, 445 (69.4%) patients were DM patients, and 196 (30.6%) were DFU patients. At follow-up, 36/445 DM patients (8.1%), and 62/196 DFU patients (31.6%), developed new ulcers; 10/196 DFU patients underwent an amputation. The annual incidence of ulceration for DM patients and amputation for DFU patients were 8.1 and 5.1%, respectively. The annual mortality of the DM patients and DMF patients were 2.8 and 14.4%, respectively. A binary logistic regression model was used to examine which risk factors were independent risk factors for foot ulceration during the follow-up period, and the final results showed that nephropathy (odds ratio 2.32), insulin level (odds ratio 3.136, 2.629), and decreased HDL (odds ratio 0.427) were associated with increased risks for foot ulceration. Complications of diabetes affecting the feet represent a serious problem in China. The incidence of foot ulcers and amputation are much higher than that of Western countries. More intensive surveillance and aggressive care following a diagnosis of DFU and earlier referral to specialty care might improve the patient outcome."
        },
        {
            "title": "Educational value of morbidity and mortality (M&M) conferences: are minor complications important?",
            "abstract": "Background:\n        \n      \n      Often, minor complications are not reported in morbidity and mortality (M&M) conference because they are considered insignificant to patient outcome. As part of an effort to improve the quality of the M&M conference, we sought to integrate a specific, focused intervention to improve the reporting of minor complications and to evaluate the perception of its educational value.\n    \n\n\n          Materials and methods:\n        \n      \n      To provide evidence-based training in recognizing, treating, and preventing minor complications, a presentation strategy was created. Surgical faculty identified 20 complications as minor complications. Each month, a junior resident was assigned to give a 10-minute presentation, assessing 1 of the 20 minor complications in depth during the M&M conference. To assess the impact of the intervention, we surveyed residents and faculty about the educational value of M&M conferences before and after implementation.\n    \n\n\n          Results:\n        \n      \n      Before introducing minor complication presentations into the M&M conference, only 58% of respondents indicated that minor complications should be reported at the conference. After the changes were implemented in minor complication reporting, 95% of respondents said that minor complications should be reported (p < 0.01). Eighty-nine percent of respondents found the minor complication presentations to be educationally beneficial. In addition, postsurvey respondents were also more likely than presurvey respondents to identify that a purpose of an M&M conference was to improve patient care (29% vs 71%, p < 0.05).\n    \n\n\n          Conclusions:\n        \n      \n      A formal, evidence-based presentation of minor complications can increase both the faculty and residents' perception of the importance of reporting minor complications at an M&M conference. Focused minor complication reporting should be incorporated into M&M curriculum."
        },
        {
            "title": "Distance visual acuity impairment and survival in African Americans and non-Hispanic Whites.",
            "abstract": "## BACKGROUND\nRegional studies conducted in the United States have shown associations between visual impairment and shorter survival in non-Hispanic Whites.\n## OBJECTIVE\nTo examine associations between visual impairment and mortality in a nationally representative sample of African Americans and non-Hispanic Whites residing in the United States.\n## DESIGN\nMortality linkage with participants from the 1974-1975 National Health and Nutrition Examination Augmentation Survey was performed by the National Center for Health Statistics in 1992.\n## SUBJECTS\nComplete data were available on 245 African Americans and 2571 non-Hispanic Whites.\n## METHODS\nUncorrected binocular distance visual acuity was assessed using Sloan letter charts. Usual-corrected visual acuity was then obtained with participants wearing glasses or contact lenses, if any. Analytical methods included Cox regression models with adjustment for sample weights and design effects as well as age, gender, smoking status, and self-rated health.\n## RESULTS\nMultivariate survival analyses found a significant interaction between race and visual impairment status; consequently, race-specific analyses were performed. There were no significant associations between uncorrected binocular visual acuity impairment (20/50 or worse) and all-cause mortality or cancer mortality. There was no significant association between impaired uncorrected acuity and cardiovascular disease mortality in African Americans (Hazard Ratio=0.67, 95% CI: 0.35-1.26), but this association was significant in non-Hispanic Whites (Hazard Ratio=1.21, 95% CI: 1.01-1.45). In multivariate models, within race groups, impaired usual-corrected visual acuity was not associated with an increased risk of all-cause mortality or mortality due to cancer or cardiovascular disease.\n## CONCLUSIONS\nUncorrected and usual-corrected binocular distance visual impairment is not associated with all-cause mortality or cancer mortality. Cardiovascular disease mortality risk may be slightly higher in non-Hispanic Whites with uncorrected visual acuity impairment.\n"
        },
        {
            "title": "Surgical Outcomes in Behcet's Disease Patients With Severe Aortic Regurgitation.",
            "abstract": "Background:\n        \n      \n      An optimal treatment for aortic regurgitation in Behcet's disease has not been established. We investigated the effect of operative technique, prosthetic material, and immunomodulation therapy on surgical outcomes.\n    \n\n\n          Methods:\n        \n      \n      In this study, 23 patients with Behcet's disease surgically treated for aortic regurgitation were assessed. Significant postoperative events were defined as death, aortic valve or graft-related problem(s), infective endocarditis, disabling stroke, and aortic valve or root reoperation. Surgical procedures were classified as isolated aortic valve replacement, bioprosthetic root replacement, and mechanical root replacement. Allograft root replacements were included in the bioprosthetic root replacement group.\n    \n\n\n          Results:\n        \n      \n      A total of 40 operations, including 39 aortic valve or root surgeries and 1 orthotopic heart transplantation, were performed on patients confirmed with Behcet's disease. However, the study only reviewed 35 of the 40 cases (4 cases with inadequately documented medical records and 1 heart transplantation case were excluded). Significant adverse events occurred in 8 of 11 (73%) isolated aortic valve replacement, 9 of 12 (75%) bioprosthetic root replacement (5 xenografts and 7 allografts), and 4 of 12 (33%) mechanical root replacement cases. Multivariate analysis revealed that the 1-month postdischarge C-reactive protein level and operative age were independent predictive factors for postoperative event-free survival. Mechanical root replacement was identified as the most significant predictive factor leading to positive outcomes (hazard ratio, 0.147; 95% confidence interval, 0.028 to 0.766; p = 0.023).\n    \n\n\n          Conclusions:\n        \n      \n      The findings suggest that mechanical root replacement combined with a low postoperative C-reactive protein level maintained through adjunctive immunomodulation therapy may lead to optimal surgical outcomes in Behcet's disease associated with severe aortic regurgitation."
        },
        {
            "title": "Occupational injury and illness in the United States. Estimates of costs, morbidity, and mortality.",
            "abstract": "Objective:\n        \n      \n      To estimate the annual incidence, the mortality and the direct and indirect costs associated with occupational injuries and illnesses in the United States in 1992.\n    \n\n\n          Design:\n        \n      \n      Aggregation and analysis of national and large regional data sets collected by the Bureau of Labor Statistics, the National Council on Compensation Insurance, the National Center for Health Statistics, the Health Care Financing Administration, and other governmental bureaus and private firms.\n    \n\n\n          Methods:\n        \n      \n      To assess incidence of and mortality from occupational injuries and illnesses, we reviewed data from national surveys and applied an attributable risk proportion method. To assess costs, we used the human capital method that decomposes costs into direct categories such as medical and insurance administration expenses as well as indirect categories such as lost earnings, lost home production, and lost fringe benefits. Some cost estimates were drawn from the literature while others were generated within this study. Total costs were calculated by multiplying average costs by the number of injuries and illnesses in each diagnostic category.\n    \n\n\n          Results:\n        \n      \n      Approximately 6500 job-related deaths from injury, 13.2 million nonfatal injuries, 60,300 deaths from disease, and 862,200 illnesses are estimated to occur annually in the civilian American workforce. The total direct ($65 billion) plus indirect ($106 billion) costs were estimated to be $171 billion. Injuries cost $145 billion and illnesses $26 billion. These estimates are likely to be low, because they ignore costs associated with pain and suffering as well as those of within-home care provided by family members, and because the numbers of occupational injuries and illnesses are likely to be undercounted.\n    \n\n\n          Conclusions:\n        \n      \n      The costs of occupational injuries and illnesses are high, in sharp contrast to the limited public attention and societal resources devoted to their prevention and amelioration. Occupational injuries and illnesses are an insufficiently appreciated contributor to the total burden of health care costs in the United States."
        },
        {
            "title": "Adherence to a Mediterranean diet, genetic susceptibility, and progression to advanced macular degeneration: a prospective cohort study.",
            "abstract": "Background:\n        \n      \n      Adherence to a Mediterranean-type diet is linked to a lower risk of mortality and chronic disease, but the association with the progression of age-related macular degeneration (AMD) and genetic susceptibility is unknown.\n    \n\n\n          Objective:\n        \n      \n      We examined the association of adherence to the Mediterranean diet and genetic susceptibility with progression to advanced AMD.\n    \n\n\n          Design:\n        \n      \n      Among 2525 subjects in the AREDS (Age-Related Eye Disease Study), 1028 eyes progressed to advanced AMD over 13 y. Baseline data for demographic and behavioral covariates were collected by using questionnaires. Dietary data were collected from food-frequency questionnaires. The alternate Mediterranean diet (aMeDi) score (range: 0-9) was constructed from individual intakes of vegetables, fruit, legumes, whole grains, nuts, fish, red and processed meats, alcohol, and the ratio of monounsaturated to saturated fats. Ten genetic loci in 7 genes [complement factor H (CFH), age-related maculopathy susceptibility 2/high-temperature requirement A serine peptidase 1 (ARMS2/HTRA1), complement component 2 (C2), complement factor B (CFB), complement component 3 (C3), collagen type VIII α 1 (COL8A1), and RAD51 paralog B (RAD51B)] were examined. Survival analysis was used to assess individual eyes for associations between incident AMD and aMeDi score, as well as interaction effects between aMeDi score and genetic variation on risk of AMD.\n    \n\n\n          Results:\n        \n      \n      A high aMeDi score (score of 6-9) was significantly associated with a reduced risk of progression to advanced AMD after adjustment for demographic, behavioral, ocular, and genetic covariates (HR: 0.74; 95% CI: 0.61, 0.91; P-trend = 0.007). The aMeDi score was significantly associated with a lower risk of incident advanced AMD among subjects carrying the CFH Y402H nonrisk (T) allele (P-trend = 0.0004, P-interaction = 0.04). The aMeDi score was not associated with AMD among subjects who were homozygous for the risk (C) allele.\n    \n\n\n          Conclusion:\n        \n      \n      Higher adherence to a Mediterranean diet was associated with reduced risk of progression to advanced AMD, which may be modified by genetic susceptibility. This trial was registered at clinicaltrials.gov as NCT00594672."
        },
        {
            "title": "Long-term results of non-valved glaucoma drainage implant surgery and glaucoma drainage implant combined with trabeculectomy.",
            "abstract": "Purpose:\n        \n      \n      The purpose was to investigate the efficacy and complications of nonvalved glaucoma drainage implant (GDI) surgery and GDI combined with trabeculectomy.\n    \n\n\n          Subjects and methods:\n        \n      \n      Serial Japanese patients who received GDI were retrospectively investigated. The survival rate of eyes was analyzed using the Kaplan-Meier method, defining death as: (1) Intraocular pressure (IOP) <6 mmHg, or ≥22 mmHg, and <20% reduction of preoperative IOP, (2) additional glaucoma surgery, (3) loss of light perception. Prognostic factors of age, sex, previous surgery, type of glaucoma, synechial closure, preoperative IOP, type of GDI (single-, double-plate Molteno, Baerveldt 350) and GDI combined with trabeculectomy were investigated employing proportional hazards analysis.\n    \n\n\n          Results:\n        \n      \n      One hundred and twenty-four eyes of 109 patients aged 53.3 ± 7.8 years old were analyzed. Types of GDI were single-plate (15.3%), double-plate Molteno (71.8%), and Baerveldt 350 (12.9%). The results of survival rate analysis were 86.1, 71.1, 71.1, and 64.6% for 1, 3, 5, and 10 years respectively. Thirty-four eyes (27.4%) died because of uncontrolled IOP (19.4%), loss of light perception (5.6%), and additional glaucoma surgery (2.4%). Single-plate Molteno was the only risk factor for failure. Persistent unphysiological hypotony (0.8%) and bullous keratopathy (5.6%) were observed.\n    \n\n\n          Conclusion:\n        \n      \n      Nonvalved GDI surgery and GDI combined with trabeculectomy using nonabsorbable tube ligature proved to be an excellent device for any type of glaucoma in Japanese patients. Hypotony and corneal endothelial loss are the most serious complication in the long-term results of our patients."
        },
        {
            "title": "Success rate of nurse-led everting sutures for involutional lower lid entropion.",
            "abstract": "PurposeTo evaluate safety and long-term recurrence rate of entropion in patients having everting sutures (ES) for involutional entropion by ophthalmic nurses in a real clinical setting.Patients and methodsRetrospective notes review of all patients who had an outpatient ES by our trained ophthalmic nurses over 2 year's time period. Outcome measures were complication and recurrence rates. Those with less than 3 years' recorded follow-up were contacted by paper questionnaire.Results90 lids of 82 patients analysed. Mean age was 78 (range 54-97). In total, 82% had no entropion surgery before, whereas 13% had previous ES and 5% one or more other procedures. Questionnaires were sent to 38, with return rate of 81%. Recurrence rate was 21.1% after 36-60 months follow up from nurse-performed ES, with mean time to recurrence of 15 months (SD 13 months). A total of 32% of patients died during the follow-up period. Mean time between the procedure and death is 20.5 months. When ES were repeated twice (11 patients), recurrence rate was still 20%. No patients had any complications.ConclusionES can be safely performed by ophthalmic nurses, with success rate comparable to the same technique performed by ophthalmologists."
        },
        {
            "title": "[Early diagnosis of lung cancer: where do we stand?].",
            "abstract": "Screening studies involving chest x-rays with and without additional sputum cytology, have failed to bring about any reduction in mortality from lung cancer. With regard to all other potential screening methods, no studies investigating tumor-specific mortality are available. Against such a background, none of the relevant societies recommend systematic screening. Sputum cytology is sensitive only for central lesions. Autofluorescence bronchoscopy is limited to the detection of early malignant and premalignant lesions in the central airways. While CT screening detects numerous round foci, only a few of these prove to be carcinomas. In contrast to these findings, we have the positive results of non-controlled screening studies employing low-dose CT, which, however, has not been widely accepted."
        },
        {
            "title": "Abnormal electrocardiographic QRS transition zone and risk of mortality in individuals free of cardiovascular disease.",
            "abstract": "Aims:\n        \n      \n      We examined the prognostic significance of abnormal electrocardiographic QRS transition zone (clockwise and counterclockwise horizontal rotations) in individuals free of cardiovascular disease (CVD).\n    \n\n\n          Methods and results:\n        \n      \n      A total of 5541 adults (age 53 ± 10.4 years, 54% women, 24% non-Hispanic black, 25% Hispanic) without CVD or any major electrocardiogram (ECG) abnormalities from the US Third National Health and Nutrition Examination Survey were included in this analysis. Clockwise and counterclockwise horizontal rotations were defined from standard 12-lead ECG using Minnesota ECG Classification. Mortality and cause of death were assessed through 2006. At baseline, 282 participants had clockwise rotation and 3500 had counterclockwise rotation. During a median follow of 14.6 years, 1229 deaths occurred of which 415 were due to CVD. In multivariable-adjusted Cox proportional hazard analysis and compared with normal rotation, clockwise rotation was significantly associated with increased risk of all-cause mortality {hazard ratio (HR) [95% confidence interval (CI)]: 1.43 (1.15-1.78); P = 0.002} and CVD mortality [HR (95% CI): 1.61 (1.09, 2.37) P = 0.016]. In contrast, counterclockwise rotation was associated with significantly lower risk of all-cause mortality [HR (95% CI): 0.86 (0.76, 0.97); P = 0.017] and non-significant association with CVD mortality [HR (95% CI): 1.07 (0.86, 1.33); P = 0.549]. These results were consistent in subgroup analysis stratified by age, sex, and race.\n    \n\n\n          Conclusion:\n        \n      \n      In a diverse community-based population free of CVD and compared with normal rotation, clockwise rotation was associated with increased risk of all-cause and CVD mortality while counterclockwise rotation was associated with lower risk of all-cause mortality and non-significant association with CVD mortality. These findings call for attention to these often neglected ECG markers, and probably call for revising the current definition of normal rotation."
        },
        {
            "title": "A randomized, placebo-controlled, clinical trial of high-dose supplementation with vitamins C and E, beta carotene, and zinc for age-related macular degeneration and vision loss: AREDS report no. 8.",
            "abstract": "Background:\n        \n      \n      Observational and experimental data suggest that antioxidant and/or zinc supplements may delay progression of age-related macular degeneration (AMD) and vision loss.\n    \n\n\n          Objective:\n        \n      \n      To evaluate the effect of high-dose vitamins C and E, beta carotene, and zinc supplements on AMD progression and visual acuity.\n    \n\n\n          Design:\n        \n      \n      The Age-Related Eye Disease Study, an 11-center double-masked clinical trial, enrolled participants in an AMD trial if they had extensive small drusen, intermediate drusen, large drusen, noncentral geographic atrophy, or pigment abnormalities in 1 or both eyes, or advanced AMD or vision loss due to AMD in 1 eye. At least 1 eye had best-corrected visual acuity of 20/32 or better. Participants were randomly assigned to receive daily oral tablets containing: (1) antioxidants (vitamin C, 500 mg; vitamin E, 400 IU; and beta carotene, 15 mg); (2) zinc, 80 mg, as zinc oxide and copper, 2 mg, as cupric oxide; (3) antioxidants plus zinc; or (4) placebo.\n    \n\n\n          Main outcome measures:\n        \n      \n      (1) Photographic assessment of progression to or treatment for advanced AMD and (2) at least moderate visual acuity loss from baseline (> or =15 letters). Primary analyses used repeated-measures logistic regression with a significance level of.01, unadjusted for covariates. Serum level measurements, medical histories, and mortality rates were used for safety monitoring.\n    \n\n\n          Results:\n        \n      \n      Average follow-up of the 3640 enrolled study participants, aged 55-80 years, was 6.3 years, with 2.4% lost to follow-up. Comparison with placebo demonstrated a statistically significant odds reduction for the development of advanced AMD with antioxidants plus zinc (odds ratio [OR], 0.72; 99% confidence interval [CI], 0.52-0.98). The ORs for zinc alone and antioxidants alone are 0.75 (99% CI, 0.55-1.03) and 0.80 (99% CI, 0.59-1.09), respectively. Participants with extensive small drusen, nonextensive intermediate size drusen, or pigment abnormalities had only a 1.3% 5-year probability of progression to advanced AMD. Odds reduction estimates increased when these 1063 participants were excluded (antioxidants plus zinc: OR, 0.66; 99% CI, 0.47-0.91; zinc: OR, 0.71; 99% CI, 0.52-0.99; antioxidants: OR, 0.76; 99% CI, 0.55-1.05). Both zinc and antioxidants plus zinc significantly reduced the odds of developing advanced AMD in this higher-risk group. The only statistically significant reduction in rates of at least moderate visual acuity loss occurred in persons assigned to receive antioxidants plus zinc (OR, 0.73; 99% CI, 0.54-0.99). No statistically significant serious adverse effect was associated with any of the formulations.\n    \n\n\n          Conclusions:\n        \n      \n      Persons older than 55 years should have dilated eye examinations to determine their risk of developing advanced AMD. Those with extensive intermediate size drusen, at least 1 large druse, noncentral geographic atrophy in 1 or both eyes, or advanced AMD or vision loss due to AMD in 1 eye, and without contraindications such as smoking, should consider taking a supplement of antioxidants plus zinc such as that used in this study."
        },
        {
            "title": "PET imaging of cerebral perfusion and oxygen consumption in acute ischaemic stroke: relation to outcome.",
            "abstract": "We used positron emission tomography (PET) to assess the relation between combined imaging of cerebral blood flow and oxygen consumption 5-18 h after first middle cerebral artery (MCA) stroke and neurological outcome at 2 months. All 18 patients could be classified into three visually defined PET patterns of perfusion and oxygen consumption changes. Pattern I (7 patients) suggested extensive irreversible damage and was consistently associated with poor outcome. Pattern II (5) suggested continuing ischaemia and was associated with variable outcome. Pattern III (6), with hyperperfusion and little or no metabolic alteration, was associated with excellent recovery, which suggests that early reperfusion is beneficial. This relation between PET and outcome was highly significant (p < 0.0005). The results suggest that within 5-18 h of stroke onset, PET is a good predictor of outcome in patterns I and III, for which therapy seems limited. The absence of predictive value for pattern II suggests that it is due to a reversible ischaemic state that is possibly amenable to therapy. These findings may have important implications for acute MCA stroke management and for patients' selection for therapeutic trials."
        },
        {
            "title": "Retinal vein occlusions and mortality: the Beijing Eye Study.",
            "abstract": "Purpose:\n        \n      \n      To assess the association between retinal vein occlusion (RVO) and mortality in a population-based setting.\n    \n\n\n          Design:\n        \n      \n      Population-based, longitudinal study.\n    \n\n\n          Methods:\n        \n      \n      At baseline in 2001, the Beijing Eye Study examined 4,335 subjects for RVO with a frequency of detected vein occlusions of 61 (1.4%) in 4,335 subjects. In 2006, all study participants were invited for a follow-up examination.\n    \n\n\n          Results:\n        \n      \n      Of the 4,335 subjects, 3,195 (73.7%) returned for follow-up examination, whereas 132 (3.0%) subjects had died and 1,008 (23.3%) subjects declined to be re-examined or had moved away. For the subjects younger than 70 years or than 65 years, respectively, RVO was associated significantly with an increased mortality rate (P = .05; 95% confidence interval [CI], 0.995 to 8.26; and P = .001; 95% CI, 2.11 to 18.73, respectively).\n    \n\n\n          Conclusions:\n        \n      \n      RVO in relatively young persons may signal a significant risk of mortality."
        },
        {
            "title": "The optimal definition of contrast-induced acute kidney injury for prediction of inpatient mortality in patients undergoing percutaneous coronary interventions.",
            "abstract": "Background:\n        \n      \n      It is unknown which definition of contrast-induced acute kidney injury (CI-AKI) in the setting of percutaneous coronary interventions is best associated with inpatient mortality and whether this association is stable across patients with various preprocedural serum creatinine (SCr) values.\n    \n\n\n          Methods:\n        \n      \n      We applied logistic regression models to multiple CI-AKI definitions used by the Kidney Disease Improving Global Outcomes guidelines and previously published studies to examine the impact of preprocedural SCr on a candidate definition's correlation with the adverse outcome of inpatient mortality. We used likelihood ratio tests to examine candidate definitions and identify those where association with inpatient mortality remained constant regardless of preprocedural SCr. These definitions were assessed for specificity, sensitivity, and positive and negative predictive values to identify an optimal definition.\n    \n\n\n          Results:\n        \n      \n      Our study cohort included 119,554 patients who underwent percutaneous coronary intervention in Michigan between 2010 and 2014. Most commonly used definitions were not associated with inpatient mortality in a constant fashion across various preprocedural SCr values. Of the 266 candidate definitions examined, 16 definition's association with inpatient mortality was not significantly altered by preprocedural SCr. Contrast-induced acute kidney injury defined as an absolute increase of SCr ≥0.3 mg/dL and a relative SCr increase ≥50% was selected as the optimal candidate using Perkins and Shisterman decision theoretic optimality criteria and was highly predictive of and specific for inpatient mortality.\n    \n\n\n          Conclusions:\n        \n      \n      We identified the optimal definition for CI-AKI to be an absolute increase in SCr ≥0.3 mg/dL and a relative SCr increase ≥50%. Further work is needed to validate this definition in independent studies and to establish its utility for clinical trials and quality improvement efforts."
        },
        {
            "title": "LGE Provides Incremental Prognostic Information Over Serum Biomarkers in AL Cardiac Amyloidosis.",
            "abstract": "Objectives:\n        \n      \n      This study sought to determine the prognostic value of cardiac magnetic resonance (CMR) late gadolinium enhancement (LGE) in amyloid light chain (AL) cardiac amyloidosis.\n    \n\n\n          Background:\n        \n      \n      Cardiac involvement is the major determinant of mortality in AL amyloidosis. CMR LGE is a marker of amyloid infiltration of the myocardium. The purpose of this study was to evaluate retrospectively the prognostic value of CMR LGE for determining all-cause mortality in AL amyloidosis and to compare the prognostic power with the biomarker stage.\n    \n\n\n          Methods:\n        \n      \n      Seventy-six patients with histologically proven AL amyloidosis underwent CMR LGE imaging. LGE was categorized as global, focal patchy, or none. Global LGE was considered present if it was visualized on LGE images or if the myocardium nulled before the blood pool on a cine multiple inversion time (TI) sequence. CMR morphologic and functional evaluation, echocardiographic diastolic evaluation, and cardiac biomarker staging were also performed. Subjects' charts were reviewed for all-cause mortality. Cox proportional hazards analysis was used to evaluate survival in univariate and multivariate analysis.\n    \n\n\n          Results:\n        \n      \n      There were 40 deaths, and the median study follow-up period was 34.4 months. Global LGE was associated with all-cause mortality in univariate analysis (hazard ratio = 2.93; p < 0.001). In multivariate modeling with biomarker stage, global LGE remained prognostic (hazard ratio = 2.43; p = 0.01).\n    \n\n\n          Conclusions:\n        \n      \n      Diffuse LGE provides incremental prognosis over cardiac biomarker stage in patients with AL cardiac amyloidosis."
        },
        {
            "title": "A comparative evaluation of radiologic and clinical scoring systems in the early prediction of severity in acute pancreatitis.",
            "abstract": "Objectives:\n        \n      \n      The early identification of clinically severe acute pancreatitis (AP) is critical for the triage and treatment of patients. The aim of this study was to compare the accuracy of computed tomography (CT) and clinical scoring systems for predicting the severity of AP on admission.\n    \n\n\n          Methods:\n        \n      \n      Demographic, clinical, and laboratory data of all consecutive patients with a primary diagnosis of AP during a two-and-half-year period was prospectively collected for this study. A retrospective analysis of the abdominal CT data was performed. Seven CT scoring systems (CT severity index (CTSI), modified CT severity index (MCTSI), pancreatic size index (PSI), extrapancreatic score (EP), ''extrapancreatic inflammation on CT'' score (EPIC), ''mesenteric oedema and peritoneal fluid'' score (MOP), and Balthazar grade) as well as two clinical scoring systems: Acute Physiology, Age, and Chronic Health Evaluation (APACHE)-II and Bedside Index for Severity in AP (BISAP) were comparatively evaluated with regard to their ability to predict the severity of AP on admission (first 24 h of hospitalization). Clinically severe AP was defined as one or more of the following: mortality, persistent organ failure and/or the presence of local pancreatic complications that require intervention. All CT scans were reviewed in consensus by two radiologists, each blinded to patient outcome. The accuracy of each imaging and clinical scoring system for predicting the severity of AP was assessed using receiver operating curve analysis.\n    \n\n\n          Results:\n        \n      \n      Of 346 consecutive episodes of AP, there were 159 (46%) episodes in 150 patients (84 men, 66 women; mean age, 54 years; age range, 21-91 years) who were evaluated with a contrast-enhanced CT scan (n = 131 episodes) or an unenhanced CT scan (n = 28 episodes) on the first day of admission. Clinically severe AP was diagnosed in 29/159 (18%) episodes; 9 (6%) patients died. Overall, the Balthazar grading system (any CT technique) and CTSI (contrast-enhanced CT only) demonstrated the highest accuracy among the CT scoring systems for predicting severity, but this was not statistically significant. There were no statistically significant differences between the predictive accuracies of CT and clinical scoring systems.\n    \n\n\n          Conclusions:\n        \n      \n      The predictive accuracy of CT scoring systems for severity of AP is similar to clinical scoring systems. Hence, a CT on admission solely for severity assessment in AP is not recommended."
        },
        {
            "title": "Association of blindness and hearing impairment with mortality in a cohort of elderly persons in a rural area.",
            "abstract": "## BACKGROUND\nStudies in developed nations have reported an association of blindness and hearing impairment with mortality in elderly persons.\n## OBJECTIVES\nTo study the association of blindness and hearing impairment with mortality in a cohort of elderly persons in rural north India.\n## MATERIALS AND METHODS\nThis community-based prospective study was conducted in eleven randomly selected villages, in Ballabgarh block, Haryana. A cohort of 1422 participants, of age 60 years and above, was examined at baseline, for their visual and hearing status. Data on the sociodemographic factors, various comorbidities, activities of daily living, and self-rated health were recorded. Baseline data was collected for the period May 2008 to August 2008. Follow-up data collection for mortality was completed in December 2009. The median follow-up period was 518 days.\n## RESULTS\nOne hundred out of 1422 elderly (7.0%) participants died during the follow-up period. Significant hazard ratios were found after adjustment for various comorbid conditions. On adjustment for sociodemographic factors (age, sex, and literacy), neither blindness nor hearing impairment was found to be significantly associated with mortality. After adjustment for all covariates in the study, hearing impairment (Hazard Ratio = 2.13; 95% CI, 1.29 - 3.54) was found to be significantly associated with mortality in the age group ≥70 years.\n## CONCLUSIONS\nThis study demonstrated that hearing impairment was an independent risk factor for mortality in people aged ≥70 years. Similar studies with a longer period of follow-up are required in India, to guide public health interventions.\n"
        },
        {
            "title": "Safety and feasibility of intravascular optical coherence tomography using a nonocclusive technique to evaluate carotid plaques before and after stent deployment.",
            "abstract": "Purpose:\n        \n      \n      To evaluate the safety and feasibility of optical coherence tomography (OCT) in patients with carotid stenosis undergoing carotid artery stenting (CAS).\n    \n\n\n          Methods:\n        \n      \n      In a prospective study, 25 consecutive patients (15 men; mean age 74±4 years) undergoing protected CAS were enrolled and underwent high-definition (homoaxial resolution 10 µm) OCT image acquisition before stent deployment, immediately after stent placement, and following postdilation of the stent (3 scans/patient). Pullbacks were started during a nonocclusive flush, mechanically injecting 24 mL of 50% diluted contrast at 6 mL/s to displace blood from the artery. Two independent physicians judged the quality of images on a predefined 1-10 scale. The proportions of specific agreement and kappa values (κ) were calculated.\n    \n\n\n          Results:\n        \n      \n      No procedural or in-hospital neurological complications occurred (any stroke/death 0%). The technical success of OCT pullbacks was 97.3% (73/75). The total amount of contrast was 86±18 mL/patient. No significant alteration in glomerular filtration rate or any other significant adverse event occurred. The images obtained were of high quality (mean value 8.1 out of 10), with good inter- and intraobserver agreement (κ = 0.81-0.87 and κ = 0.95, respectively). OCT images revealed innovative features such as rupture of the fibrous cap, plaque prolapse, and stent malapposition in a high percentage of the patients (range 24%-100%).\n    \n\n\n          Conclusion:\n        \n      \n      Intravascular OCT during a nonocclusive flush appears to be feasible and safe in carotid arteries. Since some original and unexpected information after CAS has been made available for the first time at such a high definition, future studies with OCT should focus on the interaction between carotid plaque and stent design, which might revolutionize our understanding of the mechanisms of carotid stenting, as well as influence our clinical policies."
        },
        {
            "title": "Assessment of left atrial appendage function during sinus rhythm in patients with hypertrophic cardiomyopathy: transesophageal echocardiography and tissue doppler study.",
            "abstract": "Background:\n        \n      \n      The incidence of systemic thromboembolism is high in patients with hypertrophic cardiomyopathy (HCM). The authors hypothesized that vulnerability to such vascular events could be caused by depressed left atrial appendage (LAA) function during normal sinus rhythm (SR). The aim of this cross-sectional study was to investigate LAA contractile function during SR in patients with HCM.\n    \n\n\n          Methods:\n        \n      \n      LAA function was assessed in 62 patients with HCM in SR and compared with that in 53 age-matched and sex-matched controls. Patients with histories of atrial fibrillation and documented episodes of paroxysmal atrial fibrillation on 24-hour Holter monitoring and depressed left ventricular ejection fractions (<50%) were excluded. Multiplane transesophageal echocardiography was performed for determination of the morphology and function of the LAA.\n    \n\n\n          Results:\n        \n      \n      LAA thrombi were present in five patients (8%) with HCM. LAA emptying and filling Doppler velocities were significantly depressed in the HCM group. LAA emptying and filling velocities were negatively correlated with age in controls (r = -0.4, P = .005), but these velocities were not associated with age in the HCM group. Moreover, LAA velocities were not associated with left ventricular mass index, left ventricular outflow tract gradient, or the degree of diastolic dysfunction in the HCM group. All Doppler tissue imaging velocities obtained from LAA walls were also significantly depressed in the HCM group.\n    \n\n\n          Conclusions:\n        \n      \n      LAA thrombus formation was not rare in this patient population. The significantly depressed LAA filling and emptying velocities in SR may predispose patients with HCM to thromboembolic events. The depressed Doppler tissue imaging LAA parameters in patients with HCM may indicate the presence of a possible intrinsic atrial myopathy. Thromboembolic risk should be taken into account, and the evaluation of LAA morphology and function by transesophageal echocardiography might become a component of routine workup in patients with HCM in the future."
        },
        {
            "title": "Screening for cervical cancer in low-resource settings in 2011.",
            "abstract": "Context:\n        \n      \n      Cervical cancer remains the most common malignancy in women living in low- and middle-income countries, despite the decline of the disease in countries where cervical cytology screening programs have been implemented.\n    \n\n\n          Objectives:\n        \n      \n      To review the current incidence of cervical cancer in low-resource countries, the availability and types of screening programs, and the treatment options.\n    \n\n\n          Data sources:\n        \n      \n      Literature review through PubMed, Internet search, and personal communication.\n    \n\n\n          Conclusions:\n        \n      \n      Although data are incomplete, available figures confirm that the rate of cervical cancer deaths and the availability of cervical cancer screening programs are inversely proportional and vary, in general, by the wealth of the nation. Despite the success of cervical cytology screening, many major health care organizations have abandoned screening by cytology in favor of direct visualization methods with immediate treatment of lesions by cryotherapy provided by trained, nonmedical personnel."
        },
        {
            "title": "The 'Mini Nutritional Assessment' (MNA) and the 'Determine Your Nutritional Health' Checklist (NSI Checklist) as predictors of morbidity and mortality in an elderly Danish population.",
            "abstract": "The purpose of the present study was to evaluate the capacity of the 'Determine Your Nutritional Health' Checklist (NSI Checklist) and the 'Mini Nutritional Assessment' (MNA) methods to predict nutrition-related health problems. Data were from the Danish part of the 'Survey in Europe of Nutrition in the Elderly, a Concerted Action' (SENECA) baseline survey from 1988, and the follow-up study from 1993. Based on the baseline survey thirty-nine (19.3%) of the subjects were classified at high nutritional risk, 103 (51%) were considered at moderate nutritional risk and sixty (29.7%) were within the 'good' range according to the criteria in the NSI Checklist. With the MNA, 171 subjects were classified according to their nutritional risk into a well-nourished group, comprising 78.4%, and a group who were at risk of undernutrition, comprising 21.6% at baseline. A total of 115 subjects participated in the follow-up study. The mortality rate and the prevalence of various morbidity indicators were compared between the different risk groups. The analysis showed that subjects with a high MNA score (> or = 24) had significantly lower mortality (rate ratio estimate: 0.35; 95% Cl 0.18, 0.66) compared with subjects with a low MNA score (< or = 23.5). In contrast, the NSI Checklist score was not a significant predictor of mortality (rate ratio estimate: 1.45; 95% Cl 0.78, 2.71). The sixteen Danes judged to be at high nutritional risk by the NSI Checklist in 1988, had more acute diseases (P < 0.001) than the rest of the participants, between 1988 and 1993. No significant differences were found in the participation rates, hospitalization rates, physician visits, need of help or weight loss between the groups. The thirteen Danes judged to be at risk of undernutrition in 1988 by the MNA, had a lower participation rate (P < 0.01) and higher occurrence of acute disease (P < 0.05), need of help (P < 0.05), and weight loss (P < 0.001) than the well-nourished group, between 1988 and 1993. No significant differences were found in hospitalization rates and physician visits between the two groups. In conclusion, the results indicate that modified versions of the NSI Checklist and the MNA are capable of identifying a group of 70-75-year-old subjects with increased risk of certain nutrition-related health problems. Further, an MNA score < or = 23.5 predicts mortality in a Danish population."
        },
        {
            "title": "Gamma Knife radiosurgery in the treatment of ocular melanoma.",
            "abstract": "Ocular melanoma, the most common primary adult intraocular neoplasm, has a very high morbidity and mortality despite multiple alternative therapies. Consequently, we decided to study Gamma Knife radiosurgery (GKR) for the treatment of ocular melanoma in a clinical trial format. We describe the modifications of standard GKR to treat intraocular tumors, the objective, inclusion and exclusion criteria, treatment parameters, measurement of effect, and statistical evaluation of the clinical trial. The major adverse effects (i.e., cataract, vitreous hemorrhage, blindness, and loss of the eye) of the study are correctable or infrequent, whereas the benefits (i.e., tumor stasis or regression, preservation of sight, lower metastatic potential, and preservation of the globe) are substantial. This study is designed to evaluate GKR as a treatment option for ocular melanoma, although results are not yet available."
        },
        {
            "title": "Ten-year all-cause mortality and its association with vision among Indigenous Australians within Central Australia: the Central Australian Ocular Health Study.",
            "abstract": "Background:\n        \n      \n      No studies to date have explored the association of vision with mortality in Indigenous Australians. We aimed to determine the 10-year all-cause mortality and its associations among Indigenous Australians living in Central Australia.\n    \n\n\n          Design:\n        \n      \n      Prospective observational cohort study.\n    \n\n\n          Participants:\n        \n      \n      A total of 1257 (93.0%) of 1347 patients from The Central Australian Ocular Health Study, over the age of 40 years, were available for follow-up during a 10-year period.\n    \n\n\n          Methods:\n        \n      \n      All-cause mortality and its associations with visual acuity, age and gender were analysed.\n    \n\n\n          Main outcome measures:\n        \n      \n      All-cause mortality.\n    \n\n\n          Results:\n        \n      \n      All-cause mortality was 29.3% at the end of 10 years. Mortality increased as age of recruitment increased: 14.2% (40-49 years), 22.6% (50-59 years), 50.3% (60 years or older) (χ = 59.15; P < 0.00001). Gender was not associated with mortality as an unadjusted variable, but after adjustment with age and visual acuity, women were 17.0% less likely to die (t = 2.09; P = 0.037). Reduced visual acuity was associated with increased mortality rate (5% increased mortality per one line of reduced visual acuity; t = 4.74; P < 0.0001) after adjustment for age, sex, diabetes and hypertension.\n    \n\n\n          Conclusions:\n        \n      \n      The 10-year all-cause mortality rate of Indigenous Australians over the age of 40 years and living in remote communities of Central Australia was 29.3%. This is more than double that of the Australian population as a whole. Mortality was significantly associated with visual acuity at recruitment. Further work designed to better understand this association is warranted and may help to reduce this disparity in the future."
        },
        {
            "title": "A 1993 update on biohazards affecting clinical engineers and BMETs.",
            "abstract": "In late 1993, a survey was sent to the subscribers of the Journal of Clinical Engineering to determine: (1) if employers were responding to the biohazards protection needs of BMETs and CEs; (2) if personal protective equipment was being utilized; (3) if occupational exposure perceptions and concerns had changed since a previous survey four years ago; and (4) if educational efforts targeting BMETs and CEs were still needed. The 267 respondents were divided into four groups according to certification status and job title. Results showed that employers were doing an excellent job of providing personal protective equipment, but 50% of the BMETs and CEs chose not to use it. Even though though the occupational exposure illness and disability rate increased to 21% from the 1989 survey rate of 18%, BMETs and CEs still feel confident in their abilities to recognize potential biohazard problems based on visual clues, which can be an error in judgement that can lead to serious adverse effects including illness, disability, or death."
        },
        {
            "title": "A 12-lead ECG-method for quantifying ischemia-induced QRS prolongation to estimate the severity of the acute myocardial event.",
            "abstract": "Introduction:\n        \n      \n      Studies have shown terminal QRS distortion and resultant QRS prolongation during ischemia to be a sign of low cardiac protection and thus a faster rate of myocardial cell death. A recent study introduced a single lead method to quantify the severity of ischemia by estimating QRS prolongation. This paper introduces a 12-lead method that, in contrast to the previous method, does not require access to a prior ECG.\n    \n\n\n          Methods:\n        \n      \n      QRS duration was estimated in the lead that showed the maximal ST deviation according to a novel method. The degree of prolongation was determined by subtracting the measured QRS duration in the lead that showed the least ST deviation.\n    \n\n\n          Results:\n        \n      \n      The method is demonstrated in examples of acute occlusion in two of the major coronary arteries.\n    \n\n\n          Conclusion:\n        \n      \n      This paper presents a 12-lead method to quantify the severity of ischemia, by measuring QRS prolongation, without requiring comparison with a previous ECG."
        },
        {
            "title": "Glaucoma-associated long-term mortality in a rural cohort from India: the Andhra Pradesh Eye Disease Study.",
            "abstract": "Aim:\n        \n      \n      To evaluate glaucoma-associated mortality in a rural cohort in India.\n    \n\n\n          Methods:\n        \n      \n      The study cohort comprised individuals aged 40 years and above who took part in the Andhra Pradesh Eye Disease Study (APEDS1) during 1996-2000. All participants underwent detailed comprehensive eye examination. Glaucoma was defined using International Society of Geographic and Epidemiologic Ophthalmology criteria. This cohort was followed up after a decade (June 2009 to January 2010; APEDS2). Mortality HR analysis for ocular risk factors was performed using Cox proportional hazards regression after adjusting for sociodemographic, lifestyle and clinical variables.\n    \n\n\n          Results:\n        \n      \n      In APEDS1, 2790 individuals aged more than or equal to 40 years were examined. 47.4% were male. Forty-five participants had primary open angle glaucoma (POAG) and 66 had primary angle closure disease (PACD). Ten years later, 1879 (67.3%) were available, 739 (26.5%) had died and 172 (6.2%) had migrated; whereas 22 of the 45 (48.8%) with POAG and 22 of the 66 (33.3%) with PACD had died. In univariate analysis, a higher mortality was associated with POAG (HR 1.9; 95% CI 1.23 to 2.94), pseudoexfoliation (HR 2.79; 95% CI 2.0 to 3.89), myopia (HR 1.78; 95% CI 1.54 to 2.06) and unit increase in cup:disc ratio (HR 4.49; 95% CI 2.64 to 7.64). In multivariable analysis, only cup:disc ratio remained independently associated with mortality (HR 2.5; 95% CI 1.3 to 5.1). The association remained significant when other ocular parameters were included in the model (HR 2.1; 95% CI 1.03 to 4.2).\n    \n\n\n          Conclusions:\n        \n      \n      This is the first longitudinal study to assess the association of glaucoma and mortality in a rural longitudinal cohort in India. Increased cup:disc ratio could be a potential marker for ageing and would need further validation."
        },
        {
            "title": "Extracolonic findings in an asymptomatic screening population undergoing intravenous contrast-enhanced computed tomography colonography.",
            "abstract": "Background and aim:\n        \n      \n      The purpose of this study was to evaluate extracolonic findings that could be encountered with computed tomography colonography (CTC) using intravenous (IV) contrast material in an asymptomatic screening population.\n    \n\n\n          Methods:\n        \n      \n      Intravenous contrast medium-enhanced CTC was performed in 2230 asymptomatic adults (mean age, 57.5 years). Axial images were prospectively examined for extracolonic lesions. These findings were classified into three categories: potentially important findings, likely unimportant findings, and clinically unimportant findings. Potentially important extracolonic findings were defined as those which required immediate further diagnostic studies and treatment. Clinical and radiologic follow up, missed lesions and clinical outcomes were assessed using medical records (mean duration of follow up, 1.6 years).\n    \n\n\n          Results:\n        \n      \n      A total of 115 new potentially important findings in 5.2% of subjects (115/2230) were found. Subsequent medical or surgical intervention was performed in 2.0% (45/2230). New extracolonic cancer was detected in 0.5% (12/2230), and the majority of them (83.3%) were not metastasized. Computed tomography colonography missed eight potentially important extracolonic findings in eight subjects (0.4%, 8/2230): 0.8-cm early-stage prostatic cancer, six adrenal mass and one intraductal papillary mucinous tumor. There were no severe life-threatening complications related to contrast medium.\n    \n\n\n          Conclusion:\n        \n      \n      Intravenous contrast-enhanced CTC could safely detect asymptomatic early-stage extracolonic malignant diseases without an unreasonable number of additional work-ups, thus reducing their morbidity or mortality."
        },
        {
            "title": "Silent neurological involvement in Behçet's disease.",
            "abstract": "Objective:\n        \n      \n      The aim of this study was to determine the long term clinical course and prognosis of subclinical ('silent') neurological involvement in Behçet's disease (BD).\n    \n\n\n          Methods:\n        \n      \n      We included patients with BD who did not have any neurological complaints other than headache, dizziness or other non-specific complaints, that showed abnormal neurological findings (Silent Group). We compared these patients with the patients with overt parenchymal neuro-Behçet's disease (Overt Group). Cases with at least 8 years of follow-up were included.\n    \n\n\n          Results:\n        \n      \n      There were 22 patients in the Silent Group (15M, 7F), with a mean follow-up of 12.8 +/- 4 years. Magnetic resonance imaging was abnormal in 8 of 21 patients, while neuropsychological testing revealed mild abnormalities in 15 of 20 patients. During the follow up period, 3 patients of the Silent Group had 4 overt neurological attacks. In the last visit, 21 patients were independent, while one that had previously developed overt neurological attack was deceased. The Overt Group consisted of 51 patients (45M, 6F). In the Overt Group the ratio of males was higher, nearing a marginal significance (p = 0.051); whereas age at onset of BD, and frequency of other organ manifestations of BD were not different. In the Overt Group at the final visit, 19 patients were independent (37%), while the remaining were either dependent to others, or deceased, which was significantly higher when compared to the Silent Group(p=0.005).\n    \n\n\n          Conclusion:\n        \n      \n      Silent neurological involvement in BD seems to represent a milder form of the disease, since the mortality and disability rate in this group is significantly low."
        },
        {
            "title": "Prospective independent validation of APACHE III models in an Australian tertiary adult intensive care unit.",
            "abstract": "Evaluation of the performance of the APACHE III (Acute Physiology and Chronic Health Evaluation) ICU (intensive care unit) and hospital mortality models at the Princess Alexandra Hospital, Brisbane is reported. Prospective collection of demographic, diagnostic, physiological, laboratory, admission and discharge data of 5681 consecutive eligible admissions (1 January 1995 to 1 January 2000) was conducted at the Princess Alexandra Hospital, a metropolitan Australian tertiary referral medical/surgical adult ICU ROC (receiver operating characteristic) curve areas for the APACHE III ICU mortality and hospital mortality models demonstrated excellent discrimination. Observed ICU mortality (9.1%) was significantly overestimated by the APACHE III model adjusted for hospital characteristics (10.1%), but did not significantly differ from the prediction of the generic APACHE III model (8.6%). In contrast, observed hospital mortality (14.8%) agreed well with the prediction of the APACHE III model adjusted for hospital characteristics (14.6%), but was significantly underestimated by the unadjusted APACHE III model (13.2%). Calibration curves and goodness-of-fit analysis using Hosmer-Lemeshow statistics, demonstrated that calibration was good with the unadjusted APACHE III ICU mortality model, and the APACHE III hospital mortality model adjusted for hospital characteristics. Post hoc analysis revealed a declining annual SMR (standardized mortality rate) during the study period. This trend was present in each of the non-surgical, emergency and elective surgical diagnostic groups, and the change was temporally related to increased specialist staffing levels. This study demonstrates that the APACHE III model performs well on independent assessment in an Australian hospital. Changes observed in annual SMR using such a validated model support an hypothesis of improved survival outcomes 1995-1999."
        },
        {
            "title": "Diabetes mellitus--long time survival.",
            "abstract": "The medical literature of the last decade enables us to estimate survival of diabetics. Insulin dependent diabetic (IDDM) present a 3 to 6-fold mortality and die after age 30, the most frequent causes being end stage renal and vascular diseases. Non insulin-dependent diabetic (NIDDM) mortality is 1.4 to 3.7 times that of non-diabetics. Cardiovascular events and strokes are the major causes of death. Pancreatic carcinoma occurs twice as frequently in NIDDM compared to non-diabetics. Early markers of late severe complications are hypertension and proteinuria. Retinopathy has little influence on morality if other risk factors are considered. Yet, glaucoma and lens changes are associated with three- and twofold mortalities. One of five IDDM with microalbuminuria progresses to overt nephropathy in 5 years. In NIDDM micro-albuminuria predicts cardiovascular disease with a mortality of up to 2 times. Careful treatment of cardiovascular risk factors and of microalbuminuria combined with optimal metabolic control substantially reduces mortality of diabetics."
        },
        {
            "title": "Performance status, but not the hematopoietic cell transplantation comorbidity index (HCT-CI), predicts mortality at a Canadian transplant center.",
            "abstract": "The hematopoietic cell transplantation-specific comorbidity index (HCT-CI) was developed at a single center to predict outcomes for allogeneic transplant recipients who have comorbidities. The HCT-CI has not been widely validated in unselected transplant recipients. We evaluated whether the HCT-CI and other readily available pre-transplant variables predicted NRM and OS at a Canadian transplant center. Using a prospective cohort design, we analyzed consecutive adult allogeneic HCT recipients. Of 187 patients, HCT-CI risk was low in 22 (12%), intermediate in 50 (27%), high in 104 (55%) and undetermined in 11 (6%). Two-year OS was 45% (95% CI: 24-64%), 55% (95% CI: 40-68%) and 42% (95% CI: 32-51%) in the low, intermediate and high-risk HCT-CI groups, respectively. Two-year NRM was 36% (95% CI: 17-56%), 26% (95% CI: 15-39%) and 30% (95% CI: 22-39%) in the low, intermediate and high-risk HCT-CI groups, respectively. In multivariate analysis, the HCT-CI failed to predict OS or NRM. However, KPS of <90% at HCT was a strong predictor of NRM. In conclusion, the HCT-CI was not associated with NRM or OS. In contrast, KPS was an independent indicator of survival. International multi-center studies are required before the HCT-CI is used in clinical practice."
        },
        {
            "title": "Vision screening of older drivers for preventing road traffic injuries and fatalities.",
            "abstract": "Background:\n        \n      \n      Demographic data in North America, Europe, Asia, Australia and New Zealand suggest a rapid growth in the number of persons over the age of 65 years as the baby boomer generation passes retirement age. As older adults make up an increasing proportion of the population, they are an important consideration when designing future evidence-based traffic safety policies, particularly those that lead to restrictions or cessation of driving. Research has shown that cessation of driving among older drivers can lead to negative emotional consequences such as depression and loss of independence. Older adults who continue to drive tend to do so less frequently than other demographic groups and are more likely to be involved in a road traffic crash, possibly due to what is termed the \"low mileage bias\". Available research suggests that older driver crash risk estimates based on traditional exposure measures are prone to bias. When annual driving distances are taken in to consideration, older drivers with low driving distances have an increased crash risk, while those with average or high driving distances tend to be safer drivers when compared to other age groups. In addition, older drivers with lower distance driving tend to drive in urban areas which, due to more complex and demanding traffic patterns, tend to be more accident-prone. Failure to control for actual annual driving distances and driving locations among older drivers is referred to as \"low mileage bias\" in older driver mobility research. It is also important to note that older drivers are more vulnerable to serious injury and death in the event of a traffic crash due to changes in physiology associated with normal ageing. Vision, cognition, and motor functions or skills (e.g., strength, co-ordination, and flexibility) are three key domains required for safe driving. To drive safely, an individual needs to be able to see road signs, road side objects, traffic lights, roadway markings, other vulnerable road users, and other vehicles on the road, among many other cues-all while moving, and under varying light and weather conditions. It is equally important that drivers must have appropriate peripheral vision to monitor objects and movement to identify possible threats in the driving environment. It is, therefore, not surprising that there is agreement among researchers that vision plays a significant role in driving performance. Several age-related processes/conditions impair vision, thus it follows that vision testing of older drivers is an important road safety issue. The components of visual function essential for driving are acuity, static acuity, dynamic acuity, visual fields, visual attention, depth perception, and contrast sensitivity. These indices are typically not fully assessed by licensing agencies. Also, current vision screening regulations and cut-off values required to pass a licensing test vary from country to country. Although there is a clear need to develop evidence-based and validated tools for vision screening for driving, the effectiveness of existing vision screening tools remains unclear. This represents an important and highly warranted initiative to increase road safety worldwide.\n    \n\n\n          Objectives:\n        \n      \n      To assess the effects of vision screening interventions for older drivers to prevent road traffic injuries and fatalities.\n    \n\n\n          Search methods:\n        \n      \n      For the update of this review we searched the Cochrane Injuries Group's Specialised Register, the Cochrane Central Register of Controlled Trials (CENTRAL) (The Cochrane Library), MEDLINE (OvidSP), Embase (OvidSP), PsycINFO (OvidSP) and ISI Web of Science: (CPCI-S & SSCI). The searches were conducted up to 26 September 2013.\n    \n\n\n          Selection criteria:\n        \n      \n      Randomised controlled trials (RCTs) and controlled before and after studies comparing vision screening to non-screening of drivers aged 55 years and older, and which assessed the effect on road traffic crashes, injuries, fatalities and any involvement in traffic law violations.\n    \n\n\n          Data collection and analysis:\n        \n      \n      Two review authors independently screened the reference lists for eligible articles and independently assessed the articles for inclusion against the criteria. If suitable trials had been available, two review authors would have independently extracted data using a standardised extraction form.\n    \n\n\n          Main results:\n        \n      \n      No studies were found that met the inclusion criteria for this review.\n    \n\n\n          Authors' conclusions:\n        \n      \n      Most countries require a vision screening test for the renewal of an individual's driver's licence. There is, however, lack of methodologically sound studies to assess the effects of vision screening tests on subsequent motor vehicle crash reduction. There is a need to develop valid and reliable tools of vision screening that can predict driving performance."
        },
        {
            "title": "Usefulness of T2 ratio in the diagnosis and prognosis of cardiac amyloidosis using cardiac MR imaging.",
            "abstract": "Purpose:\n        \n      \n      To detect if a difference of T2 ratio, defined as the signal intensity (SI) of the myocardium divided by the SI of the skeletal muscle on T2-weigthed cardiac magnetic resonance (CMR) imaging, exists between patients with systemic amyloidosis, by comparison to control subjects. To determine if a relationship exists between T2 ratio and the overall mortality.\n    \n\n\n          Materials and methods:\n        \n      \n      CMR imaging examinations of 73 consecutive patients (48 men, 25 women; mean age, 63 years±15[SD]) with amyloidosis and suspicion of CA and 27 control subjects were retrospectively analyzed after institutional review board approval. Final diagnosis of CA was retained in case of histological confirmation of CA, typical pattern of CA on imaging and/or positivity of 99Technetium-hydroxymethylene diphosphonate scintigraphy. Patients were divided in 2 groups according to the presence or the absence of CA. T2 ratios were calculated in patients with and those without CA and in control subjects with using analysis of variance. Prognostic value of T2 ratio was studied with a Kaplan-Meier curve.\n    \n\n\n          Results:\n        \n      \n      Thirty-five patients (51%) had CA and 33 (49%) were free from CA. T2 ratio was lower in patients with CA (1.18±0.29) than in patients without cardiac involvement (1.37±0.35) (P=0.03) and control subjects (1.45±0.24) (P=0.004). A T2 ratio of 1.36 was the best threshold value for predicting CA with a sensitivity of 63% and a specificity of 73%. Kaplan-Meier analysis showed a significant relationship between a shortened overall survival and a T2 ratio<1.36.\n    \n\n\n          Conclusion:\n        \n      \n      Patients with CA exhibit lower T2 ratio on CMR imaging by comparison with patients free of CA and control subjects."
        },
        {
            "title": "Thrombus detection in the left atrial appendage using contrast-enhanced MRI: a pilot study.",
            "abstract": "Objective:\n        \n      \n      Left atrial thrombi are an important cause for embolism-related morbidity and mortality. Transesophageal echocardiography (TEE), the clinical reference, is semiinvasive; thus, we aimed to assess the value of contrast-enhanced cardiovascular MRI for the detection of thrombus in the left atrial appendage.\n    \n\n\n          Conclusion:\n        \n      \n      The image quality was good for both 2D perfusion (grade 4 +/- 1) and 3D turbo fast low-angle shot (FLASH) (grade 4 +/- 1, n.s.). Compared with TEE, 2D perfusion, 3D turboFLASH, and the combination of both techniques yielded sensitivities of 47/35/44%, specificities of 50/67/67%, positive predictive values of 73/75/80%, and negative predictive values of 25/27/29%, respectively. The size of the thrombus was overestimated by 2D perfusion (66%) and by 3D turboFLASH (25%) and agreement for location and shape of thrombus was 50% and 75% for 2D perfusion and 75% and 50% for 3D turboFLASH, respectively. The TEE thrombus size was significantly larger in patients with true-positive diagnoses by 2D perfusion (148%) and by 3D turboFLASH (151%) when compared with patients with false-negative diagnoses (p < 0.05 for both). No such difference was found for image quality, time delay between TEE and MRI examination, and location and shape of thrombi. Contrast-enhanced MRI lacks diagnostic accuracy for the detection of thrombi in the left atrial appendage. Future technical improvements are essential to establish this technique as a noninvasive alternative to TEE."
        },
        {
            "title": "The prognostic value of T1 mapping and late gadolinium enhancement cardiovascular magnetic resonance imaging in patients with light chain amyloidosis.",
            "abstract": "Background:\n        \n      \n      Cardiac impairment is associated with high morbidity and mortality in immunoglobulin light chain (AL) type amyloidosis, for which early identification and risk stratification is vital. For myocardial tissue characterization, late gadolinium enhancement (LGE) is a classic and most commonly performed cardiovascular magnetic resonance (CMR) parameter. T1 mapping with native T1 and extracellular volume (ECV) are recently developed quantitative parameters. We aimed to investigate the prognostic value of native T1, ECV and LGE in patients with AL amyloidosis.\n    \n\n\n          Methods:\n        \n      \n      Eighty-two patients (55.5 ± 8.5 years; 52 M) and 20 healthy subjects (53.2 ± 11.7 years; 10 M) were prospectively recruited. All subjects underwent CMR with LGE imaging and T1 mapping using a Modified Look-Locker Inversion-recovery (MOLLI) sequence on a 3 T scanner. Native T1 and ECV were measured semi-automatically using a dedicated CMR software. The left ventricular (LV) LGE pattern was classified as none, patchy, and global groups. Global LGE was considered when there was diffuse, transmural LGE in more than half of the short axis images. Follow-up was performed for all-cause mortality using Cox proportional hazards regression analysis and Kaplan-Meier survival curves.\n    \n\n\n          Results:\n        \n      \n      The patients demonstrated an increase in native T1 (1438 ± 120 ms vs. 1283 ± 46 ms, P = 0.001) and ECV (43.9 ± 10.9% vs. 27.0 ± 1.7%, P = 0.001) compared to healthy controls. Native T1, ECV and LGE showed significant correlation with Mayo Stage, and ECV and LGE showed significant correlation with echocardiographic E/E' and LV ejection fraction. During the follow-up for a median time of 8 months, 21 deaths occurred. ECV ≥ 44.0% (hazard ratio [HR] 7.249, 95% confidence interval (CI) 1.751-13.179, P = 0.002) and global LGE (HR 4.804, 95% CI 1.971-12.926, P = 0.001) were independently prognostic for mortality over other clinical and imaging parameters. In subgroups with the same LGE pattern, ECV ≥ 44.0% remained prognostic (log rank P = 0.029). Median native T1 (1456 ms) was not prognostic for mortality (Tarone-Ware, P = 0.069).\n    \n\n\n          Conclusions:\n        \n      \n      During a short-term follow-up, both ECV and LGE are independently prognostic for mortality in AL amyloidosis. In patients with a similar LGE pattern, ECV remained prognostic. Native T1 was not found to be a prognostic factor."
        },
        {
            "title": "Severe vision and hearing impairment and successful aging: a multidimensional view.",
            "abstract": "Purpose:\n        \n      \n      Previous research on psychosocial adaptation of sensory-impaired older adults has focused mainly on only one sensory modality and on a limited number of successful aging outcomes. We considered a broad range of successful aging indicators and compared older adults with vision impairment, hearing impairment, and dual sensory impairments and without sensory impairment.\n    \n\n\n          Design and methods:\n        \n      \n      Data came from samples of severely visually impaired (VI; N = 121), severely hearing-impaired (HI; N = 116), dual sensory-impaired (DI; N = 43), and sensory-unimpaired older adults (UI; N = 150). Participants underwent a wide-ranging assessment, covering everyday competence, cognitive functioning, social resources, self-regulation strategies, cognitive and affective well-being, and 4-year survival status (except the DI group).\n    \n\n\n          Results:\n        \n      \n      The most pronounced difference among groups was in the area of everyday competence (lowest in VI and DI). Multigroup comparisons in latent space revealed both similar and differing relationship strengths among health, everyday competence, social resources, self-regulation, and overall well-being, depending on sensory status. After 4 years, mortality in VI (29%) and HI (30%) was significantly higher than in UI (20%) at the bivariate level, but not after controlling for confounders in a multivariate analysis.\n    \n\n\n          Implications:\n        \n      \n      A multidimensional approach to the understanding of sensory impairment and psychosocial adaptation in old age reveals a complex picture of loss and maintenance."
        },
        {
            "title": "Awareness of husband's impending death from cancer and long-term anxiety in widowhood: a nationwide follow-up.",
            "abstract": "Background:\n        \n      \n      We investigated the predictors and long-term consequences of awareness time - the length of time a woman is aware of her husband's impending death from cancer.\n    \n\n\n          Methods:\n        \n      \n      All women (n = 506) living in Sweden under 80 years of age who lost their husband/partner owing to cancer of the prostate in 1996 or of the urinary bladder in 1995 or 1996 were followed with an anonymous postal questionnaire, 2-4 years after their loss.\n    \n\n\n          Results:\n        \n      \n      We received completed questionnaires from 379 of the widows. Of these, 55 (15%) reported an awareness time of 24 hours or less, 56 (15%) of 3-6 months and 95 (26%) of one year or more. The associations between the awareness time and morbidity were of a reverted 'J-shape,' with awareness time of 24 hours or less carrying the highest risk and 3-6/6-12 months the lowest. On comparing the awareness time of 24 hours or less with 3-6 months (preformed response category), the relative risks for anxiety were found to be 1.9. (1.0-3.6) (visual digital scale) and 4.5 (1.0-20.0) for intake of tranquillising drugs. Those not informed of their husband's fatal condition or not provided with psychological support by caregivers during their husband's last months of life had an increased risk of a short awareness time.\n    \n\n\n          Conclusions:\n        \n      \n      During a man's terminal cancer illness, the wife's awareness time varies considerably and is influenced by information and psychological support from caregivers. A short awareness time may result in an additional and avoidable psychological trauma."
        },
        {
            "title": "Pterional surgery of meningiomas of the tuberculum sellae and planum sphenoidale: surgical results with special consideration of ophthalmological and endocrinological outcomes.",
            "abstract": "Object:\n        \n      \n      The authors reviewed 47 cases of suprasellar meningiomas with special attention to ophthalmological and endocrinological outcomes.\n    \n\n\n          Methods:\n        \n      \n      All patients underwent surgery performed via a unilateral pterional approach between January 1983 and January 1998. Ophthalmological and endocrinological examinations were performed before the operation as well as 1 week and 3 months postoperatively. A special scoring system was adopted to quantify the extent of ophthalmological disturbances. Complete tumor resection was possible in all but one patient. There were no fatalities and the rate of visual improvement was 80%. The best prognoses were found in patients younger than 50 years and in patients in whom the duration of symptoms was less than 1 year. Before surgery, tumor-related endocrine disturbances were present in only three women who suffered from secondary hypogonadism; two of these patients recovered after surgery. Postoperatively, no patient needed replacement therapy for pituitary dysfunction. The overall tumor recurrence rate was 2.1% (one of 47 cases). For patients in whom long-term (> 5 years) follow-up data were available, the recurrence rate was 4.2% (one of 24 cases).\n    \n\n\n          Conclusions:\n        \n      \n      In this series, complete resection of suprasellar meningiomas was possible through a unilateral pterional craniotomy and was associated with a low morbidity rate and no deaths."
        },
        {
            "title": "Validation of a stroke symptom questionnaire for epidemiological surveys.",
            "abstract": "Context and objective:\n        \n      \n      Stroke is a relevant issue within public health and requires epidemiological surveillance tools. The aim here was to validate a questionnaire for evaluating individuals with stroke symptoms in the Stroke Morbidity and Mortality Study (Estudo de Mortalidade e Morbidade do Acidente Vascular Cerebral, EMMA), São Paulo, Brazil.\n    \n\n\n          Design and setting:\n        \n      \n      This was a cross-sectional study performed among a sample of the inhabitants of Butantã, an area in the western zone of the city of São Paulo.\n    \n\n\n          Methods:\n        \n      \n      For all households in the coverage area of a primary healthcare unit, household members over the age of 35 years answered a stroke symptom questionnaire addressing limb weakness, facial weakness, speech problems, sensory disorders and impaired vision. Thirty-six participants were randomly selected for a complete neurological examination (gold standard).\n    \n\n\n          Results:\n        \n      \n      Considering all the questions in the questionnaire, the sensitivity was 72.2%, specificity was 94.4%, positive predictive value was 92.9% and negative predictive value was 77.3%. The positive likelihood ratio was 12.9, the negative likelihood ratio was 0.29 and the kappa coefficient was 0.67. Limb weakness was the most sensitive symptom, and speech problems were the most specific.\n    \n\n\n          Conclusions:\n        \n      \n      The stroke symptom questionnaire is a useful tool and can be applied by trained interviewers with the aim of identifying community-dwelling stroke patients, through the structure of the Family Health Program."
        },
        {
            "title": "An incidence estimation model for multi-stage diseases with differential mortality.",
            "abstract": "Prevalence and incidence are two important measures of the impact of a disease. For many diseases, incidence is the most useful measure for response planning. However, the longitudinal studies needed to calculate incidence are resource-intensive, so prevalence estimates are often more readily available. In 1986, Podgor and Leske (Statistics in Medicine, 5:573-578, 1986) developed a model to estimate incidence of a single disease from one survey of age-specific prevalence, even where the presence of the disease increases the mortality rate of patients. Here, we extend their model to the case of progressive diseases, where the incidence of all disease stages is desired. As an example, we consider the case of cataract disease in Africa, where ophthalmologists wish to distinguish between unilateral and bilateral cataract incidence in order to plan the number of cataract surgeries needed to prevent the occurrence of blindness as a result of the disease. Our method has successfully provided cataract incidence estimates on the basis of prevalence data from new Rapid Assessment of Avoidable Blindness surveys in Africa (Lewallen et al., Archives of Ophthalmology, 128(12):1584-1589, 2010). In this paper, we provide a more general form of the model in order to promote its applicability to other diseases."
        },
        {
            "title": "Clinical Features and Outcomes of Takayasu Arteritis with Neurological Symptoms in China: A Retrospective Study.",
            "abstract": "Objective:\n        \n      \n      To describe the clinical features and longterm outcomes of patients with Takayasu arteritis (TA) in China who experienced neurological symptoms.\n    \n\n\n          Methods:\n        \n      \n      A retrospective study was undertaken of patients with TA who attended a single study center from 2002 to 2013, who also exhibited neurological symptoms (n = 274). Clinical and imaging features were analyzed, as well as longterm outcomes.\n    \n\n\n          Results:\n        \n      \n      The mean age at disease onset was 28.2 ± 11.2 years, with a female-to-male ratio of 4.3:1. The most common neurological manifestation was dizziness (214, 78.1%), the most frequent type of TA was type III (112, 40.9%), and the most common affected artery was the left subclavian (147, 53.6%). Involvement of 3 or 4 branches of the aortic arch was observed in 28% of patients. Among 30 patients experiencing a stroke (10.9%), steno-occlusive lesions of the subclavian artery and common carotid artery were frequently observed in patients with ischemic stroke, while steno-occlusive lesions of the descending aorta, abdominal aorta, and/or renal arteries were more frequently observed with hemorrhagic stroke. Heart failure was the most common cardiovascular event in those who died (n = 6) and in surviving cohorts.\n    \n\n\n          Conclusion:\n        \n      \n      Neurological features in patients with TA were variable, and correlated with the number of arteries and the site of artery involvement. Resistant hypertension was one of the most important risk factors for hemorrhagic stroke in patients with TA."
        },
        {
            "title": "Prevalence of end-of-life visual impairment in patients followed for glaucoma.",
            "abstract": "Purpose:\n        \n      \n      To assess the prevalence of end-of-life visual impairment in patients followed for glaucoma.\n    \n\n\n          Methods:\n        \n      \n      Data of 122 patients followed for glaucoma who had died between July 2008 and July 2010 and who had visited the ophthalmology outpatient department of a large non-academic Dutch hospital were collected from the medical files. Sixty-one patients had open-angle glaucoma (OAG), and 61 patients were suspect for glaucoma or had ocular hypertension (OHT). Visual impairment was defined as a mean deviation value <-15 dB or a Snellen visual acuity <0.3 (20/60) of the better eye. We determined the number of patients with visual impairment on the last patient visit before death and investigated its main explanations.\n    \n\n\n          Results:\n        \n      \n      Overall, the mean age at death was 81.8 years after a mean follow-up period of 9.2 years. Seventy-three per cent of all patients had their last visit in the year preceding death. In OAG, 16 patients (26%) had an end-of-life visual impairment. In nine patients (15%), this was caused by glaucoma. Eight of them had substantial visual loss at the initial visit. Six (10%) impaired OAG cases were mainly explained by ocular comorbidity, and there was an equal contribution of comorbidity and glaucoma in one case. Five glaucoma suspects or patients with OHT (8%) were visually impaired at death and these were all caused by ocular comorbidity.\n    \n\n\n          Conclusion:\n        \n      \n      The prevalence of end-of-life visual impairment is considerable in patients with OAG. Substantial visual loss at baseline is an important contributing factor. In glaucoma suspects or patients with OHT, the prevalence is lower and can be attributed to ocular comorbidity."
        },
        {
            "title": "[Visual impairment -- implications on health, costs and predictions for the future].",
            "abstract": "Vision impairment in the elderly is significantly related to increased morbidity as well as mortality rates. Recent studies suggest the true costs associated with visual impairment and blindness to be much higher than reported previously. Without successful interventions the number of visually impaired or blind people world-wide will double within the next 20 years. The lack of a significant world-wide reduction in avoidable visual impairment and blindness would have substantial social as well as economical consequences. In contrast, three-quarters of all visual impairments are avoidable and therefore unnecessary. In summary, vision Impairment deserves a higher degree of attention by the public as well as from governments."
        },
        {
            "title": "Cause-specific visual impairment and mortality: results from a population-based study of older people in the United Kingdom.",
            "abstract": "Objective:\n        \n      \n      To assess the association between mortality and cause-specific visual impairment in older people.\n    \n\n\n          Methods:\n        \n      \n      Visual acuity and causes of visual impairment were collected in 13 569 participants 75 years and older participating in a randomized trial of health screening. Participants were followed up for mortality for a median of 6.1 years.\n    \n\n\n          Results:\n        \n      \n      Compared with those with 6/6 (or 20/20 Snellen) or better visual acuity, the age- and sex-adjusted rate ratio for visually impaired people (binocular visual acuity <6/18 or <20/60 Snellen) was 1.60 (95% confidence interval, 1.47-1.74), which was markedly attenuated (rate ratio, 1.17; 95% confidence interval, 1.07-1.27) after adjustment for confounding factors. People whose visual impairment was due to cataract or age-related macular degeneration had excess risks of all-cause and cardiovascular mortality, which disappeared after adjustment. People with refractive error remained at small risk, despite adjustment, probably owing to residual confounding from factors associated with minimal use of eye services rather than underlying eye disease. There were no associations with cancer mortality.\n    \n\n\n          Conclusion:\n        \n      \n      Associations reported for visual impairment and mortality or for specific causes of visual impairment reflect confounding by comorbidities, risk factors, and other factors related to susceptibility to death rather than an independent biological association of vision problems or specific eye diseases."
        },
        {
            "title": "Despair of Treatment: A Qualitative Study of Cirrhotic Patients' Perception of Treatment.",
            "abstract": "Cirrhotic patients are exposed to illness progression and life-threatening side effects. The nature of the disease, its incurability, limitations of liver transplantation, and the intensity of threatening conditions lead to psychological distress for the patients and change in their perception of the treatment. To provide holistic care, it is necessary to clarify the patient's perception of the treatment. The aim of this study was to clarify cirrhotic patients' perception of their treatment. This qualitative study was carried out through a content analysis approach. The participants were 15 cirrhotic patients. Data were collected via semistructured, in-depth interviews and analyzed on the basis of the Granheme and Landman method. Despair of treatment was revealed through four categories: (1) disease perception (quiet start and quiet death, living in an aggravating limitation, intensifying threatening conditions), (2) self-perception (living in the shadow of death, loss of self, preferring family to oneself), (3) perception of treatment (difficulty of treatment compliance, believed to be incurable, treatment conditioned to die, treatment limitation), and (4) spirituality-religion (destiny and divine test, asking God instead of doctors). The study shows that despair of treatment is considered as one of the main concerns of cirrhotic patients. Nurses should program their surveillance to support patients effectively based on the study findings."
        },
        {
            "title": "Clinically significant macular edema and survival in type 1 and type 2 diabetes.",
            "abstract": "Purpose:\n        \n      \n      To investigate the association of clinically significant macular edema (CSME) and long-term survival in individuals with type 1 and type 2 diabetes.\n    \n\n\n          Design:\n        \n      \n      Population-based cohort study.\n    \n\n\n          Methods:\n        \n      \n      The Wisconsin Epidemiologic Study of Diabetic Retinopathy (WESDR) is an ongoing prospective population-based cohort study initiated from August 21, 1980 through July 30, 1982 of individuals with diabetes diagnosed at either younger than 30 years of age (younger-onset group; n = 996) or 30 years of age or older (older-onset group; n = 1,370). Stereoscopic color retinal photographs were graded for retinopathy using the modified Airlie House classification scheme. CSME was defined by the Early Treatment Diabetic Retinopathy Study criteria.\n    \n\n\n          Results:\n        \n      \n      Prevalence of CSME was 5.9% and 7.5% for the younger- and older-onset groups, respectively. After 20 years of follow-up, 276 younger-onset and 1,197 older-onset persons died. When adjusting for age and gender, CSME was not significantly associated with all-cause mortality (hazard ratio [HR], 1.41; 95% confidence interval [CI], 0.96 to 2.07; P = .08) or ischemic heart disease mortality (HR, 1.14; 95% CI, 0.61 to 2.12; P = .68) in the younger-onset group. In the older-onset group, there was increased all-cause and ischemic heart disease mortality when CSME was present (HR, 1.55; 95% CI, 1.25 to 1.92; P < .01; and HR, 1.56; 95% CI, 1.15 to 2.13; P < .01, respectively), when adjusting for age and gender. After controlling for other risk factors, the association remained significant for ischemic heart disease (HR, 1.58; 95% CI, 1.07 to 2.35; P = .02) among those taking insulin. CSME was not significantly associated with stroke mortality in either group.\n    \n\n\n          Conclusions:\n        \n      \n      CSME seems to be a risk indicator for decreased survival in persons with older-onset diabetes mellitus. The presence of CSME may identify individuals who should be receiving care for detection and treatment of cardiovascular disease."
        },
        {
            "title": "[Relationship between carbon dioxide combining power and contrast- induced acute kidney injury in patients with ST segment elevation myocardial infarction undergoing emergency percutaneous coronary intervention].",
            "abstract": "Objective:\n        \n      \n      To study the relationship between carbon dioxide combining power(CO₂-CP) and contrast-induced acute kidney injury (CI-AKI) in patients with ST segment elevation myocardial infarction and undergoing percutaneous coronary intervention.\n    \n\n\n          Methods:\n        \n      \n      We retrospectively analyzed 174 patients admitted to our hospital from March 2012 to August 2013 with ST segment elevation myocardial infarction and underwent emergency percutaneous coronary intervention. Patients were divided into three tertiles according to pre-operative CO₂-CP: T1 (CO₂-CP < 22.62 mmol/L), T2(CO₂-CP 22.62-24.30 mmol/L), T3(CO₂-CP > 24.30 mmol/L). Baseline clinical data, CI-AKI incidence, in-hospital mortality and dialysis rate were compared among groups. An increase in serum creatinine of >26.4 µmol/L and/or >50% from baseline within 48 hours after contrast exposure was defined as CI-AKI. Univariate logistic regression analysis was used to identify the risk factors of CI-AKI. The relationship between CO₂-CP and CI-AKI was assessed by multivariate logistic regression analysis. Receiver operating characteristic curve was used to identify the optimal cutoff of the CO₂-CP for predicting CI-AKI.\n    \n\n\n          Results:\n        \n      \n      CI-AKI occurred in 25 (14.4%) patients, and lower CO₂-CP was related to higher incidence of CI-AKI (27.6% (16/58) in group T1, 5.3% (3/57) in group T2, 1.7 % (1/59) in group T3, P = 0.002) and higher in-hospital mortality (10.3% (6/58) vs. 0 and 1.7% (1/59), P = 0.010). Dialysis rate was similar among 3 groups (5.2% (3/58) vs. 0 and 1.7% (1/59), P = 0.168). The incidence of CI-AKI was significantly associated with CO₂-CP < 22.00 mmol/L in univariate analyses (OR = 6.767, 95% CI 2.731-16.768, P < 0.001). After adjusting for potential confounding risk factors, CO₂-CP < 22.00 mmol/L remained significantly associated with the incidence of CI-AKI (OR = 5.835, 95%CI 1.800-18.914, P = 0.003) in multivariate logistic regression. ROC analysis revealed that the optimal cutoff of CO₂-CP to predict CI-AKI was 22.00 mmol/L (sensitivity 64.0%, specificity 79.1%, AUC = 0.714).\n    \n\n\n          Conclusions:\n        \n      \n      Pre-percutaneous coronary intervention CO₂-CP in patients with ST segment elevation myocardial infarction undergoing percutaneous coronary intervention is related to CI-AKI. CO₂-CP < 22.00 mmol/L predicts higher risk of CI-AKI in this patient cohort."
        },
        {
            "title": "Dietary choline is positively related to overall and cause-specific mortality: results from individuals of the National Health and Nutrition Examination Survey and pooling prospective data.",
            "abstract": "Little is known about the association between dietary choline intake and mortality. We evaluated the link between choline consumption and overall as well as cause-specific mortality by using both individual data and pooling prospective studies by meta-analysis and systematic review. Furthermore, adjusted means of cardiometabolic risk factors across choline intake quartiles were calculated. Data from the National Health and Nutrition Examination Survey (1999-2010) were collected. Adjusted Cox regression was performed to determine the risk ratio (RR) and 95 % CI, as well as random-effects models and generic inverse variance methods to synthesise quantitative and pooling data, followed by a leave-one-out method for sensitivity analysis. After adjustments, we found that individuals consuming more choline had worse lipid profile and glucose homeostasis, but lower C-reactive protein levels (P < 0·001 for all comparisons) with no significant differences in anthropometric parameters and blood pressure. Multivariable Cox regression models revealed that individuals in the highest quartile (Q4) of choline consumption had a greater risk of total (23 %), CVD (33 %) and stroke (30 %) mortality compared with the first quartile (Q1) (P < 0·001 for all comparison). These results were confirmed in a meta-analysis, showing that choline intake was positively and significantly associated with overall (RR 1·12, 95 % CI 1·08, 1·17, I2 = 2·9) and CVD (RR 1·28, 95 % CI 1·17, 1·39, I2 = 9·6) mortality risk. In contrast, the positive association between choline consumption and stroke mortality became non-significant (RR 1·18, 95 % CI 0·97, 1·43, P = 0·092, I2 = 1·1). Our findings shed light on the potential adverse effects of choline intake on selected cardiometabolic risk factors and mortality risk."
        },
        {
            "title": "Risk prediction models for selection of lung cancer screening candidates: A retrospective validation study.",
            "abstract": "Background:\n        \n      \n      Selection of candidates for lung cancer screening based on individual risk has been proposed as an alternative to criteria based on age and cumulative smoking exposure (pack-years). Nine previously established risk models were assessed for their ability to identify those most likely to develop or die from lung cancer. All models considered age and various aspects of smoking exposure (smoking status, smoking duration, cigarettes per day, pack-years smoked, time since smoking cessation) as risk predictors. In addition, some models considered factors such as gender, race, ethnicity, education, body mass index, chronic obstructive pulmonary disease, emphysema, personal history of cancer, personal history of pneumonia, and family history of lung cancer.\n    \n\n\n          Methods and findings:\n        \n      \n      Retrospective analyses were performed on 53,452 National Lung Screening Trial (NLST) participants (1,925 lung cancer cases and 884 lung cancer deaths) and 80,672 Prostate, Lung, Colorectal and Ovarian Cancer Screening Trial (PLCO) ever-smoking participants (1,463 lung cancer cases and 915 lung cancer deaths). Six-year lung cancer incidence and mortality risk predictions were assessed for (1) calibration (graphically) by comparing the agreement between the predicted and the observed risks, (2) discrimination (area under the receiver operating characteristic curve [AUC]) between individuals with and without lung cancer (death), and (3) clinical usefulness (net benefit in decision curve analysis) by identifying risk thresholds at which applying risk-based eligibility would improve lung cancer screening efficacy. To further assess performance, risk model sensitivities and specificities in the PLCO were compared to those based on the NLST eligibility criteria. Calibration was satisfactory, but discrimination ranged widely (AUCs from 0.61 to 0.81). The models outperformed the NLST eligibility criteria over a substantial range of risk thresholds in decision curve analysis, with a higher sensitivity for all models and a slightly higher specificity for some models. The PLCOm2012, Bach, and Two-Stage Clonal Expansion incidence models had the best overall performance, with AUCs >0.68 in the NLST and >0.77 in the PLCO. These three models had the highest sensitivity and specificity for predicting 6-y lung cancer incidence in the PLCO chest radiography arm, with sensitivities >79.8% and specificities >62.3%. In contrast, the NLST eligibility criteria yielded a sensitivity of 71.4% and a specificity of 62.2%. Limitations of this study include the lack of identification of optimal risk thresholds, as this requires additional information on the long-term benefits (e.g., life-years gained and mortality reduction) and harms (e.g., overdiagnosis) of risk-based screening strategies using these models. In addition, information on some predictor variables included in the risk prediction models was not available.\n    \n\n\n          Conclusions:\n        \n      \n      Selection of individuals for lung cancer screening using individual risk is superior to selection criteria based on age and pack-years alone. The benefits, harms, and feasibility of implementing lung cancer screening policies based on risk prediction models should be assessed and compared with those of current recommendations."
        },
        {
            "title": "Predictors and subjective causes of loneliness in an aged population.",
            "abstract": "The aim of the study was to examine the prevalence and self-reported causes of loneliness among Finnish older population. The data were collected with a postal questionnaire from a random sample of 6,786 elderly people (>or=75 years of age). The response rate was 71.8% from community-dwelling sample. Of the respondents, 39% suffered from loneliness, 5% often or always. Loneliness was more common among rural elderly people than those living in cities. It was associated with advancing age, living alone or in a residential home, widowhood, low level of education and poor income. In addition, poor health status, poor functional status, poor vision and loss of hearing increased the prevalence of loneliness. The most common subjective causes for loneliness were illnesses, death of a spouse and lack of friends. Loneliness seems to derive from societal life changes as well as from natural life events and hardships originating from aging."
        },
        {
            "title": "Associations with weight loss and subsequent mortality risk.",
            "abstract": "Purpose:\n        \n      \n      Studies have shown a high prevalence of weight loss in older adults is associated with an increased risk of death. We investigated this in a population-based study.\n    \n\n\n          Methods:\n        \n      \n      Persons living in Beaver Dam, Wisconsin, participated in a baseline examination between 1988 and 1990 (n=4926). A medical examination and standardized questionnaire were administered. Weight loss was defined as percent loss in body weight from highest lifetime weight to measured weight at baseline.\n    \n\n\n          Results:\n        \n      \n      Weight loss was associated with older age, higher rates of diseases such as diabetes, and lower baseline levels of blood pressure and serum total cholesterol. After controlling for age, medical, and lifestyle factors, both men and women had higher mortality rates over a 10+ year period for increasing categories of weight loss (hazard ratio [ 95% CI]: 1.16 [1.06, 1.27] for men and 1.23 [1.13, 1.34] for women). Increased mortality rates with increasing weight loss was shown in stratified analyses of age, body mass index (BMI) at highest weight, smoking, and disease status, but did not always reach statistical significance. Persons on weight loss diets within the year prior to baseline did not have increased mortality with increasing weight loss.\n    \n\n\n          Conclusion:\n        \n      \n      The strong association between weight loss (likely involuntary) and mortality may be a useful way of estimating overall risks to longevity in populations."
        },
        {
            "title": "[Epidemiological study of SMON death--from the observation of death certificates during a 13-year 6-month period from July 1972].",
            "abstract": "Eight hundred and twelve vital statistics death certificates, in which either the cause of death or complications was SMON, were selected from all the death certificates filed in Japan in a 13-year 6-month period from July 1972 to December 1985. In 411, or 50.6%, of these certificates, the SMON Research Committee had records of the clinical pictures as a result of the nationwide surveys conducted earlier. The epidemiological picture presented by the death certificates and the relationship between the information in these certificates and clinical records in nationwide surveys was observed. The results are summarized as follows: 1. The male/female ratio revealed in the 812 death certificates was 0.53, which is higher than those reported in nationwide surveys (0.4). 2. Although the yearly number of certificates was more than 70 in 1974 or before, it decreased year by year. The numbers in 1984 and 1985 were less than 50. 3. The leading underlying causes of death were; SMON in 44.0% of all the certificates, heart diseases in 10.3%, cerebrovascular diseases in 9.2% and malignant neoplasm in 8.6%. 4. The presence of clinical symptoms was compared between death cases and total cases reported by the nationwide surveys. Symptoms such as visual impairment and walking incapacity were seen more frequently in death cases. 5. From these results, it was suggested that the level of the severity of the disease at onset influences the long-term prognosis of the disease, although selection bias in the death certificates observed is unavoidable."
        },
        {
            "title": "Impact of renal insufficiency in patients undergoing primary angioplasty for acute myocardial infarction.",
            "abstract": "Background:\n        \n      \n      The prognostic importance of renal insufficiency (RI) in patients undergoing primary percutaneous coronary intervention (PCI) for acute myocardial infarction (AMI) has not been well characterized.\n    \n\n\n          Methods and results:\n        \n      \n      PCI was performed in 2082 AMI patients without shock presenting within 12 hours of symptom onset in a prospective, multicenter randomized trial. RI was defined as a calculated (Cockroft-Gault) creatinine clearance (CrCl) < or =60 mL/min. RI at baseline was present in 18% of patients. Compared with patients without RI, patients with RI were older and were more likely to be female; to have hypertension, peripheral vascular disease, or cerebrovascular disease; and to present in heart failure. Mortality was markedly increased in patients with versus without baseline RI both at 30 days (7.5% versus 0.8%, P<0.0001) and at 1 year (12.7% versus 2.4%, P<0.0001). Mortality rates increased incrementally for every 10-mL/min decrease in baseline CrCl. By multivariate analysis, reduced baseline CrCl was a powerful independent predictor of 30-day mortality (hazard ratio, 5.77; P<0.0001) and remained associated with reduced survival at 1 year (hazard ratio, 1.98; P=0.08). Hemorrhagic complications and transfusion requirements were also increased more than 2-fold in patients with RI, as were severe restenosis (diameter stenosis > or =70%; 20.6% versus 11.8%, P=0.024) and infarct artery reocclusion (14.7% versus 7.3%, P=0.02).\n    \n\n\n          Conclusions:\n        \n      \n      Baseline RI in patients with AMI undergoing primary PCI is associated with a markedly increased risk of mortality, as well as bleeding and restenosis. Novel approaches are needed to improve the otherwise poor prognosis of patients with RI and AMI."
        },
        {
            "title": "Cost-effectiveness of ranibizumab in the treatment of visual impairment due to diabetic macular edema.",
            "abstract": "Objective Ranibizumab, an anti-vascular endothelial growth factor designed for ocular use, has been deemed cost-effective in multiple indications by several Health Technology Assessment bodies. This study assessed the cost-effectiveness of ranibizumab monotherapy or combination therapy (ranibizumab plus laser photocoagulation) compared with laser monotherapy for the treatment of visual impairment due to diabetic macular edema (DME). Methods A Markov model was developed in which patients moved between health states defined by best-corrected visual acuity (BCVA) intervals and an absorbing 'death' state. The population of interest was patients with DME due to type 1 or type 2 diabetes mellitus. Baseline characteristics were based on those of participants in the RESTORE study. Main outputs were costs (in 2013 CA$) and health outcomes (in quality-adjusted life-years [QALYs]) and the incremental cost-effectiveness ratio (ICER) was calculated. This cost-utility analysis was conducted from healthcare system and societal perspectives in Quebec. Results From a healthcare system perspective, the ICERs for ranibizumab monotherapy and combination therapy vs laser monotherapy were CA$24 494 and CA$36 414 per QALY gained, respectively. The incremental costs per year without legal blindness for ranibizumab monotherapy and combination therapy vs laser monotherapy were CA$15 822 and CA$20 616, respectively. Based on the generally accepted Canadian ICER threshold of CA$50 000 per QALY gained, ranibizumab monotherapy and combination therapy were found to be cost-effective compared with laser monotherapy. From a societal perspective, ranibizumab monotherapy and combination therapy provided greater benefits at lower costs than laser monotherapy (ranibizumab therapy dominated laser therapy). Conclusions Ranibizumab monotherapy and combination therapy resulted in increased quality-adjusted survival and time without legal blindness and lower costs from a societal perspective compared with laser monotherapy."
        },
        {
            "title": "Does N-terminal pro-brain natriuretic peptide add prognostic value to the Mehran risk score for contrast-induced nephropathy and long-term outcomes after primary percutaneous coronary intervention?",
            "abstract": "Purpose:\n        \n      \n      To evaluate the prognostic value of plasma N-terminal pro-brain natriuretic peptide (NT-proBNP) in relation to Mehran risk score (MRS) for contrast-induced nephropathy (CIN) in patients with ST-segment elevation myocardial infarction (STEMI) undergoing primary percutaneous coronary intervention (PPCI).\n    \n\n\n          Methods:\n        \n      \n      We prospectively enrolled 283 consecutive patients treated with PPCI for STEMI. NT-proBNP was measured, and the MRS was calculated. The primary end point was CIN, defined as an absolute increase in serum creatinine ≥0.5 mg/dL from baseline within 48-72 h after contrast medium exposure.\n    \n\n\n          Results:\n        \n      \n      The incidence of CIN was 9.2 %. Patients with CIN had higher NT-proBNP and MRS than those without CIN. The value of NT-proBNP was similar to MRS for CIN (C statistics 0.760 vs. 0.793, p = 0.689). After adjustment for MRS, elevated NT-proBNP (defined as the best cutoff point) was significantly associated with CIN. The addition of elevated NT-proBNP to MRS did not significantly improve the C statistics, over that with the original MRS model (0.833 vs. 0.793, p = 0.256). In addition, similar results were observed for in-hospital and long-term major adverse clinical events.\n    \n\n\n          Conclusions:\n        \n      \n      Although NT-proBNP did not add any prognostic value to the MRS model for CIN, NT-proBNP, as a simple biomarker, was similar to MRS, and may be another useful and rapid screening tool for CIN and death risk assessment, identifying subjects who need therapeutic measures to prevent CIN."
        },
        {
            "title": "Changes in hospital mortality associated with residency work-hour regulations.",
            "abstract": "Background:\n        \n      \n      In 2002, the Accreditation Council on Graduate Medical Education enacted regulations, effective 1 July 2003, that limited work hours for all residency programs in the United States.\n    \n\n\n          Objective:\n        \n      \n      To determine whether work-hour regulations were associated with changes in mortality in hospitalized patients.\n    \n\n\n          Design:\n        \n      \n      Comparison of mortality rates in high-risk teaching service patients hospitalized before and after July 2003, with nonteaching service patients used as a control group.\n    \n\n\n          Setting:\n        \n      \n      551 U.S. community hospitals included in the Healthcare Cost and Utilization Project's Nationwide Inpatient Survey between January 2001 and December 2004.\n    \n\n\n          Patients:\n        \n      \n      1,511,945 adult patients admitted for 20 medical and 15 surgical diagnoses.\n    \n\n\n          Measurement:\n        \n      \n      Inpatient mortality.\n    \n\n\n          Results:\n        \n      \n      In 1,268,738 medical patients examined, the regulations were associated with a 0.25% reduction in the absolute mortality rate (P = 0.043) and a 3.75% reduction in the relative risk for death. In subgroup analyses, particularly large improvements in mortality were observed among patients admitted for infectious diseases (change, -0.66%; P = 0.007) and in medical patients older than 80 years of age (change, -0.71%; P = 0.005). By contrast, in 243,207 surgical patients, regulations were not associated with statistically significant changes (change, 0.13%; P = 0.54).\n    \n\n\n          Limitations:\n        \n      \n      Teaching status was assigned according to hospital characteristics because direct information on each patient's provider was not available. Results reflect changes associated with the sum of regulations, not specifically with caps on work hours.\n    \n\n\n          Conclusions:\n        \n      \n      The work-hour regulations were associated with decreased short-term mortality among high-risk medical patients in teaching hospitals but were not associated with statistically significant changes among surgical patients in teaching hospitals."
        },
        {
            "title": "Do physiological scoring and a novel point of care metabolic screen predict 48-h outcome in admissions from the emergency department resuscitation area?",
            "abstract": "Objective:\n        \n      \n      We aimed to compare the performance of a widely used physiological score [Modified Early Warning Score (MEWS)] and a novel metabolic score (derived from a blood gas) in predicting outcome in emergency department patients.\n    \n\n\n          Design, setting and participants:\n        \n      \n      We carried out a prospective observational study using a convenience sample of 200 patients presenting to the resuscitation area of an inner-city teaching hospital over 4 months.\n    \n\n\n          Main outcome measures:\n        \n      \n      We looked primarily at whether either score predicted new organ failure at 48 h. Our secondary outcome measures were escalation of care and mortality at 48 h.\n    \n\n\n          Results:\n        \n      \n      In univariate analysis, MEWS and the metabolic score predicted 48-h organ failure [odds ratio (OR) 1.19, 95% confidence interval (CI) 1.04-1.35, P=0.009, and OR 1.34, 95% CI 1.015-1.56, P<0.001, respectively]. Both MEWS and the metabolic score predicted 48-h death (OR 1.32, 95% CI 1.02-1.71, P=0.03, and OR 1.56, 95% CI 1.18-2.06, P=0.002, respectively) in univariate analysis. Neither predicted 48-h escalation of care. The metabolic score remained statistically significant at predicting organ failure or death after controlling for MEWS parameters (OR 1.35, 95% CI 1.13-1.62, P=0.001, and OR 1.74, 95% CI 1.13-2.69, P=0.01, respectively). In contrast, MEWS was no longer associated with these outcomes; however, our study has small participant numbers.\n    \n\n\n          Conclusion:\n        \n      \n      This pilot data suggest that a blood gas-derived metabolic score on emergency department arrival may be superior to MEWS at predicting organ failure and death at 48 h."
        },
        {
            "title": "Further appraisal of APACHE II limitations and potential.",
            "abstract": "Hemodynamically unstable patients selected for invasive cardiovascular monitoring were divided into APACHE II subgroups for risk stratification to study interrelationships among monitoring, therapy, resulting cardiovascular function and outcome. When compared by regression analysis, there were no clinically relevant relationships between APACHE II scores and total intervention points (r2 = 0.02), days of invasive monitoring (r2 = 0.000001), initial cardiovascular function (r2 = 0.069) and final cardiovascular function (r2 = 0.05). Analysis of variance (ANOVA) was done between APACHE subgroups and total points (zero of 20 intragroup comparisons were different by the Scheffé test; p = 0.33), days of monitoring (zero of 20 were different; p = 0.61), initial cardiovascular function (three of 20 comparisons were different; p = 0.003) and final cardiovascular function (zero of 20 were different; p = 0.24). Opposite relationships in patients who lived and died were noted between total intervention points and APACHE II subgroups (p = 0.028, two-way ANOVA). There was an increasing number of total intervention points in patients who ultimately lived in ascending initial APACHE II subgroups. In contrast, there was a decreasing number of total intervention points in patients who ultimately died in the same APACHE II subgroups. APACHE II stratification failed to help understand the relationships among clinically important parameters. At the same time, while APACHE scores are claimed to be independent of therapy, the score seemed to be extremely sensitive to interventions, especially important in surgical populations. Should the APACHE II scores remain high in the face of continued maximal intervention, fatal outcome can be predicted. This pattern is remarkably similar across the entire initial APACHE spectrum. The predicated attributes of APACHE II scores, that is, risk stratification and independence from therapy, are neither necessary or desirable. Understanding patterns that are associated with survival or death may require alternative mathematic approaches, such as group and set theory manipulated by principles of Boolean algebra. New approaches may be more fruitful than further attempts to refine existing systems."
        },
        {
            "title": "The ten-year mortality in Behcet's syndrome.",
            "abstract": "We surveyed the 10-yr mortality among 152 Behçet's syndrome (BS) patients who had registered at a BS out-patient clinic and compared it to the expected mortality in the general population. Information on mortality was available in 79% of the study group, among whom six patients (all males) had died. The observed mortality of two patients in the 15-24 yr age bracket was significantly above that expected in the general population. BS is a cause of increased mortality in the young male patients"
        },
        {
            "title": "Scoring systems do not accurately predict outcome following abdominal aortic aneurysm repair.",
            "abstract": "Background:\n        \n      \n      Abdominal aortic aneurysm repair is associated with significant morbidity and mortality. This study aims to evaluate the efficiency of scoring systems in a group of patients undergoing abdominal aortic aneurysm repair.\n    \n\n\n          Methods:\n        \n      \n      A prospective study of 152 patients undergoing aneurysm repair was conducted. Each patient was scored according to the Acute Physiology and Chronic Health Evaluation II, Physiological and Operative Severity Score for the enUmeration of Mortality and Morbidity and Simplified Acute Physiology Score II systems. The predicted mortality for each patient was calculated. Chi(2) analysis was carried out to determine the accuracy of mortality predictions. Receiver-operator curves were drawn to compare scoring systems in terms of sensitivity and specificity.\n    \n\n\n          Results:\n        \n      \n      In the elective aneurysm repair group, all scoring systems tended to overestimate mortality. Receiver-operator curves showed inaccuracies in identifying patients who were at high risk from surgery. In contrast, predicted mortalities underestimated the true death rate among the ruptured aneurysm group. Receiver-operator curves showed better efficiency of scoring systems in the ruptured aneurysm group than in the elective repair group. There was no significant correlation between predicted and observed mortalities in either group.\n    \n\n\n          Conclusion:\n        \n      \n      In this study, all systems showed significant errors when predicting mortality. Therefore, although useful as an audit tool, scoring systems should not be used on an individual basis to guide treatment and assess prognosis after surgery."
        },
        {
            "title": "Is risk of diabetic retinopathy lower in Australia's Greek-born migrants?",
            "abstract": "Aim:\n        \n      \n      The standardized cardiovascular disease death rate for the Greek population in Crete has increased since the 1960s, unlike the all-cause and cardiovascular disease death rate for Australia's Greek migrant population, which has remained paradoxically low. A small window of opportunity remains in which the vascular profile of this interesting atypical migrant population can be characterized. This study assessed whether ethnicity modulates the risk of diabetic retinopathy in Greek-born migrants to Australia.\n    \n\n\n          Methods:\n        \n      \n      The study design was a community-based cross-sectional study of diabetic retinopathy in 107 Greek-born and Australian-born men with Type 2 diabetes, aged 44-83 years. Diabetic retinopathy was assessed by mydriatic three-field retinal photography.\n    \n\n\n          Results:\n        \n      \n      Prevalence of diabetic retinopathy was lower in Greek-born than in Australian-born participants (22 and 37%, respectively). Despite having a higher mean systolic blood pressure level (148 vs. 137 mmHg), Greek-born men had a significantly lower risk of diabetic retinopathy than Australian-born men, after adjusting for age, duration of diabetes, glycated haemoglobin, systolic blood pressure, diastolic blood pressure, albumin to creatinine ratio, and total cholesterol and triglyceride levels [odds ratio 0.32 (0.10-0.99); r(2) = 0.41, P = 0.047].\n    \n\n\n          Conclusion:\n        \n      \n      Greek ethnicity may confer some protection against diabetic retinopathy to Australia's Greek-born migrants, an effect not explained by established risk factors for diabetic retinopathy. A small window of opportunity remains in which to elucidate the ethnicity-related exposures that modulate vascular risk in this older migrant population."
        },
        {
            "title": "Clinical outcomes and prognostic factors for patients with Vibrio vulnificus infections requiring intensive care: a 10-yr retrospective study.",
            "abstract": "Objective:\n        \n      \n      Vibrio vulnificus infection is uncommon but potentially life-threatening. The aim of this study was to evaluate clinical outcomes and prognostic factors for patients with V. vulnificus infections admitted to an intensive care unit.\n    \n\n\n          Design:\n        \n      \n      Retrospective study.\n    \n\n\n          Setting:\n        \n      \n      Multidisciplinary intensive care unit in a 2300-bed teaching hospital.\n    \n\n\n          Patients:\n        \n      \n      Eighty-five adult patients (≥ 18 yrs) with V. vulnificus infections who required intensive care were enrolled and reviewed during a 10-yr period.\n    \n\n\n          Interventions:\n        \n      \n      None.\n    \n\n\n          Measurements and main results:\n        \n      \n      Thirty-four of the 85 patients died, giving an intensive care unit mortality rate of 40%. The mean Acute Physiology and Chronic Health Evaluation II score on intensive care unit admission was 18.4 (95% confidence interval, 17.1-19.8). The most common underlying disease was hepatic disease (48%) followed by diabetes mellitus (22%). Multivariate analysis showed that risk factors for intensive care unit mortality were the presence of hemorrhagic bullous skin lesions/necrotizing fasciitis (relative risk, 2.4; 95% confidence interval, 1.3-4.5; p = .006), skin/soft tissue infections involving two or more limbs (relative risk, 2.5; 95% confidence interval, 1.1-5.7; p = .025), and higher Acute Physiology and Chronic Health Evaluation II scores on intensive care unit admission (relative risk, 1.2; 95% confidence interval, 1.1-1.3; p = .0001). In contrast, surgical treatment < 24 hrs after arrival was inversely associated with intensive care unit mortality (relative risk, 0.35; 95% confidence interval, 0.15-0.79; p = .012). In addition, the area under the receiver operating characteristic curve for Acute Physiology and Chronic Health Evaluation II for predicting intensive care unit mortality was 0.945 (95% confidence interval, 0.873-0.983; p = .0001). An optimal cutoff Acute Physiology and Chronic Health Evaluation II score of ≥ 20 had a sensitivity of 97% and a specificity of 86% with a 41.4-fold increased risk of fatality (p = .0003).\n    \n\n\n          Conclusions:\n        \n      \n      This study found that V. vulnificus-infected patients with hemorrhagic bullous skin lesions/necrotizing fasciitis, skin/soft tissue infections involving two or more limbs, or higher Acute Physiology and Chronic Health Evaluation II scores have high risks of intensive care unit mortality. However, patients receiving prompt surgical treatments within 24 hrs after admission have better prognoses."
        },
        {
            "title": "Minimal preparation computed tomography instead of barium enema/colonoscopy for suspected colon cancer in frail elderly patients: an outcome analysis study.",
            "abstract": "Aim:\n        \n      \n      To evaluate the efficacy of minimal preparation computed tomography (MPCT) in diagnosing clinically significant colonic tumours in frail, elderly patients.\n    \n\n\n          Materials and methods:\n        \n      \n      A prospective study was performed in a group of consecutively referred, frail, elderly patients with symptoms or signs of anaemia, pain, rectal bleeding or weight loss. The MPCT protocol consisted of 1.5 l Gastrografin 1% diluted with sterile water administered during the 48 h before the procedure with no bowel preparation or administration of intravenous contrast medium. Eight millimetre contiguous scans through the abdomen and pelvis were performed. The scans were double-reported by two gastrointestinal radiologists as showing definite (>90% certain), probable (50-90% certain), possible (<50% certain) neoplasm or normal. Where observers disagreed the more pessimistic of the two reports was accepted. The gold standard was clinical outcome at 1 year with positive end-points defined as (1) histological confirmation of CRC, (2) clinical presentation consistent with CRC without histological confirmation if the patient was too unwell for biopsy/surgery, and (3) death directly attributable to colorectal carcinoma (CRC) with/without post-mortem confirmation. Negative end-points were defined as patients with no clinical, radiological or post-mortem findings of CRC. Patients were followed for 1 year or until one of the above end-points were met.\n    \n\n\n          Results:\n        \n      \n      Seventy-two patients were included (mean age 81; range 62-93). One-year follow-up was completed in 94.4% (n=68). Mortality from all causes was 33% (n=24). Five histologically proven tumours were diagnosed with CT and there were two probable false-negatives. Results were analysed twice: assuming all CT lesions test positive and considering \"possible\" lesions test negative [brackets] (95% confidence intervals): sensitivity 0.88 (0.47-1.0) [0.75 (0.35-0.97)], specificity 0.47 (0.34-0.6) [0.87 (0.75-0.94)], positive predictive value 0.18 [0.43], negative predictive value 0.97 [0.96], positive likelihood ratio result 1.6 [5.63], negative likelihood ratio result 0.27 [0.29], kappa 0.31 [0.43]. Tumour prevalence was 12%. A graph of conditional probabilities was generated and analysed. A variety of unsuspected pathology was also found in this series of patients.\n    \n\n\n          Conclusions:\n        \n      \n      MPCT should be double-reported, at least initially. \"Possible\" lesions should be ignored. Analysis of the graph of conditional probability applied to a group of frail, elderly patients with a high mortality from all causes (33% in our study) suggests: (1) if MPCT suggests definite or probable carcinoma, regardless of the pre-test probability, the post-test probability is high enough to warrant further action, (2) frail, elderly patients with a low pre-test probability for CRC and a negative MPCT should not have further investigation, (3) frail, elderly patients with a higher pre-test probability of CRC (such as those presenting with rectal bleeding) and a negative MPCT should have either double contrast barium enema (DCBE) or colonoscopy as further investigations or be followed clinically for 3-6 months. MPCT was acceptable to patients and clinicians and may reveal significant extra-colonic pathology."
        },
        {
            "title": "Reliability of student midwives' visual estimate of blood loss in the immediate postpartum period: a cross-sectional study.",
            "abstract": "Background:\n        \n      \n      In France, postpartum hemorrhage (blood loss≥500mL in the first 24h postpartum) is the leading direct obstetric cause of maternal mortality. In French practice, PPH is mainly diagnosed by a quantitative assessment of blood loss, performed by subjective methods such as visual estimates. Various studies have concluded that visual estimates are imprecise, tend to underestimate blood loss, and thus to delay diagnosis of PPH.\n    \n\n\n          Objectives:\n        \n      \n      The principal objective of this study was to assess the accuracy of visual estimates of blood loss by student midwives. The secondary objectives were to study intraobserver agreement of these assessments, to assess the accuracy of visual estimates for threshold values, and to look for a region effect.\n    \n\n\n          Design:\n        \n      \n      A cross-sectional multicentre study.\n    \n\n\n          Setting:\n        \n      \n      All French midwifery schools (n=35).\n    \n\n\n          Participants:\n        \n      \n      Volunteer French student midwives at their fifth (final) year (n=463).\n    \n\n\n          Methods:\n        \n      \n      The online questionnaire contained 16 photographs (8 different, each presented twice) of simulated volumes of blood loss (100, 150, 200, 300, 500, 850, 1000, and 1500mL). A 50-mL reference standard for calibration accompanied each photograph. Only one answer could be selected among the 7 choices offered for each photograph. Comparisons used χ(2) and Kappa tests.\n    \n\n\n          Results:\n        \n      \n      The participation rate was 48.43% (463/956), and 7.408 visual estimates were collected. Estimates were accurate for 35.34% of the responses. The reproducibility rate for the visual estimates (0.17≤к≤0.48) and for the accurate visual estimates (0.11≤к≤0.55) were moderate for 4 of the 8 volumes (100, 300, 1000, and 1500mL). The percentage of accurate responses was significantly higher for volumes≤300mL than for those ≥500mL (52.94% vs. 17.17%, p<0.0001) and those ≥1000mL (52.94% vs. 18.30%, p<0.0001). The percentage of accurate responses varied between the regions (p=0.042).\n    \n\n\n          Conclusion:\n        \n      \n      Despite the help of a visual aid, both the accuracy and reproducibility of the visual estimates were low."
        },
        {
            "title": "Role of diagnostic techniques in the initial evaluation of stab wounds to the anterior abdomen, back, and flank.",
            "abstract": "Despite the widespread availability of firearms, stab wounds to the abdomen, back, and flank continue to account for a significant number of injuries. The proper sequencing of diagnostic modalities in this patient group is constantly undergoing change. We report our experience with these injuries and present a new algorithm for the use of currently available diagnostic procedures. In 1987, 162 patients were seen, 103 with anterior abdominal wounds and 59 with back and flank wounds. Patients with shock, peritonitis, and evisceration were resuscitated and explored. The remainder of the cohort underwent tap and lavage, and patients with a negative study were observed. Patients with back and flank wounds underwent contrast enhanced computerized tomographic enemas (CECTE). Seventeen patients underwent immediate exploration and 108 of the 162 patients were spared exploration. Fifty-four patients were explored with six negative laparotomies. Of the 126 taps and lavages, the false positive rate was zero, and only one patient had a false negative study. Of the 47 CECTE studies, only three were interpreted as an indication for angiography which proved negative, and all patients were safely observed. The overall mortality was 4.3%, including three patients without vital signs on admission and four who expired intraoperatively due to irreversible shock. We concluded that this algorithm can be safely applied to patients with these injuries with a high degree of specificity and sensitivity."
        },
        {
            "title": "A prospective study of the Bedside Index for Severity in Acute Pancreatitis (BISAP) score in acute pancreatitis: an Indian perspective.",
            "abstract": "Introduction:\n        \n      \n      A simple and easily applicable system for stratifying patients with acute pancreatitis is lacking. The aim of our study was to evaluate the ability of BISAP score to predict mortality in acute pancreatitis patients from our institution and to predict which patients are at risk for development of organ failure, persistent organ failure and pancreatic necrosis.\n    \n\n\n          Methods:\n        \n      \n      All patients with acute pancreatitis were included in the study. BISAP score was calculated within 24 h of admission. A Contrast CT was used to differentiate interstitial from necrotizing pancreatitis within seven days of hospitalization whereas Marshall Scoring System was used to characterize organ failure.\n    \n\n\n          Results:\n        \n      \n      Among 246 patients M:F = 153:93, most common aetiology among men was alcoholism and among women was gallstone disease. 207 patients had no organ failure and remaining 39 developed organ failure. 17 patients had persistent organ failure, 16 of those with BISAP score ≥3. 13 patients in our study died, out of which 12 patients had BISAP score ≥3. We also found that a BISAP score of ≥3 had a sensitivity of 92%, specificity of 76%, a positive predictive value of 17%, and a negative predictive value of 99% for mortality.\n    \n\n\n          Discussion:\n        \n      \n      The BISAP score is a simple and accurate method for the early identification of patients at increased risk for in hospital mortality and morbidity."
        },
        {
            "title": "Perceptions and attitudes of a rural community to abortion in the Niger-delta region of Nigeria.",
            "abstract": "Objective:\n        \n      \n      To determine the perceptions and beliefs relating to unwanted pregnancy, family planning and abortion, and identify issues that can be leveraged to initiate positive attitudes towards family planning and abortion in the area.\n    \n\n\n          Materials and methods:\n        \n      \n      Focus group discussions (FGDs) and in-depth interviews (IDIs) were conducted in Amukpe, Delta State, Nigeria. A highly motivated and well-trained team versed in the local language and culture conducted the FGDs and IDIs.\n    \n\n\n          Results:\n        \n      \n      There was unanimity that unwanted pregnancies was quite common amongst women of reproductive age group and constitute a significant problem in the community. Abortion, particularly in the hands of quacks was a major option to handling an unwanted pregnancy. Almost all agreed that their culture and religion abhors abortion, yet widely practiced because of the odium associated with an unwanted pregnancy in the community. The knowledge of the Nigeria National abortion law even amongst the health workers and teachers was generally poor. The participants agreed that there were problems and complications (often severe) including death associated with abortion in the community. It was largely agreed that contraceptive knowledge and usage was poor. The reasons adduced for this include lack of knowledge, lack of spousal consent, socio-cultural taboos and misconceptions, as well as economic reasons. It was suggested that imbibing positive family values by parents in their wards and government leveraging the socio-economic status of the community will go a long way to stemming the tide.\n    \n\n\n          Conclusion:\n        \n      \n      Unwanted pregnancy, unsafe abortion and abortion complications are reported to be common amongst women of reproductive age group in Amukpe community, whilst contraceptive awareness and usage is poor."
        },
        {
            "title": "No gender difference in the extent of myocardial ischemia in non-ST elevation myocardial infarction.",
            "abstract": "Background:\n        \n      \n      Significant gender differences in angiographic severity of coronary artery disease (CAD) have been demonstrated among patients with non-ST-elevation myocardial infarction (NSTEMI). However, it is unknown if these gender differences are reflected in the extent of myocardial ischemia.\n    \n\n\n          Design and methods:\n        \n      \n      We assessed segmental myocardial wall motion and perfusion by contrast echocardiography in 110 patients (34 women and 76 men) with NSTEMI prior to scheduled coronary angiography. The extent of myocardial ischemia using a 17-segment left ventricular (LV) model was compared to quantitative coronary angiography (QCA).\n    \n\n\n          Results:\n        \n      \n      Age (70 ± 12 vs 66 ± 12 years), troponin T level (0.53 ± 0.66 vs 0.75 ± 1.32 µg/l), Thrombolysis In Myocardial Infarction (TIMI) risk score (3.2 ± 1.4 vs 3.5 ± 1.4), LV ejection fraction and cardiovascular risk factor burden did not differ between genders. As expected, women had less severe findings on coronary angiography but the extent of myocardial ischemia by contrast echocardiography was comparable in women and men. In multivariable analysis, the risk of having prognostically severe angiographic CAD increased by 29% in women and by 49% in men for every additional LV segment with ischemia, independent of TIMI risk score (both p < 0.01).\n    \n\n\n          Conclusion:\n        \n      \n      The present contrast echocardiography study in NSTEMI patients demonstrates that women with NSTEMI have the same extent of LV myocardial ischemia as men in spite of less prevalent angiographic CAD. The findings may help explain why less severe angiographic findings in women with NSTEMI are not accompanied by lower mortality."
        },
        {
            "title": "Evaluation of multifocal visual evoked potentials in patients with Graves' orbitopathy and subclinical optic nerve involvement.",
            "abstract": "Dysthyroid optic neuropathy is the most serious, although infrequent (8-10 %) complication in Graves' orbitopathy (GO). It is known that early stages of compressive optic neuropathy may produce reversible visual field defects, suggesting axoplasmic stasis rather than ganglion cell death. This observational, cross-sectional, case-control study assessed 34 consecutive patients (65 eyes) with Graves' hyperthyroidism and longstanding GO and 31 age-matched control subjects. The patients' multifocal visual evoked potentials (mfVEP) were compared to their clinical and psychophysical (standard automated perimetry [SAP]) and structural (optic coherence tomography [OCT]) diagnostic test data. Abnormal cluster defects were found in 12.3 % and 3.1 % of eyes on the interocular and monocular amplitude analysis mfVEP probability plots, respectively. As well, mfVEP latencies delays were found in 13.8 and 20 % of eyes on the interocular and monocular analysis probability plots, respectively. Interestingly, 19 % of patients with GO had ocular hypertension, and a strong correlation between intraocular pressure measured at upgaze and mfVEP latency was found. MfVEP amplitudes and visual acuity were significantly related to each other (P < 0.05), but not with the latencies delays. However, relationships between the interocular or monocular mfVEP amplitudes and latencies analysis and SAP indices or OCT data were not statistically significant. One-third of our patients with GO showed changes in the mfVEP, indicating significant subclinical optic nerve dysfunction. In this sense, the mfVEP may be a useful diagnostic tool in the clinic for early diagnosis and monitoring of optic nerve function abnormalities in patients with GO."
        },
        {
            "title": "Ethnic variation in the health burden of self-reported diabetes in adults aged 75 and older.",
            "abstract": "Objective:\n        \n      \n      The health burden of self-reported diabetes was compared across three ethnic groups of older adults.\n    \n\n\n          Methods:\n        \n      \n      Analysis of variance and logistic regression were used to compare ethnic differences in the rates of co-morbid chronic health conditions, complications, and disability for older diabetics vs non-diabetics, in a sample of 173 Mexican Americans, 201 African Americans, and 181 non-Hispanic whites, all aged 75 and older.\n    \n\n\n          Results:\n        \n      \n      The prevalence of self-reported diabetes was significantly higher in older Mexican Americans (17.6%) and African Americans (16.4%) than in non-Hispanic whites (8.5%). In all three ethnic groups, and after controlling for sociodemographic characteristics, diabetics were found to be generally at higher risk for chronic conditions such as heart disease, stroke, and hypertension, circulation and foot problems, obesity, and impaired vision and activities of daily living. Multivariate analyses indicated that the burden of diabetes appeared to be greatest among non-Hispanic white diabetics. We suggest that this is the result of higher diabetes-mortality rates among minority diabetics at earlier ages.\n    \n\n\n          Conclusions:\n        \n      \n      Diabetes is known to be increasing in prevalence and incidence, particularly among the elderly, the fastest growing segment of the population. Our findings indicate that regardless of ethnicity, diabetes carries an increased burden that affects both the functioning and the quality of life of older adults."
        },
        {
            "title": "Treatment of primary intraocular lymphoma with radiation therapy: a multi-institutional survey in Japan.",
            "abstract": "This study evaluated the clinical features and treatment outcome of 15 patients with primary intraocular lymphoma. There were nine females, with a median age of 68 years. Thirteen patients presented with bilateral lesions and median time from the onset of symptoms to diagnosis was 12 months. All but one showed the B-cell phenotype. All patients received radiation therapy (RT) with a median of 41 Gy and 10 were administered chemotherapy as well. Three patients were treated with high-dose methotrexate and nine received prophylactic cranial irradiation (PCI) with a median of 30.6 Gy. Thirteen patients obtained a complete remission. The 2-year overall and disease free survival were 74% and 58%, respectively. Although only one patient experienced local recurrence, PCI did not prevent intracranial recurrence. One patient developed a grade 3 cognitive disturbance. It was concluded that ocular RT was effective to control primary lesions. However, some modifications are indispensable to improve outcomes."
        },
        {
            "title": "Silicone oil in repair of retinal detachments caused by necrotizing retinitis in HIV infection.",
            "abstract": "Objective:\n        \n      \n      To evaluate the safety and efficacy of 1000- and 5000-centistoke silicone oil as retinal tamponades for the treatment of retinal detachments secondary to necrotizing retinitis in patients with human immunodeficiency virus (HIV) infection.\n    \n\n\n          Design:\n        \n      \n      A prospective observational study.\n    \n\n\n          Setting:\n        \n      \n      Community and university-based ophthalmology clinics.\n    \n\n\n          Patients:\n        \n      \n      Three hundred fifty patients with HIV infection, who had 407 eyes with retinal detachments secondary to necrotizing retinitis.\n    \n\n\n          Intervention:\n        \n      \n      Vitrectomy surgery for retinal detachment with 1000- or 5000-centistoke silicone oil as the retinal tamponade.\n    \n\n\n          Outcome measures:\n        \n      \n      Efficacy was measured both by anatomic success (defined as complete retinal attachment or macular attachment) and by visual acuity success (defined as preservation of visual acuity or ambulatory vision). Safety was determined by the rate of complications, including abnormal intraocular pressure and corneal and lens opacification.\n    \n\n\n          Results:\n        \n      \n      At the last follow-up examination, the retina was completely attached in 287 (73%) of 393 eyes, the macula was attached in 370 eyes (94%), 268 eyes (68%) had ambulatory vision, and visual acuity was preserved in 219 (56%) of 388 eyes. Corneal opacification, hypotony, and silicone oil emulsification were present in 4%, 2%, and 1% of eyes, respectively. One eye had elevated intraocular pressure. Of the 57 patients who had both eyes treated, 35 died, of whom four (11%) had nonambulatory vision in both eyes. Of the 293 patients who had one eye treated, 122 died, of whom 44 (36%) died with nonambulatory vision in the treated eye. The median time to cataract was 192 days; to nonambulatory vision, 474 days; and to death, 204 days.\n    \n\n\n          Conclusions:\n        \n      \n      Silicone oil repair of retinal detachments in necrotizing retinitis is an efficacious and safe procedure that delays or prevents loss of vision in advanced HIV disease."
        },
        {
            "title": "Non-invasive detection of tako-tsubo cardiomyopathy vs. acute anterior myocardial infarction by transthoracic Doppler echocardiography.",
            "abstract": "Aims:\n        \n      \n      Typical tako-tsubo cardiomyopathy (TTC) mimics acute anterior myocardial infarction (AMI) and the differential diagnosis is challenging before coronary angiography (CA) is performed; it demonstrates reduced or absent antegrade flow in the left anterior descending artery (LAD) in AMI, whereas there is no such flow limiting in TTC. At the acute phase, we tested the usefulness of the distal LAD flow visualization by transthoracic Doppler echocardiography (TDE) to distinguish between these two diseases. For this purpose, we prospectively enrolled 28 consecutive patients with TTC (75 ± 10 years, 93% females) who were compared with 28 consecutive patients with AMI treated successfully by primary angioplasty (66 ± 12 years, 79% females). All the patients underwent the assessment of the distal LAD flow just before CA, using colour and pulsed-wave TDE. In addition, the symmetric involvement of wall motion abnormalities (WMAs) based on the extent of the disease far beyond one coronary territory in TTC was searched by TDE. Non-invasive coronary flow reserve (CFR) by TDE, in the distal LAD, was also performed within 1 day after admission.\n    \n\n\n          Results:\n        \n      \n      Before CA, the distal LAD flow was visible in 38 of 56 cases (68%) in the whole population, in all cases with TTC and in 10 cases with AMI (36%). The sensitivity (Se) and specificity (Sp) of the LAD flow visualization for the diagnosis of TTC were 100 and 64%, respectively, with a diagnostic accuracy of 82%. In comparison, the pattern of WMA yielded a Se of 75% and Sp of 86%, and a diagnostic accuracy of 80%. With the combination of both tools, the Se and Sp to detect TTC were 75 and 96% respectively, with a diagnostic accuracy of 86%. After CA, the acute CFR was less severely impaired in the TTC group when compared with the AMI group (2.2 ± 0.5 vs. 1.7 ± 0.6, P < 0.01) despite a worse LV systolic dysfunction.\n    \n\n\n          Conclusion:\n        \n      \n      Non-invasive evaluation of the distal LAD flow could be helpful to differentiate TTC from AMI, and its combination with the pattern of WMA improved slightly its diagnostic accuracy. Furthermore, the acute CFR is less severely impaired in TTC compared with AMI despite poorer LV systolic dysfunction, suggesting that other mechanisms than direct microcirculatory damage are also involved in the pathogenesis of WMAs in TTC."
        },
        {
            "title": "Geographic disparity of severe vision loss - United States, 2009-2013.",
            "abstract": "Vision loss and blindness are among the top 10 disabilities in the United States, causing substantial social, economic, and psychological effects, including increased morbidity, increased mortality, and decreased quality of life.* There are disparities in vision loss based on age, sex, race/ethnicity, socioeconomic status, and geographic location. Current surveillance activities using national and state surveys have characterized vision loss at national and state levels. However, there are limited data and research at local levels, where interventions and policy decisions to reduce the burden of vision loss and eliminate disparities are often developed and implemented. CDC analyzed data from the American Community Survey (ACS) to estimate county-level prevalence of severe vision loss (SVL) (being blind or having serious difficulty seeing even when wearing glasses) in the United States and to describe its geographic pattern and its association with poverty level. Distinct geographic patterns of SVL prevalence were found in the United States; 77.3% of counties in the top SVL prevalence quartile (≥4.2%) were located in the South. SVL was significantly correlated with poverty (r = 0.5); 437 counties were in the top quartiles for both SVL and poverty, and 83.1% of those counties were located in southern states. A better understanding of the underlying barriers and facilitators of access and use of eye care services at the local level is needed to enable the development of more effective interventions and policies, and to help planners and practitioners serve the growing population with and at risk for vision loss more efficiently."
        },
        {
            "title": "Natural clinical course of progressive supranuclear palsy in Chinese patients in Hong Kong.",
            "abstract": "Introduction:\n        \n      \n      Progressive supranuclear palsy (PSP) is a common type of atypical parkinsonism. To the best of our knowledge, there has been no study of its natural clinical course among Chinese patients.\n    \n\n\n          Methods:\n        \n      \n      This retrospective study included 21 patients with PSP who had radiological evidence of midbrain atrophy (confirmed by magnetic resonance imaging) from the geriatrics clinics of Queen Mary Hospital and Tuen Mun Hospital. Clinical information was retrieved from clinical records, including age at onset, age at presentation, age at death, duration of symptoms, level of education, sex, presenting scores on Cantonese version of Mini-Mental State Examination, clinical symptoms, and history of levodopa or dopamine agonist intake and response. Clinical symptoms were clustered into the following categories and the dates of development of these symptoms were determined: motor symptoms, bulbar symptoms, cognitive symptoms, and others.\n    \n\n\n          Results:\n        \n      \n      Motor symptoms developed early in the clinical course of disease. Cox proportional hazards modelling showed that the number of episodes of pneumonia, time to vertical gaze palsy, and presence of pneumonia were predictive of mortality. Apathy, dysphagia, pneumonia, caregiver stress, and pressure injuries were predictive of mortality when analysed as time-dependent covariates. There was a significant negative correlation between the age at presentation and time to mortality from presentation (Pearson correlation=-0.54, P=0.04). Approximately 40% of caregivers complained of stress during the clinical course of disease.\n    \n\n\n          Conclusion:\n        \n      \n      Important clinical milestones, including the development of dysphagia, vertical gaze palsy, significant caregiver stress, pressure injuries, and pneumonia, may guide advanced care planning for patients with PSP."
        },
        {
            "title": "Transcatheter aortic-valve implantation with one single minimal contrast media injection.",
            "abstract": "Objectives:\n        \n      \n      Performing transcatheter aortic valve implantation (TAVI) with the use of minimal contrast in patients at high-risk for acute kidney injury (AKI).\n    \n\n\n          Background:\n        \n      \n      Contrast-induced nephropathy (CIN) is a major cause of AKI following TAVI and is associated with increased morbidity and mortality. The amount of contrast media used increases the risk for CIN.\n    \n\n\n          Methods:\n        \n      \n      Computed tomography was omitted during the screening process. For the procedure transfemoral access was default. The self-expanding CoreValve prosthesis was chosen in all patients to minimize the risk of annular rupture in case of oversizing. Valve sizing was based on echocardiography, aortography, calcification on fluoroscopy, as well as weight and height of the patient. A single contrast injection was performed to confirm correct position of the pigtail catheter at the level of the annulus. The pigtail then served as the marker for the device landing zone. Intraprocedural assessment of the implantation result relied on echocardiography and hemodynamics.\n    \n\n\n          Results:\n        \n      \n      Five patients with severe aortic stenosis and at high risk for developing CIN were included. Device success was achieved in all patients and no major complications occurred. The median dose of injected contrast media was 8 ml (4-9). All but one patient had improved renal function after the intervention compared to baseline.\n    \n\n\n          Conclusions:\n        \n      \n      Our study shows feasibility of performing TAVI with a single minimal contrast media injection, using a self-expandable valve. This technique has the potential to reduce the incidence of CIN."
        },
        {
            "title": "[Noninvasive mechanical ventilation in severe pneumonia due to H1N1 virus].",
            "abstract": "Objective:\n        \n      \n      The use of noninvasive mechanical ventilation was evaluated in our series of patients admitted to our ICU with pneumonia due to influenza A virus H1N1, assessing the need for intubation, arterial blood gases and clinical improvement, the development of complications and ICU and hospital stay.\n    \n\n\n          Design:\n        \n      \n      Retrospective and observational study.\n    \n\n\n          Setting:\n        \n      \n      ICU of Castellón University General Hospital (Castellón, Spain).\n    \n\n\n          Population:\n        \n      \n      Patients admitted to ICU with pneumonia due to influenza A virus H1N1 and acute hypoxemic respiratory failure.\n    \n\n\n          Interventions:\n        \n      \n      Boussignac CPAP, Helmet system and BiPAP Vision(®) were used.\n    \n\n\n          Results:\n        \n      \n      Five of 10 patients with pneumonia and hypoxemia were analyzed, showing 100% effectiveness of noninvasive mechanical ventilation in terms of clinical and arterial blood gas improvement, and avoiding intubation in all cases. There were no patient deaths in ICU or in hospital. The duration (median) of ventilation was 6 (4-11) days, with an ICU stay of 9 (7-11) days. The number of complications was low (except for urinary tract infection due to Pseudomonas aeruginosa), and only the noise produced by CPAP was underscored. There were no infections among the staff.\n    \n\n\n          Conclusions:\n        \n      \n      Based on our results, increased use of noninvasive mechanical ventilation in future epidemics coujld be proposed."
        },
        {
            "title": "Assessment of coronary artery calcium using dual-energy subtraction digital radiography.",
            "abstract": "Cardiovascular disease is the leading cause of global mortality, yet its early detection remains a vexing problem of modern medicine. Although the computed tomography (CT) calcium score predicts cardiovascular risk, relatively high cost ($250-400) and radiation dose (1-3 mSv) limit its universal utility as a screening tool. Dual-energy digital subtraction radiography (DE; <$60, 0.07 mSv) enables detection of calcified structures with high sensitivity. In this pilot study, we examined DE radiography's ability to quantify coronary artery calcification (CAC). We identified 25 patients who underwent non-contrast CT and DE chest imaging performed within 12 months using documented CAC as the major inclusion criteria. A DE calcium score was developed based on pixel intensity multiplied by the area of the calcified plaque. DE scores were plotted against CT scores. Subsequently, a validation cohort of 14 additional patients was independently evaluated to confirm the accuracy and precision of CAC quantification, yielding a total of 39 subjects. Among all subjects (n = 39), the DE score demonstrated a correlation coefficient of 0.87 (p < 0.0001) when compared with the CT score. For the 13 patients with CT scores of <400, the correlation coefficient was -0.26. For the 26 patients with CT scores of ≥400, the correlation coefficient yielded 0.86. This pilot study demonstrates the feasibility of DE radiography to identify patients at the highest cardiovascular risk. DE radiography's accuracy at lower scores remains unclear. Further evaluation of DE radiography as an inexpensive and low-radiation imaging tool to diagnose cardiovascular disease appears warranted."
        },
        {
            "title": "Radiotherapy of choroidal metastases.",
            "abstract": "Purpose:\n        \n      \n      This retrospective study was undertaken to clarify the role of high energy external beam radiation therapy (EBRT) and to determine its safety and efficacy on local control and visual acuity in patients suffering from choroidal metastases (CM).\n    \n\n\n          Materials and methods:\n        \n      \n      The records of 58 consecutive patients treated with EBRT between 1970 and 1993 were analyzed. The female to male ratio was 2.9 and the median age was 59 years (range 40-81 years). Thirty-six patients (62%) had unilateral CM and 22 patients had bilateral CM. The mean number of lesions per eye was two. Retinal detachment was present in 65% of cases. The primary tumour (PT) was breast carcinoma for 38 patients (75%), lung carcinoma for 10 patients (17%) and gastrointestinal, genitourinary or unknown PT for the remaining 10 patients. The median interval of time between the PT and the CM was 55 months (range 0-228 months). All patients were treated with megavoltage irradiation. The median prescribed dose was 35.5 Gy (range 20-53 Gy) normalized at a 2 Gy per fraction schedule with an alpha/beta value of 10 Gy. Various techniques were used and whenever possible the lens was spared. Ten patients with unilateral disease were treated in both eyes.\n    \n\n\n          Results:\n        \n      \n      The tumour response was slow. When assessed after 3 months or more, the complete response rate was 53% with significantly better results for doses higher than 35.5 Gy (72 versus 33%; P = 0.009). Visual acuity was improved or stabilized in 62% of patients, with also significantly better results when doses higher than 35.5 Gy (P = 0.014) were administered. Amongst 26 patients with unilateral CM who had no elective contralateral irradiation, three developed metastasis in the opposite eye versus none of the 10 patients who had bilateral irradiation. Five complications occurred (three cataracts, one retinopathy and one glaucoma).\n    \n\n\n          Conclusion:\n        \n      \n      Radiation therapy is an efficient and safe palliative treatment for choroidal metastases and it helps the preservation of vision. Thus, there is a major impact on the quality of life in a group of patients with an almost uniformly fatal prognosis. Both tumour response and visual acuity are significantly improved if doses higher than 35.5 Gy are administered. Whenever possible, a lens sparing technique should be used."
        },
        {
            "title": "Impacts of flood on health: epidemiologic evidence from Hanoi, Vietnam.",
            "abstract": "Background:\n        \n      \n      Vietnam is one of the most disaster-prone countries in the world. The country suffers from many kinds of natural disasters, of which the most common and serious one is flooding. Long and heavy rainfall during the last days of October and the first week of November 2008 resulted in a devastating flood unseen for over three decades in the capital city of Hanoi. It caused a substantial health impact on residents in and around the city and compromised the capacity of local health services.\n    \n\n\n          Objective:\n        \n      \n      The aim of this study is to ascertain the vulnerability and health impacts of the devastating flood in Hanoi by identifying the differences in mortality, injuries, and morbidity patterns (dengue, pink eye, dermatitis, psychological problems, and hypertension) between flood affected and non-affected households.\n    \n\n\n          Design:\n        \n      \n      A cross-sectional study was carried out involving 871 households in four selected communes (two heavily flood affected and two comparatively less affected) from two severely flooded districts of Hanoi. Participants were interviewed and information collected on the social, economic, and health impacts of the devastation within 1 month after the flood.\n    \n\n\n          Results:\n        \n      \n      The self-reported number of deaths and injuries reported in this study within 1 month after the heavy rainfall were a bit higher in severely affected communes as compared to that of the less affected communes of our study. The findings showed higher incidences of dengue fever, pink eye, dermatitis, and psychological problems in communes severely affected by flood as compared to that of the controlled communes.\n    \n\n\n          Conclusions:\n        \n      \n      For people in flood prone areas (at risk for flooding), flood prevention and mitigation strategies need to be seriously thought through and acted upon, as these people are exposed to greater health problems such as psychological issues and communicable diseases such as pink eye or dermatitis."
        },
        {
            "title": "Cardiac Magnetic Resonance in Stable Coronary Artery Disease: Added Prognostic Value to Conventional Risk Profiling.",
            "abstract": "Aims:\n        \n      \n      Cardiovascular magnetic resonance (CMR) permits a comprehensive evaluation of stable coronary artery disease (CAD). We sought to assess whether, in a large contemporaneous population receiving optimal medical therapy, CMR independently predicts prognosis beyond conventional cardiovascular risk factors (RF).\n    \n\n\n          Methods:\n        \n      \n      We performed a single centre, observational prospective study that enrolled 465 CAD patients (80% males; 63±11 years), optimally treated with ACE-inhibitors/ARB, aspirin, and statins (76-85%). Assessments included conventional evaluation (clinical history, atherosclerosis RF, electrocardiography, and echocardiography) and a comprehensive CMR with LV dimensions/function, late gadolinium enhancement (LGE), and stress perfusion CMR (SPCMR).\n    \n\n\n          Results:\n        \n      \n      During a median follow-up of 62 months (IQR 23-74) there were 50 deaths and 92 major adverse cardiovascular events (MACE). CMR variables improved multivariate model prediction power of mortality and MACE over traditional RF alone (F-test p<0.05 and p<0.001, respectively). LGE was an independent prognostic factor of mortality (hazard ratio [95% CI]: 3.4 [1.3-8.8]); moreover, LGE (3.3 [1.7-6.3]) and SPCMR (2.1 [1.4-3.2]) were the best predictors of MACE.\n    \n\n\n          Conclusion:\n        \n      \n      LGE is an independent noninvasive marker of mortality in the long term in patients with stable CAD and optimized medical therapy. Furthermore, LGE and SPCMR independently predict MACE beyond conventional risk stratification."
        },
        {
            "title": "Restricting intersection visibility to reduce approach speeds.",
            "abstract": "This paper reports the field test of a visual restriction treatment for a rural intersection with a high rate of injury crashes. A human factors analysis of the asymmetric pattern of crashes at the site suggested that most of the crashes were the result of anticipatory decision-making occasioned by visual characteristics of the eastbound approach to the intersection. The field test examined the effectiveness of a visual restriction treatment directed at eliminating drivers' anticipatory decision-making. The treatment consisted of a hessian screen erected along the eastbound approach to the intersection beginning 125 m prior to intersection and ending 25 m prior to intersection. Over 2 days of testing, approximately 300 drivers' reactions at the intersection were observed and their responses to a brief survey recorded. The test indicated a 23% reduction in the 80th percentile and mean approach speeds and elimination of all approach speeds over 57 km/h following introduction of the treatment. Survey results showed that the treatment was visually acceptable to the majority of drivers using the intersection and did not affect its perceived safety. Follow-on analyses compared speed data before the treatment, and 2, 21, and 37 weeks after installation of the treatment. These analyses showed that approach speeds remained low; 30% lower than pre-treatment speeds for both the 80th percentile and the average approach speeds. Of perhaps the greatest significance, no crash resulting in serious injury or death has occurred at the intersection since installation of the treatment to the present time."
        },
        {
            "title": "Evaluating adherence to recommended diets in adults: the Alternate Healthy Eating Index.",
            "abstract": "Objective:\n        \n      \n      The Healthy Eating Index (HEI), designed to assess adherence to the Dietary Guidelines for Americans and the Food Guide Pyramid, was previously associated with only a small reduction in major chronic disease risk in US adult men and women. We assessed whether an alternate index would better predict risk.\n    \n\n\n          Design:\n        \n      \n      Dietary intake reported by men and women from two prospective cohorts was scored according to an a priori designed Alternate Healthy Eating Index (AHEI). In contrast with the original HEI, the AHEI distinguished quality within food groups and acknowledged health benefits of unsaturated oils. The score was then used to predict development of CVD, cancer or other causes of death in the same population previously tested.\n    \n\n\n          Subjects:\n        \n      \n      67,271 women from the Nurses' Health Study and 38 615 men from the Health Professionals' Follow-up Study.\n    \n\n\n          Results:\n        \n      \n      Men and women with AHEI scores in the top vs. bottom quintile had a significant 20% and 11% reduction in overall major chronic disease, respectively. Reductions were stronger for CVD risk in men (RR = 0.61, 95% CI 0.49-0.75) and women (RR = 0.72, 95% CI 0.60-0.86). The score did not predict cancer risk.\n    \n\n\n          Conclusions:\n        \n      \n      The AHEI was twice as strong at predicting major chronic disease and CVD risk compared to the original HEI, suggesting that major chronic disease risk can be further reduced with more comprehensive and detailed dietary guidance."
        },
        {
            "title": "Dobutamine Stress Echocardiography for Management of Low-Flow, Low-Gradient Aortic Stenosis.",
            "abstract": "Background:\n        \n      \n      In the American College of Cardiology/American Heart Association guidelines, patients are considered to have true-severe stenosis when the mean gradient (MG) is ≥40 mm Hg with an aortic valve area (AVA) ≤1 cm2 during dobutamine stress echocardiography (DSE). However, these criteria have not been previously validated.\n    \n\n\n          Objectives:\n        \n      \n      The aim of this study was to assess the value of these criteria to predict the presence of true-severe AS and the occurrence of death in patients with low-flow, low-gradient aortic stenosis (LF-LG AS).\n    \n\n\n          Methods:\n        \n      \n      One hundred eighty-six patients with low left ventricular ejection fraction (LVEF) LF-LG AS were prospectively recruited and underwent DSE, with measurement of the MG, AVA, and the projected AVA (AVAProj), which is an estimate of the AVA at a standardized normal flow rate. Severity of AS was independently corroborated by macroscopic evaluation of the valve at the time of valve replacement in 54 patients, by measurement of the aortic valve calcium by computed tomography in 25 patients, and by both methods in 8 patients. According to these assessments, 50 of 87 (57%) patients in the study cohort had true-severe stenosis.\n    \n\n\n          Results:\n        \n      \n      Peak stress MG ≥40 mm Hg, peak stress AVA ≤1 cm2, and the combination of peak stress MG ≥40 mm Hg and peak stress AVA ≤1 cm2 correctly classified AS severity in 48%, 60%, and 47% of patients, respectively, whereas AVAProj ≤1 cm2 was better than all the previous markers (p < 0.007), with 70% correct classification. Among the subset of 88 patients managed conservatively (47% of the cohort), 52 died during a follow-up of 2.8 ± 2.5 years. After adjustment for age, sex, functional capacity, chronic kidney failure, and peak stress LVEF, peak stress MG and AVA were not predictors of mortality in this subset. In contrast, AVAProj ≤1 cm2 was a strong predictor of mortality under medical management (hazard ratio: 3.65; p = 0.0003).\n    \n\n\n          Conclusions:\n        \n      \n      In patients with low LVEF LF-LG AS, the DSE criteria of a peak stress MG ≥40 mm Hg, or the composite of a peak stress MG ≥40 mm Hg and a peak stress AVA ≤1 cm2 proposed in the guidelines to identify true-severe AS and recommend valve replacement, have limited value to predict actual stenosis severity and outcomes. In contrast, AVAProj better distinguishes true-severe AS from pseudo-severe AS and is strongly associated with mortality in patients under conservative management. (Multicenter Prospective Study of Low-Flow Low-Gradient Aortic Stenosis [TOPAS]; NCT01835028)."
        },
        {
            "title": "CT of blunt trauma bowel and mesenteric injury: typical findings and pitfalls in diagnosis.",
            "abstract": "Detection of bowel and mesenteric injury can be challenging in patients after blunt abdominal trauma. Early diagnosis and treatment are critical to decrease patient morbidity and mortality. Computed tomography (CT) has become the primary modality for the imaging of these patients. Signs of bowel perforation such as free air and contrast material are virtually pathognomonic. Bowel-wall thickening, free fluid, and mesenteric infiltration may be seen with this type of injury and partial thickness injuries. The authors present and discuss the range of CT findings seen with bowel and mesenteric injuries. Examples of observation and interpretation errors are also provided to highlight pitfalls encountered in the evaluation of abdominopelvic CT scans in patients after blunt trauma."
        },
        {
            "title": "Ocular manifestations in herpes zoster ophthalmicus.",
            "abstract": "Background:\n        \n      \n      Ocular complications of herpes zoster ophthalmicus (HZO) may lead to substantial visual disability, severe post-herpetic neuralgia and rarely fatal cerebral complications.\n    \n\n\n          Aim:\n        \n      \n      To identify the pattern of ocular manifestation in herpes zoster ophthalmicus.\n    \n\n\n          Materials and methods:\n        \n      \n      A cross-sectional descriptive study was under taken including the clinically diagnosed cases of HZO. All of them underwent a complete ophthalmological evaluation.\n    \n\n\n          Results:\n        \n      \n      Sixty-eight cases of HZO were examined, of which 37 (54.4 %) were male and 31 (45.6%) female. The mean age was 48.7 ± 18.5 years. Most of the patients (64.7 %) were above the age of 40 years. 77.94 % of the patients had some form of ocular involvement. Pain (77.9 %) was the commonest ocular complaint. In young patients less than 35 years, HIV was the most common risk factor (19.3 %).Visual status was good in the majority (73.5 %) of patients at presentation. Lid and adnexal findings (45.8 %) were most common ocular involvement followed by conjunctivitis (41.1 %). Corneal complication was seen in 38.2 % of cases, uveitis in 19.1 % and post-herpetic neuralgia (PHN) and secondary glaucoma each in 5.8 %.\n    \n\n\n          Conclusion:\n        \n      \n      Eyelid and ocular adnexal involvement is most commonly found in patients with herpes zoster ophthalmicus followed by corneal complication and uveitis. There needs to be awareness of ocular involvement, which can be sight threatening, among the HZO patients and other medical departments and an increased emphasis on regular ophthalmic examination."
        },
        {
            "title": "Fluorine-18 fluorodeoxyglucose positron emission tomography/computed tomography for improving diagnosis of infection in patients on CF-LVAD: longing for more 'insights'.",
            "abstract": "Aim:\n        \n      \n      Presence and consequent extent of infection in patients on continuous-flow left ventricular assist devices (CF-LVADs) can be challenging with the current diagnostic tools. The present study sought to demonstrate the diagnostic power of 18F-Fluorodeoxyglucose-Positron-Emission Tomography/Computed Tomography (18F-FDG PET/CT) in detecting infection in patients supported with CF-LVAD.\n    \n\n\n          Background:\n        \n      \n      The present study sought to demonstrate the diagnostic power of 18F-fluorodeoxyglucose-positron-emission tomography/computed tomography (18F-FDG PET/CT) in detecting infection in patients supported with CF-LVAD.\n    \n\n\n          Methods and results:\n        \n      \n      Between July 2009 and April 2016, 61 PET/CT examinations were performed in 47 patients (median age 64.13 years, IQR 18.77) supported with a CF-LVAD. PET/CT assessments were performed qualitatively and quantitatively at three different levels: at the piercing site of driveline (first level), along the intracorporeal course of driveline (second level), and around the device (third level). Final diagnosis of LVAD infection was prospectively performed and was based upon microbiological samples taken at hospital admission, during the surgical revision/transplantation and recurrence of symptoms on long-term follow-up. At last follow-up a total of 40 (65.57%) final diagnoses of LVAD-infection could be ascertained. Matching the final diagnosis with the PET/CT assessments the sensitivity, specificity, and positive and negative predictive value were 90.0, 71.4, 85.71, and 78.94%, respectively. Level sub-analyses of SUV max showed an optimal discriminator power for levels 1 and 2 (AUC of level 1-0.824, P < 0.001; AUC of level 2-0.849, P < 0.001, respectively). At the third level semi-quantitative analysis showed poor discriminator power (AUC 0.589, P = 0.33). Qualitative visual analysis instead indicated a trend toward significance (P = 0.07).\n    \n\n\n          Conclusions:\n        \n      \n      Quantitative 18F-FDG PET/CT is an optimal diagnostic tool in detecting superficial and deep driveline infections. However, diagnostic accuracy with regard to the diagnosis of pump housing infection is limited. Here, clinical and qualitative PET/CT analyses must be better considered."
        },
        {
            "title": "Matrix metalloproteinase-9 (MMP-9) and myeloperoxidase (MPO) levels in patients with nonobstructive coronary artery disease detected by coronary computed tomographic angiography.",
            "abstract": "Rationale and objectives:\n        \n      \n      The aim of this study was to evaluate whether matrix metalloproteinase-9 (MMP-9) and myeloperoxidase (MPO) are elevated in patients with nonobstructive coronary artery disease.\n    \n\n\n          Materials and methods:\n        \n      \n      Eighty-four patients with nonobstructive coronary artery disease (group A) and 90 patients with no coronary plaques (group B) were enrolled. MMP-9 and MPO levels were compared between the two groups. The relationships between these biomarkers and Framingham risk score were analyzed. Receiver-operating characteristic curves were used to evaluate the ability of these biomarkers to predict the presence of coronary artery plaques.\n    \n\n\n          Results:\n        \n      \n      The MMP-9 and MPO values in group A were significantly higher than in group B (P < .001). The levels of MMP-9 and MPO showed significant correlations with Framingham risk score (r = 0.796, P < .001, and r = 0.409, P < .001, respectively). The areas under the receiver-operating characteristic curves for MMP-9 and MPO were 0.80 (95% confidence interval, 0.74-0.87) and 0.74 (95% confidence interval, 0.66-0.81), respectively.\n    \n\n\n          Conclusions:\n        \n      \n      Levels of MMP-9 and MPO are positively correlated with Framingham risk score. Additionally, in patients with nonobstructive coronary artery disease, elevated levels of MMP-9 and MPO may identify patients at risk for future myocardial infarction or sudden cardiac death."
        },
        {
            "title": "Vision impairment predicts five-year mortality.",
            "abstract": "## PURPOSE\nTo describe predictors of mortality in the 5-year follow-up of the Melbourne Visual Impairment Project (VIP) cohort.\n## METHODS\nThe Melbourne VIP was a population-based study of the distribution and determinants of age-related eye disease in a cluster random sample of Melbourne residents aged 40 years and older. Baseline examinations were conducted between 1992 and 1994. In 1997, 5-year follow-up examinations of the original cohort commenced. Causes of death were obtained from the National Death Index for all reported deaths.\n## RESULTS\nOf the original 3,271 participants, 231 (7.1%) were reported to have died in the intervening 5 years. Of the remaining 3,040 participants eligible to return for follow-up examinations, 2,594 (85% of eligible) did participate, 51 (2%) had moved interstate or overseas, 83 (3%) could not be traced, and 312 (10%) refused to participate. Best corrected visual acuity < 6/12 and cortical cataract were associated with a significantly increased risk of mortality, as were increasing age, male sex, increased duration of cigarette smoking, increased duration of hypertension, and arthritis.\n## CONCLUSIONS\nEven mild visual impairment increases the risk of death more than twofold.\n"
        },
        {
            "title": "Transfemoral access assessment for transcatheter aortic valve replacement: evidence-based application of computed tomography over invasive angiography.",
            "abstract": "Background:\n        \n      \n      Although computed tomography (CT) is commonly used for iliofemoral evaluation for transfemoral transcatheter aortic valve replacement, many centers worldwide use invasive angiography alone for this purpose. No study to date has evaluated the value of CT over angiography for the prediction of vascular complications. In addition, no data exist for the value of noncontrast CT.\n    \n\n\n          Methods and results:\n        \n      \n      Of the 588 transcatheter aortic valve replacement patients, we reviewed 496 consecutive transfemoral cases. Vessel diameters were measured by CT or angiography. Sheath-related complication (SRC) was defined as an iliofemoral arterial injury not including a cannulation site. Receiver operating characteristic models were generated using sheath-to-iliofemoral artery ratios as a variable and SRC as an end point. In patients undergoing both contrast CT and angiography (n=283; 35 SRCs), contrast CT showed a greater predictive value than angiography by area under the curve P<0.001): 0.87 (95% confidence interval: 0.82-0.91) versus 0.72 (95% confidence interval: 0.66-0.77). In patients undergoing both noncontrast CT and angiography (n=103; 17 SRCs), there was no difference between noncontrast CT and angiography: 0.79 (95% confidence interval: 0.70-0.86) versus 0.73 (95% confidence interval: 0.63-0.81). Three-dimensional assessments of calcification and tortuosity provided limited additional value for SRC prediction.\n    \n\n\n          Conclusions:\n        \n      \n      Contrast CT has a greater predictive value for post-transcatheter aortic valve replacement vascular complications than angiography. Because these complications increase mortality, an accurate assessment of the vasculature is a critical component of proper access selection."
        },
        {
            "title": "Prognostic Role of Late Gadolinium Enhancement in Patients With Hypertrophic Cardiomyopathy and Low-to-Intermediate Sudden Cardiac Death Risk Score.",
            "abstract": "Sudden cardiac death (SCD) is the most life-threating complication of hypertrophic cardiomyopathy. Guidelines of the European Society of Cardiology (ESC) suggest the implantation of an implantable cardioverter defibrillator in primary prevention according to a 5-year risk SCD score ≥6%. The aim of the study is to evaluate the prognostic role of late gadolinium enhancement (LGE) in patients with a 5-year risk SCD score <6%. In this multicenter study, we performed cardiac magnetic resonance in 354 consecutive hypertrophic cardiomyopathy patients (257 males, range of age 54 ± 17) with a risk SCD score <6% (302 with <4% and 52 with ≥4 and <6% risk). Hard cardiac events, including SCD, resuscitated cardiac arrest, appropriate implantable cardioverter defibrillator interventions, sustained ventricular tachycardia, occurred in 22 patients. LGE was detected in a high proportion (92%) of patients with hard cardiac events (p = 0.002). At receiver-operating characteristic curve analysis, LGE extent ≥10% was the best threshold to predict major arrhythmic events (area under the curve: 0.74). Kaplan-Meier curves showed that patients with LGE ≥10% had a worse prognosis than those with lower extent (p < 0.0001). LGE extent was the best independent predictor of hard cardiac events (hazard ratio 1.05; 95% confidence interval [CI] 1.03 to 107; p < 0.0001). The estimates 5-year risk of hard cardiac event was 2.5% (95% CI 0.8 to 4.2) in patients with LGE extent <10% and 23.4% (95% CI 10.2 to 36.5) for those with LGE extent ≥10%. In conclusion, this study demonstrates as the extent of LGE ≥10% is able to recognize additional patients at increased risk for malignant arrhythmic episodes in a population with low-to-intermediate ESC SCD risk score."
        },
        {
            "title": "Patient Health Questionnaire-9 score and adverse cardiac outcomes in patients hospitalized for acute cardiac disease.",
            "abstract": "Objective:\n        \n      \n      The Patient Health Questionnaire-9 (PHQ-9) is increasingly used as a depression assessment tool in cardiac patients. However, in contrast to older depression instruments, there is little data linking PHQ-9 scores to adverse cardiac outcomes. Our goal was to evaluate whether higher PHQ-9 scores were predictive of subsequent cardiac readmissions among depressed patients hospitalized for an acute cardiac event.\n    \n\n\n          Methods:\n        \n      \n      Patients diagnosed with depression during hospitalization for acute coronary syndrome, heart failure, or arrhythmia were enrolled in a randomized depression management trial. Participants were administered PHQ-9 at enrollment, and data was collected regarding cardiac readmissions and mortality over the next 6months. To evaluate the independent association of PHQ-9 score with subsequent cardiac readmission, Cox regression analysis that included relevant sociodemographic and medical covariates was used. Survival analysis examining time to first event, stratified by quartile of initial PHQ-9 score, was performed using Kaplan-Meier curves and log-rank test for trend. Analyses were then repeated using a composite (cardiac readmission or mortality) outcome.\n    \n\n\n          Results:\n        \n      \n      Among 172 subjects, 62 (36.0%) had a cardiac-related rehospitalization. Higher initial PHQ-9 score predicted cardiac-related rehospitalization, independent of multiple relevant covariates (hazard ratio 1.09 [95% confidence interval=1.02-1.17]; p=0.015). On survival analysis, log-rank test for trend revealed a significant rise in event rates across increasing PHQ-9 quartiles (χ(2)=6.36; p=0.012). Findings were similar (p<.05) for the composite outcome.\n    \n\n\n          Conclusion:\n        \n      \n      In depressed cardiac patients, each additional point on the PHQ-9 was independently associated with a 9% greater risk of cardiac readmission over the subsequent 6months."
        },
        {
            "title": "Incidence estimation using a single cross-sectional age-specific prevalence survey with differential mortality.",
            "abstract": "Here, we present a method for incidence estimation of a curable, non-recurring disease when data from a single cross-sectional survey are used together with population-level mortality rates and an assumption of differential mortality of diseased versus non-diseased individuals. The motivating example is cataract, and the VISION2020 goal to eliminate avoidable blindness globally by 2020. Reliable estimates of current and future cataract disease burden are required to predict how many surgeries would need to be performed to meet the VISION2020 goals. However, incidence estimates, needed to derive future burden, are not as easily available, due to the cost of conducting cohort studies. Disease is defined at the person-level in accordance with the WHO person-level definition of blindness. An extension of the standard time homogeneous illness-death model to a four-state model is described, which allows the disease to be cured, whereby surgery is performed on at least one diseased eye. Incidence is estimated, and the four-state model is used to predict disease burden assuming different surgical strategies whilst accounting for the competing risk of death. The method is applied to data from approximately 10,000 people from a survey of visual impairment in Nigeria."
        },
        {
            "title": "Hepatic enhancement in colorectal cancer: texture analysis correlates with hepatic hemodynamics and patient survival.",
            "abstract": "Rationale and objectives:\n        \n      \n      Perfusion imaging of the liver has attracted interest as a potential means for earlier detection of hepatic metastases, but the techniques are complex and do not form part of routine imaging protocols. This study assesses whether the hemodynamic status of the liver of patients with colorectal cancer but apparently normal hepatic morphology is reflected by texture features within a single portal-phase contrast enhanced computed tomography (CT) image and correlates texture with overall survival.\n    \n\n\n          Materials and methods:\n        \n      \n      Portal-phase CT images from 27 patients with colorectal cancer but no apparent hepatic metastases were processed using a band-pass filter that highlighted image features at different spatial frequencies. A range of parameters reflecting liver texture on filtered images were correlated against CT hepatic perfusion index (HPI) and patient survival.\n    \n\n\n          Results:\n        \n      \n      After image filtration, entropy values from hepatic regions were inversely correlated with HPI (r=-0.503978, P=.007355), and directly correlated with survival (r=0.489642, P=.009533). An entropy value below 2.0 identified four patients who died within 36 months of their CT scan with sensitivity 100% and specificity 65% (P<.03).\n    \n\n\n          Conclusion:\n        \n      \n      The hemodynamic status of the liver is reflected by subtle changes in coarse texture on portal phase images that can be revealed by texture analysis. Texture analysis has the potential to identify colorectal cancer patients with an apparently normal portal phase hepatic CT but reduced survival."
        },
        {
            "title": "Age-related macular degeneration and risk of total and cause-specific mortality over 15 years.",
            "abstract": "Objective:\n        \n      \n      We aimed to investigate the independent association between AMD and risk of ischemic heart disease (IHD), stroke, and cardiovascular (CVD) mortality, and all-cause mortality over 15 years.\n    \n\n\n          Methods:\n        \n      \n      3654 participants aged 49+ years at baseline were followed over 15 years. AMD was assessed from retinal photographs. Deaths and cause of death were confirmed by data linkage with the Australian National Death Index. Hazard ratios (HRs) and 95% confidence intervals (CIs) were assessed using Cox models.\n    \n\n\n          Results:\n        \n      \n      71.4% (n=162) and 34.6% (n=1037) of participants with any AMD and no AMD, respectively, died over 15 years. After multivariable-adjustment, no significant associations were observed between AMD and total- and cause-specific mortality in the overall cohort. However, among men, late AMD at baseline was associated with an increased risk of all-cause mortality (n=22; 95.7%), 15 years later: multivariable-adjusted HR, 1.80 (95% CI 1.04-3.11). Women with late AMD had 2-fold increased risk of stroke mortality (n=15; 28.9%), HR 2.10 (95% CI 1.08-4.06). Early-stage AMD was not associated with mortality risk.\n    \n\n\n          Conclusion:\n        \n      \n      Late AMD independently predicted all-cause mortality in men and stroke mortality in women, over 15 years. Although underlying mechanisms are unclear, these findings indicate that late AMD is a marker of biological aging."
        },
        {
            "title": "Identifying Patients at Risk for Prehospital Sudden Cardiac Arrest at the Early Phase of Myocardial Infarction: The e-MUST Study (Evaluation en Médecine d'Urgence des Stratégies Thérapeutiques des infarctus du myocarde).",
            "abstract": "Background:\n        \n      \n      In-hospital mortality of ST-segment-elevation myocardial infarction (STEMI) has decreased drastically. In contrast, prehospital mortality from sudden cardiac arrest (SCA) remains high and difficult to reduce. Identification of the patients with STEMI at higher risk for prehospital SCA could facilitate rapid triage and intervention in the field.\n    \n\n\n          Methods:\n        \n      \n      Using a prospective, population-based study evaluating all patients with STEMI managed by emergency medical services in the greater Paris area (11.7 million inhabitants) between 2006 and 2010, we identified characteristics associated with an increased risk of prehospital SCA and used these variables to build an SCA prediction score, which we validated internally and externally.\n    \n\n\n          Results:\n        \n      \n      In the overall STEMI population (n=8112; median age, 60 years; 78% male), SCA occurred in 452 patients (5.6%). In multivariate analysis, younger age, absence of obesity, absence of diabetes mellitus, shortness of breath, and a short delay between pain onset and call to emergency medical services were the main predictors of SCA. A score built from these variables predicted SCA, with the risk increasing 2-fold in patients with a score between 10 and 19, 4-fold in those with a score between 20 and 29, and >18-fold in patients with a score ≥30 compared with those with scores <10. The SCA rate was 28.9% in patients with a score ≥30 compared with 1.6% in patients with a score ≤9 (P for trend <0.001). The area under the curve values were 0.7033 in the internal validation sample and 0.6031 in the external validation sample. Sensitivity and specificity varied between 96.9% and 10.5% for scores ≥10 and between 18.0% and 97.6% for scores ≥30, with scores between 20 and 29 achieving the best sensitivity and specificity (65.4% and 62.6%, respectively).\n    \n\n\n          Conclusions:\n        \n      \n      At the early phase of STEMI, the risk of prehospital SCA can be determined through a simple score of 5 routinely assessed predictors. This score might help optimize the dispatching and management of patients with STEMI by emergency medical services."
        },
        {
            "title": "[Epidemiological study on the visual ability of official public transport drivers in Bamako].",
            "abstract": "The aim of this work was to evaluate the public transport drivers' visual driving ability in Bamako. We carried out a descriptive cross-sectional study from May 15 to July 15, 2013. The probabilistic method was used. 385 drivers were examined at the University Hospital of the Institute of Tropical Ophthalmology of Africa (IOTA). The average age was 44.69 years. 296 drivers (76.9%) were able to drive public transport vehicles. Among drivers over 50 years old, 42.5% are unfit for driving. Color vision was normal in 98.7 % of cases. Ametropia, cataract and neuropathies accounted for 48.5%, 25.1% and 17.5% respectively, of the causes of impaired vision of drivers. According to European legislation, official transport drivers must be subject to periodic eye examinations. In the absence of such measures in Malian legislation, 42.5% of drivers over 50 years old are unfit to drive. The improvement of road safety requires the renewal and application of the texts for the delivery and renewal of the driver's license of public transport drivers."
        },
        {
            "title": "Predictors of clinical outcome in acute pulmonary embolism: Correlation of CT pulmonary angiography with clinical, echocardiography and laboratory findings.",
            "abstract": "Rationale and objectives:\n        \n      \n      The aims of this study were to retrospectively evaluate whether computed tomographic (CT) parameters were predictors of in-hospital mortality within 30 days of CT imaging and to compare CT parameters with clinical, echocardiographic, and laboratory findings in patients with acute pulmonary embolism (PE).\n    \n\n\n          Materials and methods:\n        \n      \n      A total of 122 patients (61 women, 61 men; mean age, 64 ± 15 years) with CT scans positive for acute PE were reviewed. Two independent readers who were blinded to clinical outcomes scored pulmonary artery obstructions, evaluated cardiovascular measurements, and assessed qualitative findings. Reports of echocardiographic, clinical, and laboratory findings and clinical outcome were reviewed. Results were correlated with patient outcomes using Wilcoxon's rank-sum, χ², and Student's t tests. Logistic regression analyses were performed to determine predictors of patient outcomes.\n    \n\n\n          Results:\n        \n      \n      Thirteen patients (11%) died related to PE within 30 days in the hospital. There were significant differences in the ratio of arterial partial pressure of oxygen to inspired fraction of oxygen and in heart rate between survivors and nonsurvivors (P < .05). No CT or echocardiographic predictor was associated with mortality.\n    \n\n\n          Conclusions:\n        \n      \n      The ratio of arterial partial pressure of oxygen to inspired fraction of oxygen and heart rate strongly predicted mortality due to PE. Neither CT pulmonary angiographic variables nor echocardiography could successfully predict in-hospital mortality in patients with acute PE."
        },
        {
            "title": "Assessing tumor response and detecting recurrence in metastatic renal cell carcinoma on targeted therapy: importance of size and attenuation on contrast-enhanced CT.",
            "abstract": "Objective:\n        \n      \n      The aim of this study was to improve response assessment in patients with metastatic renal cell carcinoma (RCC) on antiangiogenic targeted therapy by evaluating changes in both tumor size and attenuation and by detecting unique patterns of contrast enhancement on contrast-enhanced CT (CECT).\n    \n\n\n          Materials and methods:\n        \n      \n      Tumor long-axis measurements and volumetric mean tumor attenuation of target lesions on CECT images were correlated with time to progression in 53 patients with metastatic clear cell RCC treated with first-line sorafenib or sunitinib. The frequencies of specific patterns of tumor progression were assessed. The data were used to develop new imaging criteria, the size and attenuation CT (SACT) criteria. CECT findings were evaluated using the SACT criteria, Response Evaluation Criteria in Solid Tumors (RECIST), and modified Choi criteria, and the Kaplan-Meier method was used to estimate survival functions.\n    \n\n\n          Results:\n        \n      \n      One or more target metastatic lesions had decreased attenuation of >or=40 HU in 59% of patients with progression-free survival of >250 days (n=44) after initiating targeted therapy; 0% of patients with earlier disease progression (n=9) had this finding. A favorable response based on SACT criteria had a sensitivity of 75% and specificity of 100% for identifying patients with progression-free survival of >250 days, versus 16% and 100%, respectively, for RECIST and 93% and 44% for the modified Choi criteria.\n    \n\n\n          Conclusion:\n        \n      \n      Objectively measuring changes in both tumor size and attenuation on the first CECT study after initiating targeted therapy for metastatic RCC markedly improves response assessment. Distinct patterns of disease recurrence are seen in patients with metastatic RCC on targeted therapy."
        },
        {
            "title": "Comparison of clinical outcomes for patients with large choroidal melanoma after primary treatment with enucleation or proton beam radiotherapy.",
            "abstract": "Purpose:\n        \n      \n      To evaluate survival and clinical outcome for patients with a large uveal melanoma treated by either enucleation or proton beam radiotherapy (PBRT).\n    \n\n\n          Procedures:\n        \n      \n      This retrospective non-randomized study evaluated 132 consecutive patients with T3 and T4 choroidal melanoma classified according to TNM stage grouping.\n    \n\n\n          Results:\n        \n      \n      Cumulative all-cause mortality, melanoma-related mortality and metastasis-free survival were not statistically different between the two groups (log-rank test, p = 0.56, p = 0.99 and p = 0.25, respectively). Eye retention of the tumours treated with PBRT at 5 years was 74% (SD 6.2%). In these patients at diagnosis, 73% of eyes had a best-corrected visual acuity (BCVA) of 0.1 or better. After 12 and 60 months, BCVA of 0.1 or better was observed in 47.5 and 32%, respectively.\n    \n\n\n          Conclusion and message:\n        \n      \n      Although enucleation is the most common primary treatment for large uveal melanomas, PBRT is an eye-preserving option that may be considered for some patients."
        },
        {
            "title": "Meta-analysis and systematic review of the long-term predictive value of assessment of coronary atherosclerosis by contrast-enhanced coronary computed tomography angiography.",
            "abstract": "Objectives:\n        \n      \n      We conducted a systematic review and meta-analysis to determine the predictive value of findings of coronary computed tomography angiography for incident cardiovascular events.\n    \n\n\n          Background:\n        \n      \n      Initial studies indicate a prognostic value of the technique; however, the level of evidence as well as exact independent risk estimates remain unclear.\n    \n\n\n          Methods:\n        \n      \n      We searched PubMed, EMBASE, and the Cochrane Library through January 2010 for studies that followed up ≥ 100 subjects for ≥ 1 year and reported at ≥ 1 hazard ratio (HR) of interest. Risk estimates for the presence of significant coronary stenosis (primary endpoint; ≥ 50% diameter stenosis), left main coronary artery stenosis, each coronary stenosis, 3-vessel disease, any plaque, per coronary segment containing plaque, and noncalcified plaque were derived in random effect regression analysis, and causes of heterogeneity were determined in meta-regression analysis.\n    \n\n\n          Results:\n        \n      \n      We identified 11 eligible articles including 7,335 participants (age 59.1 ± 2.6 years, 62.8% male) with suspected coronary artery disease. The presence of ≥ 1 significant coronary stenosis (9 studies, 3,670 participants, and 252 outcome events [6.8%] with 62% revascularizations) was associated with an annualized event rate of 11.9% (6.4% in studies excluding revascularization). The corresponding HR was 10.74 (98% confidence interval [CI]: 6.37 to 18.11) and 6.15 (95% CI: 3.22 to 11.74) in studies excluding revascularization. Adjustment for coronary calcification did not attenuate the prognostic significance (p = 0.79). The estimated HRs for left main stenosis, presence of plaque, and each coronary segment containing plaque were 6.64 (95% CI: 2.6 to 17.3), 4.51 (95% CI: 2.2 to 9.3), and 1.23 (95% CI: 1.17 to 1.29), respectively.\n    \n\n\n          Conclusions:\n        \n      \n      Presence and extent of coronary artery disease on coronary computed tomography angiography are strong, independent predictors of cardiovascular events despite heterogeneity in endpoints, categorization of computed tomography findings, and study population."
        },
        {
            "title": "Vision Preference Value Scale and Patient Preferences in Choosing Therapy for Symptomatic Vitreomacular Interface Abnormality.",
            "abstract": "Importance:\n        \n      \n      While symptomatic vitreomacular interface abnormalities (VIAs) are common, assessment of vision preference values and treatment preferences of these may guide treatment recommendations by physicians and influence third-party payers.\n    \n\n\n          Objective:\n        \n      \n      To determine preference values that individuals with VIA assign to their visual state and preferences of potential treatments.\n    \n\n\n          Design, setting, and participants:\n        \n      \n      In this cross-sectional one-time questionnaire study conducted between December 2015 and January 2017, 213 patients from tertiary care referral centers in Thailand, the United Kingdom, and the United States were studied. Patients with symptomatic VIA diagnosed within 1 year of data collection, visual acuity less than 20/20 OU, and symptoms ascribed to VIAs were included. Data were analyzed from January 2017 to November 2017.\n    \n\n\n          Main outcomes and measures:\n        \n      \n      The primary end points were overall mean preference value that individuals with VIA assigned to their visual state and patients' preferences for potential treatments. Preference values were graded on a scale from 0 to 1, with 0 indicating death and 1 indicating perfect health with perfect vision.\n    \n\n\n          Results:\n        \n      \n      Of the 213 included patients, 139 (65.3%) were women, and the mean (SD) age was 65.6 (7.7) years. Diagnoses included epiretinal membrane (n = 100 [46.9%]), macular hole (n = 99 [46.5%]), and vitreomacular traction (n = 14 [6.6%]). The mean (SD) vision preference value was 0.76 (0.15), without differences identified among the 3 VIA types. More participants were enthusiastic about vitrectomy (150 [71.1%]) compared with intravitreal injection (120 [56.9%]) (difference, 14.2%; 95% CI, 5.16-23.3; P = .002). Adjusted analyses showed enthusiasm for vitrectomy was associated with fellow eye visual acuity (odds ratio, 10.99; 95% CI, 2.01-59.97; P = .006) and better-seeing eye visual acuity (odds ratio, 0.03; 95% CI, 0.001-0.66; P = .03). Overall enthusiasm for treatment was associated with fellow eye visual acuity (odds ratio, 7.22; 95% CI, 1.29-40.40; P = .02). Overall, most participants (171 [81.0%]) were enthusiastic about surgery, injection, or both.\n    \n\n\n          Conclusions and relevance:\n        \n      \n      Study participants reported similar preference values among 3 types of VIAs. The data suggest that most patients with these conditions would be enthusiastic about undergoing vitrectomy or an injection to treat it, likely because of the condition's effect on visual functioning, although there may be a slight preference for vitrectomy at this time."
        },
        {
            "title": "Five-Year Outcomes of Panretinal Photocoagulation vs Intravitreous Ranibizumab for Proliferative Diabetic Retinopathy: A Randomized Clinical Trial.",
            "abstract": "Importance:\n        \n      \n      Ranibizumab is a viable treatment option for eyes with proliferative diabetic retinopathy (PDR) through 2 years. However, longer-term results are needed.\n    \n\n\n          Objective:\n        \n      \n      To evaluate efficacy and safety of 0.5-mg intravitreous ranibizumab vs panretinal photocoagulation (PRP) over 5 years for PDR.\n    \n\n\n          Design, setting, and participants:\n        \n      \n      Diabetic Retinopathy Clinical Research Network multicenter randomized clinical trial evaluated 394 study eyes with PDR enrolled February through December 2012. Analysis began in January 2018.\n    \n\n\n          Interventions:\n        \n      \n      Eyes were randomly assigned to receive intravitreous ranibizumab (n = 191) or PRP (n = 203). Frequency of ranibizumab was based on a protocol-specified retreatment algorithm. Diabetic macular edema could be managed with ranibizumab in either group.\n    \n\n\n          Main outcomes and measures:\n        \n      \n      Mean change in visual acuity (intention-to-treat analysis) was the main outcome. Secondary outcomes included peripheral visual field loss, development of vision-impairing diabetic macular edema, and ocular and systemic safety.\n    \n\n\n          Results:\n        \n      \n      The 5-year visit was completed by 184 of 277 participants (66% excluding deaths). Of 305 enrolled participants, the mean (SD) age was 52 (12) years, 135 (44%) were women, and 160 (52%) were white. For the ranibizumab and PRP groups, the mean (SD) number of injections over 5 years was 19.2 (10.9) and 5.4 (7.9), respectively; the mean (SD) change in visual acuity letter score was 3.1 (14.3) and 3.0 (10.5) letters, respectively (adjusted difference, 0.6; 95% CI, -2.3 to 3.5; P = .68); the mean visual acuity was 20/25 (approximate Snellen equivalent) in both groups at 5 years. The mean (SD) change in cumulative visual field total point score was -330 (645) vs -527 (635) dB in the ranibizumab (n = 41) and PRP (n = 38) groups, respectively (adjusted difference, 208 dB; 95% CI, 9-408). Vision-impairing diabetic macular edema developed in 27 and 53 eyes in the ranibizumab and PRP groups, respectively (cumulative probabilities: 22% vs 38%; hazard ratio, 0.4; 95% CI, 0.3-0.7). No statistically significant differences between groups in major systemic adverse event rates were identified.\n    \n\n\n          Conclusions and relevance:\n        \n      \n      Although loss to follow-up was relatively high, visual acuity in most study eyes that completed follow-up was very good at 5 years and was similar in both groups. Severe vision loss or serious PDR complications were uncommon with PRP or ranibizumab; however, the ranibizumab group had lower rates of developing vision-impairing diabetic macular edema and less visual field loss. Patient-specific factors, including anticipated visit compliance, cost, and frequency of visits, should be considered when choosing treatment for patients with PDR. These findings support either anti-vascular endothelial growth factor therapy or PRP as viable treatments for patients with PDR.\n    \n\n\n          Trial registration:\n        \n      \n      ClinicalTrials.gov Identifier: NCT01489189."
        },
        {
            "title": "Cognitive and sensory declines in old age: gauging the evidence for a common cause.",
            "abstract": "Resource accounts of behavioral aging postulate that age-associated impairments within and across intellectual and sensory domains reflect, in part, a common set of senescent alterations in the neurochemistry and neuroanatomy of the aging brain. Hence, these accounts predict sizeable correlations of between-person differences in rates of decline, both within and across intellectual and sensory domains. The authors examined reliability-adjusted variances and covariances in longitudinal change for 8 cognitive measures and for close visual acuity, distant visual acuity, and hearing in 516 participants in the Berlin Aging Study (ages 70 to 103 years at 1st measurement). Up to 6 longitudinal measurements were distributed over up to 13 years. Individual differences in rates of cognitive decline were highly correlated, with a single factor accounting for 60% of the variance in cognitive change. This amount increased to 65% when controlling for age at first measurement, distance to death, and risk of dementia. Contrary to expectations, the correlations between cognitive and sensory declines were only moderate in size, underscoring the need to delineate both domain-general and function-specific mechanisms of behavioral senescence."
        },
        {
            "title": "Current multivariate risk scores in patients undergoing non-cardiac surgery.",
            "abstract": "Several indexes to predict perioperative cardiovascular risk have been proposed overtime. The most widely used is the Revised Cardiac Risk Index (RCRI) developed by Lee since 1999. It predicts major cardiac outcomes from five independent clinical determinants: history of ischemic heart disease, history of cardiovascular disease, heart failure, insulin-dependent diabetes mellitus, and chronic renal failure (i.e. serum creatinine &gt;2 mg/dl). In external validation studies, the RCRI showed high negative predictive value in all groups of age, indicating that it may be used to identify people at low risk for perioperative adverse cardiovascular events in noncardiac surgery. However its accuracy is suboptimal in many clinical settings. More recently the National Surgical Quality Improvement Program database) (NSQIP) hasdeveloped a new index to predict perioperative myocardial infarction (MI) or cardiac arrest (MICA) from a cohort of 211,410 patients (the Gupta index) and afterwards a universal surgical risk estimation tool has been developed, using standardized clinical data from 393 ACSNSQIP hospitals in US (a cohort based on 1,414,006 patients), showing a good performance for mortality (C-statistic = 0.944) and morbidity (C-statistic =0.816) as compared with procedure-specific models. Other risk scores include the Vascular events In noncardiac Surgery patIents cOhort evaluatioN (VISION), which has evaluated cardiac complications in 15,065 patients, the Physiological and Operative Severity Score for the enUmeration of Mortality and Morbidity (POSSUM) and the large Preoperative Score to Predict Postoperative Mortality (POSPOM) that was built up from data collected in the National Hospital Discharge Data Base (NHDBB) including a cohort of 7.059.447 patients. In Italy a new risk index (the Orion score) builkt up from a cohort of 9000 patients generated four classes of major cardiovascular adverse events perioperative risk ranging from 1 (0.6%); 2 (2.4%); 3 (7.4%) and 4 (23.1%). The AUROC curves showed higher accuracy as compared to the RCRI score both in the derivation than in the validation cohort (AUROC= 0.872 ± 0.028 vs 0.807 ± 0.037). Thus, many risk indices are available nowadays. Despite the latest European guidelines recommended them for risk stratification (class I, level of evidence B), their use in clinical practice is still scarce."
        },
        {
            "title": "Self-reported visual impairment, physical activity and all-cause mortality: The HUNT Study.",
            "abstract": "Aims:\n        \n      \n      To examine the associations of self-reported visual impairment and physical activity (PA) with all-cause mortality.\n    \n\n\n          Methods:\n        \n      \n      This prospective cohort study included 65,236 Norwegians aged ⩾20 years who had participated in the Nord-Trøndelag Health Study (HUNT2, 1995-1997). Of these participants, 11,074 (17.0%) had self-reported visual impairment (SRVI). The participants' data were linked to Norway's Cause of Death Registry and followed throughout 2012. Hazard ratios and 95% confidence intervals (CI) were assessed using Cox regression analyses with age as the time-scale. The Cox models were fitted for restricted age groups (<60, 60-84, ⩾85 years).\n    \n\n\n          Results:\n        \n      \n      After a mean follow-up of 14.5 years, 13,549 deaths were identified. Compared with adults with self-reported no visual impairment, the multivariable hazard ratios among adults with SRVI were 2.47 (95% CI 1.94-3.13) in those aged <60 years, 1.22 (95% CI 1.13-1.33) in those aged 60-84 years and 1.05 (95% CI 0.96-1.15) in those aged ⩾85 years. The strength of the associations remained similar or stronger after additionally controlling for PA. When examining the joint associations, the all-cause mortality risk of SRVI was higher for those who reported no PA than for those who reported weekly hours of PA. We found a large, positive departure from additivity in adults aged <60 years, whereas the departure from additivity was small for the other age groups.\n    \n\n\n          Conclusions:\n        \n      \n      Adults with SRVI reporting no PA were associated with an increased all-cause mortality risk. The associations attenuated with age."
        },
        {
            "title": "The association between visual impairment and mortality in elderly people.",
            "abstract": "By linking together the results from two surveys of elderly people in an English community it has been possible to consider visual impairment as a possible risk factor for mortality in people aged 75 years and over. Although minor degrees of visual impairment are associated with an increased mortality rate, blind people survive better than those with less serious visual impairments. Associations are considered between visual impairment and other known risk factors for mortality in the elderly. Visual impairment is shown to be associated with degree of social contact, whether a person lives alone, dementia score, physical activity score and number of unmet needs."
        },
        {
            "title": "[Senior citizen health conditions and hospitalization on geriatric, general and surgical floors, a population study conducted in Toledo].",
            "abstract": "Background:\n        \n      \n      The health condition and some clinical aspects configure a group of senior citizens in need of further care who could benefit from specialized geriatric care, although no consensus exists as to how to identify these patients. The aim of this study is to describe the profile of those patients over 64 years of age who are hospitalized in a geriatric unit and to compare this profile to the senior citizens hospitalized in general medical and surgical units.\n    \n\n\n          Method:\n        \n      \n      A cohort representative of the population over age 64 in the Judicial District of Toledo (n = 3214) was studied over an eighteen-month period for the purpose of identifying the income and length of stays at the public hospitals in the health care district in question. The health condition-related variables were gathered by means of personal interviews, and the income and the different aspects thereof by way of hospital admissions department data.\n    \n\n\n          Results:\n        \n      \n      A total of 410 individuals were admitted (12.8%), 168 patients (30.7%) in geriatrics, 204 (37.3%) in medical units and 174 (32.0%) in surgical units. In geriatrics, the average age was significantly higher (age 77.4), there being no differences in the average length of stay (12.8 days; CI 95%) 10.6-14.0), 44 patients (8.1%) having died, 26 (59.1%) hospitalized in geriatrics. More females, younger patients having minor vision and hearing impairments were admitted to the surgical units. In geriatrics, as compared to the medical units, more patients over 80 years of age, living in senior citizen living facilities, having no spouse, moderate-to-severe functional dependence, impaired cognitive function, depression, poor quality of life and scanty social resources.\n    \n\n\n          Conclusions:\n        \n      \n      No differences were found to exist between the health conditions of those over age 64 who were hospitalized in non-surgical and surgical units. In geriatrics, as compared to the other groups of units, the patients were older, in worse condition, had a higher death rate and similar average length of stay."
        },
        {
            "title": "Azathioprine in Behcet's syndrome: effects on long-term prognosis.",
            "abstract": "Objective:\n        \n      \n      To assess the effect of azathioprine (AZA) treatment on long-term prognosis in Behçet's syndrome.\n    \n\n\n          Methods:\n        \n      \n      Patients (all male) who took part in a double-blind, placebo-controlled trial of AZA a mean +/- SD of 94 +/- 10 months previously were reevaluated.\n    \n\n\n          Results:\n        \n      \n      The emergence of blindness (log rank chi2 = 5.6, P = 0.02) and a 2-line drop in the visual acuity of the right eye (log rank chi2 = 5.9, P = 0.015) occurred significantly more frequently among the patients originally allocated to the placebo group compared with patients who originally received AZA, despite posttrial treatment for patients in both groups when needed. There was also a trend toward more frequent occurrence of extraocular complications in the placebo group. The beneficial effect of AZA was especially pronounced among patients who had eye involvement of short duration prior to their entry into the trial.\n    \n\n\n          Conclusion:\n        \n      \n      Early treatment with AZA tends to favorably affect the long-term prognosis of Behçet's syndrome."
        },
        {
            "title": "Outcomes After Proton Beam Therapy for Large Choroidal Melanomas in 492 Patients.",
            "abstract": "Purpose:\n        \n      \n      To evaluate proton beam therapy (PBT) as a means to preserve the eye and spare some vision while not deteriorating survival in patients with large choroidal melanomas.\n    \n\n\n          Design:\n        \n      \n      This is a retrospective, consecutive cohort study of patients with T3-4 choroidal melanomas according to the 7th edition of the American Joint Cancer Classification treated with PBT over a 24-year period.\n    \n\n\n          Results:\n        \n      \n      A total of 492 patients were included. Mean (range) tumor thickness and diameter were 8.77 (2-15) mm and 14.91 (7-24.1) mm, respectively. Mean macular and optic disc distance were 4.56 (0-19.9) mm and 4.59 (0-22.1) mm, respectively. Mean follow-up was 61.9 months. Rates of neovascular glaucoma (NVG) and enucleation (mainly for local recurrence or NVG) were 27.0% and 19.5%, respectively. Enucleation rates decreased over time. The 5-year local control was 94%. Mean baseline visual acuity was 20/63, and visual acuity ≥20/200 was preserved in 20% of patients. At 5 years, 25% of T3 patients presented with metastasis; overall and specific survival rates were 65% and 75%, respectively.\n    \n\n\n          Conclusion:\n        \n      \n      Local control after PBT remained good with increasingly manageable complications and fewer secondary enucleations over time for these large melanomas. As PBT does not seem to deteriorate survival in these patients having a high risk of metastasis, PBT may be considered as a safe and efficient alternative to enucleation in patients with large choroidal melanoma, and may help to spare some vision."
        },
        {
            "title": "Prognostic value of coronary computed tomographic angiography in comparison with calcium scoring and clinical risk scores.",
            "abstract": "Background:\n        \n      \n      Several studies have demonstrated a high accuracy of coronary computed tomography angiography (CCTA) for detection of obstructive coronary artery disease (CAD), whereas some studies have also shown a good prediction of cardiac events. However, it remains to be proven whether CCTA is better predictive of events than conventional risk scores or calcium scoring. Therefore, we compared CCTA with calcium scoring and clinical risk scores for the ability to predict cardiac events.\n    \n\n\n          Methods and results:\n        \n      \n      Patients (n=2223) with suspected CAD undergoing CCTA were followed up for a median of 28 months. The end point was the occurrence of cardiac events (cardiac death, nonfatal myocardial infarction, unstable angina requiring hospitalization, and coronary revascularization later than 90 days after CCTA). Patients with obstructive CAD had a significantly higher event rate (2.9% per year; 95% confidence interval, 2.1 to 4.0) than those without obstructive CAD, having an event rate 0.3% per year (95% confidence interval, 0.1 to 0.5; hazard ratio, 13.5; 95% confidence interval, 6.7 to 27.2; P<0.001). CCTA had significant incremental predictive value when compared with calcium scoring, both with scores assessing the degree of stenosis (P<0.001) and with scores assessing the number of diseased coronary segments (P=0.027).\n    \n\n\n          Conclusions:\n        \n      \n      In patients with suspected CAD, CCTA not only detects coronary stenosis but also improves prediction of cardiac events over and above conventional risk scores and calcium scoring. This may result in a reclassification of cardiovascular risk in a substantial proportion of patients."
        },
        {
            "title": "The impact of ocular tuberculosis on vision after two months of intensive therapy.",
            "abstract": "Tuberculosis is an infectious disease of global importance with major economic and social burden accounting for 25% of all avoidable deaths in developing countries. Extrapulmonary involvement may occur either in association with clinically apparent pulmonary tuberculosis or in isolation. This cross-sectional descriptive study aimed to evaluate the impact of ocular tuberculosis in visual acuity at baseline and after two months of intensive anti-tuberculous therapy. A sample of 133 pulmonary tuberculosis patients, seven disseminated tuberculosis, and three pleural tuberculosis patients was evaluated. All patients underwent routine ophthalmic evaluation, including assessment of visual acuity, biomicroscopy, applanation tonometry, indirect ophthalmoscopy, and fluorescent angiography as appropriate. None of the patients had impaired visual acuity due to tuberculosis. A rate of 4.2% (6/143) of ocular involvement was found. None of the patients with ocular involvement were HIV-infected. Of the six patients with ocular involvement, five met the diagnostic criteria for probable and one for possible ocular lesions. As for the type of ocular lesions, two patients had bilateral findings: one had sclerouveitis and the second had choroidal nodules. The other four patients presented with unilateral lesions: peripheral retinal artery occlusion in the right eye (one case), choroidal nodules in the left eye (one case), and choroidal nodules in the right eye (two cases). Patients progressed favorably after two month of intensive therapy, with no significant reduction in vision."
        },
        {
            "title": "Estimating quality-adjusted life-year loss due to noncommunicable diseases in Korean adults through to the year 2040.",
            "abstract": "Objectives:\n        \n      \n      To estimate the loss in quality-adjusted life-years (QALYs) in Korean adults due to 13 noncommunicable diseases (NCDs) in 2010 and predict changes in QALY loss through to the year 2040.\n    \n\n\n          Methods:\n        \n      \n      Thirteen NCDs (hypertension, diabetes mellitus, hyperlipidemia, stroke, myocardial infarction, angina, arthritis, osteoporosis, asthma, allergic rhinitis, atopic dermatitis, cataract, and depression) were selected from the Korean Community Health Survey 2010. The EuroQol five-dimensional questionnaire index from the Korean Community Health Survey 2010 and the Korean valuation set were used to estimate utility weights according to sex, age, and disease. Morbidity data were also obtained from the Korean Community Health Survey 2010. Mortality data according to disease and life expectancy were retrieved from the Korean Statistical Information Service. To predict future QALY loss, future population projection data from the Korean Statistical Information Service were used as substitutes for 2010 population size.\n    \n\n\n          Results:\n        \n      \n      Among the assessed 13 NCDs, the largest total QALY loss was for hypertension (513,113 QALYs; units are omitted hereafter), followed by arthritis (509,317) and stroke (431,049). The largest QALY loss due to mortality was stroke (306,733), whereas the largest QALY loss due to morbidity was arthritis (502,513). By applying the middle estimate of future population, the largest increase in total QALY loss between 2010 and 2040 was for hypertension (840,582), followed by stroke (719,076) and diabetes mellitus (474,607).\n    \n\n\n          Conclusions:\n        \n      \n      Hypertension, arthritis, and stroke are important in terms of total QALY loss, which will continuous to increase because of aging. These results could be used to develop cost-effective interventions that reduce the burden of NCDs."
        },
        {
            "title": "Relationship Between the Urine Flow Rate and Risk of Contrast-Induced Nephropathy After Emergent Percutaneous Coronary Intervention.",
            "abstract": "A low urine flow rate is a marker of acute kidney injury. However, it is unclear whether a high urine flow rate is associated with a reduced risk of contrast-induced nephropathy (CIN) in high-risk patients. We conducted this study to evaluate the predictive value of the urine flow rate for the risk of CIN following emergent percutaneous coronary intervention (PCI). We prospectively examined 308 patients undergoing emergent PCI who provided consent. The predictive value of the 24-hour postprocedural urine flow rate, adjusted by weight (UR/W, mL/kg/h) and divided into quartiles, for the risk of CIN was assessed using multivariate logistic regression analysis. The cumulative incidence of CIN was 24.4%. In particular, CIN was observed in 29.5%, 19.5%, 16.7%, and 32.0% of cases in the UR/W quartile (Q)-1 (≤0.94 mL/kg/h), Q2 (0.94-1.30 mL/kg/h), Q3 (1.30-1.71 mL/kg/h), and Q4 (≥1.71 mL/kg/h), respectively. Moreover, in-hospital death was noted in 7.7%, 3.9%, 5.1%, and 5.3% of patients in Q1, Q2, Q3, and Q4, respectively. After adjusting for potential confounding predictors, multivariate analysis indicated that compared with the moderate urine flow rate quartiles (Q2 + Q3), a high urine flow rate (Q4) (odds ratio [OR], 2.69; 95% confidence interval [CI], 1.27-5.68; P = 0.010) and low urine flow rate (Q1) (OR, 2.23; 95% CI, 1.03-4.82; P = 0.041) were significantly associated with an increased risk of CIN. Moreover, a moderate urine flow rate (0.94-1.71 mL/kg/h) was significantly associated with a decreased risk of mortality. Our data suggest that higher and lower urine flow rates were significantly associated with an increased risk of CIN after emergent PCI, and a moderate urine flow rate (0.94-1.71 mL/kg/h) may be associated with a decreased risk of CIN with a good long-term prognosis after emergent PCI."
        },
        {
            "title": "Long-term (20 years) outcome and mortality of Type 1 diabetic patients in Soweto, South Africa.",
            "abstract": "Aims:\n        \n      \n      To assess the long-term (20 years) mortality, with causes of death, in a cohort of Type 1 diabetic patients resident in Soweto, South Africa.\n    \n\n\n          Methods:\n        \n      \n      A cohort of Type 1 diabetic patients attending the Diabetic Clinic of Baragwanath Hospital, Soweto were studied in 1982. They were followed over the subsequent 20 years, the final investigation being in 2002. Numbers dying during the period were recorded, as well as year of death and cause. The complication status of survivors was also assessed.\n    \n\n\n          Results:\n        \n      \n      Of the original cohort of 88 Type 1 patients, 21 died during the follow-up period. There were 39 lost to follow-up, giving a crude 20 years' mortality of 43%. Kaplan-Meier analysis showed mortality hazard of 33%. Of those dying, most (9/21) were as a result of renal failure. Other causes were hypoglycaemia (6), ketoacidosis (2), infection (2) and undetermined (2). Of the survivors, comparing data at 0 and 20 years' follow-up, there was a significant increase in rates of retinopathy (P<0.02) and hypertension (P<0.005), but not of other complications.\n    \n\n\n          Conclusions:\n        \n      \n      This is the first long-term outcome study of Type 1 diabetes in sub-Saharan Africa. Although the mortality was substantial, it is similar to equivalent studies of United States (US) Afro-Americans with Type 1 diabetes. The major cause of death was renal failure related to diabetic nephropathy, and reflects lack of adequate facilities for renal replacement therapy. Despite the deprivation, poverty, political upheaval and recent AIDS epidemic in Soweto, Type 1 diabetes carries a reasonable long-term prognosis, and survivors are generally free of debilitating complications."
        },
        {
            "title": "Using claims data to examine mortality trends following hospitalization for heart attack in Medicare.",
            "abstract": "Objective:\n        \n      \n      To see if changes in the demographics and illness burden of Medicare patients hospitalized for acute myocardial infarction (AMI) from 1995 through 1999 can explain an observed rise (from 32 percent to 34 percent) in one-year mortality over that period.\n    \n\n\n          Data sources:\n        \n      \n      Utilization data from the Centers for Medicare and Medicaid Services (CMS) fee-for-service claims (MedPAR, Outpatient, and Carrier Standard Analytic Files); patient demographics and date of death from CMS Denominator and Vital Status files. For over 1.5 million AMI discharges in 1995-1999 we retain diagnoses from one year prior, and during, the case-defining admission.\n    \n\n\n          Study design:\n        \n      \n      We fit logistic regression models to predict one-year mortality for the 1995 cases and apply them to 1996-1999 files. The CORE model uses age, sex, and original reason for Medicare entitlement to predict mortality. Three other models use the CORE variables plus morbidity indicators from well-known morbidity classification methods (Charlson, DCG, and AHRQ's CCS). Regressions were used as is--without pruning to eliminate clinical or statistical anomalies. Each model references the same diagnoses--those recorded during the pre- and index admission periods. We compare each model's ability to predict mortality and use each to calculate risk-adjusted mortality in 1996-1999.\n    \n\n\n          Principal findings:\n        \n      \n      The comprehensive morbidity classifications (DCG and CCS) led to more accurate predictions than the Charlson, which dominated the CORE model (validated C-statistics: 0.81, 0.82, 0.74, and 0.66, respectively). Using the CORE model for risk adjustment reduced, but did not eliminate, the mortality increase. In contrast, adjustment using any of the morbidity models produced essentially flat graphs.\n    \n\n\n          Conclusions:\n        \n      \n      Prediction models based on claims-derived demographics and morbidity profiles can be extremely accurate. While one-year post-AMI mortality in Medicare may not be worsening, outcomes appear not to have continued to improve as they had in the prior decade. Rich morbidity information is available in claims data, especially when longitudinally tracked across multiple settings of care, and is important in setting performance targets and evaluating trends."
        },
        {
            "title": "Cerebral hemodynamic and metabolic profiles in fulminant hepatic failure: relationship to outcome.",
            "abstract": "The purpose of this retrospective study was to examine the potential role of cerebral hemodynamic and metabolic factors in the outcome of patients with fulminant hepatic failure (FHF). Based on the literature, a hypothetical model was proposed in which physiologic changes progress sequentially in five phases, as defined by intracranial pressure (ICP) and cerebral blood flow (CBF) measurements. Seventy-six cerebral physiologic profiles were obtained in 26 patients (2 to 5 studies each) within 6 days of FHF diagnosis. ICP was continuously measured by an extradural fiber optic monitor. Global CBF estimates were obtained by xenon clearance techniques. Jugular venous and peripheral artery catheters permitted calculation of cerebral arteriovenous oxygen differences (AVDO2), from which cerebral metabolic rate for oxygen (CMRO2) was derived. A depressed CMRO2 was found in all patients. There was no evidence of cerebral ischemia as indicated by elevated AVDO2s. Instead, over 65% of the patients revealed cerebral hyperemia. Eight of the 26 patients underwent orthotopic liver transplantation-all recovered neurologically, including 6 with elevated ICPs. Of the 18 patients receiving medical treatment only, all 7 with increased ICP died in contrast to 9 survivors whose ICP remained normal (P < 0.004). Hyperemia, per se, was not related to outcome, although it occurred more frequently at the time of ICP elevations. Six patients were studied during brain death. All 6 revealed malignant intracranial hypertension, preceded by hyperemia. In conclusion, the above findings are consistent with the hypothetical model proposed. Prospective longitudinal studies are recommended to determine the precise evolution of the pathophysiologic changes."
        },
        {
            "title": "The elderly diabetic's eyes.",
            "abstract": "This review article attempts to clarify current and future issues concerning diabetic retinopathy in the elderly. This retinopathy is often part of multiple geriatric ophthalmological diseases (cataract, glaucoma, age-related macular degeneration [ARMD]). Current management is insufficient. A variety of factors come together to aggravate the situation: the increase in the number of elderly diabetic patients and the decrease in the number of ophthalmologists. Through a review of the literature, seriously lacking in prospective studies specific to the geriatric population, we discuss the epidemiology, the screening problems, and the various issues concerning the overall ophthalmologic and diabetologic management inherent to these patients' age and condition. We stress the seriousness of the visual disability of the older subject, but also the overall morbidity and mortality. We observe that recommendations can only be based on expert opinion. Each section includes a warning on the high iatrogenic risk in this area. The caregiver should avoid two pitfalls: a lax attitude related to fear or defeatism and excessive interventionism that may be inappropriate to the patient's condition."
        },
        {
            "title": "An evaluation of fluoroscopy time and correlation with outcomes after percutaneous coronary intervention.",
            "abstract": "Objective:\n        \n      \n      We evaluated short-term prognosis and resource utilization of consecutive patients treated with percutaneous coronary intervention (PCI) as a function of fluoroscopy time.\n    \n\n\n          Background:\n        \n      \n      Advances in interventional cardiology are reflected in the growing complexity of PCI leading to an increasing use of fluoroscopic guidance. The relationship between fluoroscopy time and in-hospital outcomes after PCI has not been addressed.\n    \n\n\n          Methods:\n        \n      \n      In a retrospective analysis of a prospectively collected database including a total of 9,650 patients, the mean fluoroscopy time was 18.3 +/- 12.2 minutes. Outcomes were stratified by fluoroscopy time.\n    \n\n\n          Results:\n        \n      \n      Compared to patients within the 75th percentile, those with prolonged fluoroscopy time were older and had a higher prevalence of prior coronary artery bypass surgery (CABG), chronic renal insufficiency, peripheral arterial disease, type B2/C lesions, and baseline TIMI flow 0-2. Patients with prolonged fluoroscopy time had higher rates of in-hospital death (3.3% vs. 0.3%; p <0.0001), emergent CABG (2.1% vs. 0.3%; p = 0.0001), stent thrombosis (2.9% vs. 1.3%; p = 0.17), retroperitoneal hematoma (0.9% vs. 0.2%; p = 0.01), and contrast-induced nephropathy (6.7% vs. 4.5%; p = 0.03). Resource utilization was significantly higher (p <0.0001) in patients with prolonged fluoroscopy time. By multivariate analysis, prolonged fluoroscopy time was most strongly associated with prior CABG (OR = 2.39), ostial lesion (OR = 2.87), severe lesion calcification (OR = 2.14), baseline TIMI flow 0-2 (OR = 3.71) (all p <0.0001), lesion eccentricity (OR = 1.96; p = 0.0063), and peripheral arterial disease (OR = 1.91; p = 0.0068).\n    \n\n\n          Conclusions:\n        \n      \n      Prolonged fluoroscopy time is associated with higher complexity of treated lesions and increased rates of periprocedural complications including early mortality, emergent CABG, contrast-induced nephropathy, and increased resource utilization."
        },
        {
            "title": "[Report on the conservative treatment of melanoma of the uvea at the Lausanne University Ophthalmologic Clinic].",
            "abstract": "A great number of techniques are currently available for the conservative treatment of uveal melanomas: ocular applicators emitting gamma rays 60Co, 125I) or high-energy beta rays (106Ru/106Rh), light photocoagulation, surgical excision, and accelerated proton beam irradiation. Life expectancy following conservative treatment is equal to or better than that following enucleation. This is demonstrated by nonrandomized comparative studies, and by the authors' own long-term results following the conservative treatment of melanomas by 60Co applicators: mortality due to metastases of small melanomas was 3% (V = smaller than 10 x 10 x 3 mm), with medium-size melanomas it was 12% (V = 10 x 10 x 3-15 x 15 x 5 mm), and with large melanomas 21% (V = larger than 15 x 15 x 5 mm). Accelerated proton beam irradiation of uveal melanomas is currently the method of choice for the conservative treatment of uveal melanomas. The sharp boundaries of the irradiated zone, the uniformly distributed irradiation dose, and beam-splitting are the main advantages of this technique. During the last three years, 310 cases of uveal melanoma have been treated in Switzerland with an accelerated proton beam. Of these, 214 were followed up for more than one year. Eight patients (3.9%) died of metastases. Visual acuity was identical or superior to initial visual acuity in 60.3% of the cases, while 39.6% exhibited a deterioration of vision or a functional loss. Favorable results achieved by conservative treatment of uveal melanomas considerably limited the indications for enucleation, which is now only performed in exceptional situations."
        },
        {
            "title": "Discrete time representation of the formula for calculating DALYs.",
            "abstract": "The global burden of disease (GBD) was measured using a new indicator called disability-adjusted life years (DALYs). The formula to calculate DALYs is based on the idea of time being a continuous variable, which is not consistent with the way economic and health data are collected and reported, and is also different from the concept of time used in the vast majority of policy analyses. Use of this formula gives rise to a time-aggregation bias in the estimates of GBD. Based on discrete time representation and the key principles outlined in the GBD study, a new formula for estimating DALYs is derived in this paper. The properties of the two formulae are compared and contrasted and the implications of using the new formula are discussed. The results show that there is an appreciable difference in percentage terms (14.06%) between the burden of cataract in Sub-Saharan Africa in 1990 calculated using the new and the old formulae. The global burden of diseases and injuries as previously reported in the GDB study may, therefore, be underestimated and the relative positions of some diseases and injuries, and hence the relative priorities of related interventions, may shift if the more appropriate, discrete-time formula is used. The difference is greatest for diseases of short duration (e.g. infectious diseases)."
        },
        {
            "title": "Improved visual, acoustic, and neurocognitive functions after carotid endarterectomy in patients with minor stroke from severe carotid stenosis.",
            "abstract": "Objective:\n        \n      \n      Carotid endarterectomy (CEA) is an established operation performed to prevent strokes, but its other potential effects, such as improving neurocognitive, visual, and auditory functions, remain unconfirmed. This study examined these effects of CEA on patients with symptomatic carotid stenosis.\n    \n\n\n          Methods:\n        \n      \n      This was a prospective controlled study that included 80 patients with minor strokes who had severe extracranial internal carotid stenoses (>70%). Forty patients, who did not receive or who postponed the CEA due to concerns about age, fear of surgery, limited life expectancy because of cancer, or financial problems, formed the medicine-treatment group. Another 40 patients who received CEA 1 week after recruitment formed the CEA group. For both groups, visual acuity chart tests, perimetry tests, audiometry tests, and neurologic scales (National Institutes of Health Stroke Scale, Mini Mental State Examination, and Barthel Index of Activities of Daily Living) were used to assess ophthalmic functions, auditory acuity, and neurocognitive functions before treatment and 3 months after treatment. Intragroup and intergroup comparisons were conducted to examine the effect of CEA.\n    \n\n\n          Results:\n        \n      \n      No deaths or strokes occurred during the 3-month follow-up. The intragroup and intergroup comparisons of ipsilateral function showed that CEA could improve visual acuity, visual field, and auditory acuity at all tested frequencies (250 Hz, 500 Hz, 1000 Hz, 2000 Hz, and 4000 Hz) and could improve the visual field and the auditory acuity for contralateral functions at 1000 Hz. The auditory acuity at 2000 Hz and 4000 Hz were unchanged in the intragroup comparison but showed no deterioration in the intergroup comparison with the medicine group. General neurocognitive function and independent living ability were significantly improved by CEA, as shown by intergroup comparisons (change rate of National Institutes of Health Stroke Scale: -8.1% ± 9.0% vs -2.7% ± 3.0%, P < .001; change rate of Mini Mental State Examination: 15.5% ± 10.5% vs 1.6% ± 2.6%, P < .001; change rate of Barthel Index: 28.0% ± 24.6% vs 2.0% ± 5.5%, P < .001).\n    \n\n\n          Conclusions:\n        \n      \n      In patients with minor strokes caused by severe carotid stenosis, CEA improves neurocognitive, ophthalmic, and acoustic functions. Studies with a larger sample and longer follow-up are needed to substantiate these results, and the underlying mechanisms need further investigation."
        },
        {
            "title": "[Blindness in Germany: dimensions and perspectives].",
            "abstract": "Blindness and visual impairment in Germany and the Western world will increase further over the coming years and decades. This is primarily due to current demographic trends leading to increased visual impairment, mainly in the elderly. In Germany the current situation of the blind and visually impaired is less well documented than in other European countries. The impact of this development can be differentiated into individual and societal dimensions, of which quality of life, morbidity, mortality, and cost are the best-documented aspects. Because of the socioeconomic and cultural importance of vision in Western society, the negative effects of an increase in blindness and visual impairment in a continuously aging population are considerable. Healthcare policy makers need to consider this when discussing future planning and resource allocation. Preventive and rehabilitative services should be given a much higher priority."
        },
        {
            "title": "Stress Perfusion CMR in Patients With Known and Suspected CAD: Prognostic Value and Optimal Ischemic Threshold for Revascularization.",
            "abstract": "Objectives:\n        \n      \n      This study sought to determine the ischemia threshold and additional prognostic factors that identify patients for safe deferral from revascularizations in a large cohort of all-comer patients with known or suspected coronary artery disease (CAD).\n    \n\n\n          Background:\n        \n      \n      Stress-perfusion cardiac magnetic resonance (CMR) is increasingly used in daily practice for ischemia detection. However, there is insufficient evidence about the ischemia burden that identifies patients who benefit from revascularization versus those with a good prognosis who receive drugs only.\n    \n\n\n          Methods:\n        \n      \n      All patients with known or suspected CAD referred to stress-perfusion CMR for myocardial ischemia assessment were prospectively enrolled. The CMR examination included standard functional adenosine stress first-pass perfusion (gadobutrol 0.1 mmol/kg Gadovist, Bayer AG, Zurich, Switzerland) and late gadolinium enhancement (LGE) acquisitions. Presence of ischemia and ischemia burden (number of ischemic segments on a 16-segment model), and of scar and scar burden (number and transmurality of scar segments in a 17-segment model) were assessed. The primary endpoint was a composite of cardiac death, nonfatal myocardial infarction (MI), and late coronary revascularization (>90 days post-CMR); the secondary endpoint was a composite of cardiac death and nonfatal MI.\n    \n\n\n          Results:\n        \n      \n      During a follow-up of 2.5 ± 1.0 years, 86 and 32 of 1,024 patients (1,103 screened patients) experienced the primary and secondary endpoints, respectively. On Kaplan-Meier curves for the primary and secondary endpoints, patients without ischemia had excellent outcomes that did not differ from patients with <1.5 ischemic segments. In multivariate Cox regression analyses of the entire population and of the subgroups, ischemia burden (threshold: ≥1.5 ischemic segments) was consistently the strongest predictor of the primary and secondary endpoints with hazard ratios (HRs) of 7.42 to 8.72 (p < 0.001), whereas age (≥67 years), left ventricular ejection fraction (≤40%), and scar burden (LGE score ≥0.03) contributed significantly, but to a lesser extent, in all models with HRs of 2.01 to 3.48, 1.75 to 1.96, and 1.66 to 1.76, respectively.\n    \n\n\n          Conclusions:\n        \n      \n      In a large all-comer patient cohort with known and suspected CAD, an ischemia burden of ≥1.5 ischemic segments on stress-perfusion CMR was the strongest predictor of the primary and secondary endpoints. Patients with zero or 1 ischemic segment can be safely deferred from revascularizations."
        },
        {
            "title": "Prognostic value of cardiovascular parameters in computed tomography pulmonary angiography in patients with acute pulmonary embolism.",
            "abstract": "The value of various computed tomography parameters for prognosis and risk stratification in acute pulmonary embolism is controversial. Our objective was to evaluate the impact of specific cardiovascular computed tomography pulmonary angiography parameters on short- and long-term clinical outcomes.We analysed radiological and clinical data of 1950 patients with acute pulmonary embolism who participated in an international randomised clinical trial on anticoagulants. Parameters included right/left ventricular ratio, septal bowing, cardiothoracic ratio, diameters of pulmonary trunk and aorta, and intrahepatic/azygos vein contrast medium backflow. Associations with mortality, recurrent venous thromboembolism (VTE), hospitalisation, bleeding and adverse events were assessed over the short term (1 week and 1 month) and long term (12 months).Pulmonary trunk enlargement was the only parameter significantly associated with mortality over both the short and long term (OR 4.18 (95% CI 1.04-16.76) at 1 week to OR 2.33 (95% CI 1.36-3.97) after 1 year), as well as with recurrent VTE and hospitalisation.Most of the evaluated radiological parameters do not have strong effects on the short- or long-term outcome in patients with acute pulmonary embolism. Only an enlarged pulmonary trunk diameter carries an increased risk of mortality and recurrent VTE up to 12 months, and can be used for risk stratification."
        },
        {
            "title": "Prognostic utility of the Perugini grading of 99mTc-DPD scintigraphy in transthyretin (ATTR) amyloidosis and its relationship with skeletal muscle and soft tissue amyloid.",
            "abstract": "Aims:\n        \n      \n      High-grade (Perugini grade 2 or 3) cardiac uptake on bone scintigraphy with 99mTechnetium labelled 3,3-diphosphono-1,2-propanodicarboxylic acid (99mTc-DPD) has lately been confirmed to have high diagnostic sensitivity and specificity for cardiac transthyretin (ATTR) amyloidosis. We sought to determine whether patient stratification by Perugini grade on 99mTc-DPD scintigraphy has prognostic significance in ATTR amyloidosis.\n    \n\n\n          Methods and results:\n        \n      \n      Patient survival from time of 99mTc-DPD scintigraphy was determined in 602 patients with ATTR amyloidosis, including 377 with wild-type ATTR (ATTRwt) and 225 with mutant ATTR (ATTRm) amyloidosis. Patients were stratified according to Perugini grade (0-3) on 99mTc-DPD scan. The prognostic significance of additional patient and disease-related factors at baseline were determined. In the whole cohort, the finding of a Perugini grade 0 99mTc-DPD scan (n = 28) was invariably associated with absence of cardiac amyloid according to consensus criteria as well as significantly better patient survival compared to a Perugini grade 1 (n = 28), 2 (n = 436) or 3 (n = 110) 99mTc-DPD scan (P < 0.005). There were no differences in survival between patients with a grade 1, grade 2 or grade 3 99mTc-DPD scan in ATTRwt (n = 369), V122I-associated ATTRm (n = 92) or T60A-associated ATTRm (n = 59) amyloidosis. Cardiac amyloid burden, determined by equilibrium contrast cardiac magnetic resonance imaging, was similar between patients with Perugini grade 2 and Perugini grade 3 99mTc-DPD scans but skeletal muscle/soft tissue to femur ratio was substantially higher in the latter group (P < 0.001).\n    \n\n\n          Conclusion:\n        \n      \n      99mTc-DPD scintigraphy is exquisitely sensitive for identification of cardiac ATTR amyloid, but stratification by Perugini grade of positivity at diagnosis has no prognostic significance."
        },
        {
            "title": "A population-based survey of hospitalized work-related ocular injury: diagnoses, cause of injury, resource utilization, and hospitalization outcome.",
            "abstract": "Occupational injury is a major source of ocular trauma and is often preventable. A statewide population-based survey of severe work-related ocular injury was generated by using the California Hospital discharge database to identify hospitalized ocular injury and workers compensation as principal payor to identify work-relatedness. Information concerning diagnoses, procedures, causes of injury, length of hospital stay, total hospital charges and disposition at hospital discharge were obtained for injuries occurring during the calendar year 1988. A total of 455 admissions for work-related ocular trauma were identified. The most common work-related ocular trauma diagnoses associated with hospitalizations were open globe injury (46%), adnexal wounds (20%), orbital fractures (11%), and traumatic hyphema (11%). The most common causes of work-related ocular trauma were foreign-body or projectile objects (19%), transport vehicles (18%), cutting or piercing objects (17%), and assaults (9%). Approximately 8% reported other than routine disposition at time of hospital discharge, including long-term nursing or rehabilitation services and death. Mean hospital stay when ocular trauma was the principal admitting diagnosis was 3.7 days. Results differed significantly for admissions reporting ocular trauma as the principal admitting diagnosis compared to admissions that did not. Hospitalized work-related ocular trauma is represented by a wide spectrum of injuries with substantial morbidity and economic costs. Projected to the United States population, these data indicate annual hospital charges excluding professional fees of $14.6 million when work-related ocular trauma is the principal admitting diagnosis and $40 million for admissions where ocular trauma is either a principal or secondary diagnosis."
        },
        {
            "title": "The impact of diabetes-related complications on healthcare costs: results from the United Kingdom Prospective Diabetes Study (UKPDS Study No. 65).",
            "abstract": "Aims:\n        \n      \n      To develop a model for estimating the immediate and long-term healthcare costs associated with seven diabetes-related complications in patients with Type 2 diabetes participating in the UK Prospective Diabetes Study (UKPDS).\n    \n\n\n          Methods:\n        \n      \n      The costs associated with some major complications were estimated using data on 5102 UKPDS patients (mean age 52.4 years at diagnosis). In-patient and out-patient costs were estimated using multiple regression analysis based on costs calculated from the length of admission multiplied by the average specialty cost and a survey of 3488 UKPDS patients' healthcare usage conducted in 1996-1997.\n    \n\n\n          Results:\n        \n      \n      Using the model, the estimate of the cost of first complications were as follows: amputation pound 8459 (95% confidence interval pound 5295, pound 13 200); non-fatal myocardial infarction pound 4070 ( pound 3580, pound 4722); fatal myocardial infarction pound 1152 ( pound 941, pound 1396); fatal stroke pound 3383 ( pound 1935, pound 5431); non-fatal stroke pound 2367 ( pound 1599, pound 3274); ischaemic heart disease pound 1959 ( pound 1467, pound 2541); heart failure pound 2221 ( pound 1690, pound 2896); cataract extraction pound 1553 ( pound 1320, pound 1855); and blindness in one eye pound 872 ( pound 526, pound 1299). The annual average in-patient cost of events in subsequent years ranged from pound 631 ( pound 403, pound 896) for heart failure to pound 105 ( pound 80, pound 142) for cataract extraction. Non-in-patient costs for macrovascular complications were pound 315 ( pound 247, pound 394) and for microvascular complications were pound 273 ( pound 215, pound 343) in the year of the event. In each subsequent year the costs were, respectively, pound 258 ( pound 228, pound 297) and pound 204 ( pound 181, pound 255).\n    \n\n\n          Conclusions:\n        \n      \n      These results provide estimates of the immediate and long-term healthcare costs associated with seven diabetes-related complications."
        },
        {
            "title": "Diagnostic test accuracy of a novel smartphone application for the assessment of attention deficits in delirium in older hospitalised patients: a prospective cohort study protocol.",
            "abstract": "Background:\n        \n      \n      Delirium is a common and serious clinical syndrome which is often missed in routine clinical care. The core cognitive feature is inattention. We developed a novel bedside neuropsychological test for assessing inattention in delirium implemented on a smartphone platform (DelApp). We aim to evaluate the diagnostic performance of the DelApp in a representative cohort of older hospitalised patients.\n    \n\n\n          Methods:\n        \n      \n      This is a prospective study of older non-scheduled hospitalised patients (target n = 500, age ≥ 65), recruited from elderly care and acute orthopaedic wards. Exclusion criteria are: non-English speakers; severe vision or hearing impairment; photosensitive epilepsy. A structured reference standard delirium assessment based on DSM-5 criteria will be used, which includes a cognitive test battery administered by a trained assessor (Orientation-Memory-Concentration Test, Abbreviated Mental Test-10, Delirium Rating Severity Scale-Revised-98, digit span, months and days backwards, Vigilance A' test) and assessment of arousal (Observational Scale of Level of Arousal, Richmond Agitation Sedation Scale). Prior change in cognition will be documented using the Informant Questionnaire on Cognitive Decline in the Elderly. Patients will be categorized as delirium (with/without dementia), possible delirium, dementia, no cognitive impairment, or undetermined. A separate assessor (blinded to diagnosis and assessments) will administer the DelApp index test within 3 h of the reference standard assessment. The DelApp comprises assessment of arousal (score 0-4) and sustained attention (score 0-6), yielding a total score between 0 and 10 (higher score = better performance). Outcomes (length of stay, mortality and discharge location) will be collected at 12 weeks. We will evaluate a priori cutpoints derived from a previous case-control study. Measures of the accuracy of DelApp will include sensitivity, specificity, positive and negative predictive values, and area under the ROC curve. We plan repeat assessments on up to 4 occasions in a purposive subsample of 30 patients (15 delirium, 15 no delirium) to examine changes over time.\n    \n\n\n          Discussion:\n        \n      \n      This study evaluates the diagnostic test accuracy of a novel smartphone test for delirium in a representative cohort of older hospitalised patients, including those with dementia. DelApp has the potential to be a convenient, objective method of improving delirium assessment for older people in acute care.\n    \n\n\n          Trial registration:\n        \n      \n      Clinical trials.gov, NCT02590796 . Registered on 29 Oct 2015. Protocol version 5, dated 25 July 2016."
        },
        {
            "title": "Cranio-orbital Resection Does Not Appear to Improve Survival of Patients With Lacrimal Gland Carcinoma.",
            "abstract": "Purpose:\n        \n      \n      To ascertain long-term outcome of treatment for primary epithelial malignancies of the lacrimal gland and compare outcomes after cranio-orbital resection or after macroscopic tumor resection with radiotherapy.\n    \n\n\n          Methods:\n        \n      \n      Comparative case series of 79 patients (49 male; 62%) treated for primary epithelial malignancies of the lacrimal gland at Moorfields Eye Hospital between 1972 and 2014. Patients were identified from clinical and pathological databases and, where available, the clinical, pathological, and imaging records reviewed. The primary outcome measures were overall survival after diagnosis, disease-free survival, and final visual acuity for patients having cranio-orbital resection (exenteration plus local bone removal), compared with macroscopic tumor resection plus radiotherapy.\n    \n\n\n          Results:\n        \n      \n      The mean age at presentation was 48 years (median: 50 years; range: 13-84 years), with 53 (67%) having adenoid cystic carcinoma, 15 (19%), primary adenocarcinoma, and 11 (14%) carcinoma ex-pleomorphic adenoma (malignant mixed tumor). The overall survival probability of the cohort (79 patients) was 0.59 at 5 years and 0.52 at 10 years, with 36/79 (46%) patients suffering tumor-related deaths; 14 patients died from other causes, and 4 patients were lost to follow up after the minimum follow-up period. The probability of disease-free survival at 5 years for patients with adenoid cystic carcinoma, adenocarcinoma, and malignant mixed tumor was 0.52, 0.4, and 0.64, respectively, with the comparable figures at 10 years being 0.44, 0.40, and 0.64. Most importantly, the 9 patients undergoing cranio-orbital resection and the 44 having solely macroscopic tumor resection plus radiotherapy had similar overall survival (p = 0.59) and disease-free survival (p = 0.89). Subgroup analysis of the 2 treatment modalities for patients with adenoid cystic carcinoma (8 cranio-orbital resection and 32 debulking and radiotherapy) demonstrated similar results for disease-free survival (p = 0.87). Likewise, there were no significant differences between rates of recurrences between the 2 different treatments. For the 50 patients who had eye-preserving surgery and long-term visual acuity data, the final acuity was better or equal to 0.6 logMAR (6/24 Snellen) in 25 (50%).\n    \n\n\n          Discussion:\n        \n      \n      There is no difference in either survival or tumor recurrence for lacrimal gland carcinoma treated with cranio-orbital resection, or eye-preserving tumor excision and radiotherapy. The authors, therefore, continue to advocate local resection and radiotherapy for almost all patients with primary epithelial malignancies of the lacrimal gland-this treatment having lower morbidity, causing less disfigurement, and, importantly, preserving useful vision in most patients."
        },
        {
            "title": "Pituitary Apoplexy: Should Endoscopic Surgery Be the Gold Standard?",
            "abstract": "Background:\n        \n      \n      Pituitary apoplexy is an uncommon, potentially fatal condition due to spontaneous ischemia or hemorrhage in a pituitary adenoma. The treatment of this disorder has long been a matter of debate.\n    \n\n\n          Methods:\n        \n      \n      Retrospective cohort study including all patients admitted with pituitary apoplexy in our department between 2005 and 2015 was undertaken. Clinical symptoms and signs on admission, treatment (conservative vs. surgical), neurologic deficit on discharge and at 6 months' follow-up, and endocrinologic evaluation at 6 months' follow-up were analyzed. The statistical analysis was performed with STATA 13.0. Endocrinologic and visual outcomes at 6 months in the different groups according to treatment were compared by applying an independent multinomial probit regression test. Outcomes between the conservative and the surgical (endoscopic and microscopic considered together) groups also were compared and the differences between surgical treated groups were analyzed with logistic regression analysis. P values <0.05 were considered significant.\n    \n\n\n          Results:\n        \n      \n      Twenty-three patients were included in this study; 60.9% (n = 14) were treated surgically (5 microsurgically; 9 endoscopically) and 39.1% (n = 9) conservatively. Statistical analysis revealed no significant differences in the visual function between the 3 treatment groups in both univariate and multivariate analysis (P > 0.05). The endocrinologic outcome was better in the surgical group (P = 0.017; adjusted P = 0.027), with a significant difference between the conservative group and the endoscopic group (P = 0.004; adjusted P = 0.005). When we compared both surgical groups, the endoscopic group has a better endocrinologic outcome (P = 0.020; adjusted P = 0.012).\n    \n\n\n          Conclusions:\n        \n      \n      Our results support endoscopic intranasal transsphenoidal surgery as a treatment of pituitary apoplexy patients, as it probably decreases the need for long-term hormonal replacement."
        },
        {
            "title": "[Screening for colorectal cancer].",
            "abstract": "This piece summarizes new recommendations from the Preventive Services Task Force of the United States of America concerning screening for colorectal cancer (CRC). These recommendations update and replace ones that were issued in 1996. The Task Force strongly recommends that physicians carry out CRC screening tests for both men and women who are 50 years of age or older. The Task Force found fair or good evidence that: 1) several screening methods are effective in reducing mortality from CRC, 2) the benefits of screening outweigh its risks, although the quality of the tests, the magnitude of the benefits, and the potential harms vary according to the method, and 3) periodic fecal occult blood testing (FOBT) reduces mortality from CRC. In addition, there is fair evidence that sigmoidoscopy, either alone or in combination with FOBT, reduces CRC mortality. There is no direct evidence that screening colonoscopy is effective in reducing CRC mortality, nor is it clear if the greater accuracy of colonoscopy in comparison to other tests compensates for its additional complications, inconvenience, and costs. Double-contrast barium enema is less sensitive than colonoscopy, and there is no direct evidence that it is effective in lowering mortality rates. There are insufficient data to determine which screening strategy is best in terms of the balance of benefits, potential harms, and cost-effectiveness. Regardless of the strategy chosen, CRC screening is likely to be cost-effective (less than US$ 30 000 per year of life gained)."
        },
        {
            "title": "Plaque radiotherapy for juxtapapillary choroidal melanoma. Visual acuity and survival outcome.",
            "abstract": "Objectives:\n        \n      \n      To assess the effect of plaque radiotherapy on the visual acuity of patients with juxtapapillary choroidal melanoma and to determine the clinical predictive factors for radiation retinopathy, radiation papillopathy, local tumor recurrence, and distant metastasis.\n    \n\n\n          Design:\n        \n      \n      A retrospective review of the medical records of 93 patients with juxtapapillary choroidal melanoma who were treated initially with plaque radiotherapy.\n    \n\n\n          Results:\n        \n      \n      During a mean follow-up of 78 months, radiation retinopathy developed in 81 patients (87%) and radiation papillopathy developed in 48 patients (52%) after a mean interval of 21 and 27 months, respectively. The univariate variables that were significant predictors of radiation retinopathy were history of diabetes mellitus (P = .05) and use of a notched radioactive plaque (P = .04). The factors predictive of radiation papillopathy were age (> 45 years; P = .01), history of diabetes mellitus (P = .05), mushroom-shaped tumor configuration (P = .006), and nasal location of the tumor (P = .04). By using Kaplan-Meier survival curves, we found that the proportion of the 93 patients with radiation retinopathy was 87 (94%) at 5 years and with radiation papillopathy was 53 (57%) at 5 years. By using life-table analysis, we found that the proportion of the 93 patients who experienced a decrement of at least 3 lines of visual acuity was 67 (72%) by 50 to 60 months. Local tumor recurrence was documented in 14 patients (15%) after a mean interval of 41 months. The age of the patient (< 35 years; P = .02) and the superior (P = .004) and inferior (P = .05) locations of the tumor were predictive of local tumor recurrence. Distant metastasis developed in 11 patients (12%) after a mean interval of 44 months. The factors predictive of distant metastasis were a tumor with a basal diameter larger than 6.0 mm (P = .05), the superior location of the tumor (P = .01), and local tumor recurrence (P < .001).\n    \n\n\n          Conclusion:\n        \n      \n      Based on these observations, plaque radiotherapy remains a potential option vs enucleation for the management of juxtapapillary choroidal melanoma."
        },
        {
            "title": "Clinical Evaluation of Sepsis-1 and Sepsis-3 in the ICU.",
            "abstract": "Background:\n        \n      \n      There has been considerable controversy between sepsis-1 and sepsis-3 criteria.\n    \n\n\n          Methods:\n        \n      \n      Patients with infection meeting two or more systemic inflammatory response syndrome (SIRS) criteria (sepsis-1) or a Sequential Organ Failure Assessment (SOFA) score ≥ 2 (sepsis-3) on the first day after ICU admission were selected from the Medical Information Mart for Intensive Care-III database, and their outcomes were compared using all-cause death as the end point. Subgroup analysis was also performed based on prior chronic organ dysfunction.\n    \n\n\n          Results:\n        \n      \n      There were 21,491 infected patients included. Of those meeting the diagnostic criteria for sepsis-1, 13.42% did not satisfy sepsis-3 criteria, and this population had a 21-day mortality rate of 6.96%. In contrast, 7.00% of the patients meeting sepsis-3 criteria did not meet sepsis-1 criteria, and their 21-day mortality rate was 10.76%. When excluding preexisting organ conditions, 18.41% of patients with sepsis-1 did not meet sepsis-3 criteria, with a 21-day mortality rate of 6.39%, and 6.00% of patients with sepsis-3 did not meet sepsis-1 criteria, with a 21-day mortality rate of 9.11%. When two or more SIRS criteria or SOFA score ≥ 2 were applied to predict 21-day all-cause mortality in infected patients without prior chronic organ dysfunction, the sensitivity was 96.0% or 91.0%, respectively. Although the areas under the receiver operator curve of both SOFA and SIRS criteria could be used for predicting mortality, SOFA score represented the severity of the condition, whereas SIRS score represented a clinically evident host response to infection.\n    \n\n\n          Conclusions:\n        \n      \n      Sepsis-3 diagnostic criteria narrow the sepsis population at the expense of sensitivity, and the resulting false negatives may delay disease diagnosis. It may be inappropriate to compare the prediction performance of SIRS and SOFA criteria when sepsis-3 is defined."
        },
        {
            "title": "Prognostic value of CT in the early assessment of patients with acute pancreatitis.",
            "abstract": "Objective:\n        \n      \n      This study investigates the prognostic value of early CT in acute pancreatitis, the role of pancreatic necrosis as a indicator of prognosis, and the need for the routine use of IV iodinated contrast material in early CT to assess prognosis in these patients.\n    \n\n\n          Materials and methods:\n        \n      \n      We conducted a retrospective review of 148 patients who underwent unenhanced and contrast-enhanced helical CT within 72 hr after onset of symptoms of a first episode of acute pancreatitis. Patients were classified by CT grade and grouped into two categories (mild: grades A, B, C; and severe: grades D and E) that were correlated with complications and death. In the grades including patients with pancreatic necrosis, it was also correlated with complications and death.\n    \n\n\n          Results:\n        \n      \n      All complications (n = 15) and deaths (n = 4) occurred in patients with a CT grade of severe disease; differences as compared with mild grade were significant (p < 0.001 and p < 0.03, respectively). CT grade had a sensitivity and specificity of 100% and 61.6%, respectively, for predicting morbidity and 100% and 56.9% for predicting mortality. The 13 patients with necrosis were all in the severe group (p < 0.001). Necrosis detection on early CT had a sensitivity and specificity of 53.3% and 90.2%, respectively, for predicting morbidity and 75% and 83.8% for mortality.\n    \n\n\n          Conclusion:\n        \n      \n      Early unenhanced CT alone was a good indicator of severity of acute pancreatitis in our selected population. CT grade was sensitive for predicting outcome in acute pancreatitis. Pancreatic necrosis, estimated on early, contrast-enhanced CT and seen only in patients having severe disease, was a specific predictor of morbidity and mortality. These findings lead us to suggest that the use of iodinated contrast material to assess necrosis can be reserved for only those patients classified as having severe disease on unenhanced CT."
        },
        {
            "title": "A cluster randomized, controlled trial of breast and cervix cancer screening in Mumbai, India: methodology and interim results after three rounds of screening.",
            "abstract": "Cervix and Breast cancers are the most common cancers among women worldwide and extract a large toll in developing countries. In May 1998, supported by a grant from the NCI (US), the Tata Memorial Hospital, Mumbai, India, started a cluster-randomized, controlled, screening-trial for cervix and breast cancer using trained primary health workers to provide health-education, visual-inspection of cervix (with 4% acetic acid-VIA) and clinical breast examination (CBE) in the screening arm, and only health education in the control arm. Four rounds of screening at 2-year intervals will be followed by 8 years of monitoring for incidence and mortality from cervix and breast cancers. The methodology and interim results after three rounds of screening are presented here. Good randomization was achieved between the screening (n = 75360) and control arms (n = 76178). In the screening arm we see: High screening participation rates; Low attrition; Good compliance to diagnostic confirmation; Significant downstaging; Excellent treatment completion rate; Improving case fatality ratios. The ever-screened and never-screened participants in the screening arm show significant differences with reference to the variables religion, language, age, education, occupation, income and health-seeking behavior for gynecological and breast-related complaints. During the same period, in the control arm we see excellent participation rate for health education; Low attrition and a good number of symptomatic referrals for both cervix and breast."
        },
        {
            "title": "Preventive eye care in people with diabetes is cost-saving to the federal government. Implications for health-care reform.",
            "abstract": "Objective:\n        \n      \n      Diabetic retinopathy, which leads to macular edema and retinal neovascularization, is the leading cause of blindness among working-age Americans. Previous research has demonstrated significant cost savings associated with detection of eye disease in Americans with type I diabetes. However, detection and treatment of eye disease among those with type II diabetes was previously thought not to be cost-saving. Our purpose was to estimate the current and potential federal savings resulting from the screening and treatment of retinopathy in patients with type II diabetes, based on recently available data concerning efficacy of treating both macular edema and neovascularization along with new data on federal budgetary costs of blindness.\n    \n\n\n          Research design and methods:\n        \n      \n      We used computer modeling, incorporating data from population-based epidemiological studies and multicenter clinical trials. Monte Carlo simulation was used, combined with sensitivity analysis and present value analysis of cost savings.\n    \n\n\n          Results:\n        \n      \n      Screening and treatment for eye disease in patients with type II diabetes generates annual savings of $247.9 million to the federal budget and 53,986 person-years of sight, even at current suboptimal (60%) levels of care. If all patients with type II diabetes receive recommended care, the predicted net savings (discounted at 5%) exceeds $472.1 million and 94,304 person-years of sight. Nearly all savings are associated with detection and treatment of diabetic macular edema. Enrolling each additional person with type II diabetes into currently recommended ophthalmological care results in an average net savings of $975/person, even if all costs of care are borne by the federal government.\n    \n\n\n          Conclusions:\n        \n      \n      Our analysis indicates that prevention programs aimed at improving eye care for patients with diabetes not only reduce needless vision loss but also will provide a financial return on the investment of public funds."
        },
        {
            "title": "Improving visual prognosis of the diabetic patients during the past 30 years based on the data of the Finnish Register of Visual Impairment.",
            "abstract": "Purpose:\n        \n      \n      To evaluate changes in visual impairment (VI) due to diabetic retinopathy (DR) recorded in the Finnish Register of Visual Impairment (RVI) during the past 30 years.\n    \n\n\n          Methods:\n        \n      \n      Data from the visually impaired diabetic persons included in the RVI were analysed using three 10-year cohorts (1982-90, 1991-2000, 2001-10). Information on the age at the time of the first VI registration, severity of VI determined according to the World Health Organisation (WHO) definition, and the age at death was collected. VI due to proliferative (PDR) and non-proliferative (NPDR) DR were analysed separately.\n    \n\n\n          Results:\n        \n      \n      Data of 4080 patients whose primary cause for VI was DR were analysed. The median age at the time of notification of VI for the three cohorts was 39, 62 and 59 years in the PDR group and 71, 73 and 73 in the NPDR group, respectively. The proportion of blind persons was 42%, 22% and 15% in the PDR group and 10%, 9% and 4% in the NPDR group, respectively. The median age at death in the three cohorts was 54, 73 and 72 years in PDR group and 76, 79 and 80 years in the NPDR group, respectively. The standardized mortality ratio (SMR) compared with the general population was 8.3, 2.9 and 1.4 in persons with PDR and 3.4, 2.0 and 1.2 in those with NPDR, respectively.\n    \n\n\n          Conclusions:\n        \n      \n      A significant change in the profile of the VI in the PDR group has taken place in Finland. It was characterized by increased age at the time of VI notification, decreased severity of VI and higher age at death. Most evidently these improvements took place in the 1990s. The profile of VI in the NPDR group has changed only modestly. Compared with the general population, SMRs improved both in NPDR and PDR groups continuously."
        },
        {
            "title": "Does coronary CT angiography improve risk stratification over coronary calcium scoring in symptomatic patients with suspected coronary artery disease? Results from the prospective multicenter international CONFIRM registry.",
            "abstract": "Aims:\n        \n      \n      The prognostic value of coronary artery calcium (CAC) scoring is well established and has been suggested for use to exclude significant coronary artery disease (CAD) for symptomatic individuals with CAD. Contrast-enhanced coronary computed tomographic angiography (CCTA) is an alternative modality that enables direct visualization of coronary stenosis severity, extent, and distribution. Whether CCTA findings of CAD add an incremental prognostic value over CAC in symptomatic individuals has not been extensively studied.\n    \n\n\n          Methods and results:\n        \n      \n      We prospectively identified symptomatic patients with suspected but without known CAD who underwent both CAC and CCTA. Symptoms were defined by the presence of chest pain or dyspnoea, and pre-test likelihood of obstructive CAD was assessed by the method of Diamond and Forrester (D-F). CAC was measured by the method of Agatston. CCTAs were graded for obstructive CAD (>70% stenosis); and CAD plaque burden, distribution, and location. Plaque burden was determined by a segment stenosis score (SSS), which reflects the number of coronary segments with plaque, weighted for stenosis severity. Plaque distribution was established by a segment-involvement score (SIS), which reflects the number of segments with plaque irrespective of stenosis severity. Finally, a modified Duke prognostic index-accounting for stenosis severity, plaque distribution, and plaque location-was calculated. Nested Cox proportional hazard models for a composite endpoint of all-cause mortality and non-fatal myocardial infarction (D/MI) were employed to assess the incremental prognostic value of CCTA over CAC. A total of 8627 symptomatic patients (50% men, age 56 ± 12 years) followed for 25 months (interquartile range 17-40 months) comprised the study cohort. By CAC, 4860 (56%) and 713 (8.3%) patients had no evident calcium or a score of >400, respectively. By CCTA, 4294 (49.8%) and 749 (8.7%) had normal coronary arteries or obstructive CAD, respectively. At follow-up, 150 patients experienced D/MI. CAC improved discrimination beyond D-F and clinical variables (area under the receiver-operator characteristic curve 0.781 vs. 0.788, P = 0.004). When added sequentially to D-F, clinical variables, and CAC, all CCTA measures of CAD improved discrimination of patients at risk for D/MI: obstructive CAD (0.82, P < 0.001), SSS (0.81, P < 0.001), SIS (0.81, P = 0.003), and Duke CAD prognostic index (0.82, P < 0.0001).\n    \n\n\n          Conclusion:\n        \n      \n      In symptomatic patients with suspected CAD, CCTA adds incremental discriminatory power over CAC for discrimination of individuals at risk of death or MI."
        },
        {
            "title": "A Proactive Approach to Quantification of Blood Loss in the Perinatal Setting.",
            "abstract": "Objective:\n        \n      \n      To educate nurses and physicians on changing practice from visual estimation of blood loss to quantification of blood loss (QBL) and to replace estimation of blood loss with QBL for at least 85% of vaginal births during a 3-month period.\n    \n\n\n          Design:\n        \n      \n      Quality improvement project.\n    \n\n\n          Setting/local problem:\n        \n      \n      A midwestern U.S. urban community hospital with 1,200 annual births, where postpartum blood loss was being measured by using visual estimation.\n    \n\n\n          Participants:\n        \n      \n      A convenience sample of 43 intrapartum nurses and 17 physicians.\n    \n\n\n          Intervention/measurements:\n        \n      \n      A goal was set to use the QBL method for at least 85% of vaginal births for 3 consecutive months. Study participants were surveyed at baseline to assess their knowledge of the QBL method; they then received a 10-minute educational presentation by the clinical nurse specialist (CNS) on QBL. The CNS attended births on both 12-hour shifts to give support, evaluate correct use of the new drapes, and answer questions. Midway through the project, a brief survey was distributed to participants for their feedback. The CNS conducted a chart audit to determine the compliance rate for the QBL process.\n    \n\n\n          Results:\n        \n      \n      Data analysis indicated an average 89% compliance rate with the QBL process for the time period studied.\n    \n\n\n          Conclusion:\n        \n      \n      Education on the QBL method increased nurses' and physicians' awareness of the importance of using this method as the new standard of care for assessment of postpartum blood loss. Accuracy of postpartum blood loss measurement is critical to help prevent maternal morbidity and mortality. Nurses play a key role in the development and implementation of practice changes to use QBL measurement."
        },
        {
            "title": "Screening for lung cancer revisited and the role of sputum cytology and fluorescence bronchoscopy in a high-risk group.",
            "abstract": "Lung cancer is an epidemic disease that is underrepresented in the research funding for early detection and chemoprevention arenas. Screening programs have been discouraged for both financial and political reasons. Yet, increasing evidence suggests that screening and early detection may improve outcome in lung cancer. Sputum cytology examination has been shown in several studies to lead to detection of lung cancer at an earlier stage, resulting in an improved 5-year survival rate. Monoclonal antibody detection, fluorescence bronchoscopy, and low-dose spiral CT increase diagnostic sensitivity and improve the ability to localize early-stage lesions. Utilizing these new techniques and improving the definition of high-risk groups may improve the success and cost-effectiveness of early detection based on sputum cytology. The ultimate goal of improving long-term survival in lung cancer will be achieved only when cancer can be detected in its early stages and lesions can be localized in large numbers. Advances in the last 15 years offer an encouraging vision for the value of early detection and effective treatment for lung cancer."
        },
        {
            "title": "Relationship between ICU design and mortality.",
            "abstract": "Background:\n        \n      \n      Architectural design of health-care facilities can influence patient safety; however, it is unknown whether patient outcomes are significantly affected by ICU design.\n    \n\n\n          Methods:\n        \n      \n      Six hundred sixty-four patients admitted to the medical ICU (MICU) of Columbia University Medical Center during 2008 were included in this retrospective study. Patient outcome measures, which included hospital mortality, ICU mortality, ICU length of stay (LOS), and ventilator-free days, were compared based on random room assignment. Rooms that were not visible from the MICU central nursing station were designated as low-visible rooms (LVRs), whereas the remaining rooms were designated as high-visible rooms (HVRs).\n    \n\n\n          Results:\n        \n      \n      Overall hospital mortality did not differ among patients assigned to LVRs vs HVRs; however, severely ill patients (those with Acute Physiology and Chronic Health Evaluation II scores > 30) had significantly higher hospital mortality when admitted to an LVR than did similarly ill patients admitted to an HVR (82.1% and 64.0%, n = 39 and 75, respectively; P = .046). ICU mortality showed a similar pattern. ICU LOS and ventilator-free days did not differ significantly between groups.\n    \n\n\n          Conclusions:\n        \n      \n      Severely ill patients may experience higher mortality rates when assigned to ICU rooms that are poorly visualized by nursing staff and physicians."
        },
        {
            "title": "Efficacy and safety of on-pump beating heart surgery.",
            "abstract": "Background:\n        \n      \n      Beating-heart surgery with warm blood perfusion, instead of cardioplegic solution, has been widely accredited to be a feasible technique in the cardiac operation. However, few studies have addressed the efficacy and safety of on-pump beating-heart surgery, especially with large numbers of patients. In this study, the efficacy and safety of on-pump beating-heart surgery was evaluated by surveying 701 patients with cardiac disease.\n    \n\n\n          Methods:\n        \n      \n      Preoperative risk factors, intraoperative techniques, and postoperative complications were documented and evaluated in 701 consecutive patients (from January 1, 2002, to December 30, 2006) who underwent beating-heart surgery with continuous antegrade or retrograde warm blood perfusion at The People's Hospital of Guangxi Zhuang Autonomous Region.\n    \n\n\n          Results:\n        \n      \n      Among the 701 patients with beating-heart surgery, antegrade perfusion was used in 556 patients (79.32%); retrograde perfusion was used in 40 patients (5.71%); and retrograde perfusion followed by antegrade perfusion was performed in 93 patients (13.27%). Cardioplegic arrest was required in 12 patients (1.71%) for inadequate visualization. In 4 of 701 patients (0.57%) low cardiac output syndrome occurred. Hemoglobinuria occurred in 16 patients (2.28%). No air embolization or permanent high-degree atrioventricular block occurred in these patients. The crude mortality of the surveyed patients was 2.43% (17 of 701).\n    \n\n\n          Conclusions:\n        \n      \n      Our results indicate that on-pump beating-heart surgery is a relatively safe and reliable technique for treatment of cardiac diseases."
        },
        {
            "title": "Carbohydrate nutrition and inflammatory disease mortality in older adults.",
            "abstract": "Background:\n        \n      \n      Several studies suggest that carbohydrate nutrition is related to oxidative stress and inflammatory markers.\n    \n\n\n          Objective:\n        \n      \n      We examined whether dietary glycemic index (GI), dietary fiber, and carbohydrate-containing food groups were associated with the mortality attributable to noncardiovascular, noncancer inflammatory disease in an older Australian cohort.\n    \n\n\n          Design:\n        \n      \n      Analysis included 1490 postmenopausal women and 1245 men aged ge 49 y at baseline (1992-1994) from a population-based cohort who completed a validated food-frequency questionnaire. Cox proportional hazards ratios were calculated both for death from diseases in which inflammation or oxidative stress was a predominant contributor and for cardiovascular mortality.\n    \n\n\n          Results:\n        \n      \n      Over a 13-y period, 84 women and 86 men died of inflammatory diseases. Women in the highest GI tertile had a 2.9-fold increased risk of inflammatory death compared with women in the lowest GI tertile [multivariate hazard ratio in energy-adjusted tertile 3 (tertile 1 as reference): 2.89; 95% CI: 1.52, 5.51; P for trend: 0.0006, adjusted for age, smoking, diabetes, and alcohol and fiber consumption]. Increasing intakes of foods high in refined sugars or refined starches (P = 0.04) and decreasing intakes of bread and cereals (P = 0.008) or vegetables other than potatoes (P = 0.007) also independently predicted a greater risk, with subjects' GI partly explaining these associations. In men, only an increased consumption of fruit fiber (P = 0.005) and fruit (P = 0.04) conferred an independent decrease in risk of inflammatory death. No associations were observed with cardiovascular mortality.\n    \n\n\n          Conclusion:\n        \n      \n      These data provide new epidemiologic evidence of a potentially important link between GI and inflammatory disease mortality among older women."
        },
        {
            "title": "The long-term effects of laser photocoagulation treatment in patients with diabetic retinopathy: the early treatment diabetic retinopathy follow-up study.",
            "abstract": "Objectives:\n        \n      \n      To evaluate the long-term natural history and effects of laser photocoagulation treatment in patients with diabetic retinopathy.\n    \n\n\n          Design:\n        \n      \n      Follow-up study of the 214 surviving patients enrolled originally at the Johns Hopkins Clinical Center for the Early Treatment Diabetic Retinopathy Study (ETDRS), which was a clinical trial designed to evaluate the role of laser photocoagulation and aspirin treatment in patients with diabetic retinopathy.\n    \n\n\n          Methods:\n        \n      \n      Early Treatment Diabetic Retinopathy Study patients enrolled in the Johns Hopkins Clinical Center had complete eye examinations, including best-corrected visual acuity measurements, fundus photographs, and medical questionnaires throughout the 7-year study. They had the same examinations at the final long-term follow-up visit at the National Eye Institute, National Institutes of Health, 13 to 19.5 years after the initial laser photocoagulation (median, 16.7 years).\n    \n\n\n          Main outcome measures:\n        \n      \n      The major outcomes were mortality and the rates of moderate and severe vision loss. The secondary outcomes were progression of diabetic retinopathy and need for other eye surgery.\n    \n\n\n          Results:\n        \n      \n      Of the 214 patients who were alive at the end of the original ETDRS in 1989, 130 (61%) were deceased at the time of the re-examination. Of the 84 who were alive, 71 (85%) were examined at their long-term follow-up visit at the National Institutes of Health. At the long-term follow-up examination, 42% had visual acuity of 20/20 or better, and 84% had visual acuity of 20/40 or better in the better eye. Compared with baseline, 20% of patients had moderate vision loss (loss of 3 lines or more vision) in the better eye at follow-up. Only one patient had visual acuity of 20/200 bilaterally. He had visual acuity loss secondary to age-related macular degeneration. No patient had severe vision loss (worse than 5/200). All the initially untreated eyes of patients who had severe nonproliferative diabetic retinopathy or worse by the time of the ETDRS closeout visit of the original study received scatter photocoagulation treatment. Focal photocoagulation was performed in 43% bilaterally and 22% unilaterally. Cataract surgery was performed in 31% of the patients, vitrectomy in 17%, and glaucoma surgery in one patient.\n    \n\n\n          Conclusions:\n        \n      \n      As previously reported, the mortality rate of patients with diabetic retinopathy is much higher than that of the general population. For those who survived, aggressive follow-up, with treatment when indicated, seems to be associated with maintenance of good long-term visual acuity for most patients. The need for laser scatter photocoagulation with long-term follow-up seems to be high."
        },
        {
            "title": "The Nine-Step Minnesota Grading System for Eyebank Eyes With Age Related Macular Degeneration: A Systematic Approach to Study Disease Stages.",
            "abstract": "Purpose:\n        \n      \n      To refine the Minnesota Grading System (MGS) using definitions from the Age-Related Eye Disease Studies (AREDS) into a nine-step grading scale (MGS-9).\n    \n\n\n          Methods:\n        \n      \n      A nine-step grading scale descriptive analysis using three key phenotypic features (total drusen area, increased, and decreased pigmentation) of human eyebank eyes that were graded according to definitions from the AREDS criteria in order to harmonize studies of disease progression for research involving human tissue. From 2005 through February 2017, we have analyzed 1159 human eyes, procured from two eyebanks. Each macula was imaged using high-resolution, stereoscopic color fundus photography with both direct- and transillumination. Fundus images were digitally overlaid with a grading template and triangulated for foveal centration.\n    \n\n\n          Results:\n        \n      \n      We documented and stratified risk for each globe by applying the AREDS nine-step grading scale to the key clinical features from the MGS-9. We found a good distribution within the MGS categories (1-9) with few level eight globes. Eyes were processed within 12.1 ± 6.3, hours from the time of death through imaging, dissection, and freezing or fixation. Applying the MGS-9 to 331 pairs (662 eyes were simultaneously graded), 84% were within one-grading step and 93% within two steps of the fellow eye. We also document reticular pseudodrusen, basal laminar drusen, and pattern dystrophy.\n    \n\n\n          Conclusions:\n        \n      \n      The MGS nine-step grading scale enables researchers using human tissue to refine the risk assessment of donor tissue. This analysis will harmonize results among researchers when grading human tissue using MGS criteria. Most importantly, the MGS-9 links directly to the known risk for progression from the AREDS."
        },
        {
            "title": "Drug reactions reported in a survey of South Carolina.",
            "abstract": "A survey of the membership of the South Carolina Ophthalmological Society revealed an average of 3 1/2 patient drug reactions per year per physician using topical diagnostic drugs in the office. The reactions to the drugs included topical allergic responses, acute glaucoma, cardiovascular collapse, and one death. All of the reactions had previous been attributed to these drugs in the literature. Suggestions are presented to prevent these reactions from occurring or to better prepare the ophthalmologist and his staff to manage these problems when they do arise."
        },
        {
            "title": "Automated utility assessment of global health.",
            "abstract": "The objective of this study was to characterize the performance of an automated utility assessment instrument for measuring preferences for overall health. The study population consisted of 83 subjects recruited from the cafeteria of a large tertiary care hospital. We assessed utilities for current health relative to perfect health and death using the rating scale, time tradeoff and standard gamble metrics. To validate the instrument, we compared utilities with the General Health subscale of the SF-36 Health Survey instrument, satisfaction with current health, and degree of bother due to current health. We evaluated interview failure rate based on irrational orderings of two practice assessments (monocular and binocular blindness) or inability to complete the interview. As expected, utility for overall health was statistically significantly associated with the General Health subscale score and measures of satisfaction with current health and degree of bother. There is substantial variation in utilities among patients with similarly severe overall health, and substantial overlap in utilities among subjects with different levels of overall health. The failure rate in the study was acceptable (9.6%). Automated assessment of utility for overall health provides a feasible means for estimating individual preferences."
        },
        {
            "title": "Decline in measured glomerular filtration rate is associated with a decrease in endurance, strength, balance and fine motor skills.",
            "abstract": "Aim:\n        \n      \n      Physical performance in chronic kidney disease affects morbidity and mortality. The aim was to find out which measures of physical performance are important in chronic kidney disease (CKD) and if there are associations with declining measured glomerular filtration rate (GFR).\n    \n\n\n          Methods:\n        \n      \n      Endurance was assessed by 6 min walk test (6-MWT) and stair climbing, muscular endurance by 30 s sit to stand, heel rises and toe lifts, strength by quadriceps- and handgrip-strength, balance by functional reach and Berg's balance scale, and fine motor skills by Moberg's picking-up test. GFR was measured by Iohexol clearance.\n    \n\n\n          Results:\n        \n      \n      The study comprised 101 patients with CKD 3b-5 not started dialysis, 40 women and 61 men, with a mean age of 67 ± 13 (range: 22 - 87) years. All measures of physical performance were impaired. A decrease in GFR of 10 mL/min per 1.73 m2 corresponded to a 35 metre shorter walking distance in the 6-MWT. Multivariable linear regression analysis showed significant relationships between decline in GFR and the 6-MWT (P = 0.04), isometric quadriceps strength left (P = 0.04), balance measured as functional reach (P = 0.02) and fine motor skills in the left hand as measured by Moberg's picking-up test (P = 0.01), respectively, after sex, age, comorbidity and the interaction between sex and age had been taken into account.\n    \n\n\n          Conclusion:\n        \n      \n      Endurance, muscular endurance, strength, balance and fine motor skills were impaired in patients with CKD 3b-5. Walking capacity, isometric quadriceps strength, balance, and fine motor skills were associated with declining GFR. The left extremities were more susceptible to GFR, ageing and comorbidities and seem thus to be more sensitive."
        },
        {
            "title": "The development and validation of the King's Sarcoidosis Questionnaire for the assessment of health status.",
            "abstract": "Rationale:\n        \n      \n      Health status is impaired in patients with sarcoidosis. There is a paucity of tools that assess health status in sarcoidosis. The objective of this study was to develop and validate the King's Sarcoidosis Questionnaire (KSQ), a new modular health status measure.\n    \n\n\n          Methods:\n        \n      \n      Patients with sarcoidosis were recruited from outpatient clinics. The development of the questionnaire consisted of three phases: item generation; item reduction, Rasch analysis to create unidimensional scales and validation; repeatability testing.\n    \n\n\n          Results:\n        \n      \n      207 patients with sarcoidosis (organ involvement: 184 lung, 54 skin, 45 eye disease) completed a 65-item preliminary questionnaire. 36 items were removed due to redundancy or poor fit to the Rasch model. The final version of the KSQ consisted of five modules (General health status, Lung, Skin, Eye, Medications). Internal consistency assessed with Cronbach's α coefficient was 0.70-0.93 for KSQ modules. Concurrent validity of the Lung module was high compared with St George's Respiratory Questionnaire (r=-0.83) and moderate when compared to forced vital capacity (r=0.49). Concurrent validity with skin-specific and eye-specific measures ranged from r=-0.4 to 0.8. The KSQ was repeatable over 2 weeks (n=39), intraclass correlation coefficients for modules were 0.90-0.96.\n    \n\n\n          Conclusions:\n        \n      \n      The KSQ is a brief, valid, self-completed health status measure for sarcoidosis. It can be used in the clinic to assess sarcoidosis from the patients' perspective."
        },
        {
            "title": "Evaluation of POSSUM and P-POSSUM scoring systems for predicting the mortality in elective neurosurgical patients.",
            "abstract": "A simple way of evaluating surgical outcomes is to compare mortality and morbidity. Such comparisons may be misleading without a proper case mix. The POSSUM scoring system was developed to overcome this problem. The score can be used to derive predictive mortality and morbidity for surgical procedures. POSSUM and a modified version P-POSSUM have been evaluated in various groups of surgical patients for the accuracy of predicting mortality. These scoring systems have not been evaluated in neurosurgical patients. Thus, we tried to evaluate the usefulness of POSSUM and P-POSSUM scoring systems in neurosurgical patients in predicting in-hospital mortality. POSSUM physiological and operative variables were collected from all neurosurgical patients undergoing elective craniotomy, from April 2005 to Feb 2006. In-hospital mortality was obtained from the hospital mortality register. The physiological score, operative score, POSSUM predicted mortality rate and P-POSSUM predicted mortality rate were calculated using a calculator. The observed number of deaths was compared against the predicted deaths. A total of 285 patients with a mean age of 38 +/- 15 years were studied. Overall observed mortality was nine patients (3.16%). The mortality predicted by the P-POSSUM model was also nine patients (3.16%). Mortality predicted by POSSUM was poor with predicted deaths in 31 patients (11%). The difference between observed and predicted deaths at different risk levels was not significant with P-POSSUM (p = 0.424) and was significantly different with POSSUM score (p < 0.001). P-POSSUM scoring system was highly accurate in predicting the overall mortality in neurosurgical patients. In contrast, POSSUM score was not useful for prediction of mortality."
        },
        {
            "title": "Quality of life and survival prediction in terminal cancer patients: a multicenter study.",
            "abstract": "Background:\n        \n      \n      It remains unclear whether health-related quality of life (HRQoL) measurements from patients and staff can be combined with medical data to predict survival in patients with terminal cancer.\n    \n\n\n          Methods:\n        \n      \n      The correlations between survival and potential health-related quality-of-life (HRQoL) prognostic variables were explored in 2 independent cohorts of patients with terminal cancer (248 patients in Cohort 1 and 756 patients in Cohort 2) after adjusting for clinical and demographics variables using Cox regression models.\n    \n\n\n          Results:\n        \n      \n      At the onset of the terminal phase (Cohort 1), the hazards of dying increased by 28% in the presence of dyspnea and by 68% in the presence of nausea/emesis; however, the most important predictors of worse survival were the presence of liver metastases (hazard ratio [HR], 2.5; 95% confidence interval [95% CI], 1.8-3.8), lung tumor (HR, 2.4; 95% CI, 1.7-3.4), and tumor burden (HR, 2.0; 95% CI, 1.4-2.7). In contrast, for patients who were seen in later stages of their terminal disease (Cohort 2), dyspnea (HR, 1.5; 95% CI, 1.1-1.9) and the coexistence of weakness with a diagnosis of digestive tumors (HR, 5.2; 95% CI, 1.2-21.8), breast tumors (HR, 3.1; 95% CI, 1.6-6.2), and genitourinary tumors (HR, 3.5; 95% CI, 1.6-7.8) were more predictive of survival than the type of tumor primary. Emotional functioning along with anxiety, spiritual distress, and lack of insight were not associated consistently with survival in both cohorts.\n    \n\n\n          Conclusions:\n        \n      \n      Health care professionals should focus on physical HRQoL indicators, such as nausea and emesis, dyspnea, and weakness, to gather prognostic clues in patients with terminal cancer. These symptoms may reflect consequences of cancer cachexia and the progress of patients toward this terminal syndrome. Psychosocial distress did not appear to be associated consistently with survival; however, future studies should clarify further the prognostic significance of \"positive attitudes\", such as hope and optimism, in patients with advanced cancer."
        },
        {
            "title": "Management and Systemic Implications of Diabetic Neovascular Glaucoma.",
            "abstract": "Objectives:\n        \n      \n      To study the efficacy and safety of different treatments for diabetic neovascular glaucoma (NVG). We additionally attempt to determine if the presence of NVG could be a predictor of cardiovascular disease or death.\n    \n\n\n          Method:\n        \n      \n      This is a retrospective, observational cohort study including patients diagnosed with diabetic NVG from 2006 to 2016 at the Hospital Clínico Universitario de Valladolid (Spain). Extracted data included clinical characteristics of the patients, glycated haemoglobin levels, and ocular treatment. Visual acuity (VA), intraocular pressure (IOP), cardiovascular events, and deaths were registered.\n    \n\n\n          Results:\n        \n      \n      30 eyes from 23 patients were followed for a mean of 4.48 years (SD = 2.82 years). The IOP-lowering intervention groups were: Ahmed implant (11 eyes), laser cyclo-photocoagulation (CPC; 6 eyes), both (4 eyes), or none (9 eyes). IOP success was achieved in 100% of the eyes with Ahmed and/or laser CPC and in 44.4% of the eyes with no IOP-lowering procedure (p= 0.002). Most eyes with Ahmed implant (with or without CPC) maintained or improved their VA (100 and 63.6%, respectively). 33.3% of the eyes with laser CPC and 25% of those with no IOP-lowering intervention maintained or improved their VA (p = 0.028). Hypotony was the only adverse effect (after laser CPC). No statistically significant difference could be established between low VA (finger count or worse), poor IOP control, or bad metabolic control and mortality or cardiovascular event (p > 0.05), however, the four patients who died had poor VA at the time of NVG diagnosis.\n    \n\n\n          Conclusions:\n        \n      \n      Ahmed implant surgery is a safe and effective treatment option for diabetic NVG. Medical treatment alone is not the best option for most cases. Advanced NVG could be an indicator of higher mortality risk in diabetic patients."
        },
        {
            "title": "Cognitive impairment after stroke: clinical determinants and its associations with long-term stroke outcomes.",
            "abstract": "Objectives:\n        \n      \n      To identify factors that were associated with cognitive impairment 3 months after stroke, and to examine the associations of cognitive impairment with stroke outcomes up to 4 years after stroke.\n    \n\n\n          Design:\n        \n      \n      Observational study.\n    \n\n\n          Setting:\n        \n      \n      Population-based stroke register.\n    \n\n\n          Participants:\n        \n      \n      Six hundred forty-five subjects with first-ever stroke, identified from the register.\n    \n\n\n          Measurements:\n        \n      \n      Subjects were assessed for cognition using the Mini-Mental State Examination (MMSE) 3 months after stroke. Cognitively impaired subjects (MMSE <24, n = 248 (38%)) were compared with cognitively intact subjects (MMSE 24-30, n = 397) in terms of demographic details, stroke risk factors, laterality of stroke, and initial poststroke impairments. Outcome data collected at 1, 3, and 4 years poststroke included disability assessed by the Barthel Index (BI) and the Frenchay Activity Index, case fatality, and institutionalization.\n    \n\n\n          Results:\n        \n      \n      Two hundred forty-eight (38%) of 645 subjects were cognitively impaired 3 months after stroke. Using multivariate analyses, cognitive impairment was associated with age of 75 and older (odds ratio (OR) = 2.5, 95% confidence interval (CI) = 1.5-4.2), ethnicity (Caribbean/African (OR = 1.9, 95% CI = 1.2-3.2) and Asian (OR = 3.4, 95% CI = 1.1-10.2), lower socioeconomic class (OR = 2.1, 95% CI = 1.3-3.3), left hemispheric lesion (OR = 1.6, 95% CI = 1.01-2.4), visual field defect (OR = 2.0, 95% CI = 1.2-3.2), and urinary incontinence (OR = 4.8, 95% CI = 3.1-7.3). Using multivariate analyses, cognitive impairment was associated with death or disability (BI <15) at 4 years after stroke (OR = 2.2, 95% CI = 1.1-4.5). In univariate analyses, it was also associated with higher institutionalization 4 years after stroke (P =.001).\n    \n\n\n          Conclusions:\n        \n      \n      Cognitive impairment is common 3 months after stroke and is independently associated with older age, ethnicity, lower social class, left hemispheric stroke, visual field defect, and urinary incontinence. It is associated with poor long-term outcomes, including survival and disability, up to 4 years after stroke. Because physical and cognitive impairments after stroke have independent prognostic implications, measures that evaluate both functions should be used in future studies of stroke outcome and in care of stroke patients."
        },
        {
            "title": "Visual acuity, contrast sensitivity, and mortality in older women: Study of osteoporotic fractures.",
            "abstract": "## OBJECTIVES\nTo determine whether poorer visual acuity and contrast sensitivity are independent risk factors for all-cause and traumatic mortality in older women.\n## DESIGN\nTwelve-year prospective cohort study (1986-2003).\n## SETTING\nFour U.S. clinical centers.\n## PARTICIPANTS\nNine thousand seven hundred four postmenopausal white women aged 65 and older.\n## MEASUREMENTS\nHabitually corrected binocular visual acuity and low- and high-frequency contrast sensitivity were measured at baseline using a standard protocol. A study physician adjudicated the primary cause of death from death certificates and medical record review.\n## RESULTS\nDuring an average of 12.2 years of follow-up, 3,427 women died (35%), 72 (0.7%) from traumatic events. In multivariate models adjusted for age, chronic medical problems, and smoking, all-cause mortality risk was 19% greater for persons in the worst quartile of visual acuity than for those in the best (hazard ratio (HR) = 1.19, P = .008) and 39% greater for persons with the worst contrast sensitivity (HR = 1.39, P < .001) than for those with the best. Traumatic mortality risk was 2.4 times greater for women with the worst contrast sensitivity than for those with the best (HR = 2.44, P = .03).\n## CONCLUSION\nPoorer visual acuity and contrast sensitivity are associated with greater risk of traumatic and all-cause mortality in older women, even after controlling for demographic and clinical characteristics. Although further research is necessary to determine how treating reversible causes of visual impairment or improving current refraction affects mortality in older women, clinical detection and follow-up of these visual impairments holds promise for identifying those who are at risk of mortality from other systemic conditions.\n"
        },
        {
            "title": "Prediction of intracerebral haemorrhage expansion with clinical, laboratory, pharmacologic, and noncontrast radiographic variables.",
            "abstract": "Background:\n        \n      \n      Hematoma expansion confers excess mortality in intracerebral haemorrhage, and is potentially preventable if at-risk patients can be identified. Contrast extravasation on initial computed tomographic angiography strongly predicts hematoma expansion but is not very sensitive, and most centers have not yet integrated computed tomographic angiography into acute intracerebral haemorrhage management. We therefore asked whether other presentation variables can predict hematoma expansion.\n    \n\n\n          Methods:\n        \n      \n      We searched the electronic medical records of a large integrated healthcare delivery system to identify patients with a hospitalization discharge diagnosis of intracerebral haemorrhage between the years 2008 and 2010. Hematoma expansion was defined as radiographic increase by 1/3 or by 12·5 ml within 48 h of presentation. Pre-specified patient demographic and clinical presentation variables were extracted. Stepwise multivariable logistic regression was performed to model hematoma expansion. Because some patients may have died from hematoma expansion without a second head computed tomography, we constructed a separate model including patients that died without a second head computed tomography in 48 h, hematoma expansion or death.\n    \n\n\n          Results:\n        \n      \n      Ninety-one of 257 patients (35%) had hematoma expansion. Antithrombotic use (odds ratio = 1·9, P = 0·04) and initial mNIHSS (modified National Institutes of Health Stroke Scale; odds ratio = 1·06, P = 0·001) were significant predictors in the hematoma expansion model (area under the Receiver-Operator Characteristics curve, AUROC = 0·6712, pseudo-r(2) = 0·0641). 163 of 343 patients (48%) had hematoma expansion or death. Age (odds ratio = 1·02, P = 0·02), initial mNIHSS (odds ratio = 1·07, P < 0·001), and initial hematoma volume (odds ratio = 1·01, P = 0·03) were significant predictors of hematoma expansion or death (AUROC = 0·7579, pseudo-r(2) = 0·1722).\n    \n\n\n          Conclusion:\n        \n      \n      Clinical and noncontrast radiographic variables only weakly predict hematoma expansion. Examination of other indicators, such as computed tomographic angiography contrast extravasation (the 'spot sign'), may prove more valuable in acute intracerebral haemorrhage care."
        },
        {
            "title": "What predicts a poor outcome in older stroke survivors? A systematic review of the literature.",
            "abstract": "Purpose:\n        \n      \n      To identify factors in the early post-stroke period that have a predictive value for a poor outcome, defined as institutionalization or severe disability.\n    \n\n\n          Methods:\n        \n      \n      MEDLINE, PSYCINFO, EMBASE and CINAHL were systematically searched for observational cohort studies in which adult and/or elderly stroke patients were assessed ≤ 1 month post-stroke and poor outcome was determined after a follow-up of ≥ 3 months.\n    \n\n\n          Results:\n        \n      \n      Thirty three articles were selected from 4063 records, describing 27 independent cohort studies. There are rather consistent findings that greater age, a more severe stroke (measured through a clinical evaluation scale), the presence of urinary incontinence (with impaired awareness) and a larger stroke volume (measured through brain imaging techniques) predict poor stroke outcome. In contrast to clinical expectations, the prognostic value of ADL-dependency and impaired cognition remains unclear, and factors in the domains of emotional and communicative functioning rarely feature. Studies using a selected group of stroke patients tended to identify different predictors.\n    \n\n\n          Conclusions:\n        \n      \n      The current evidence is insufficient for the development of a clinical prediction tool that is better than physicians' informal predictions. Future research should focus on the selection of optimal screening instruments in multiple domains of functioning, including the timing of assessment. We suggest developing prediction tools stratified by more homogeneous, clinically distinguished stroke subtypes.\n    \n\n\n          Implications for rehabilitation:\n        \n      \n      A reliable prognosis soon after a stroke is highly relevant to patients who ultimately have a poor outcome, because it enables early planning of care tailored to their needs. In view of the development of a clinical prediction tool that is better than physicians' informal predictions, future research should focus on optimal screening instruments in multiple domains of functioning, including emotional and communicative functioning. Clinical prediction tools stratified by more homogeneous, clinically distinguished stroke subtypes, could enable more accurate prognosis in individual stroke patients."
        },
        {
            "title": "Diabetic retinopathy and coronary artery disease from the cardiac surgeon's perspective.",
            "abstract": "Coronary artery disease is the leading cause of death in diabetics; therefore, the main purpose of managing coronary artery disease in diabetics should be to lengthen life expectancy. Recent evidence demonstrates that the severity of diabetic retinopathy is associated with a graded, increased risk of death from coronary artery disease and myocardial infarction. Recently, we found that the survival benefit of coronary artery bypass grafting over percutaneous coronary intervention is more apparent in patients with diabetic retinopathy than in diabetic patients without it. In this article, we review published studies evaluating the association between diabetic retinopathy and coronary artery disease, and we propose that coronary artery bypass surgery should be the first choice for revascularization of patients with diabetic retinopathy, especially in its early stage. Furthermore, coronary artery disease complicating diabetic retinopathy is often underdiagnosed, and all diabetic retinopathy patients should undergo screening for coronary artery disease followed by coronary artery bypass grafting. Future studies will probably comprise carefully performed cost-effective analyses of treatment effectiveness and prospective randomized studies comparing survival after coronary artery bypass grafting with that of survival after percutaneous coronary intervention, stratified by the stage of retinopathy."
        },
        {
            "title": "Toward clinical risk assessment in hypertrophic cardiomyopathy with gadolinium cardiovascular magnetic resonance.",
            "abstract": "Objectives:\n        \n      \n      We sought to assess whether hyperenhancement by gadolinium cardiovascular magnetic resonance (CMR) occurs in hypertrophic cardiomyopathy (HCM) and correlates with the risk of heart failure and sudden death.\n    \n\n\n          Background:\n        \n      \n      The myocardial interstitium is abnormal in HCM at post-mortem. Focally increased interstitial myocardial space appears as hyperenhancement with gadolinium CMR.\n    \n\n\n          Methods:\n        \n      \n      In a blinded, prospective study, HCM patients were selected for the presence (n = 23) or absence (n = 30) of an increased clinical risk of sudden death and/or progressive adverse left ventricular (LV) remodeling. Gadolinium-enhanced CMR was performed.\n    \n\n\n          Results:\n        \n      \n      Myocardial hyperenhancement was found in 42 patients (79%), affecting 10.9% (range 0% to 48%) of the LV mass. There was a greater extent of hyperenhancement in patients with progressive disease (28.5% vs. 8.7%, p < 0.001) and in patients with two or more risk factors for sudden death (15.7% vs. 8.6%, p = 0.02). Improved discrimination was seen in patients >40 years old (29.6% vs. 6.7%, p < 0.001) for progressive disease and for patients <40 years old for risk factors for sudden death (15.7% vs. 2.1%, p = 0.002). Patients with diffuse rather than confluent enhancement had two or more risk factors for sudden death (87% vs. 33%, p = 0.01).\n    \n\n\n          Conclusions:\n        \n      \n      Gadolinium CMR reveals myocardial hyperenhancement in HCM. The extent of hyperenhancement is associated with progressive ventricular dilation and markers of sudden death."
        },
        {
            "title": "Sensitivity of diagnostic examinations for colorectal polyps.",
            "abstract": "The removal of adenomatous polyps of the large bowel reduces mortality from colorectal cancer (CRC). Faecal occult blood testing only reveals 20.40% of polyps. The flexible rectosigmoidoscope explores less than half of the large bowel. Its use should always be coupled with faecal occult blood testing which, if positive, requires a total colonoscopy. The sensitivity of double-contrast barium enema for the search of polyps is 35%. Colonoscopy does not reach the caecum in about 10% of cases. It misses 15-20% of polyps with diameter <10 mm and about 6% of polyps with diameter >10 mm. Virtual colonoscopy has substantially the same sensitivity as optical colonoscopy for polyps > or =7 mm in diameter."
        },
        {
            "title": "Predictive value of GRACE risk scores for contrast-induced acute kidney injury in patients with ST-segment elevation myocardial infarction before undergoing primary percutaneous coronary intervention.",
            "abstract": "Objectives:\n        \n      \n      Contrast-induced acute kidney injury (CI-AKI) is a well-known serious complication of percutaneous coronary intervention (PCI) and may cause increased morbidity and mortality. We aim to identify the predictive value of Global Registry for Acute Coronary Events (GRACE) risk scores for CI-AKI in patients with ST-segment elevation myocardial infarction (STEMI) before primary PCI, allowing pre-procedural decisions regarding prevention therapy for CI-AKI.\n    \n\n\n          Methods:\n        \n      \n      We enrolled 251 consecutive patients with STEMI undergoing primary PCI. Receiver operating characteristic curves were used to identify the optimal sensitivity for the observed range of GRACE risk scores. CI-AKI was defined as any of the following: absolute increase in serum creatinine (SCr) of ≥ 0.3 or ≥ 0.5 mg/dL within 48-72 h after contrast exposure, or a percentage increase in SCr level of ≥ 50 %.\n    \n\n\n          Results:\n        \n      \n      Forty-three patients (17.1 %) developed CI-AKI0.3, 22 (8.8 %) CI-AKI0.5, and 19 (7.6 %) CI-AKI50. The GRACE quartiles were as follows: Q1 (<136), Q2 (136-159), Q3 (159-180), and Q4 (>180). Patients with high GRACE risk scores had higher risk for CI-AKI0.3, 0.5, and 50 (6.6, 6.6, 23.4, 31.7 %, respectively, p < 0.001; 1.6, 1.6, 9.4, 22.2 %, respectively, p < 0.001; and 3.3, 3.2, 9.4, 14.3 %, respectively, p = 0.009). ROC showed that a GRACE risk score >160 was a fair discriminator for CI-AKI0.3, 0.5, and 50 (C statistic = 0.723, 0.788, 0.668, respectively). After adjusting for potential confounding predictors, GRACE risk score >160 remained significantly associated with CI-AKI0.3 or 0.5 (OR 3.84; 95 % CI 1.61-9.17; p = 0.002, or OR 5.54; 95 % CI 1.42-21.66; p = 0.014), and high-sensitivity C-reactive protein (Hs-CRP) >15.5 mg/L was a highly significant predictor of CI-AKI0.3, 0.5, and CI-AKI50.\n    \n\n\n          Conclusions:\n        \n      \n      GRACE risk score (>160) and post-procedural Hs-CRP >15.5 mg/L are independent and significant predictors of CI-AKI in patients with STEMI before primary PCI."
        },
        {
            "title": "Common comorbidity scales were similar in their ability to predict health care costs and mortality.",
            "abstract": "Objective:\n        \n      \n      To compare the ability of commonly used measures of medical comorbidity (ambulatory care groups [ACGs], Charlson comorbidity index, chronic disease score, number of prescribed medications, and number of chronic diseases) to predict mortality and health care costs over 1 year.\n    \n\n\n          Study design and setting:\n        \n      \n      A prospective cohort study of community-dwelling older adults (n=3,496) attending a large primary care practice.\n    \n\n\n          Results:\n        \n      \n      For predicting health care charges, the number of medications had the highest predictive validity (R(2)=13.6%) after adjusting for demographics. ACGs (R(2)=16.4%) and the number of medications (15.0%) had the highest predictive validity for predicting ambulatory visits. ACGs and the Charlson comorbidity index (area under the receiver operator characteristic [ROC] curve=0.695-0.767) performed better than medication-based measures (area under the ROC curve=0.662-0.679) for predicting mortality. There is relatively little difference, however, in the predictive validity across these scales.\n    \n\n\n          Conclusion:\n        \n      \n      In an outpatient setting, a simple count of medications may be the most efficient comorbidity measure for predicting utilization and health-care charges over the ensuing year. In contrast, diagnosis-based measures have greater predictive validity for 1-year mortality. Current comorbidity measures, however, have only poor to moderate predictive validity for costs or mortality over 1 year."
        },
        {
            "title": "Increased association of the ERG oncoprotein expression in advanced stages of prostate cancer in Filipinos.",
            "abstract": "Background:\n        \n      \n      Filipinos with prostate cancer (CaP) are at increased risk of harboring advanced stages and lower survival rates compared to other Asians. This study aims to investigate prevalence of ETS-related gene (ERG) oncoprotein overexpression in Filipinos as surrogate of TMPRSS2-ERG gene fusions, using a highly specific monoclonal antibody (ERG-MAb), and conduct the first attempt to study the role of genetic alterations in the aggressive tumor biologic behaviour of CaP among Filipinos.\n    \n\n\n          Methods:\n        \n      \n      This case-matched, case-control retrospective study evaluated ERG expression in Filipino patients diagnosed with CaP and its effect on stage and Gleason grade of their disease. Men who underwent radical prostatectomy for organ-confined disease at the University of the Philippines-Philippine General Hospital (UP-PGH) comprised the organ-confined cohort. Age-matched adults who had trans-rectal ultrasound-guided prostate (TRUSP) biopsy or trans-urethral resection of the prostate (TURP) with bilateral orchiectomy for T4 or stage IV CaP composed the advanced disease cohort.\n    \n\n\n          Results:\n        \n      \n      Overall ERG expression frequency of 23.08% (N = 104) was demonstrated, with a higher rate observed in the advanced disease cohort (32.69%) compared to the organ-confined group (13.46%). Furthermore, ERG overexpression was only detected among intermediate and high-risk tumors. A high-specificity (98.08%) of the ERG-MAb for malignant prostatic cells was likewise demonstrated.\n    \n\n\n          Conclusions:\n        \n      \n      In contrast to higher ERG frequency in Western countries, it is much lower in Filipino CaP, which is similar to lower rates noted from other Asian countries. The 98.08% specificity of ERG oncoprotein for prostate tumor cells combined with its increased association in advanced disease, suggests for prognostic potential of ERG that may aid clinicians in treatment decisions for Filipino CaP patients."
        },
        {
            "title": "A nationwide population-based study of social demographic factors, associated diseases and mortality of keratoconus patients in Denmark from 1977 to 2015.",
            "abstract": "Purpose:\n        \n      \n      To study sociodemographic factors, associated diseases and survival of Danish keratoconus patients.\n    \n\n\n          Methods:\n        \n      \n      All patients diagnosed with keratoconus 1977-2015 (n = 2679) were matched to 10 persons who had not been diagnosed with keratoconus (n = 26 790). Conditional logistic regression assessed whether sociodemographic factors and specific systemic diseases were associated with the odds of keratoconus. Mortality was assessed with time-to-event analysis.\n    \n\n\n          Results:\n        \n      \n      After adjustment, non-Europeans had more than threefold higher odds of keratoconus compared to Europeans (OR, 3.34; 96% CI 2.94-3.80). Single persons had 27% higher odds (OR, 1.27; 95% CI 1.13-1.43), and divorced persons had 18% lower odds (OR 0.82; 95% CI 0.68-0.97) of keratoconus compared with persons in a relationship. Persons living in cities with <500 and 500-4999 inhabitants had 40% (OR, 0.60; 95% CI 0.51-0.71) and 30% (OR, 0.70; 95% CI 0.61-0.81) lower odds of keratoconus, respectively, compared with those living in the capital (>1 000 000 inhabitants). Persons receiving government substitution had 68% higher odds of keratoconus (OR, 1.68; 95% CI 1.30-2.17) compared to self-employed. Keratoconus patients had more than twofold higher odds of asthma (OR, 2.21; 95% CI 1.91-2.55), more than threefold higher odds of allergic rhinitis (OR, 3.44; 95% CI 2.75-4.30), more than sevenfold higher odds of atopic dermatitis (OR, 7.97; 95% CI, 6.21-10.21) and 69% higher odds of depression (OR, 1.69; 95% CI 1.18-2.43). Mortality rates were similar among keratoconus patients and controls (HR, 1.02; 95% CI 0.90-1.16).\n    \n\n\n          Conclusion:\n        \n      \n      Danish keratoconus patients differ from controls on several sociodemographic factors and have higher risk of allergic rhinitis, asthma, atopic dermatitis and depression. They do not have excess mortality compared to controls."
        },
        {
            "title": "The impact of probable anxiety and mood disorder on self-reported collisions: a population study.",
            "abstract": "Background:\n        \n      \n      Individuals diagnosed with psychiatric disorder are at significantly increased risk of death and serious injury, to which motor vehicle collisions may be important contributors. This study examined the association between probable anxiety or mood disorder (AMD) and self-reported collision risk in a large representative sample of the adult population in Ontario.\n    \n\n\n          Methods:\n        \n      \n      Based on data from a regionally stratified general-population telephone survey of adults conducted from 2002 through 2009 (N=12,830), a logistic regression analysis examined self-reported collision involvement in the previous 12 months by measures of demographic characteristics, driving exposure, impaired driving behaviour, and probable AMD.\n    \n\n\n          Results:\n        \n      \n      Controlling for demographic variables and potential confounders, probable AMD was associated with an increased risk of collision involvement (OR=1.78, 95% CI=1.37, 2.31).\n    \n\n\n          Limitations:\n        \n      \n      The use of self-report measures and the potential for bias created by groups excluded because they do not have access to landline telephones represent limitations to the current findings. Nevertheless, the benefits of a large sample derived from general population survey data far outweigh these limitations.\n    \n\n\n          Conclusions:\n        \n      \n      The results suggest that the increased risk of injury and mortality associated with some psychiatric disorders is at least partially related to increased risk of collision involvement. The magnitude of the increase in risk associated with probable AMD is similar to that seen among individuals who drive after drinking or using cannabis. In view of these findings, more work to understand this risk among individuals experiencing probable AMD and how it can be avoided is necessary."
        },
        {
            "title": "Mehran contrast nephropathy risk score: Is it still useful 10 years later?",
            "abstract": "Background:\n        \n      \n      Nowadays, contrast-induced nephropathy (CIN) is the third cause of acquired acute renal impairment in hospital. CIN is related to increased in-hospital morbidity, mortality, costs of medical care, and long admissions. Because of this, we hypothesized it would be useful to determine the risk of CIN with scores such as the Mehran score. The aim of this study was to validate the Mehran score in a contemporary cohort of Spanish patients with acute coronary syndrome (ACS).\n    \n\n\n          Methods:\n        \n      \n      We assessed the calibration and discriminatory capacity of Mehran score to predict CIN in a cohort of 1520 patients with a definitive diagnosis of ACS and who underwent coronary angiography between March 2008 and June 2012. We excluded patients on chronic dialysis and those without data of contrast volume. The calibration of the model was assessed with the Hosmer-Lemeshow goodness-of-fit test and discriminatory capacity was assessed by C-statistic, which is equivalent to the area under the receiver-operating characteristic curve.\n    \n\n\n          Results:\n        \n      \n      From the total group, 118 patients (7.8%) developed CIN. They were older, with higher rates of diabetes (DM) and hypertension and worse renal function and anemia (p<0.001). The odds ratios for different score components in Mehran's population versus our study were similar except for DM, hypotension, and intra-aortic balloon pump (1.6%, 2.68%, 2.55% vs 0.9%, 1.89%, and 2.86%, respectively). Calibration and discriminatory capacity of Mehran score were excellent with a Hosmer-Lemeshow p=0.7, C-statistic value >0.8.\n    \n\n\n          Conclusions:\n        \n      \n      Mehran risk score has been validated in our study as a good score for predicting CIN in patients with ACS who underwent coronary angiography. According to this, we support its use in patients hospitalized for ACS in order to identify the ones at risk, and to optimize CIN prophylactic therapy prior to and after catheterization."
        },
        {
            "title": "Cost-effectiveness model for colon cancer screening.",
            "abstract": "Background & aims:\n        \n      \n      The relative efficacy and effectiveness of different colon screening programs has not been assessed. The purpose of this analysis was to provide a model for comparing several colon screening programs and to determine the key variables that impact program effectiveness.\n    \n\n\n          Methods:\n        \n      \n      Five screening programs were compared: annual fecal occult blood test (FOBT) alone, flexible sigmoidoscopy, flexible sigmoidoscopy and FOBT combined, one-time colonoscopy, and air-contrast barium enema. Key variables were adjusted for sensitivity analyses. Cost-effectiveness was defined as the cost per cancer death prevented.\n    \n\n\n          Results:\n        \n      \n      FOBT alone prevents fewer cancer deaths than the other programs. The addition of flexible sigmoidoscopy to the FOBT increases the rate of cancer prevention. One-time colonoscopy has the greatest impact on colorectal cancer mortality, largely because of assumptions that cancer would be prevented in most patients who undergo polypectomy. FOBT alone is the most cost-effective of the programs, but the cost is sensitive to several key variables.\n    \n\n\n          Conclusions:\n        \n      \n      The model shows key variables that impact the cost-effectiveness of colon screening programs. Compliance is an important determinant of effectiveness of all of the screening programs. Future study should be focused on methods of patient education that improve patient compliance with screening."
        },
        {
            "title": "[CT angiography of the aorta].",
            "abstract": "Aortic disease is associated with high morbidity and mortality and thus require an efficient and accurate diagnostic approach, especially in the acute setting. Multislice computed tomography (MSCT) with the option of high-resolution CT angiography (CTA) has emerged as the standard of reference in diagnosis and follow-up of patients with acquired aortic disease. Aortic dissection is the most common aortic emergency, but it remains undiscovered in up to 38% of cases. Sensitivity and specificity of MSCT in the assessment of aortic dissection are greater than 99%. The sensitivity of CT in the detection of inflammatory changes is 83%; its specificity is almost 100%; and its diagnostic accuracy is ca. 94%. This article outlines state-of-the-art principles in diagnostic CT imaging of acquired aortic disease."
        },
        {
            "title": "Prevalence of selected risk behaviors and chronic diseases--Behavioral Risk Factor Surveillance System (BRFSS), 39 steps communities, United States, 2005.",
            "abstract": "Problem:\n        \n      \n      Behavioral risk factors (e.g., tobacco use, poor diet, and physical inactivity) can lead to chronic diseases. In 2005, of the 10 leading causes of death in the United States, seven (heart disease, cancer, stroke, chronic lower respiratory diseases, diabetes, Alzheimer's disease, and kidney disease) were attributable to chronic disease. Chronic diseases also adversely affect the quality of life of an estimated 90 million persons in the United States, resulting in illness, disability, extended pain and suffering, and major limitations in daily living.\n    \n\n\n          Reporting period covered:\n        \n      \n      2005.\n    \n\n\n          Description of the system:\n        \n      \n      CDC's Steps Program funds 40 selected U.S. communities to address six leading causes of death and disability and rising health-care costs in the United States: obesity, diabetes, asthma, physical inactivity, poor nutrition, and tobacco use. In 2005, a total of 39 Steps communities conducted a survey to collect adult health outcome data. The survey instrument was a modified version of the Behavioral Risk Factor Surveillance System (BRFSS) survey, a community-based, random-digit--dialing telephone survey with a multistage cluster design. The survey instrument collected information on health risk behaviors and preventive health practices among noninstitutionalized adults aged >/=18 years.\n    \n\n\n          Results:\n        \n      \n      Prevalence estimates of risk behaviors and chronic conditions varied among the 39 Steps communities that reported data for 2005. The proportion of the population that achieved Healthy People 2010 (HP 2010) objectives also varied among the communities. The estimated prevalence of obesity (defined as having a body mass index [BMI] of >/=30.0 kg/m(2) as calculated from self-reported weight and height) ranged from 15.6% to 44.0%. No communities reached the HP2010 objective of reducing the proportion of adults who are obese to 15.0%. The prevalence of diagnosed diabetes (excluding gestational diabetes) ranged from 4.3% to 16.6%. Eighteen communities achieved the HP2010 objective to increase the proportion of adults with diabetes who have at least an annual foot examination to 75.0%; five communities achieved the HP2010 objective to increase the proportion of adults with diabetes who have an annual dilated eye examination to 75.0%. The prevalence of reported asthma ranged from 7.0% to 17.6%. Among those who reported having asthma, the prevalence of having no symptoms of asthma during the preceding 30 days ranged from 15.4% to 40.3% for 10 communities with sufficient data for estimates. The prevalence of respondents who engaged in moderate physical activity for >/=30 minutes at least five times a week or who reported vigorous physical activity for >/=20 minutes at least three times a week ranged from 42.0% to 62.2%. The prevalence of consumption of fruits and vegetables at least five times a day ranged from 15.6% to 30.3%. The estimated prevalence among respondents aged >/=18 years who reported having smoked >/=100 cigarettes in their lifetime and who were current smokers on every day or some days at the time of the survey ranged from 11.0% to 39.7%. One community achieved the HP2010 objective to reduce the proportion of adults who smoke to 12.0%. Among smokers, the prevalence of having stopped smoking for >/=1 day as a result of trying to quit smoking during the previous 12 months ranged from 47.8% to 63.3% for 31 communities. No communities reached the HP2010 objective of increasing smoking cessation attempts by adult smokers to 75%.\n    \n\n\n          Interpretation:\n        \n      \n      The findings in this report indicate variations in health risk behaviors, chronic conditions, and use of preventive health screenings and health services. These findings underscore the continued need to evaluate intervention programs at the community level and to design and implement policies to reduce morbidity and mortality caused by chronic disease.\n    \n\n\n          Public health action:\n        \n      \n      Steps BRFSS data can be used to monitor the prevalence of specific health behaviors, diseases, conditions, and use of preventive health services. Steps Program staff at the national, state, local, and tribal levels can use BRFSS data to demonstrate accountability to stakeholders, monitor progress in meeting program objectives, focus programs on activities with the greatest promise of results, identify opportunities for strategic collaboration, and identify and disseminate successes and lessons learned."
        },
        {
            "title": "Enhancement of a small bowel obstruction model using the gastrografin® challenge test.",
            "abstract": "Background:\n        \n      \n      Based on a previous published data on small bowel obstruction (SBO), a management model for predicting the need for exploration has been adopted in our institution. In our model, patients presenting with three criteria-the history of obstipation, the presence of mesenteric edema, and the lack of small bowel fecalization on computed tomography (CT)-undergo exploration. Patients with two or less features were managed nonoperatively. An alternative tool for predicting need for operative intervention is Gastrografin (GG) challenge test.\n    \n\n\n          Hypothesis:\n        \n      \n      We hypothesized that the GG challenge test, when used in combination with our prior model, will decrease the rate of explorations in patients not meeting the criteria for immediate operation.\n    \n\n\n          Methods:\n        \n      \n      An approval from IRB was obtained to review patients admitted with a diagnosis of SBO from November 2010 to September 2011. All patients presenting with signs of ischemia, patients with all three model criteria defined previously, and those who had an abdominal operation within 6 weeks of diagnosis were excluded. All patients had an abdominal/pelvic CT and GG challenge at the time of diagnosis. Patients were compared to historic controls managed without the GG challenge (from July to December 2009). Successful GG challenge was defined as the presence of contrast in the colon after a follow-up film or a bowel movement. Data were presented as medians or percentages; significance was considered at p < 0.05.\n    \n\n\n          Results:\n        \n      \n      One hundred and twenty-five patients with a diagnosis of small bowel obstruction were identified wherein 47 % were males. Fifty-three received a GG challenge (study), and 72 did not have a GG challenge (historic). There was no difference in age (70 vs 65 years), history of prior SBO (51 vs 49 %), history of diabetes mellitus (21 vs 18 %), history of malignancy (32 vs 39 %), or cardiac disease (30 vs 39 %). Both groups had similar number of previous abdominal operations (two vs two). The presence of mesenteric edema (68 vs 75 %), the lack of small bowel fecalization (47 vs 46 %), and a history of obstipation (25 vs 24 %) were similar in both groups. Patients in the study group had a lesser rate of abdominal exploration (25 vs 42 %, p = 0.05) and fewer complications (13 vs 31 %, p = 0.02) compared to the historic control group. There was equivalent incidence of ischemic bowel (4 vs 7 %), duration of hospital stay (4 vs 7 days), duration from admission to operation (2 vs 3 days), and mortality (8 vs 6 %); 44 patients had a successful GG challenge with nine failures. There was a greater rate of exploration in patients with a failed challenge compared to those with a successful challenge (89 vs 11 %, p < 0.01).\n    \n\n\n          Conclusion:\n        \n      \n      The use of the GG challenge enhanced the SBO prediction model by decreasing the need for exploration in patients not meeting the criteria for immediate operation. Patients who failed the GG challenge test were much more likely to undergo an exploration."
        },
        {
            "title": "Auckland proliferative diabetic vitrectomy fellow eye study.",
            "abstract": "Background:\n        \n      \n      To review medical and ophthalmic findings of primary diabetic vitrectomy patients to examine indices important in progression to fellow eye surgery.\n    \n\n\n          Methods:\n        \n      \n      A retrospective analysis was undertaken of all diabetic patients undergoing vitreoretinal surgery at Auckland Public Hospital between January 1992 and July 1996. Kaplan-Meier survival analysis was performed along with univariate and multivariate (Cox Proportional Hazards) data analysis.\n    \n\n\n          Results:\n        \n      \n      One hundred and fourteen primary diabetic vitrectomy cases were reviewed with mean follow-up duration of 4 years. Thirty-eight per cent (n = 43) of the study group underwent fellow eye surgery at a mean time of 1.6 years after first eye surgery. Fourteen patients were already blind in the fellow eye at baseline, and five patients refused second eye surgery on intention to treat. Thus there were 62 (54%) patients with severe (surgical threshold) fellow eye disease diagnosed within the follow-up period. The presence of either tractional retinal detachment or combined rhegmatogenous/tractional retinal detachment but without vitreous haemorrhage in the presenting eye was, in this series, a risk factor for fellow eye surgery (OR 5.56; 95% CI 1.96-15.8). Maori and Pacific Islander ethnicity was significantly associated with traction retinal detachment (OR 2.23; 95% CI 1.05-4.7). At data analysis 57% (n = 60) of the study patients had died. The mean time to death was 4.3 years, with 84% of these patients having evidence of renal disease at the time of their first eye surgery. Good visual function in at least one eye was maintained in many patients.\n    \n\n\n          Conclusions:\n        \n      \n      A substantial proportion of diabetic vitrectomy patients require fellow eye surgery. Absence of vitreous haemorrhage in the presenting eye (i.e. tractional or combined rhegmatogenous/tractional retinal detachments but without vitreous haemorrhage) was predictive of need for fellow eye surgery. The need for diabetic vitrectomy correlates with poor survival in this study population."
        },
        {
            "title": "The natural history of non-arteritic anterior ischaemic optic neuropathy.",
            "abstract": "Seventy one patients with non-arteritic anterior ischaemic optic neuropathy were studied retrospectively. Sixty three (89%) were followed to the end of the study or death, mean follow up time was 5.3 years. Whilst twenty (28%) had diabetes or hypertension, in thirty nine (55%) no predisposing condition was identified. In those who had monocular disease at presentation (68), subsequent involvement of the second eye occurred in seventeen (25%), seven within the first year. Nineteen patients died within the study period. Of these, nine died from myocardial infarction and four from cerebrovascular disease. This is a significant increase above figures calculated from the Office of Population Census and Surveys (p less than 0.001 for all causes, p less than 0.002 for myocardial infarction and cerebrovascular disease). Such an increase in mortality has not been previously reported, and implies that this condition carries a more sinister systemic prognosis than is frequently supposed."
        },
        {
            "title": "CT Chest with IV Contrast Compared with CT Angiography after Blunt Trauma.",
            "abstract": "Blunt aortic injury (BAI) after chest trauma is a potentially lethal condition. Rapid diagnosis is important to appropriately treat patients. The purpose of this study was to compare CT with intravenous contrast (CTI) to CT with angiography (CTA) in the initial evaluation of blunt chest trauma patients. This was a retrospective review of all blunt trauma patients who received a CTI or CTA during the initial evaluation at an urban Level I trauma center from January 1, 2010 to December 31, 2013. Two-hundred and eighty-one trauma patients met inclusion criteria. Most, 167/281 (59%) received CTI and 114/281 (41%) received CTA. There were no differences between cohorts in age, gender, initial heart rate, systolic blood pressure, and Glasgow Coma Scale in emergency department. Mortality rates were similar for CTI and CTA (4% vs 8%, P = 0.20). CTI identified an injury in 54 per cent compared with 46 per cent in CTA (P = 0.05). Overall, 2 per cent of patients had BAI with similar rates in CTI and CTA (2% vs 2%, P = 0.80). BAI was not missed using either CTI or CTA. Trauma patients studied with CTI had similar diagnostic findings as CTA. CTI may be preferable to CTA during the initial assessment for possible BAI because of a single contrast injection for whole body CT."
        },
        {
            "title": "Survival Bias When Assessing Risk Factors for Age-Related Macular Degeneration: A Tutorial with Application to the Exposure of Smoking.",
            "abstract": "Purpose:\n        \n      \n      We illustrate the effect of survival bias when investigating risk factors for eye disease in elderly populations for whom death is a competing risk. Our investigation focuses on the relationship between smoking and late age-related macular degeneration (AMD) in an observational study impacted by censoring due to death.\n    \n\n\n          Methods:\n        \n      \n      Statistical methodology to calculate the survivor average causal effect (SACE) as a sensitivity analysis is described, including example statistical computing code for Stata and R. To demonstrate this method, we examine the causal effect of smoking history at baseline (1990-1994) on the presence of late AMD at the third study wave (2003-2007) using data from the Melbourne Collaborative Cohort Study.\n    \n\n\n          Results:\n        \n      \n      Of the 40,506 participants eligible for inclusion, 38,092 (94%) survived until the start of the third study wave, 20,752 (51%) were graded for AMD (60% female, aged 47-85 years, mean 65 ± 8.7 years). Late AMD was detected in 122 participants. Logistic regression showed strong evidence of an increased risk of late AMD for current smokers compared to non-smokers (adjusted naïve odds ratio 2.99, 95% confidence interval, CI, 1.74-5.13). Among participants expected to be alive at the start of follow-up regardless of their smoking status, the estimated SACE odds ratio comparing current smokers to non-smokers was at least 3.42 (95% CI 1.57-5.15).\n    \n\n\n          Conclusions:\n        \n      \n      Survival bias can attenuate associations between harmful exposures and diseases of aging. Estimation of the SACE using a sensitivity analysis approach should be considered when conducting epidemiological research within elderly populations."
        },
        {
            "title": "Falls in Older people with Cataract, a longitudinal evalUation of impact and riSk: the FOCUS study protocol.",
            "abstract": "Background:\n        \n      \n      Falls result in >$1 billion in treatment, disability, lost output and mortality each year in Australia and people with cataract are at increased risk. Previous research is inconclusive; one large Australian study using linked hospital data found no protective effect of cataract surgery. We aim to examine the impact of cataract-related vision impairment on falls risk and the additional effects of delays in access to surgery, refractive management (type of spectacles and changes to spectacle prescription) and the resulting level of function, particularly binocular function which can impact balance.\n    \n\n\n          Method/design:\n        \n      \n      A prospective, 24-month cohort study is planned involving over 700 patients aged 70 years or older with bilateral cataract presenting for surgery at five public hospital eye clinics in Sydney, Melbourne and Perth, Australia. The primary outcomes will be self-reported falls and falls requiring medical care, assessed objectively using administrative data sets. Secondary outcomes include community participation, quality of life, mood and depressive symptoms. McNemar's test will be used to evaluate differences in falls rate before, after first eye and after second eye cataract surgery. Generalised Estimating Equations linear regression analysis will be undertaken to examine factors associated with falls risk and the secondary outcomes.\n    \n\n\n          Discussion:\n        \n      \n      With limited resources to further shorten public waiting lists, there is a need to better understand an individuals' risk of fall injury or other negative consequences while waiting for surgery. The findings of this project will inform the development of strategies to reduce falls risk in the many older people with cataract."
        },
        {
            "title": "Acceptance of cataract surgery in a cohort of Tanzanians with operable cataract.",
            "abstract": "Background:\n        \n      \n      In spite of recent increases in the number of surgeries carried out within some hospitals and programmes in sub-Saharan Africa, there are indications that the acceptance of cataract surgery remains quite low.\n    \n\n\n          Methods:\n        \n      \n      We conducted a population-based prospective (cohort) study of cataract patients from 12 villages in Hai district of Kilimanjaro region, Tanzania. Those identified with operable cataract were informed of the regular community programmes (within 5 km) in place providing transportation and high-quality surgery. At years 1 and 2 after the survey, we traced the patients to determine uptake of cataract surgery.\n    \n\n\n          Results:\n        \n      \n      Among patients eligible for surgery (128), 31 could not be followed up after 1 year due to deaths, moving, and refusal. Among the remaining patients, 18 accepted surgery in the first year and four accepted in the second year. Among these 22 patients, only five were blind or with severe visual impairment. The most elderly were those least likely to accept surgery.\n    \n\n\n          Discussion:\n        \n      \n      Even with bridging strategies in place to make cataract surgery accessible and affordable, the uptake of cataract surgery remains low. Strategies aimed to identifying and referring all patients recognizing vision loss as a personal disability rather than using predefined vision cutoffs will likely be most successful in reducing the burden of vision loss due to cataract."
        },
        {
            "title": "A targeted decision aid for the elderly to decide whether to undergo colorectal cancer screening: development and results of an uncontrolled trial.",
            "abstract": "Background:\n        \n      \n      Competing causes of mortality in the elderly decrease the potential net benefit from colorectal cancer screening and increase the likelihood of potential harms. Individualized decision making has been recommended, so that the elderly can decide whether or not to undergo colorectal cancer (CRC) screening. The objective is to develop and test a decision aid designed to promote individualized colorectal cancer screening decision making for adults age 75 and over.\n    \n\n\n          Methods:\n        \n      \n      We used formative research and cognitive testing to develop and refine the decision aid. We then tested the decision aid in an uncontrolled trial. The primary outcome was the proportion of patients who were prepared to make an individualized decision, defined a priori as having adequate knowledge (10/15 questions correct) and clear values (25 or less on values clarity subscale of decisional conflict scale). Secondary outcomes included overall score on the decisional conflict scale, and preferences for undergoing screening.\n    \n\n\n          Results:\n        \n      \n      We enrolled 46 adults in the trial. The decision aid increased the proportion of participants with adequate knowledge from 4% to 52% (p < 0.01) and the proportion prepared to make an individualized decision from 4% to 41% (p < 0.01). The proportion that preferred to undergo CRC screening decreased from 67% to 61% (p = 0. 76); 7 participants (15%) changed screening preference (5 against screening, 2 in favor of screening)\n    \n\n\n          Conclusion:\n        \n      \n      In an uncontrolled trial, the elderly participants appeared better prepared to make an individualized decision about whether or not to undergo CRC screening after using the decision aid."
        },
        {
            "title": "Measuring health in a vacuum: examining the disability weight of the DALY.",
            "abstract": "The Disability Adjusted Life Year (DALY) is a widely used summary measure of population health combining years of life lost due to mortality and years of healthy life lost due to disability. A feature of the DALY is that, in the assessment of morbidity, each health condition is associated with a disability weight. The disability weight lies on a scale between 0 (indicating the health condition is equivalent to full health) and 1 (indicating the health condition is equivalent to death). The disability weight associated with each health condition is currently fixed across all social, cultural and environmental contexts. Thus blindness in the United Kingdom has the same disability weight as blindness in Niger in spite of structural interventions in the UK that make the disability less severe than in Niger. Although the fixed disability weight is defended on grounds that it supports a strongly egalitarian flavour in the DALY, we argue that the lack of consideration of realistic contexts results in a measure that will underestimate the burden associated with morbidity in disadvantaged populations and overestimate the burden in advantaged populations. There is, consequently, a loss of information on possible non-clinical points of intervention. Disaggregated estimates of the burden of disease such as those in the World Health Report 2000 should be interpreted with caution."
        },
        {
            "title": "Central serous chorioretinopathy and risk of ischaemic stroke: a population-based cohort study.",
            "abstract": "Background:\n        \n      \n      Central serous chorioretinopathy (CSCR) is a common maculopathy that features choroidal circulatory disturbance. This population-based cohort study aimed to explore the relationship between CSCR and the future development of ischaemic stroke.\n    \n\n\n          Methods:\n        \n      \n      Data were obtained from Taiwan's national health insurance research database. From 2000 to 2007, 1814 patients with newly diagnosed CSCR were eligible for inclusion in the study cohort. Using stratified random sampling, 9648 enrollees matched with the study subjects in terms of sex, age, monthly income, geographical location and time of enrolment were selected as the control group. Stroke-free survival analysis was assessed using a Kaplan-Meier method. Cox proportional hazard regressions were performed to calculate the HR of ischaemic stroke for the two groups after adjusting for possible confounding variables.\n    \n\n\n          Results:\n        \n      \n      Of the sampled patients, 45 (2.5%) from the CSCR cohort and 157 (1.6%) from the control group developed ischaemic stroke during a mean follow-up period of 3.9 ± 2.2 years. CSCR patients had a significantly higher incidence of ischaemic stroke than those without a diagnosis of CSCR (p=0.003). After adjusting for age, sex and chronic comorbidities at baseline, CSCR patients were found to have a 1.56-fold (95% CI 1.11 to 2.18, p=0.010) greater risk of a subsequent ischaemic stroke than the matched controls.\n    \n\n\n          Conclusions:\n        \n      \n      CSCR is an independent indicator for the increased risk of subsequent ischaemic stroke development."
        },
        {
            "title": "Contrast-enhanced intraoperative ultrasonography during surgery for hepatocellular carcinoma in liver cirrhosis: is it useful or useless? A prospective cohort study of our experience.",
            "abstract": "Background:\n        \n      \n      Preliminary results showed that contrast-enhanced intraoperative ultrasonography (CEIOUS) could provide information not obtainable with conventional IOUS during surgery for hepatocellular carcinoma (HCC). The aim of the study was to prospectively validate the role of CEIOUS on the basis of a larger experience and to establish a new classification that takes into account its findings.\n    \n\n\n          Methods:\n        \n      \n      Eighty-seven consecutive patients underwent hepatecomies for HCC. Those patients with new lesions at IOUS underwent CEIOUS: for that patients received intravenously 4.8 mL sulphurhexafluoride microbubbles. Pattern of enhancement was classified in 4 categories: A1 (full enhancement in the arterial phase and wash-out in the delayed phases), A2 (intralesional signs of neovascularization during all phases), A3 (no nodular enhancement but detectability during the liver enhancement), and B (undetectability during the liver enhancement). Resection was recommended for A1-3 nodules and no treatment for B nodules.\n    \n\n\n          Results:\n        \n      \n      Twenty-nine patients (33%) had 59 new lesions at IOUS and underwent CEIOUS. Twenty-seven nodules showed a B pattern at CEIOUS and were not removed; 32 nodules were classified as A1 in 5 patients, A2 in 11 patients, and A3 in 16 patients. The nodules were removed, and by histology, five A1, nine A2, and six A3 nodules were confirmed to be HCC. CEIOUS modified the operative decision making in 79% of these patients.\n    \n\n\n          Conclusions:\n        \n      \n      CEIOUS is useful during surgery for HCC; it complements the accuracy of IOUS and affects the radicalness of the surgical. Specificity of CEIOUS has to be further improved, although intrinsic drawbacks exist in the diagnostic criterion of tumor vascularity."
        },
        {
            "title": "[Analysis and long-term observation after surgical treatment of patients with tumours of the iris].",
            "abstract": "Background:\n        \n      \n      Uveal melanoma is the most common primary intraocular malignancy in adults. Iris melanomas are rare tumours: they account for 2-3 % of all uveal melanomas. The clinical differentiation between benign iris nevi and malignant iris melanomas can be difficult.\n    \n\n\n          Patients and methods:\n        \n      \n      The aim of this study was the registration, analysis and observation of all patients with tumours of the anterior uvea who had been treated surgically between 1992 and 2011 at the ophthalmic department of the University Hospital of Jena. 40 patients were analysed and compared concerning their preoperative states, operating methods including complications, histological results, postoperative function, subjective complaints as well as the risk of metastasis and the associated dependence on mortality of the dignity of the tumours. In this time period 26 patients has been observed in a follow-up visit. Patients with a malignant tumour were offered an examination.\n    \n\n\n          Results:\n        \n      \n      The histological examination revealed for 24 patients a benign tumour and for 16 patients a malignant tumour. After an exact analysis of multiple parameters there was only a statistically significant difference in the preoperative visual acuity (p = 0.025) and in the tumour size (p = 0.011) between the two analysed groups of patients. The rate of serious postoperative complications was 11.4 %. One fourth of the patients complained of subjective problems after the surgical intervention. In the follow-up visit a visual acuity of 0.5 or better was achieved in 68 % of all interventions.\n    \n\n\n          Conclusions:\n        \n      \n      A reliable diagnosis is only possible after histological examination. The analysed parameters can only give indications for the dignity of the tumour. The strategy of the ophthalmic department of the University Hospital of Jena to remove a tumour of uncertain dignity at an early state makes sense, because there are few postoperative complications, few patients complain about subjective problems and the chances for achieving good visual acuity are high."
        },
        {
            "title": "[Surgical treatment of meningiomas of medial part of sphenoid ala and en-plaque meningiomas of perisellar area].",
            "abstract": "The authors present some clinical aspects of their own experience in the operative treatment of clinoidal and en-plaque suprasellar meningiomas which account for 5.6% of all intracranial meningiomas in Department of Neurosurgery treated in the years 1987-93. Visual loss was the first symptom of disease. In conclusion, the authors stress the still unacceptably long period between the onset of symptoms and correct diagnosis. Possibilities of total removal are limited by the size of tumour and compression of adjacent structures, and the chance for visual improvement is low for that reasons. Visual improvement was observed in only one case after surgery, however in 5 cases worsening of visual acuity was noted in postoperative period. Mortality rate was 6.2%. Adjuvant radiotherapy was performed in non patient. Long term results in ophthalmological and neurological aspects are discussed."
        },
        {
            "title": "Differences in natural history of low- and high-gradient aortic stenosis from nonsevere to severe stage of the disease.",
            "abstract": "Background:\n        \n      \n      The aim of the present study was to assess and compare the disease progression of aortic stenosis (AS) subtypes from nonsevere to severe disease on the basis of measures of gradient and flow.\n    \n\n\n          Methods:\n        \n      \n      Seventy-seven patients with AS (mean aortic valve area, 1.3 ± 0.3 cm(2) at baseline) underwent echocardiographic examination, including two-dimensional speckle-tracking strain measurements. Patients were retrospectively grouped according to mean transvalvular pressure gradient (40 mm Hg) into low-gradient (LG/AS) and high-gradient (HG/AS) groups. The LG/AS group was further subdivided into low-flow (LF/LG; i.e., stroke volume index < 35 mL/m(2)) and normal-flow (NF/LG) groups. For subanalysis, the LF/LG group was split into two groups: \"paradoxical\" (P-LF/LG; ejection fraction > 50%) and \"classical\" LF/LG (C-LF/LG; ejection fraction < 50%). Follow-up echocardiography was performed in patients with severe AS after 3.3 ± 1.7 years. Survival status was ascertained after 5.0 ± 2.0 years.\n    \n\n\n          Results:\n        \n      \n      Coronary artery disease was more frequent in LG/AS than HG/AS patients. Already at baseline, LF/LG patients showed reduced left ventricular global systolic strain and reduced systemic arterial compliance compared with HG/AS patients (HG/AS, 1.0 ± 0.4 mL · mm Hg-(1) · m(-2); NF/LG, 0.9 ± 0.2 mL · mm Hg-(1) · m(-2); LF/LG, 0.6 ± 0.2 mL · mm Hg(-1) · m(-2); P < .001). The initially elevated valvuloarterial impedance increased significantly more in LG/AS than in the other groups (HG/AS, 2.2 ± 0.9 mm Hg · mL-(1) · m(-2); NF/LG, 2.2 ± 0.5 mm Hg · mL-(1) · m(-2); LF/LG, 3.2 ± 0.8 mm Hg · mL(-1) · m-(2); P < .001), while aortic valve area decreased by 42% in HG/AS versus 34% in NF/LG and 32% in LF/LG (P < .001). At follow-up, global systolic strain was significantly reduced in C-LF/LG (7.7 ± 2.5 vs 13.5 ± 2.9 in P-LF/LG, P < .001). In P-LF/LG, mitral E/E' ratio increased significantly from 8.9 ± 4.0 to 26.4 ± 9.2 (P < .05).\n    \n\n\n          Conclusions:\n        \n      \n      In patients with AS with high-gradient physiology, the valve constitutes the primary problem. By contrast, low-gradient AS is a systemic disease with valvular, vascular, and myocardial components, resulting in a slower progression of transvalvular gradient, but worse clinical outcome. In C-LF/LG, impaired systolic function leads to an LG flow pattern, whereas the pathophysiology in P-LF/LG is predominantly a diastolic dysfunction."
        },
        {
            "title": "Use of a regional wall motion score to enhance risk stratification of patients receiving an implantable cardioverter-defibrillator.",
            "abstract": "Objectives:\n        \n      \n      We postulated that preoperative assessment of both regional wall motion and left ventricular ejection fraction would serve as an accurate prognostic indicator of long-term cardiac mortality and functional outcome in patients treated with an implantable cardioverter-defibrillator.\n    \n\n\n          Background:\n        \n      \n      Long-term cardiac mortality has remained high in patients receiving an implantable cardioverter-defibrillator. The ability to risk stratify patients before defibrillator implantation is becoming increasingly important from a medical and economic standpoint.\n    \n\n\n          Methods:\n        \n      \n      The hypothesis was retrospectively tested in 74 patients who had received an implantable cardioverter-defibrillator. Left ventricular ejection fraction and regional wall motion score, derived from centerline chord motion analysis, were calculated for each patient from the preoperative right anterior oblique contrast ventriculogram. Wall motion score was the only significant independent predictor of long-term cardiac mortality and functional status by multivariate analysis because of its enhanced prognostic capability in patients with an ejection fraction in the critical range of 30% to 40%.\n    \n\n\n          Results:\n        \n      \n      Patients with an ejection fraction > 40% had a 3-year cardiac mortality rate of 0% compared with 25% for those with an ejection fraction of 30% to 40% and 48% for those with an ejection fraction < 30% (p < 0.05). Similarly, 75% of patients with an ejection fraction > 40% were in New York Heart Association functional class I or II during long-term follow-up compared with 59% of those with an ejection fraction 30% to 40% and 29% of those with an ejection fraction < 30%. Among patients with an ejection fraction of 30% to 40%, those with a wall motion score > 16% had a 3-year cardiac mortality rate of 0% compared with 71% of those with a wall motion score < or = 16% (p = 0.002). In addition, 86% of patients with a wall motion score > 16% were in functional class I or II during long-term follow-up compared with 13% of those with a wall motion score < or = 16% (p = 0.001).\n    \n\n\n          Conclusions:\n        \n      \n      Long-term cardiac mortality and functional outcome in patients receiving an implantable cardioverter-defibrillator can be predicted if the left ventricular ejection fraction and regional wall motion score are measured preoperatively."
        },
        {
            "title": "Rates and predictors of hydroxychloroquine retinal toxicity in patients with rheumatoid arthritis and systemic lupus erythematosus.",
            "abstract": "Objective:\n        \n      \n      Hydroxychloroquine (HCQ) retinopathy is of concern because of the potential seriousness of visual loss and the medicolegal consequences of failure to detect toxicity. However, there have been limited demographic data on which to base recommendations for screening. We have studied the largest unselected series of patients to date to evaluate the risk of toxicity and the relevance of purported risk factors.\n    \n\n\n          Methods:\n        \n      \n      We studied 3,995 patients with rheumatoid arthritis or systemic lupus erythematosus who had used HCQ, including 1,538 current users. We screened for self-reported toxicity, and followed up on positive cases with detailed interviews and specialist confirmation. We categorized cases as \"definite or probable\" if there was bull's eye maculopathy or visual field loss.\n    \n\n\n          Results:\n        \n      \n      Of the lifetime users of HCQ, 6.5% discontinued therapy because of an eye problem, including 1.8% who reported HCQ retinal problems. However, definite or probable toxicity was documented in only 0.65% (95% confidence interval 0.31-0.93). The risk of toxicity was low in the initial 7 years of exposure, and was approximately 5 times greater after 7 years of usage (or 1,000 gm total exposure). Toxicity was unrelated to age, weight, or daily dosage. Eye examinations were obtained annually by 50.5% and every 6 months by 40.4% of patients.\n    \n\n\n          Conclusion:\n        \n      \n      HCQ toxicity remains uncommon, but increases markedly with the duration of therapy and exceeds 1% after 5-7 years. Toxicity was unassociated with age, daily dosage, or weight. These findings will aid the reformulation of screening guidelines."
        },
        {
            "title": "Accelerometer-determined physical activity and mortality in a national prospective cohort study: Considerations by visual acuity.",
            "abstract": "Background:\n        \n      \n      Previous research demonstrates that visual impairment (VI) is associated with increased all-cause mortality risk and is also associated with reduced physical activity participation. Although physical activity is reduced among those with VI, no studies have examined the relationship between physical activity and all-cause mortality across different visual function statuses, which is noteworthy of investigation as physical activity is linked with greater survival.\n    \n\n\n          Methods:\n        \n      \n      Data from the 2003-2006 NHANES were employed, with physical activity assessed via accelerometry and visual function assessed using the ARK-760 autorefractor.\n    \n\n\n          Results:\n        \n      \n      For those with normal vision, and after adjustments, for every 60min increase in physical activity, normal-sighted adults had an 18% (HR=0.82; 95% CI: 0.72-0.93) reduced risk of all-cause mortality. Similarly, after adjustments and for every 60min increase in physical activity for those with uncorrected refractive error and VI, respectively, there was a 15% (HR=0.85; 95% CI: 0.72-1.00) and 35% (HR=0.65; 95% CI: 0.43-0.98) reduced risk of all-cause mortality. Among all three visual status groups, sedentary behavior was not associated with mortality status.\n    \n\n\n          Conclusion:\n        \n      \n      Among those with varying degrees of visual loss, sedentary behavior was not associated with mortality, but physical activity demonstrated survival benefits."
        },
        {
            "title": "Comparison of treatment regimens for cytomegalovirus retinitis in patients with AIDS in the era of highly active antiretroviral therapy.",
            "abstract": "Purpose:\n        \n      \n      To describe the outcomes of different treatment approaches for cytomegalovirus (CMV) retinitis in the era of highly active antiretroviral therapy (HAART).\n    \n\n\n          Design:\n        \n      \n      Prospective cohort study, the Longitudinal Study of the Ocular Complications of AIDS.\n    \n\n\n          Participants:\n        \n      \n      A total of 250 patients with CMV retinitis and a CD4+ T-cell count <100 cells/μl (n = 221) at enrollment or incident retinitis (n = 29) during cohort follow-up.\n    \n\n\n          Methods:\n        \n      \n      The effects of systemic therapy (vs. intraocular therapy only) on systemic outcomes and the effect of intraocular therapies (ganciclovir implants, intravitreal injections) on ocular outcomes were evaluated.\n    \n\n\n          Main outcome measures:\n        \n      \n      Mortality, CMV dissemination, retinitis progression, and treatment side effects.\n    \n\n\n          Results:\n        \n      \n      Regimens containing systemic anti-CMV therapy were associated with a 50% reduction in mortality (adjusted hazard ratio [HR], 0.5; 95% confidence interval [CI], 0.3-0.7; P = 0.006), a 90% reduction in new visceral CMV disease (adjusted HR, 0.1; 95% CI, 0.04-0.4; P = 0.004), and among those with unilateral CMV retinitis at presentation, an 80% reduction in second eye disease (adjusted HR, 0.2; 95% CI, 0.1-0.5; P = 0.0005) when compared with those using only intraocular therapy (implants or injections). Compared with systemic treatment only, regimens containing intravitreal injections had greater rates of retinitis progression (adjusted HR, 3.4; P = 0.004) and greater visual field loss (for loss of one half of the normal field, adjusted HR, 5.5; P < 0.01). Intravitreal implants were not significantly better than systemic therapy (adjusted HR for progression, 0.5; P = 0.26; adjusted HR for loss of one half of the visual field, 0.5; P = 0.45), but the sample size was small. Hematologic and renal side effect rates were similar between those groups with and without systemic anti-CMV therapy. The rate of endophthalmitis was 0.017 per eye-year (EY) (95% CI, 0.006-0.05) among those treated with intravitreal injections and 0.01 per EY (95% CI, 0.002-0.04) among those treated with an implant.\n    \n\n\n          Conclusions:\n        \n      \n      In the HAART era, systemic anti-CMV therapy, while there is immune compromise, seems to provide benefits in terms of longer survival and decreased CMV dissemination.\n    \n\n\n          Financial disclosure(s):\n        \n      \n      Proprietary or commercial disclosure may be found after the references."
        },
        {
            "title": "Myocardial fibrosis on cardiac magnetic resonance and cardiac outcomes in hypertrophic cardiomyopathy: a meta-analysis.",
            "abstract": "Objective:\n        \n      \n      Late gadolinium enhancement (LGE) on cardiac MRI that indicates the extent of myocardial fibrosis in hypertrophic cardiomyopathy (HCM) is a potential risk factor of sudden cardiac death (SCD) in non-high-risk patients according to conventional clinical markers.\n    \n\n\n          Methods:\n        \n      \n      The present study was designed to systematically review prospective trials and assess the association between LGE and SCD in HCM. We systematically searched the electronic databases, MEDLINE, PubMed, Embase and Cochrane for prospective cohort studies of the effects of LGE on clinical outcomes (SCD/aborted SCD, all-cause mortality, cardiac and heart failure death) in HCM.\n    \n\n\n          Results:\n        \n      \n      We identified six clinical studies, examining 1414 patients without LGE and 1653 with LGE and an average follow-up of 3.05 years. The incidence of SCD/aborted SCD in patients with HCM and LGE was significantly increased as compared with patients without LGE (OR 2.52, 95% CI 1.44 to 4.4, p=0.001). The all-cause mortality and cardiac death rates were also significantly increased in patients with LGE. The extent of LGE was not significantly related to the risk of SCD.\n    \n\n\n          Conclusions:\n        \n      \n      LGE is significantly associated with SCD risk, cardiac mortality and all-cause mortality in patients with non-high-risk HCM according to conventional risk factors."
        },
        {
            "title": "An observational study using eye tracking to assess resident and senior anesthetists' situation awareness and visual perception in postpartum hemorrhage high fidelity simulation.",
            "abstract": "Background:\n        \n      \n      The postpartum hemorrhage (PPH) is the leading cause of maternal mortality in the world. Human factors and especially situation awareness has primarily responsibility to explain suboptimal cares. Based on eye tracking and behavior analysis in high fidelity simulation of PPH management, the goal of this study is to identify perceptual and cognitive key parameters of the expertise.\n    \n\n\n          Methods:\n        \n      \n      Two groups of fifteen anesthetists (residents and experienced anesthetists) watched the beginning of a severe simulated PPH management. During this first experimental phase, situation awareness was assessed using SAGAT (Situation Awareness Global Assessment Technique) questionnaire and visual behavior was analyzed with eye tracking. In the continuity of the video sequence, they have to step in the PPH situation and to provide care to the simulated patient. Performance of cares was evaluated and self-assessed as well as cognitive load.\n    \n\n\n          Results:\n        \n      \n      No statistical difference between the residents and experienced anesthetists was observed on performance of simulated PPH management. The mean expected practice score was 76.9 ± 13.9%). Assessment of situation awareness (65 ± 7%), cognitive load (74.4 ± 11.3%) and theoretical knowledge of PPH (52.4 ± 3.5%) were also not statistically different between the two groups. Only results of self-assessed performance (respectively 66.1 ± 16.6 and 47.0 ± 20.8 for experts and residents) and eye-tracking data revealed that experts tended to get accurate evaluation of their performance and to monitor more the blood loss of the patient. Experts have in average 8.28% more fixating points than Novices and gazed the blood loss region longer (865 ms ± 439 vs. 717 ms ± 362).\n    \n\n\n          Conclusions:\n        \n      \n      This study pointed out the limits of classical assessment of performance, and human factors based on questionnaires to identify expertise in simulated PPH care. A neuroscientific approach with new technology like eye tracking could provide new objective and more sensitive insights on human factors in simulated medical emergency situations."
        },
        {
            "title": "Chronic diseases and life events accounted for 2-18 % population attributable risks for adult hearing loss: UK Adult Psychiatric Morbidity Survey, 2007.",
            "abstract": "Links between chronic diseases and hearing loss in adults have emerged. However, previous investigations were not complete, and the role of life events was unclear. Therefore, it was aimed to examine the relationships of common chronic diseases and life events and adult hearing loss in a country-wide and population-based study. Data were retrieved from UK Adult Psychiatric Morbidity Survey, 2007, being cross-sectional, including demographics, self-reported prior health conditions and hearing loss (ever and in the last 12 months), and several major life events. Analyses included Chi square test, t test, logistic regression model, and population attributable risk estimation. People who had prior health conditions including cancer, migraine, dementia, depression, cataracts, chronic bronchitis, allergy, bowel problem, bladder problem, arthritis, muscle problem or skin problem tended to report hearing loss than their counterparts. People who have experienced major life events including post-traumatic stress disorder, serious illness of close relatives, death of family, serious problems with friends, major financial crisis, valuables stolen, being bullied, violence at home, sexual abuse or running away from home were also more likely to experience ever hearing loss problem or that in the last 12 months. 2.0-13.1 % adult hearing loss could be delayed or prevented by managing chronic diseases while 4.1-18.1 % might be delayed or prevented by minimizing the negative effects of life events. Chronic diseases and life events were associated with hearing loss in adults. Better managing lifestyle to minimize detrimental impacts in future health and nursing programs would be suggested."
        },
        {
            "title": "Mortality in patients with small choroidal melanoma. COMS report no. 4. The Collaborative Ocular Melanoma Study Group.",
            "abstract": "Objective:\n        \n      \n      To describe the clinical characteristics and survival experience of a prospectively followed up group of patients with small choroidal melanoma.\n    \n\n\n          Methods:\n        \n      \n      The Collaborative Ocular Melanoma Study (COMS) is a set of clinical trials designed to compare the role of radiotherapy and enucleation in the treatment of medium and large-size choroidal melanoma. From December 1986 to August 1989, patients with small choroidal melanoma, not large enough to be eligible for the COMS clinical trials, were offered participation in a nonrandomized prospective follow-up study. Small choroidal melanomas were defined as 1.0 to 3.0 mm in apical height and at least 5.0 mm in basal diameter. A total of 204 patients were enrolled in the study. Patients were followed up annually through August 1989. Two additional assessments of treatment status and mortality were conducted in 1993-1994 and 1995-1996. The median length of follow-up was 92 months. Eight percent of patients were treated at the time of study enrollment and an additional 33% were treated during follow-up.\n    \n\n\n          Results:\n        \n      \n      Twenty-seven patients have died; 6 deaths were reported by the clinical center as due to metastatic melanoma. The Kaplan-Meier estimate of 5-year all-cause mortality was 6.0% (95% confidence interval, 2.7%-9.3%) and 8-year all-cause mortality was 14.9% (95% confidence interval, 9.6%-20.2%).\n    \n\n\n          Conclusions:\n        \n      \n      Otherwise healthy patients, average age of 60 years, without a previous diagnosis of malignant disease who have small choroidal lesions judged to be melanoma have a low risk of dying within 5 years."
        },
        {
            "title": "Conventional MRI does not reliably distinguish radiation necrosis from tumor recurrence after stereotactic radiosurgery.",
            "abstract": "Distinguishing radiation necrosis (RN) from tumor recurrence after stereotactic radiosurgery (SRS) for brain metastases is challenging. This study assesses the sensitivity (SN) and specificity (SP) of an MRI-based parameter, the \"lesion quotient\" (LQ), in characterizing tumor progression from RN. Records of patients treated with SRS for brain metastases between 01/01/1999 and 12/31/2009 and with histopathologic analysis of a subsequent contrast enhancing enlarging lesion at the treated site at a single institution were examined. The LQ, the ratio of maximal nodular cross sectional area on T2-weighted imaging to the corresponding maximal cross sectional area of T1-contrast enhancement, was calculated by a neuroradiologist blinded to the histopathological outcome. Cutoffs of <0.3, 0.3-0.6, and >0.6 have been previously suggested to have correlated with RN, mixed findings and tumor recurrence, respectively. These cutoff values were evaluated for SN, SP, positive predictive value (PPV) and negative predictive value (NPV). Logistic regression analysis evaluated for associated clinical factors. For the 51 patients evaluated, the SN, SP, PPV and NPV for identifying RN (LQ < 0.3) were 8, 91, 25 and 73 %, respectively. For the combination of recurrent tumor and RN (LQ 0.3-0.6) the SN, SP, PPV and NPV were 0, 64, 0 and 83 %. The SN, SP, PPV and NPV of the LQ for recurrent tumor (LQ > 0.6) were 59, 41, 62 and 39 %, respectively. Standard MRI techniques do not reliably discriminate between tumor progression and RN after treatment with SRS for brain metastases. Additional imaging modalities are warranted to aid in distinguishing between these diagnoses."
        },
        {
            "title": "[Choroid metastases in metastatic breast cancer--a rare metastatic site].",
            "abstract": "Between 1985 and 1989, four patients with uvea metastases of breast cancers were treated in the Dept. of Gynaecology and Obstetrics of the University of the Saar, Medical School, in Homburg/Saar. One of these patients developed binocular metastases. The patient's age at the primary diagnosis of breast cancer was 48 years (median), the others were in pre- or peri-menopausal status. Uvea metastases appeared in median five years after primary diagnosis, always in coincidence with at least one more metastasis of different localisation. All cases with uvea metastases have been treated by radiation therapy with 40 gy reference dose. In three out of five cases, complete remission of the visus restriction could be achieved. In a further case, a temporary partial remission occurred. Two relapses were observed after remission induction, one and four years after treatment respectively."
        },
        {
            "title": "Effect of calcium dobesilate on occurrence of diabetic macular oedema (CALDIRET study): randomised, double-blind, placebo-controlled, multicentre trial.",
            "abstract": "Background:\n        \n      \n      Medical treatment for diabetic retinopathy could have an important role in prevention of complications such as visual loss. We aimed to assess the effect of calcium dobesilate on occurrence of diabetic macular oedema.\n    \n\n\n          Methods:\n        \n      \n      We undertook a randomised, double-blind, placebo-controlled, multicentre study in 40 centres in 11 countries. We enrolled outpatients with adult-onset type 2 diabetes and mild-to-moderate non-proliferative diabetic retinopathy, and randomly allocated them via sealed envelopes either calcium dobesilate (1500 mg per day) or placebo. The primary endpoint was development of clinically significant macular oedema (CSME) within a follow-up period of 5 years. Patients who dropped out of the study early were censored. Analysis was by intention to treat.\n    \n\n\n          Findings:\n        \n      \n      We enrolled 635 patients. 324 were randomly allocated calcium dobesilate and 311 were assigned placebo. In the calcium dobesilate group, 86 patients developed CSME compared with 69 in the placebo group. Accounting for censored cases, estimated cumulative 5-year CSME probability was 35% and 28%, respectively (hazard ratio 1.32, 95% CI 0.96-1.81; p=0.0844). Adverse events did not differ between treatment groups (78 [24%] on calcium dobesilate and 90 [29%] with placebo). No relevant drug-related complications were noted. Nine patients (3%) died in the calcium dobesilate group and eight (3%) deaths were recorded on placebo.\n    \n\n\n          Interpretation:\n        \n      \n      Calcium dobesilate did not reduce the risk of development of CSME."
        },
        {
            "title": "Improving function in age-related macular degeneration: design and methods of a randomized clinical trial.",
            "abstract": "Age-Related Macular Degeneration (AMD) is the leading cause of severe vision loss in older adults and impairs the ability to read, drive, and live independently and increases the risk for depression, falls, and earlier mortality. Although new medical treatments have improved AMD's prognosis, vision-related disability remains a major public health problem. Improving Function in AMD (IF-AMD) is a two-group randomized, parallel design, controlled clinical trial that compares the efficacy of Problem-Solving Therapy (PST) with Supportive Therapy (ST) (an attention control treatment) to improve vision function in 240 patients with AMD. PST and ST therapists deliver 6 one-hour respective treatment sessions to subjects in their homes over 2 months. Outcomes are assessed masked to treatment assignment at 3 months (main trial endpoint) and 6 months (maintenance effects). The primary outcome is targeted vision function (TVF), which refers to specific vision-dependent functional goals that subjects highly value but find difficult to achieve. TVF is an innovative outcome measure in that it is targeted and tailored to individual subjects yet is measured in a standardized way. This paper describes the research methods, theoretical and clinical aspects of the study treatments, and the measures used to evaluate functional and psychiatric outcomes in this population."
        },
        {
            "title": "Comparative effectiveness of angiotensin-converting-enzyme inhibitors and angiotensin II receptor blockers in patients with type 2 diabetes and retinopathy.",
            "abstract": "Background:\n        \n      \n      Angiotensin-converting-enzyme (ACE) inhibitors and angiotensin II receptor blockers (ARBs) are effective treatments for diabetic retinopathy, but randomized trials and meta-analyses comparing their effects on macrovascular complications have yielded conflicting results. We compared the effectiveness of these drugs in patients with pre-existing diabetic retinopathy in a large population-based cohort.\n    \n\n\n          Methods:\n        \n      \n      We conducted a propensity score-matched cohort study using Taiwan's National Health Insurance Research Database. We included adult patients prescribed an ACE inhibitor or ARB within 90 days after diagnosis of diabetic retinopathy between 2000 and 2010. Primary outcomes were all-cause death and major adverse cardiovascular events (myocardial infarction, ischemic stroke or cardiovascular death). Secondary outcomes were hospital admissions with acute kidney injury or hyperkalemia.\n    \n\n\n          Results:\n        \n      \n      We identified 11 246 patients receiving ACE inhibitors and 15 173 receiving ARBs, of whom 9769 patients in each group were matched successfully by propensity scores. In the intention-to-treat analyses, ARBs were similar to ACE inhibitors in risk of all-cause death (hazard ratio [HR] 0.94, 95% confidence interval [CI] 0.87-1.01) and major adverse cardiovascular events (HR 0.95, 95% CI 0.87-1.04), including myocardial infarction (HR 1.03, 95% CI 0.88-1.20), ischemic stroke (HR 0.94, 95% CI 0.85-1.04) and cardiovascular death (HR 1.01, 95% CI 0.88-1.16). They also did not differ from ACE inhibitors in risk of hospital admission with acute kidney injury (HR 1.01, 95% CI 0.91-1.13) and hospital admission with hyperkalemia (HR 1.01, 95% CI 0.86-1.18). Results were similar in as-treated analyses.\n    \n\n\n          Interpretation:\n        \n      \n      Our study showed that ACE inhibitors were similar to ARBs in risk of all-cause death, major adverse cardiovascular events and adverse effects among patients with pre-existing diabetic retinopathy."
        },
        {
            "title": "Fronto-basal interhemispheric approach for tuberculum sellae meningiomas; long-term visual outcome.",
            "abstract": "We report our experience with the treatment of tuberculum sellae meningiomas using the fronto-basal interhemispheric approach. A retrospective analysis was performed on a series of 24 patients with tuberculum sellae meningiomas who were operated between March 2000 and January 2007. Patients' presenting symptoms, radiological images, operative reports, and clinical follow-up data were reviewed with special consideration for visual outcome. Visual deterioration was the presenting symptom in all patients, followed by headache in 9 patients (37.5%). The average duration of visual symptoms was 17.6 months. The average tumor diameter was 2.63 cm; encasement of the carotid artery was identified in 7 patients (29%). Complete tumor removal was achieved in 21 patients (87.5%). Mean follow-up period was 52 months. Vision improved in 19 patients (79%), remained stable in 4 (17%) and deteriorated in 1 patient (4%). The degree of tumor removal or visual outcome were both unrelated to the tumor size (p = 0.2 and p = 0.6 respectively). While the degree of preoperative visual deficit did not affect the visual improvement rate in the whole group (p = 0.9), those patients with improvement to good functional vision (>20/40) after the surgery, had a less severe preoperative deficit (p < 0.001). The most common complication was anosmia (29.1%) and there was no mortality. The frontobasal interhemispheric approach is safe and provides a direct anatomical approach to tuberculum sellae meningiomas with relatively low incidence of complications. Patients with improved vision to good functional level had a better preoperative visual status."
        },
        {
            "title": "Study protocol for valuing EQ-5D-3L and EORTC-8D health states in a representative population sample in Sri Lanka.",
            "abstract": "Background:\n        \n      \n      Economic evaluations to inform decisions about allocation of health resources are scarce in Low and Middle Income Countries, including in Sri Lanka. This is in part due to a lack of country-specific utility weights, which are necessary to derive appropriate Quality Adjusted Life Years. The EQ-5D-3 L, a generic multi-attribute instrument (MAUI), is most widely used to measure and value health states in high income countries; nevertheless, the sensitivity of generic MAUIs has been criticised in some conditions such as cancer. This article describes a protocol to produce both a generic EQ-5D-3 L and cancer specific EORTC-8D utility index in Sri Lanka.\n    \n\n\n          Method:\n        \n      \n      EQ-5D-3 L and EORTC-8D health states will be valued using the Time Trade-Off technique, by a representative population sample (n = 780 invited) identified using stratified multi-stage cluster sampling with probability proportionate to size method. Households will be randomly selected within 30 clusters across four districts; one adult (≥ 18 years) within each household will be selected using the Kish grid method.Data will be collected via face-to-face interview, with a Time Trade-Off board employed as a visual aid. Of the 243 EQ-5D-3 L and 81,290 EORTC-8D health states, 196 and 84 respectively will be directly valued. In EQ-5D-3 L, all health states that combine level 3 on mobility with either level 1 on usual activities or self-care were excluded. Each participant will first complete the EQ-5D-3 L, rank and value 14 EQ-5D-3 L states (plus the worst health state and \"immediate death\"), and then rank and value seven EORTC-8D states (plus \"immediate death\"). Participant demographic and health characteristics will be also collected.Regression models will be fitted to estimate utility indices for EQ-5D-3 L and EORTC-8D health states for Sri Lanka. The dependent variable will be the utility value. Different specifications of independent variables will be derived from the ordinal EQ-5D-3 L to test for the best-fitting model.\n    \n\n\n          Discussion:\n        \n      \n      In Sri Lanka, a LMIC health state valuation will have to be carried out using face to face interview instead of online methods. The proposed study will provide the first country-specific health state valuations for Sri Lanka, and one of the first valuations to be completed in a South Asian Country."
        },
        {
            "title": "Predicting late myocardial recovery and outcomes in the early hours of ST-segment elevation myocardial infarction traditional measures compared with microvascular obstruction, salvaged myocardium, and necrosis characteristics by cardiovascular magnetic resonance.",
            "abstract": "Objectives:\n        \n      \n      The aim of this study was to determine whether a very early imaging strategy improves the prediction of late systolic dysfunction and poor outcomes in ST-segment elevation myocardial infarction (STEMI) compared with traditional predictors.\n    \n\n\n          Background:\n        \n      \n      Earlier prediction of poor outcomes after STEMI is desirable, because it will allow tailored therapy at the earliest possible time, when benefits might be greatest.\n    \n\n\n          Methods:\n        \n      \n      One hundred and three patients with acute STEMI were studied by contrast-enhanced cardiovascular magnetic resonance within 12 h of primary angioplasty and at 6 months and followed >2 years. The primary end point was left ventricular (LV) dysfunction, whereas poor outcomes were a key secondary end point.\n    \n\n\n          Results:\n        \n      \n      Traditional risk factors were only modest predictors of late LV dysfunction. Late gadolinium enhancement (LGE) volume maintained a stronger association to LV ejection fraction change than infarct transmurality, microvascular obstruction, or myocardial salvage during STEMI (p = 0.02). Multivariable logistic regression identified LGE volume during STEMI as the best predictor of late LV dysfunction (odds ratio: 1.36, p = 0.03). An LGE >or=23% of LV during STEMI accurately predicted late LV dysfunction (sensitivity 89%, specificity 74%). The LGE volume provided important incremental benefit for predicting late dysfunction (area under the curve = 0.92, p <or= 0.03 vs. traditional risk factors). Twenty-three patients developed poor outcomes (1 death, 2 myocardial infarctions, 5 malignant arrhythmias, 4 severe LV dysfunction <35%, 11 hospital stays for heart failure) over 2.6 +/- 0.9 years; LGE volume remained a strong independent predictor of poor outcomes, whereas LGE >or=23% carried a hazard ratio of 6.1 for adverse events (p < 0.0001).\n    \n\n\n          Conclusions:\n        \n      \n      During the hyperacute phase of STEMI, LGE volume provides the strongest association and incremental predictive value for late systolic dysfunction and discerns poor late outcomes."
        },
        {
            "title": "Vision Loss in Older Adults.",
            "abstract": "Vision loss affects 37 million Americans older than 50 years and one in four who are older than 80 years. The U.S. Preventive Services Task Force concludes that current evidence is insufficient to assess the balance of benefits and harms of screening for impaired visual acuity in adults older than 65 years. However, family physicians play a critical role in identifying persons who are at risk of vision loss, counseling patients, and referring patients for disease-specific treatment. The conditions that cause most cases of vision loss in older patients are age-related macular degeneration, glaucoma, ocular complications of diabetes mellitus, and age-related cataracts. Vitamin supplements can delay the progression of age-related macular degeneration. Intravitreal injection of a vascular endothelial growth factor inhibitor can preserve vision in the neovascular form of macular degeneration. Medicated eye drops reduce intraocular pressure and can delay the progression of vision loss in patients with glaucoma, but adherence to treatment is poor. Laser trabeculoplasty also lowers intraocular pressure and preserves vision in patients with primary open-angle glaucoma, but long-term studies are needed to identify who is most likely to benefit from surgery. Tight glycemic control in adults with diabetes slows the progression of diabetic retinopathy, but must be balanced against the risks of hypoglycemia and death in older adults. Fenofibrate also slows progression of diabetic retinopathy. Panretinal photocoagulation is the mainstay of treatment for diabetic retinopathy, whereas vascular endothelial growth factor inhibitors slow vision loss resulting from diabetic macular edema. Preoperative testing before cataract surgery does not improve outcomes and is not recommended."
        },
        {
            "title": "Dosing of iodinated contrast volume: a new simple algorithm to stratify the risk of contrast-induced nephropathy in patients with acute coronary syndrome.",
            "abstract": "Objectives and background:\n        \n      \n      Previous studies on contrast-induced nephropathy (CIN) have identified contrast volume (CV) as a risk factor. The aim of our research was to define the safe dose of contrast media based on absolute CV, maximum allowable contrast dose (MACD) and estimated glomerular filtrate rate (eGFR).\n    \n\n\n          Methods and results:\n        \n      \n      A total of 940 consecutive patients with acute coronary syndrome (ACS) were enrolled. Fifty-four patients developed CIN. MACD was defined as 5*body weight/serum creatinine. When using a CV higher than MACD, CIN-risk was increased 19-fold (OR 9.810-39.307, P < 0.001). For the CV/eGFR ratio, we found that for every increase of one-tenth, CIN-risk increased by 4.9% (OR 1.037-1.061, P < 0.001). The discriminative ability of CV (C statistic = 0.626 ± 0.038) was significantly lower than for the CV/MACD (C statistic = 0.782 ± 0.036, P = 0.003) and CV/eGFR (C statistics: 0.796 ± 0.033 for MDRD-4, 0.796 ± 0.034 for Cockcroft-Gault, and 0.803 ± 0.033 for CKD-EPI; P < 0.001). There were no differences in the discriminative ability to predict CIN between the three eGFR equations. The combination of CV/MACD and CV/eGFR in a single protocol increases the positive predictive value of the Mehran risk score (40.7% vs. 8.8%) with the same sensitivity (90.7% vs. 83.3%). High doses of relative CV (CV/MACD and CV/eGFR) were also significantly associated with higher in-hospital mortality, reinfarction, and heart failure.\n    \n\n\n          Conclusions:\n        \n      \n      A sequential protocol based on CV/MACD and CV/eGFR appropriately identified those ACS patients who developed CIN, with predictive values similar to a Mehran score, reducing the false positive rate. It is also useful to predict risk of in-hospital cardiac events regardless of GRACE score."
        },
        {
            "title": "Role of early contrast enhanced CT scan in severity prediction of acute pancreatitis.",
            "abstract": "Severe pancreatitis occurs in approximately 15-25% of patients with acute pancreatitis. The objective of our study was to compare the CT Severity Index (CTSI) with a clinical score (BISAP score) to predict severity of acute pancreatitis. Forty-eight consecutive patients with acute pancreatitis who underwent contrast enhanced CT scan within 72 hours of presentation were included. Results of our study showed that both CTSI and BISAP score were reliable predictors of mortality (p value = 0.019 and <0.001 respectively) and need for mechanical ventilation (p value = .002 and .006 respectively). Positive predictive value of CTSI to predict recovery without intervention was 91.4% as compared to 78% for that of BISAP score. Receiver Operating Characteristics (ROC) Curves showed CT scan was superior to BISAP Score in predicting need of percutaneous or surgical intervention. Early CT scan may be utilized for prediction of clinical course of patients with acute pancreatitis."
        },
        {
            "title": "Coronary computed tomography angiography predicts subsequent cardiac outcome events: results of the Visipaque CCTA registry study.",
            "abstract": "Objectives:\n        \n      \n      To evaluate the diagnostic performance and predictive value of coronary computed tomography angiography (CCTA) on subsequent cardiac outcomes.\n    \n\n\n          Background:\n        \n      \n      CCTA has been suggested as an alternative method to invasive coronary angiography for detection of and ruling out coronary artery disease (CAD). However, the usefulness of CCTA findings in predicting patient outcome in routine clinical practice is still uncertain.\n    \n\n\n          Materials and methods:\n        \n      \n      A prospective, multicenter registry study of CCTA with a Visipaque injection 320 mg I/ml was carried out in symptomatic patients suspected of having CAD as part of their medical care. CCTA findings were used to guide patient management decisions. Patient cardiac outcomes were followed at 1, 6, and 12 months after the CCTA procedure for the occurrence of major adverse cardiac event (MACE) (cardiac death, nonfatal myocardial infarction, or unstable angina requiring hospitalization). All cardiac outcome events or deaths were adjudicated independently.\n    \n\n\n          Results:\n        \n      \n      Of 874 patients (mean age=59 years; 51% men) who received Visipaque, 857 were included in the efficacy analysis. Using cardiac outcomes as the endpoint, the sensitivity of CCTA was 96.1, 95.8, and 94.7%, specificity was 84.5, 86.6, and 87.0%, and negative predictive value more than 99.0% at 1, 6, and 12 months, respectively. At 12 months, the rate of MACE was 5.7% (10/174) in patients with a positive CCTA (one or more ≥50% stenosis) and 0.1% (1/683) in patients with a negative CCTA (99.9% MACE-free survival rate). The Cox proportional hazards analysis with CCTA outcome, age, sex, reasons for CCTA, and cardiac risk factors as covariates showed a hazard ratio of 87.6 for positive versus negative CCTA (P=0.0001).\n    \n\n\n          Conclusion:\n        \n      \n      CCTA is a highly accurate, noninvasive tool to detect or rule out subsequent cardiovascular events in patients with intermediate pretest probability of CAD or an uninterpretable/equivocal stress test. A positive CCTA finding contributed significantly toward the prediction of subsequent MACE whereas a negative CCTA carried excellent prognostic outcomes at 12 months."
        },
        {
            "title": "[Causes and treatment of acute visual dysfunction after transsphenoidal surgery of pituitary adenoma].",
            "abstract": "Objective:\n        \n      \n      Visual dysfunction is a sever complication of transsphenoidal surgery of pituitary adenoma. In this study, we explored the mechanisms and strategies of prevention and treatment of early acute visual deterioration after transsphenoidal surgery for removal of pituitary adenoma.\n    \n\n\n          Methods:\n        \n      \n      Clinical data of 12 patients with transsphenoidal pituitary adenoma resection of early postoperative acute visual deterioration were analyzed retrospectively. All the cases were come from Peking Union Medical College Hospital during Jan 1991 to May 2014.\n    \n\n\n          Results:\n        \n      \n      Among 12 cases, 8 were with intrasellar haematoma, 2 with suprasellar haematoma, 2 with vascular spasm. After all these treatments, good recovery achieved in 10 cases, not so well in 1 with rupture of intracranial aneurysm, and 1 case death with suprasellar haematoma and intracranial infection.\n    \n\n\n          Conclusions:\n        \n      \n      Blood circulation variation of the optic nerve and optic chiasma could be caused by long-term compressing with the pituitary adenoma, on this basis, combined with the intrasellar haematoma, direct injury during the operation, and acute ischemia of the optic chiasma and tamping too tight in the sella turcica, etc, resulting in acute visual function disorder after pituitary adenomas transsphenoidal surgery, find the reason and early treatment can be effective to save the patient's visual acuity."
        },
        {
            "title": "Mortality associated with bevacizumab intravitreal injections in age-related macular degeneration patients after acute myocardial infarct: a retrospective population-based survival analysis.",
            "abstract": "Background:\n        \n      \n      Intraocular injections of antivascular endothelial growth factor (VEGF) agents are currently the main therapy in age-related macular degeneration (AMD). The safety of bevacizumab, an anti-VEGF compound frequently delivered off label, is debated, particularly for high-group risks. We aim to analyze the mortality associated with intravitreal injections of bevacizumab for AMD in patients previously diagnosed with acute myocardial infarct (MI).\n    \n\n\n          Methods:\n        \n      \n      In a national database, we identified bevacizumab-treated AMD patients with a diagnosis of MI prior to their first bevacizumab injection, delivered between September 2008 and October 2014 (n = 2100). We then generated sub-groups of patients treated within 3 months (n = 11), 6 months (n = 24), 12 months (n = 52), and 24 months (n = 124) after MI. Those patients were compared to age- and gender-matched members that had a MI at the same time and had never been exposed to anti-VEGF. Survival analysis was performed using propensity score-adjusted Cox regression.\n    \n\n\n          Results:\n        \n      \n      Bevacizumab-treated patients were slightly and insignificantly older than controls (mean age 83.25 vs 83.19 year, P = .75). Gender distribution was similar. In a Cox regression adjusted with propensity score, the following differences in mortality were found: within 3 months between MI and initiation of bevacizumab treatment, OR = 6.22 (95% C.I 1.08-35.97, P < .05); within 6 months, OR = 2.37 (95% C.I 0.93-6.02, P = .071); within 12 months, OR = 3.00 (95% C.I 1.44-6.28, P < .01); within 24 months after MI, OR = 2.24 (95% C.I 1.35-3.70, P < .01); and MI any time prior to first bevacizumab injection, OR = 1.71 (95% C.I 1.53-1.92, P < .001).\n    \n\n\n          Conclusions:\n        \n      \n      We report increased mortality associated with the use of intravitreal bevacizumab in AMD patients after MI, compared to age- and gender-matched post-MI patients with no exposure to any anti-VEGF agent. Caution should be taken while offering bevacizumab to AMD patients after MI."
        },
        {
            "title": "A comparison of the recording of comorbidity in primary and secondary care by using the Charlson Index to predict short-term and long-term survival in a routine linked data cohort.",
            "abstract": "Objective:\n        \n      \n      Hospital admission records provide snapshots of clinical histories for a subset of the population admitted to hospital. In contrast, primary care records provide continuous clinical histories for complete populations, but might lack detail about inpatient stays. Therefore, combining primary and secondary care records should improve the ability of comorbidity scores to predict survival in population-based studies, and provide better adjustment for case-mix differences when assessing mortality outcomes.\n    \n\n\n          Design:\n        \n      \n      Cohort study.\n    \n\n\n          Setting:\n        \n      \n      English primary and secondary care 1 January 2005 to 1 January 2010.\n    \n\n\n          Participants:\n        \n      \n      All patients 20 years and older registered to a primary care practice contributing to the linked Clinical Practice Research Datalink from England.\n    \n\n\n          Outcome:\n        \n      \n      The performance of the Charlson index with mortality was compared when derived from either primary or secondary care data or both. This was assessed in relation to short-term and long-term survival, age, consultation rate, and specific acute and chronic diseases.\n    \n\n\n          Results:\n        \n      \n      657,264 people were followed up from 1 January 2005. Although primary care recorded more comorbidity than secondary care, the resulting C statistics for the Charlson index remained similar: 0.86 and 0.87, respectively. Higher consultation rates and restricted age bands reduced the performance of the Charlson index, but the index's excellent performance persisted over longer follow-up; the C statistic was 0.87 over 1 year, and 0.85 over all 5 years of follow-up. The Charlson index derived from secondary care comorbidity had a greater effect than primary care comorbidity in reducing the association of upper gastrointestinal bleeding with mortality. However, they had a similar effect in reducing the association of diabetes with mortality.\n    \n\n\n          Conclusions:\n        \n      \n      These findings support the use of the Charlson index from linked data and show that secondary care comorbidity coding performed at least as well as that derived from primary care in predicting survival."
        },
        {
            "title": "Prognostic value of dobutamine stress myocardial contrast perfusion echocardiography.",
            "abstract": "Background:\n        \n      \n      Myocardial perfusion (MP) imaging with real-time contrast echocardiography (RTCE) improves the sensitivity of dobutamine stress echocardiography for detecting coronary artery disease. Its prognostic value is unknown. We sought to determine the value of MP and wall motion (WM) analysis during dobutamine stress echocardiography in predicting the outcome of patients with known or suspected coronary artery disease.\n    \n\n\n          Methods and results:\n        \n      \n      We retrospectively studied 788 patients with RTCE during dobutamine stress echocardiography using intravenous commercially available contrast agents. The incremental prognostic value of MP imaging over clinical risk factors and other echocardiographic data was examined through the use of a log-likelihood test (Cox model). During a median follow-up of 20 months, 75 events (9.6%) occurred (58 deaths, 17 nonfatal myocardial infarctions). Abnormal MP had significant incremental value over clinical factors, resting ejection fraction, and WM responses in predicting events (P<0.001). By multivariate analysis, the independent predictors of death and nonfatal myocardial infarction were resting left ventricular ejection fraction <50% (relative risk [RR], 1.9; 95% CI, 1.2 to 3.2; P=0.01), hypercholesterolemia (RR, 0.5; 95% CI, 0.3 to 0.9; P=0.01), and abnormal MP (RR, 5.2; 95% CI, 3.0 to 9.0; P<0.0001). The 3-year event free survival was 95% for patients with normal WM and MP, 82% for normal WM and abnormal MP, and 68% for abnormal WM and MP.\n    \n\n\n          Conclusions:\n        \n      \n      MP imaging during dobutamine stress RTCE provides incremental prognostic information in patients with known or suspected coronary artery disease. Patients with normal MP have a better outcome than patients with normal WM."
        },
        {
            "title": "Biostatistical analysis of the collaborative glaucoma study. I. Summary report of the risk factors for glaucomatous visual-field defects.",
            "abstract": "A prospective collaborative study was conducted in five centers during a 13-year period to identify factors that influence the development of visual-field defects (GVFDs) of open angle glaucoma. In 5,000 subjects, GVFDs developed in only 1.7% of eyes. Statistical analysis of 26 factors at first examination identified five that were significantly related to the development of GVFDs--outflow facility, age, applanation pressure, cup-disc ratio, and pressure change after water drinking. Their absolute initial value, and not its change with time, was the important predictor. Multivariate analysis showed their collective predictive power to be undesirably poor, indicating that other factors must play an important role in the development of GVFDs. Mortality-table analysis indicated that during a period of five years, 98.54% of eyes with initial pressure less than 20 mm Hg continued to be free from GVFDs as compared with 93.34% of those with pressure of 20 mm Hg or greater."
        },
        {
            "title": "Longitudinal rates of annual eye examinations of persons with diabetes and chronic eye diseases.",
            "abstract": "Objective:\n        \n      \n      To assess the rate of annual eye examinations over time among older Americans with diabetes and chronic eye diseases.\n    \n\n\n          Design:\n        \n      \n      Longitudinal analysis of Medicare claims data.\n    \n\n\n          Participants:\n        \n      \n      Random sample of Medicare beneficiaries aged 65 years or older.\n    \n\n\n          Methods:\n        \n      \n      Beneficiaries were followed between 1991 and 1999, unless mortality or enrollment in a health maintenance organization for > 6 months in a given 12-month period intervened. All claims data (both physician and facility) during this time were analyzed for the presence of International Classification of Diseases 9 codes consistent with 1 of the 3 study conditions and the performance of eye examinations.\n    \n\n\n          Main outcome measures:\n        \n      \n      Claims submitted by optometrists, ophthalmologists, or other providers of eye care for subjects with diabetes, glaucoma, or age-related macular degeneration (ARMD). Rates were calculated on the basis of a 15-month time window for annual examinations rather than for 12 months to allow for less than full compliance with the guidelines for various reasons (e.g., bad weather).\n    \n\n\n          Results:\n        \n      \n      Among those with diabetes in this population, 50% to 60% had annual eye examinations in a 15-month period. Of those followed for at least 75 months after diagnosis, about three quarters had one or more 15-month gaps between visits. For subjects diagnosed with glaucoma, most visit rates were in the 70% to 90% range per 15-month period. The percentage of subjects with at least one 15-month period with no visits was considerably lower than for diabetes. The patterns for those with ARMD were in between those for diabetes and glaucoma. Over a nine-year period, only slightly over half of persons with at least one of the study conditions complied with practice guidelines.\n    \n\n\n          Conclusions:\n        \n      \n      Annual eye examinations for persons diagnosed with diabetes, glaucoma, and ARMD are important for detecting potentially treatable vision loss among those already diagnosed with these conditions. Currently, actual rates of eye examinations for persons diagnosed with the study conditions fall far short of recommended rates. As such, approaches to enhancing longitudinal follow-up of those already in the eye care system are needed."
        },
        {
            "title": "Validation of a predictive risk score for radiocontrast-induced nephropathy following percutaneous coronary intervention.",
            "abstract": "Objective:\n        \n      \n      We sought to externally validate the William Beaumont Hospital (WBH) risk score for radiocontrast-induced nephropathy (RCIN) following percutaneous coronary intervention (PCI). Background. RCIN is associated with increased mortality and morbidity following PCI and accounts for increased hospital costs and length of stay.\n    \n\n\n          Methods:\n        \n      \n      A total of 4,814 PCI procedures were used for validation of the WBH risk score, using a >1.0 mg/dl increase in serum creatinine (Cr) as the definition of RCIN. Clinical and procedural details were identified within the Mayo Clinic PCI registry. Multiple imputation was used to impute values where missing. Five imputation sets were created and averaged to compute the final estimate.\n    \n\n\n          Results:\n        \n      \n      Follow-up Cr was available in 3,213 (67%) of procedures and RCIN occurred in 1.9% of cases. Baseline Cr clearance was missing in 13%. All other risk factors used to calculate the risk score were missing in 25% of the procedures. The risk score has the ability to discriminate well between patients at low and high risk of post-PCI RCIN; c-statistic = 0.86. In-hospital mortality occurred in 6.6% (4/61) with RCIN vs. 1.2% (37/3152) without RCIN. The odds ratio for in-hospital mortality is 5.3 (95% CI, 1.9, 15.0; p = 0.002) for those with RCIN vs. those without.\n    \n\n\n          Conclusions:\n        \n      \n      The WBH risk score can identify patients at high and low risk of RCIN following PCI. Use of this risk score can identify patients at high risk of RCIN development and direct the use of preventative measures to the highest-risk population, improving patient outcome and prognosis."
        },
        {
            "title": "Effects of sensory aids on the quality of life and mortality of elderly people: a multivariate analysis.",
            "abstract": "The present study aimed at clarifying the relationships between the use of sensory aids and the quality of life (QOL) and mortality of elderly people suffering from sensory deprivation. We carried out a cross-sectional survey on the QOL and the sensory status of an elderly cohort and a 6-year longitudinal follow-up of mortality rates among 1192 non-institutionalized people aged 70-75 years in a North Italian town. We classified respondents into three groups: those with functionally adequate visual and hearing acuity (n = 275); those with sensory impairment, corrected by the use of sensory aids (n = 680), and those with uncorrected sensory impairment (n = 245). In the whole sample, multiple logistic regression analyses showed that an uncorrected sensory deprivation was associated with a significant and independent impairment of mood, self-sufficiency in instrumental activities of daily living and social relationships. Such impairments were not apparent in the subjects with sensory impairments who were using sensory aids. In men with uncorrected sensory impairment the unadjusted 6-year mortality rate was almost twice that of the other two groups, which did not differ from each other. No corresponding differences were detected in women. Multivariate analysis showed that the effect of the sensory aid status on mortality was indirect and mediated through the global physical health status and the social relationships. We conclude that our cross-sectional data demonstrate an association between uncorrected sensory deprivation and a low QOL; such an association was not present in subjects with corrected sensory deprivation."
        },
        {
            "title": "Long-term functional outcome after ileal pouch anal anastomosis in 191 patients with ulcerative colitis.",
            "abstract": "Background:\n        \n      \n      A long-lasting good functional outcome of the pelvic pouch and a subsequent satisfying quality of life (QoL) are mandatory. Long-term functional outcome and QoL in a single-center cohort were assessed.\n    \n\n\n          Patients and methods:\n        \n      \n      A questionnaire was sent to all patients with an IPAA for UC, operated between 1990 and 2010 in our department. Pouch function was assessed using the Öresland Score (OS) and the 'Pouch Functional Score' (PFS). QoL was assessed using a Visual Analogue Score (VAS).\n    \n\n\n          Results:\n        \n      \n      250 patients (42% females) with a median age at surgery of 38 years (interquartile range (IQR): 29-48 years) underwent restorative proctocolectomy. Median follow-up was 11 years (IQR: 6-17 years). Response rate was 81% (n=191). Overall pouch function was satisfactory with a median OS of 6/15 (IQR: 4-8) and a median PFS of 6/30 (IQR: 3-11). 24-hour bowel movement is limited to 8 times in 68% of patients (n=129), while 55 patients (29%) had less than 6 bowel movements. 12 patients (6.5%) were regularly incontinent for stools, while 154 patients (82%) reported a good fecal continence. Fecal incontinence during nighttime was more common (n=72, 39%). Pouch function had little impact on social activity (4/10; IQR: 2-6) and on professional activity (3/10; IQR: 1-6). 172 patients (90%) reported to experience an overall better health condition since their operation. The OS and the PFS correlated well (Pearson's correlation coefficient=0.83). Overall pouch function was stable over time.\n    \n\n\n          Conclusion:\n        \n      \n      Majority of patients report a good pouch function on the long-term with limited impact on QoL."
        },
        {
            "title": "Utility values among glaucoma patients: an impact on the quality of life.",
            "abstract": "Aim:\n        \n      \n      To ascertain utility values and associated quality of life with different severity and duration of glaucoma among Indian patients.\n    \n\n\n          Methods:\n        \n      \n      Utility values of 105 consecutive patients with primary glaucoma of at least 12 months' duration were evaluated in a cross sectional study. Utility values were ascertained in five groups using both the time-trade off and standard gamble methods: group 1 (best corrected visual acuity in the better eye of 6/9 or better), group 2 (best corrected visual acuity in the better eye of 6/18 to 6/12), group 3 (best corrected visual acuity in the better eye of 6/36 to6/24), group 4 (best corrected visual acuity in the better eye of 3/60 to 6/60), and group 5 (best corrected visual acuity in the better eye of 3/60 or worse).\n    \n\n\n          Results:\n        \n      \n      The mean utility value for the glaucoma group as a whole was 0.64 (SD 0.69; 95% confidence interval (CI), 0.58 to 0.70) with the time-trade off method and 0.86 (SD 1.00; 95% CI, 0.81 to 0.90) with the standard gamble method for a gamble of death and 0.97 (SD 1.00; 95% CI, 0.94 to 0.99) for a gamble of blindness. The mean utility results by the time-trade off method were as follows: group 1 = 0.66, group 2 = 0.66, group 3 = 0.62, group 4 = 0.55, and group 5 = 0.61. The utility value was much lower (0.46) in those with no formal education or only primary education compared to those with postgraduate education (0.75) (p = 0.038). Those patients with glaucoma of less than 5 years' duration had a utility score of 0.62 while those with glaucoma for more than 10 years had a score of 0.74 (p = 0.40).\n    \n\n\n          Conclusions:\n        \n      \n      Visual acuity loss occurring secondary to glaucoma is associated with a substantial decrease in patient utility value (and quality of life) in a developing country like India. The utility value is directly dependent on the degree of visual acuity loss associated with the disease and educational status and not on the duration of disease, the number of medications, or the visual field indices."
        },
        {
            "title": "Quantification of epicardial and intrathoracic fat volume does not provide an added prognostic value as an adjunct to coronary artery calcium score and myocardial perfusion single-photon emission computed tomography.",
            "abstract": "Aims:\n        \n      \n      To compare the predictive value of epicardial and intrathoracic fat volume (EFV, IFV), coronary artery calcium (CAC) score, and single-photon emission computed tomography (SPECT) myocardial perfusion imaging (MPI) for major adverse cardiac events (MACE).\n    \n\n\n          Methods and results:\n        \n      \n      Follow-up was obtained in 275 patients with known or suspected coronary artery disease (CAD), who underwent SPECT-MPI including non-contrast cardiac computed tomography (CT) for attenuation correction to evaluate ischaemic heart disease and in whom EFV, IFV, and CAC score were calculated from non-contrast CT. Associations between fat volume, traditional cardiovascular risk factors, CAC score, and SPECT-MPI results were assessed and MACE predictors identified by Cox proportional hazard regression and global χ(2) statistics. After a median follow-up of 2.9 years, MACE were recorded in 38 patients. In univariate Cox regression analysis, EFV and IFV were predictors of MACE (P = 0.013 and P = 0.004, respectively). In multivariate analysis, EFV and IFV provided incremental predictive value beyond traditional cardiovascular risk factors (P < 0.05 and P < 0.01). However, after adjustment for CAC score and SPECT-MPI results, EFV and IFV fell short of statistical significance as independent outcome predictors.\n    \n\n\n          Conclusion:\n        \n      \n      Quantification of EFV and IFV is associated with MACE and may improve risk stratification beyond traditional cardiovascular risk factors. However, once CAC score and/or SPECT-MPI results are known, EFV and IFV do not provide any added clinically relevant prognostic value. Further studies may identify the subpopulation with the largest relative merit of EFV and IFV as an adjunct to SPECT-MPI and CAC score."
        },
        {
            "title": "Risk Stratification for Cardiovascular Disease in Women in the Primary Care Setting.",
            "abstract": "Background:\n        \n      \n      Traditional risk assessment tools classify the majority of middle-aged women at low risk despite cardiovascular (CV) disease's affecting >50% of women and remaining the leading cause of death. Ultrasound-determined carotid intima-media thickness (CIMT) and/or computed tomographic coronary artery calcium score (CACS) quantify subclinical atherosclerosis and add incremental prognostic value. The aim of this study was to assess the utility of CIMT and CACS to detect subclinical atherosclerosis in younger women.\n    \n\n\n          Methods:\n        \n      \n      Asymptomatic women aged 50 to 65 years with at least one CV risk factor and low Framingham risk scores were identified prospectively at primary care and cardiology clinics. Mean intimal thickness, plaque on CIMT, and Agatston calcium score for CACS were obtained.\n    \n\n\n          Results:\n        \n      \n      Of 86 women (mean age, 58 ± 4.6 years; mean Framingham risk score, 1.9 ± 1.2; mean low-density lipoprotein cholesterol level, 138.9 ± 37.0 mg/dL), 53 (62%) had high-risk CIMT (51% plaque, 11% CIMT > 75th percentile). In contrast, three women (3.5%) had CACS > 100, all of whom had plaque by CIMT. Of the 58 women with CACS of 0, 32 (55%) had high-risk CIMT (48% plaque, 7% CIMT > 75th percentile).\n    \n\n\n          Conclusions:\n        \n      \n      In patients referred by their physicians for assessment of CV risk, CIMT in asymptomatic middle-aged women with at least one CV risk factor and low risk by the Framingham risk score identified a large number with advanced subclinical atherosclerosis despite low CACS. Our results suggest that CIMT may be a more sensitive method for CV risk assessment than CACS or traditional risk tools in this population. Further studies are needed to determine if earlier detection would be of clinical benefit."
        },
        {
            "title": "Added value of delayed computed tomography angiography in primary intracranial hemorrhage and hematoma size for predicting spot sign.",
            "abstract": "Background The computed tomography angiography (CTA) spot sign represents active contrast extravasation within acute primary intracerebral hemorrhage (ICH) and is an independent predictor of hematoma expansion (HE) and poor clinical outcomes. The spot sign could be detected on first-pass CTA (fpCTA) or delayed CTA (dCTA). Purpose To investigate the additional benefits of dCTA spot sign in primary ICH and hematoma size for predicting spot sign. Material and Methods This is a retrospective study of 100 patients who underwent non-contrast CT (NCCT) and CTA within 24 h of onset of primary ICH. The presence of spot sign on fpCTA or dCTA, and hematoma size on NCCT were recorded. The spot sign on fpCTA or dCTA for predicting significant HE, in-hospital mortality, and poor clinical outcomes (mRS ≥ 4) are calculated. The hematoma size for prediction of CTA spot sign was also analyzed. Results Only the spot sign on dCTA could predict high risk of significant HE and poor clinical outcomes as on fpCTA ( P < 0.05). With dCTA, there is increased sensitivity and negative predictive value (NPV) for predicting significant HE, in-hospital mortality, and poor clinical outcomes. The XY value (product of the two maximum perpendicular axial dimensions) is the best predictor (area under the curve [AUC] = 0.82) for predicting spot sign on fpCTA or dCTA in the absence of intraventricular and subarachnoid hemorrhage. Conclusion This study clarifies that dCTA imaging could improve predictive performance of CTA in primary ICH. Furthermore, the XY value is the best predictor for CTA spot sign."
        },
        {
            "title": "Emotion and motivation I: defensive and appetitive reactions in picture processing.",
            "abstract": "Emotional reactions are organized by underlying motivational states--defensive and appetitive--that have evolved to promote the survival of individuals and species. Affective responses were measured while participants viewed pictures with varied emotional and neutral content. Consistent with the motivational hypothesis, reports of the strongest emotional arousal, largest skin conductance responses, most pronounced cardiac deceleration, and greatest modulation of the startle reflex occurred when participants viewed pictures depicting threat, violent death, and erotica. Moreover, reflex modulation and conductance change varied with arousal, whereas facial patterns were content specific. The findings suggest that affective responses serve different functions-mobilization for action, attention, and social communication-and reflect the motivational system that is engaged, its intensity of activation, and the specific emotional context."
        },
        {
            "title": "Contrast-enhanced abdominal computed tomography scanning and prediction of severity of acute pancreatitis: a prospective study.",
            "abstract": "One hundred and fifty-two patients were admitted to a single hospital with a diagnosis of acute pancreatitis during a 31-month period. Of these, 126 patients had contrast-enhanced abdominal computed tomography (CT) scans within 72 h of admission; 92 of these attacks were clinically mild, 34 were clinically severe. A single consultant radiologist reported the scans 'blind' and noted whether pancreatic enhancement was normal, increased or decreased, and whether there was loss of peripancreatic tissue planes. The maximum anteroposterior measurement of the pancreatic head and body were multiplied together to produce a 'pancreatic size index' (cm2). Significantly more patients with severe attacks had decreased pancreatic enhancement (79 versus 58 per cent, P = 0.01) and loss of peripancreatic tissue planes (82 versus 54 per cent, P = 0.006). The median (range) pancreatic size index for clinically severe attacks was 12.8 cm2 (3.0-52.5), and for mild attacks was 6.0 cm2 (1.1-23.4), P less than 0.0001. Modified Glasgow criteria had a sensitivity of 85 per cent and specificity of 79 per cent for clinically severe attacks. A pancreatic size index of greater than or equal to 10 cm2 had a sensitivity of 71 per cent and specificity of 77 per cent for clinically severe attacks. In conclusion, although there were highly significant differences between the clinically severe and mild groups with respect to pancreatic enhancement, peripancreatic tissue planes and pancreatic size indices, these CT criteria did not improve on modified Glasgow criteria for prediction of disease severity."
        },
        {
            "title": "Classification versus Prediction of Mortality Risk using the SIRS and qSOFA Scores in Patients with Infection Transported by Paramedics.",
            "abstract": "Objective: Identifying patients with sepsis in the prehospital setting is an important opportunity to increase timely care. When assessing clinical tools designed for paramedic sepsis identification, predicted risk may provide more useful information to support decision-making, compared to traditional estimates of classification accuracy (i.e., sensitivity and specificity). We sought to contrast classification accuracy versus predicted risk of a modified version of the Systemic Inflammatory Response Syndrome score (i.e., excluding white blood cell measure which is often unavailable to paramedics; mSIRS) and quick Sepsis Related Organ Failure Assessment (qSOFA) for determining mortality risk among patients with infection transported by paramedics. Methods: A one-year cohort of patients with infections transported to the Emergency Department (ED) by paramedics was linked to in-hospital administrative databases. Scores were calculated using the first reported vital sign measure for each patient. We calculated sensitivity and specificity of mSIRS and qSOFA for classifying hospital mortality at different score thresholds, and estimated discrimination (using the C-statistic) and calibration (using calibration curves). Regression models for predicting hospital mortality were constructed using the mSIRS or qSOFA scores for each patient as the predictor. Results: A total of 10,409 patients with infection who were transported by paramedics were successfully linked, with an overall mortality rate of 9.2%. The mSIRS score had higher sensitivity estimates than qSOFA for classifying hospital mortality at all thresholds (mSIRS ≥ 1: 0.83 vs. qSOFA≥ 1: 0.80, mSIRS ≥ 3: 0.11 vs. qSOFA ≥ 3: 0.08), but the qSOFA score had better discrimination (C-statistic qSOFA: 0.72 vs. mSIRS: 0.63) and calibration. The risk of hospital mortality predicted by the mSIRS score ranged from 8.0 to 19% across score values, whereas the risk predicted by the qSOFA score ranged from 10 to 51%. Conclusion: Assessing the predicted risk for the mSIRS and qSOFA scores instead of classification accuracy reveals that the qSOFA score provides more information to clinicians about a patient's mortality risk, supporting its use in clinical decision-making."
        },
        {
            "title": "Mortality in diabetic subjects: an eleven-year follow-up of a community-based population.",
            "abstract": "In 1979, all the known diabetic subjects (849) were identified from a community (population 81851), of whom 717 (85%) were reviewed by a single observer. Using the NHS Central Register, follow-up was completed for 98% of subjects. After 11 years, 306 (42.7%) diabetic subjects had died, of whom 65 were insulin treated and 241 were non-insulin treated. Circulatory disease accounted for 168 (54.9%) deaths, of which 124 (73.8%) were due to ischaemic heart disease. The standardized mortality ratio (SMR) for all causes of death, based on data from England and Wales, was significantly raised for both insulin-treated and non-insulin-treated patients (1.75, 95% CI 1.35 to 2.24 and 1.32, 95% CI 1.15 to 1.50, respectively). SMRs for all cause mortality were significantly greater for diabetic subjects in the 45-64 (SMR, 1.97, 95% CI 1.34 to 2.80), 65-74 (SMR 1.59, 95% CI 1.27 to 1.97 and 75 years and over (SMR 1.26, 95% CI 1.08 to 1.45) age ranges. Using a proportional hazards model, after adjusting for age and gender, systolic blood pressure and vibration threshold were significant predictors of all cause mortality in insulin-treated subjects. For non-insulin-treated subjects, blood glucose, systolic blood pressure, glycated haemoglobin, retinopathy, proteinuria, coronary artery disease, and stroke were significant baseline predictors of mortality. No association was found for serum cholesterol, body mass index, diastolic pressure or cigarette smoking in either treatment group."
        },
        {
            "title": "Abridged geriatric assessment is a better predictor of overall survival than the Karnofsky Performance Scale and Physical Performance Test in elderly patients with cancer.",
            "abstract": "Objectives:\n        \n      \n      Comprehensive geriatric assessment (CGA) is a complex and interdisciplinary approach to evaluate the health status of elderly patients. The Karnofsky Performance Scale (KPS) and Physical Performance Test (PPT) are less time-consuming tools that measure functional status. This study was designed to assess and compare abridged geriatric assessment (GA), KPS and PPT as predictive tools of mortality in elderly patients with cancer.\n    \n\n\n          Materials and methods:\n        \n      \n      This prospective interventional study included all individuals aged >70years who were diagnosed with cancer during the study period. Subjects were interviewed directly using a procedure that included a clinical test and a questionnaire composed of the KPS, PPT and abridged GCA. Overall survival (OS) was the primary endpoint. The log rank test was used to compare survival curves, and Cox's regression model (forward procedure) was used for multivariate survival analysis.\n    \n\n\n          Results:\n        \n      \n      One hundred patients were included in this study. Abridged GA was the only tool found to predict mortality [median OS for unfit patients (at least two impairments) 467days vs 1030days for fit patients; p=0.04]. Patients defined as fit by mean PPT score (>20) had worse median OS (560 vs 721days); however, this difference was not significant (p=0.488 on log rank). Although median OS did not differ significantly between patients with low (≤80) and high (>80) KPS scores (467 and 795days, respectively; p=0.09), survival curves diverged after nearly 120days of follow-up. Visual and hearing impairments were the only components of abridged GA of prognostic value.\n    \n\n\n          Conclusion:\n        \n      \n      Neither KPS nor PPT were shown to predict mortality in elderly patients with cancer whereas abridged GA was predictive. This study suggests a possible role for visual and hearing assessment as screening for patients requiring CGA."
        },
        {
            "title": "Methanol poisoning: 27 years experience at a tertiary care hospital.",
            "abstract": "Methanol toxicity can result in serious morbidity and mortality without timely diagnosis and treatment. Many cases of methanol poisoning outbreaks have been noted in our population but no study has been performed to estimate methanol exposure and its outcomes and complications. A retrospective study was conducted to review all the cases of methanol poisoning admitted from January 1988 to December 2015 at the Aga Khan University Hospital. A total of 35 methanol poisoning cases were reported. All the patients were male, and the mean age was 36.2±8.6 years. The mean Glasgow Coma Scale score on presentation in the emergency was 10.4 ± 4.4. Blurring of vision was present in 17 (48%) patients while 10 (28%) had complete blindness. Mean arterial pH was 6.8±0.5 on arrival. Ethanol was given to 30(88%) patients and 12(32%) patients received bicarbonate for immediate treatment. A total of 15 (42.8%) patients underwent dialysis, out of which only 5 (33.3%) patients survived. Overall, 19 (54.3%) patients expired secondary to methanol ingestion."
        },
        {
            "title": "Noncalcified plaque: relationship between results of multislice computed tomography, risk factors, and late clinical outcome.",
            "abstract": "Background:\n        \n      \n      Contrast-enhanced multislice computed tomographic angiography (MSCTA) detects noncalcified plaque (NCP) in coronary arteries and associated coronary stenoses. However, the clinical relevance of NCP is poorly defined.\n    \n\n\n          Objectives:\n        \n      \n      Our goal was to examine the relationship NCP, risk factors (RFs), and clinical follow-up in unselected outpatients undergoing MSCTA.\n    \n\n\n          Methods:\n        \n      \n      Five hundred six patients undergoing contrast MSCTA were evaluated for NCP (intraluminal density 25 < Hounsfield units < 130). One hundred twenty-four patients (24.5%) had calcium scores (CAC) of zero. Of these, 111 patients were examined for RFs and followed clinically for a mean of 34 months.\n    \n\n\n          Results:\n        \n      \n      Of 124 patients with zero CAC, 111 (89.5%) included 52 (46.8%) with no NCP, 40 (36.0%) with NCP, and mild luminal stenosis, 14 (12.6%) and 5 (4.5%) with NCP causing significant and severe stenosis, respectively. Patients in each group were similar in age but differed significantly in number of RFs. Current or former smokers, hypertensive, and obese patients had more NCP and associated stenosis. At a mean of 34 months, there were no events in the no NCP group, 2/54 (3.7%) events in the NCP without severe stenosis group (one sudden cardiac death and one ventricular tachycardia), and 2/5 (40.0%) patients had revascularization in the NCP with severe stenosis group.\n    \n\n\n          Conclusions:\n        \n      \n      (1) In patients with zero CAC, presence of NCP on MSCTA was associated with more RFs, especially smoking, obesity, and hypertension. (2) NCP can result in severe coronary stenosis. (3) NCP detected by MSCTA in patients with zero CAC may identify patients with late cardiac events."
        },
        {
            "title": "Medical student and patient perceptions of quality of life associated with vision loss.",
            "abstract": "Objective:\n        \n      \n      Because most medical schools in the United States and Canada require no formal ophthalmology training, the authors queried medical student and ophthalmic patients to compare their perceptions of the quality of life (QOL) associated with vision loss.\n    \n\n\n          Design:\n        \n      \n      Cross-sectional comparative study of consecutive medical students and patients with vision loss using a validated, reliable, time trade-off utility instrument.\n    \n\n\n          Participants:\n        \n      \n      Consecutive Jefferson Medical College medical students (cohort 1: 145 second-year student; cohort 2: 112 third-year/fourth-year students) and 283 patients with vision loss (patient cohort).\n    \n\n\n          Methods:\n        \n      \n      Time trade-off vision utilities with anchors of 0.0 (death) to 1.0 (normal vision permanently) were used to quantify the QOL associated with vision loss. Students were asked to assume they had: (i) mild vision loss (20/40 to 20/50 vision in the better-seeing eye), (ii) legal blindness (20/200 in the better-seeing eye), and (iii) absolute blindness (no light perception bilaterally).\n    \n\n\n          Results:\n        \n      \n      Mean utilities for cohort 1/cohort 2 were 0.96/0.95 (p = 0.20) for mild vision loss, 0.88/0.84 for legal blindness (p = 0.009), and 0.80/0.67 (p < 0.0001) for absolute blindness. Medical student/patient mean utilities were 0.96/0.79 (p < 0.0001) for mild vision loss, 0.85/0.62 for legal blindness (p < 0.0001), and 0.74/0.26 (p < 0.0001) for absolute blindness. Overall, medical students underestimated the QOL associated with vision loss referent to patients with vision loss by 153%-425%.\n    \n\n\n          Conclusions:\n        \n      \n      Medical students dramatically underestimated the impact of vision loss on patient QOL. Clinical training slightly improved medical student perceptions. Trivialization of vision loss could result in systemic health harm, less ophthalmic research dollars, loss of the finest medical students entering ophthalmology, and overall adverse financial effects for the field."
        },
        {
            "title": "Survival Rates and Mortality Risk Factors of Thai Patients with Type 2 Diabetes Mellitus.",
            "abstract": "Background:\n        \n      \n      Type 2 diabetes mellitus (T2DM) is a common public health problem due to both its micro- and macro-vascular chronic complications. Data on survival rates and mortality risk factors of T2DM in Thailand need to be investigated and updated.\n    \n\n\n          Objective:\n        \n      \n      To assess the survival rate and mortality risk factors in T2DM patients.\n    \n\n\n          Material and method:\n        \n      \n      This study is a part of the Thai DMS Diabetes Complications project which is a prospective observational 4-year study of Thai T2DM patients. All patients were recruited from out-patient departments of seven public hospitals and received standard treatment from their doctors. Their clinical and laboratory status were evaluated annually over 4 years, with particular emphasis on diabetic nephropathy, retinopathy and neuropathy. Outcomes at the end of the four-year study were expressed as survival or death, and causes of mortality were identified.\n    \n\n\n          Results:\n        \n      \n      1,097 from 1,120 stable T2DM patients were enrolled. After 4 years of follow-up, 80 patients (7.3%) had died. Causes of death were: cardiovascular disease (20 cases, 25.0%); infection (20 cases, 25.0%); malignancy (10 cases, 12.5%); end-stage renal disease (4 cases, 5.0%); and other causes (26 cases, 32.5%). Survival rates at 1, 2, 3, and 4 years were 98.9, 97.5, 96.2 and 92.7% respectively. Hazard ratios (95% CI) of all-cause mortality were being over 60 years old 1.84 (1.15-2.94) and having diabetic nephropathy 1.75 (1.12-2.75). Survival rates from cardiovascular mortality at 1, 2, 3, and 4 years were 99.2, 98.4, 97.4 and 94.5% respectively. Hazard ratios (95% CI) of cardiovascular mortality were: female gender 1.75 (1.05-2.94); diabetic nephropathy 1.72 (1.03-2.88); and diabetic retinopathy 1.74 (1.02-2.94).\n    \n\n\n          Conclusion:\n        \n      \n      The survival rate of Thai patients with T2DM over the 4 years was 92.7%. Being over 60 years old and having diabetic nephropathy were associated with all-cause mortality. Female gender, diabetic nephropathy and diabetic retinopathy were associated with cardiovascular mortality."
        },
        {
            "title": "Mortality in Behcet's syndrome.",
            "abstract": "Behçet's syndrome significantly increases mortality especially when seen in the young male, while it is less severe among females and the aged. In many patients, the condition abates with the passage of time. The main cause of mortality is large vessel disease, especially bleeding pulmonary artery aneurysms (PAA), almost exclusively seen among men. Central nervous system disease comes second. Interestingly, not much increased atherosclerosis is seen in Behçet's syndrome when compared to other inflammatory diseases. In controlled studies, there has been no increase in history of increased angina or myocardial infarction. Similarly, atherosclerotic plaque formation is not increased by ultrasound. On the other hand, intermittent claudication can be seen. However, this is not due to arterial involvement but due to venous disease of the lower extremities. Recently there has been a substantial decrease in mortality due to PAA thanks to prompt disease recognition and treatment."
        },
        {
            "title": "Long-term results of Krupin-Denver valve implants in filtering surgery for neovascular glaucoma.",
            "abstract": "The long-term results obtained with the Krupin eye short valve shut in 28 eyes with neovascular glaucoma were retrospectively analyzed by means of Kaplan-Meier survival curve. The preoperative intraocular pressures (IOPs) ranged from 28 to 62 mm Hg (mean, 36.8 +/- 5.8 mm Hg). Success was considered an IOP of less than 22 mm Hg and greater than 5 mm Hg without medication (complete success) or with medication (qualified success) without additional glaucoma filtering surgery or devastating complications. Postoperative success was obtained in 10 out of 28 eyes after a mean follow-up period of 58.4 +/- 23.02 months (range, 10-108 months). The 3- and 6-year life table success rates were 66 and 34%, respectively. Early complications were: shallow or flat anterior chamber (15 patients, 53.6%), hypotony (16 patients, 57.1%), hypertony (7 patients, 25%), serous choroidal effusion (7 patients, 25%), fibrinous uveitis (5 patients, 17.9%), blockage of the intracameral portion of the tube by fibrin (5 patients, 17.9%), choroidal hemorrhage (2 patients, 7.1%). Late complications were: external conjunctival bleb failure (12 patients, 42.9%), blockage of the intracameral portion of the tube by fibrovascular tissue (5 patients, 17.9%), cataract (2 patients, 7.1%), bullous keratopathy (2 patients, 7.1%), external erosion of the Silastic valve (2 patients, 7.1%), phthisis bulbi (2 patients, 7.1%). Mortality during long-term follow-up was high in our series. The complications of an underlying diabetes mellitus were the most common cause of death (15 out of 22 patients). The high mortality of patients subjected to valve implantation makes it difficult to interpret the results of long-term studies. However, the valve implant is still today an alternative surgical procedure for controlling IOP in eyes with neovascular glaucoma that have visual potential."
        },
        {
            "title": "Association of anthropometric measures with SF-36v2 PCS and MCS in a multi-ethnic Asian population.",
            "abstract": "Purpose:\n        \n      \n      Obesity adversely affects health-related quality of life (HRQoL). Most studies have used body mass index (BMI) to measure obesity. Other measures of obesity, such as waist circumference (WC) or waist-to-hip ratio (WHR), may be better predictors of cardiovascular disease and mortality. We, therefore, examined the associations between other anthropometric measures and HRQoL in a multi-ethnic Asian population.\n    \n\n\n          Methods:\n        \n      \n      In this follow-up study from four previous cross-sectional surveys, HRQoL was measured, at follow-up, using the Short-Form 36 version 2 (SF-36v2) questionnaire. Linear regression was used to assess the relationship between anthropometric measures [BMI, WC, waist residuals (WR) (generated by regressing WC on BMI), WHR, waist-to-height ratio (WHtR) and height (Ht)] and HRQoL. We compared the models' R2, Akaike's information criteria (AIC), and Schwarz Bayesian information criteria (BIC) from the different models.\n    \n\n\n          Results:\n        \n      \n      Among 4,981 subjects, 47.6% were men aged 50.6 ± 12.2 and women aged 49.3 ± 11.6 years. All gender-specific anthropometric measures were significantly correlated with BMI, except WR. After adjusting for known determinants of HRQoL, we found significant associations between BMI, WC and WHtR with SF-36v2 Physical Component Summary (PCS) scores in women but not men. In contrast, after adjusting for known determinants of HRQoL, WR and WHR were significantly associated with SF-36v2 Mental component summary (MCS) scores in men, but not women. R², AIC and BIC were similar for all anthropometric measures in the final model.\n    \n\n\n          Conclusions:\n        \n      \n      The associations between measures of central obesity and HRQoL differed between men and women. In women, associations were seen with SF-36v2 PCS, but measures of central obesity did not have significant associations with HRQoL after controlling for BMI. In men, an association between WC and SF-36v2 MCS was statistically significant independent of BMI. These gender differences require further investigation."
        },
        {
            "title": "Accuracy of plasma neutrophil gelatinase-associated lipocalin in the early diagnosis of contrast-induced acute kidney injury in critical illness.",
            "abstract": "Purpose:\n        \n      \n      Neutrophil gelatinase-associated lipocalin (NGAL) is a promising biomarker for acute kidney injury (AKI). We evaluated the diagnostic and prognostic accuracies of plasma NGAL (pNGAL) for contrast-induced AKI (CI-AKI) in critically ill patients.\n    \n\n\n          Methods:\n        \n      \n      In a prospective observational study in two adult intensive care units in a university hospital, 100 consecutive critically ill patients with stable serum creatinine concentrations up to 48 h before contrast medium (CM) injection were enrolled. Serial blood sampling for pNGAL analysis was performed at enrolment, 2, 6, and 24 h after CM injection. The primary outcome was CI-AKI, defined by AKIN criteria, within the first 72 h following CM injection. Secondary outcomes were the need for renal replacement therapy (RRT) and mortality.\n    \n\n\n          Results:\n        \n      \n      Of the 98 patients analyzed, 30 developed CI-AKI. The pNGAL levels did not differ in patients with or without CI-AKI, and were higher in septic patients compared to nonseptic patients, and in patients with AKI preceding CM injection. The discriminative value of pNGAL to predict CI-AKI and mortality was poor; although, it did predict the need for RRT requirement after CM injection (area under receiver-operating characteristic curve, 0.85, 0.80, 0.83 and 0.86 at H0, H2, H6 and H24, respectively).\n    \n\n\n          Conclusion:\n        \n      \n      CI-AKI was common in critically ill patients. pNGAL levels were higher in patients with sepsis or previous AKI, but did not help to diagnose CI-AKI any earlier than serum creatinine after CM injection. However, pNGAL could be of interest to detect patients at risk of subsequent RRT requirement."
        },
        {
            "title": "Serum cortisol levels in patients admitted to the department of medicine: Prognostic correlations and effects of age, infection, and comorbidity.",
            "abstract": "Background:\n        \n      \n      In contrast to healthy adults or critically ill patients, data on serum cortisol levels in noncritically ill patients admitted to general internal medicine wards has not been well characterized. We aimed to describe the distribution and range of serum cortisol levels in patients admitted to the department of medicine, to discover whether old age, severe infections, or comorbidity induced a blunted hypothalamic-pituitary-adrenal (HPA) response and whether initial serum cortisol value had a prognostic significance.\n    \n\n\n          Methods:\n        \n      \n      Morning (8 am) serum cortisol level together with epidemiologic, clinical, and laboratory data were analyzed for 252 consecutive adult (age > or = 18 yrs) patients admitted to the department of internal medicine during a 6-weeks period.\n    \n\n\n          Results:\n        \n      \n      The mean serum cortisol level (541 +/- 268 nmol/L) was within the normal range. Only one patient had a low serum cortisol level of 72 nmol/L, whereas the majority of patients had either normal (80%) or increased (19%) serum cortisol levels. Older age, sepsis, prolonged duration of fever, higher comorbidity score, and higher serum creatinine level were each associated with significantly higher serum cortisol level. In addition, a higher serum cortisol level was significantly related to longer hospitalization and higher in-hospital mortality rate.\n    \n\n\n          Conclusions:\n        \n      \n      Serum cortisol level positively correlated with age, disease severity, and outcome. All admitted patients, except one, had normal to high serum cortisol. Whether this increased cortisol level is an adequate HPA response or less than required for the disease-induced stress should be investigated in further studies."
        },
        {
            "title": "Patient preferences for diabetic retinopathy health States.",
            "abstract": "PURPOSE. To develop standardized descriptions of health states that characterize vision-specific functional impacts of diabetic retinopathy (DR) according to levels of visual acuity and contrast sensitivity and to elicit preferences for these health states from persons with DR and assign weighted values to them. METHODS. Vision-specific descriptions of health states were developed based on a literature review and patient and physician interviews. The content was based on items from the National Eye Institute Visual Functioning Questionnaire (VFQ) and reflected functional impacts experienced by DR patients. Values were assigned to the range of health states, anchored by the extremes full vision and death, by using the time-tradeoff method in a sample of 98 Canadian DR patients from three clinical centers. RESULTS. The mean age of the sample was 60.4 years, and 56% were men. Mean preferences decreased from 0.98 (better-eye logMAR [Snellen equivalent] acuity, > or =20/40; worse-eye Snellen equivalent, > or =20/200) to 0.67 (Snellen equivalent visual acuity, < or =20/200, contrast sensitivity, < or =21 letters bilaterally). Preferences decreased with increasing severity of functional deficits and did not vary significantly by sex, age, VFQ quartile, or better- or worse-eye acuity. CONCLUSIONS. This is the first study that has been conducted to estimate preferences for standardized DR-specific health states, accounting for visual acuity and contrast sensitivity in both eyes. The results showed that the development and progression of DR are associated with substantial declines in preferences. In addition to the progressively greater impact from declining ETDRS visual acuity and contrast sensitivity, preference weights declined with increasing bilateral disparity. These preference values are useful for comparing the cost effectiveness of ophthalmic treatments."
        },
        {
            "title": "Common values in assessing health outcomes from disease and injury: disability weights measurement study for the Global Burden of Disease Study 2010.",
            "abstract": "Background:\n        \n      \n      Measurement of the global burden of disease with disability-adjusted life-years (DALYs) requires disability weights that quantify health losses for all non-fatal consequences of disease and injury. There has been extensive debate about a range of conceptual and methodological issues concerning the definition and measurement of these weights. Our primary objective was a comprehensive re-estimation of disability weights for the Global Burden of Disease Study 2010 through a large-scale empirical investigation in which judgments about health losses associated with many causes of disease and injury were elicited from the general public in diverse communities through a new, standardised approach.\n    \n\n\n          Methods:\n        \n      \n      We surveyed respondents in two ways: household surveys of adults aged 18 years or older (face-to-face interviews in Bangladesh, Indonesia, Peru, and Tanzania; telephone interviews in the USA) between Oct 28, 2009, and June 23, 2010; and an open-access web-based survey between July 26, 2010, and May 16, 2011. The surveys used paired comparison questions, in which respondents considered two hypothetical individuals with different, randomly selected health states and indicated which person they regarded as healthier. The web survey added questions about population health equivalence, which compared the overall health benefits of different life-saving or disease-prevention programmes. We analysed paired comparison responses with probit regression analysis on all 220 unique states in the study. We used results from the population health equivalence responses to anchor the results from the paired comparisons on the disability weight scale from 0 (implying no loss of health) to 1 (implying a health loss equivalent to death). Additionally, we compared new disability weights with those used in WHO's most recent update of the Global Burden of Disease Study for 2004.\n    \n\n\n          Findings:\n        \n      \n      13,902 individuals participated in household surveys and 16,328 in the web survey. Analysis of paired comparison responses indicated a high degree of consistency across surveys: correlations between individual survey results and results from analysis of the pooled dataset were 0·9 or higher in all surveys except in Bangladesh (r=0·75). Most of the 220 disability weights were located on the mild end of the severity scale, with 58 (26%) having weights below 0·05. Five (11%) states had weights below 0·01, such as mild anaemia, mild hearing or vision loss, and secondary infertility. The health states with the highest disability weights were acute schizophrenia (0·76) and severe multiple sclerosis (0·71). We identified a broad pattern of agreement between the old and new weights (r=0·70), particularly in the moderate-to-severe range. However, in the mild range below 0·2, many states had significantly lower weights in our study than previously.\n    \n\n\n          Interpretation:\n        \n      \n      This study represents the most extensive empirical effort as yet to measure disability weights. By contrast with the popular hypothesis that disability assessments vary widely across samples with different cultural environments, we have reported strong evidence of highly consistent results.\n    \n\n\n          Funding:\n        \n      \n      Bill & Melinda Gates Foundation."
        },
        {
            "title": "Comparison of the correlation of the Selvester QRS scoring system with cardiac contrast-enhanced magnetic resonance imaging-measured acute myocardial infarct size in patients with and without thrombolytic therapy.",
            "abstract": "Background:\n        \n      \n      After an acute myocardial infarction (MI), it is important to define the infarct size because it is related to mortality and morbidity. The Selvester QRS Score is an electrocardiographic (ECG) method that has been developed for estimating MI size. It has been shown to correlate well with postmortem anatomically measured sizes of single MI in patients who did not receive thrombolytic therapy. The aim of this study was to test the hypothesis that correlation between Selvester QRS Score-estimated MI size and contrast-enhanced magnetic resonance imaging (ceMRI)-measured MI size is equivalent in patients who did vs those who did not receive thrombolytic therapy.\n    \n\n\n          Methods:\n        \n      \n      Thirty-six patients with MI (24 with thrombolytic therapy and 12 without) received ceMRI and ECG at admission and at 1 or 6 months after admission. Indeed, in 23 of the patients, the therapy was intravenous only. The Selvester QRS Score was calculated using the 1-month ECG or, if not available, the 6-month ECG. The correlation between the 2 measures of MI size was determined for all patients and for the 2 groups separately.\n    \n\n\n          Results:\n        \n      \n      The mean MI size in the group that did not receive thrombolytic therapy was 8.5% +/- 6.4% estimated by the Selvester QRS Score and 11.7% +/- 10.2% measured by ceMRI. For the group that received thrombolytic therapy, Selvester QRS Score was 13.9% +/- 11.1% and ceMRI was 20.2% +/- 11.3%. The mean MI size in both groups combined was 12.1% +/- 10.0% estimated by the Selvester QRS Score and 17.3% +/- 11.5% measured by ceMRI. The Spearman rank correlation coefficient between Selvester QRS Score and ceMRI was 0.74 (P < .0001) for all patients, 0.74 (P < .0001) for the group that received thrombolytic therapy, and 0.64 (P = .024) for the group that did not receive thrombolytic therapy.\n    \n\n\n          Conclusions:\n        \n      \n      The associations between Selvester QRS Score and ceMRI-based MI were statistically significant and similar in both groups."
        },
        {
            "title": "Association of Mortality with Ocular Diseases and Visual Impairment in the Age-Related Eye Disease Study 2: Age-Related Eye Disease Study 2 Report Number 13.",
            "abstract": "Purpose:\n        \n      \n      To evaluate the association of mortality with visual acuity (VA) impairment, age-related macular degeneration (AMD), and cataract surgery.\n    \n\n\n          Design:\n        \n      \n      Cohort study.\n    \n\n\n          Participants:\n        \n      \n      Participants with at least intermediate AMD enrolled in a randomized controlled clinical trial of lutein/zeaxanthin and/or omega-3 fatty acids, the Age-Related Eye Disease Study 2 (AREDS2), for treatment of AMD and cataract.\n    \n\n\n          Methods:\n        \n      \n      Baseline and annual eye examinations included best-corrected visual acuity (BCVA) assessments, slit-lamp examinations, and stereoscopic fundus photographs that were centrally graded for development of late AMD (central geographic atrophy or neovascular AMD) or pseudophakia. Cause-specific mortality was determined on the basis of the International Classification of Diseases 9th or 10th Revision codes. Risk of all-cause and cause-specific mortality was assessed with Cox proportional hazards models adjusted for age, sex, AMD severity, VA, history of cataract surgery, and assigned AREDS2 study treatment. Analyses included baseline covariates: race, education, smoking status, diabetes, and cardiovascular disease.\n    \n\n\n          Results:\n        \n      \n      During follow-up (median 5 years), 368 (9%) of the 4203 AREDS2 participants died. Participants with neovascular AMD in 1 eye at baseline had a statistically significant increased risk for mortality compared with participants with no or few drusen (hazard ratio [HR], 1.56; 95% confidence interval [CI], 1.21-2.01; P < 0.001). Poorer survival was associated with bilateral cataract surgery before enrollment compared with baseline bilateral phakia (HR, 1.63; 95% CI, 1.29-2.07; P < 0.001) and with BCVA of less than 20/40 compared with participants with 20/40 or better (HR, 1.56; 95% CI, 1.06-2.30; P = 0.024), adjusted for age, sex, and statistically significant covariates. Participants who received antivascular endothelial growth factor therapies for neovascular AMD had decreased mortality compared with those who did not (HR, 0.71; 95% CI, 0.57-0.88; P = 0.002). The association between all-cause mortality and AREDS2 treatment whether assessing the main or individual treatment effect was not significantly different (omega-3 fatty acids main effect HR, 1.18; 95% CI, 0.96-1.45; P = 0.12; lutein/zeaxanthin main effect HR, 1.04; 95% CI, 0.85-1.28; P = 0.71).\n    \n\n\n          Conclusions:\n        \n      \n      In AREDS2, the presence of late AMD, bilateral cataract surgery, and VA less than 20/40 was associated with decreased survival. However, oral supplementation with omega-3 fatty acids, lutein plus zeaxanthin, zinc, or beta-carotene had no statistically significant impact on mortality."
        },
        {
            "title": "The influence of perceptions of HIV infection, care, and identity on care entry.",
            "abstract": "The benefits of accessing HIV care after diagnosis (e.g., improved clinical outcomes and reduced transmission) are well established. However, many persons who are aware that they are HIV infected have never received HIV medical care. During 2008-2010, we conducted 43 in-depth interviews in three health department jurisdictions among adults who had received an HIV diagnosis but who had never accessed HIV medical care. Respondents were selected from the HIV/AIDS Reporting System, a population-based surveillance system. We explored how respondents perceived HIV infection and HIV medical care. Most respondents associated HIV with death. Many respondents said that HIV medical care was not necessary until one is sick. Further, we explored how these perceptions may have conflicted with one's identity and thus served as barriers to timely care entry. Most respondents perceived themselves as healthy. All respondents acknowledged their HIV serostatus, but many did not self-identify as HIV-positive. Several respondents expressed that they were not ready to receive HIV care immediately but felt that they would eventually attempt to access care. Some stated that they needed time to accept their HIV diagnosis before entering care. To improve timely linkage to care, we suggest that during the posttest counseling session and subsequent linkage-to-care activities, counselors and service providers discuss patient perceptions of HIV, particularly to address beliefs that HIV infection is a \"death sentence\" or that HIV care is necessary only for those who exhibit symptoms."
        },
        {
            "title": "Impact of Visual Impairment and Eye diseases on Mortality: the Singapore Malay Eye Study (SiMES).",
            "abstract": "We investigated the relationship of visual impairment (VI) and age-related eye diseases with mortality in a prospective, population-based cohort study of 3,280 Malay adults aged 40-80 years between 2004-2006. Participants underwent a full ophthalmic examination and standardized lens and fundus photographic grading. Visual acuity was measured using logMAR chart. VI was defined as presenting (PVA) and best-corrected (BCVA) visual acuity worse than 0.30 logMAR in the better-seeing eye. Participants were linked with mortality records until 2012. During follow-up (median 7.24 years), 398 (12.2%) persons died. In Cox proportional-hazards models adjusting for relevant factors, participants with VI (PVA) had higher all-cause mortality (hazard ratio[HR], 1.57; 95% confidence interval[CI], 1.25-1.96) and cardiovascular (CVD) mortality (HR 1.75; 95% CI, 1.24-2.49) than participants without. Diabetic retinopathy (DR) was associated with increased all-cause (HR 1.70; 95% CI, 1.25-2.36) and CVD mortality (HR 1.57; 95% CI, 1.05-2.43). Retinal vein occlusion (RVO) was associated with increased CVD mortality (HR 3.14; 95% CI, 1.26-7.73). No significant associations were observed between cataract, glaucoma and age-related macular degeneration with mortality. We conclude that persons with VI were more likely to die than persons without. DR and RVO are markers of CVD mortality."
        },
        {
            "title": "Surveillance testing for metastasis from primary uveal melanoma and effect on patient survival.",
            "abstract": "Purpose:\n        \n      \n      To evaluate the quality of evidence about effectiveness of regular periodic surveillance testing for metastasis in patients with primary uveal melanoma (PUM) following treatment of the primary tumor in prolonging survival.\n    \n\n\n          Design:\n        \n      \n      Literature review and personal perspective of the authors.\n    \n\n\n          Methods:\n        \n      \n      Identification and analysis of peer-reviewed articles on human PUM published between 1980 and 2009 that reported on \"screening,\" \"surveillance,\" or \"systemic follow-up evaluation\" for metastasis in patients with PUM following treatment of primary tumor.\n    \n\n\n          Results:\n        \n      \n      Of 4222 identified articles, only 31 were considered satisfactory for inclusion in this study. Satisfactory articles reported levels of specific biomarkers when metastasis was first confirmed (14), percentage of patients with abnormal results on surveillance testing (13), values of diagnostic markers (eg, sensitivity, specificity) associated with evaluated components of a surveillance regimen (7), survival time after first detection of metastasis from primary uveal melanoma (7), total survival time after initial diagnosis or initial treatment of primary uveal melanoma (3), percentage of patients whose metastatic tumors were detected by presymptomatic testing (5), surveillance regimens employed by different groups (1), and relationship with generally accepted clinical and histopathologic prognostic factors for primary uveal melanoma metastasis (1). However, none of these articles reported survival times of comparable subgroups of patients in which regular periodic surveillance for metastasis vs no surveillance was performed.\n    \n\n\n          Conclusion:\n        \n      \n      Available evidence from the peer-reviewed literature does not provide any compelling evidence of survival benefit for any regimen or frequency of surveillance for metastasis relative to no such testing. In view of this, advisability of periodic surveillance for metastasis in routine clinical practice must be questioned."
        },
        {
            "title": "Coronary computed tomographic angiography as a gatekeeper to invasive diagnostic and surgical procedures: results from the multicenter CONFIRM (Coronary CT Angiography Evaluation for Clinical Outcomes: an International Multicenter) registry.",
            "abstract": "Objectives:\n        \n      \n      This study sought to examine patterns of follow-up invasive coronary angiography (ICA) and revascularization (REV) after coronary computed tomography angiography (CCTA).\n    \n\n\n          Background:\n        \n      \n      CCTA is a noninvasive test that permits direct visualization of the extent and severity of coronary artery disease (CAD). Post-CCTA patterns of follow-up ICA and REV are incompletely defined.\n    \n\n\n          Methods:\n        \n      \n      We examined 15,207 intermediate likelihood patients from 8 sites in 6 countries; these patients were without known CAD, underwent CCTA, and were followed up for 2.3 ± 1.2 years for all-cause mortality. Coronary artery stenosis was judged as obstructive when ≥50% stenosis was present. A multivariable logistic regression was used to estimate ICA use. A Cox proportional hazards model was used to estimate all-cause mortality.\n    \n\n\n          Results:\n        \n      \n      During follow-up, ICA rates for patients with no CAD to mild CAD according to CCTA were low (2.5% and 8.3%), with similarly low rates of REV (0.3% and 2.5%). Most ICA procedures (79%) occurred ≤3 months of CCTA. Obstructive CAD was associated with higher rates of ICA and REV for 1-vessel (44.3% and 28.0%), 2-vessel (53.3% and 43.6%), and 3-vessel (69.4% and 66.8%) CAD, respectively. For patients with <50% stenosis, early ICA rates were elevated; over the entirety of follow-up, predictors of ICA were mild left main, mild proximal CAD, respectively, or higher coronary calcium scores. In patients with <50% stenosis, the relative hazard for death was 2.2 (p = 0.011) for ICA versus no ICA. Conversely, for patients with CAD, the relative hazard for death was 0.61 for ICA versus no ICA (p = 0.047).\n    \n\n\n          Conclusions:\n        \n      \n      These findings support the concept that CCTA may be used effectively as a gatekeeper to ICA."
        },
        {
            "title": "Frailty syndrome: a transitional state in a dynamic process.",
            "abstract": "Frailty has long been considered synonymous with disability and comorbidity, to be highly prevalent in old age and to confer a high risk for falls, hospitalization and mortality. However, it is becoming recognized that frailty may be a distinct clinical syndrome with a biological basis. The frailty process appears to be a transitional state in the dynamic progression from robustness to functional decline. During this process, total physiological reserves decrease and become less likely to be sufficient for the maintenance and repair of the ageing body. Central to the clinical concept of frailty is that no single altered system alone defines it, but that multiple systems are involved. Clinical consensus regarding the phenotype which constitutes frailty, drawing upon the opinions of numerous authors, shows the characteristics to include wasting (loss of both muscle mass and strength and weight loss), loss of endurance, decreased balance and mobility, slowed performance, relative inactivity and, potentially, decreased cognitive function. Frailty is a distinct entity easily recognized by clinicians, with multiple manifestations and with no single symptom being sufficient or essential in its presentation. Manifestations include appearance (consistent or not with age), nutritional status (thin, weight loss), subjective health rating (health perception), performance (cognition, fatigue), sensory/physical impairments (vision, hearing, strength) and current care (medication, hospital). Although the early stages of the frailty process may be clinically silent, when depleted reserves reach an aggregate threshold leading to serious vulnerability, the syndrome may become detectable by looking at clinical, functional, behavioral and biological markers. Thus, a better understanding of these clinical changes and their underlying mechanisms, beginning in the pre-frail state, may confirm the impression held by many geriatricians that increasing frailty is distinguishable from ageing and in consequence is potentially reversible. We therefore provide an update of the physiopathology and clinical and biological characteristics of the frailty process and speculate on possible preventative approaches."
        },
        {
            "title": "The Association of Statin Use with Age-Related Macular Degeneration Progression: The Age-Related Eye Disease Study 2 Report Number 9.",
            "abstract": "Purpose:\n        \n      \n      To evaluate the association of statin use with progression of age-related macular degeneration (AMD).\n    \n\n\n          Design:\n        \n      \n      Preplanned, prospective cohort study within a controlled clinical trial of oral supplementation for age-related eye diseases.\n    \n\n\n          Participants:\n        \n      \n      Age-Related Eye Disease Study 2 (AREDS2) participants, aged 50 to 85 years.\n    \n\n\n          Methods:\n        \n      \n      Factors, including age, gender, smoking status, aspirin use, and history of diabetes, hypertension, heart disease, angina, and stroke-all known to be associated with statin use-were included in a logistic regression model to estimate propensity scores for each participant. Age-adjusted proportional hazards regression models, with and without propensity score matching, were performed to evaluate the association of statin use with progression to late AMD. Analyses adjusting for the competing risk of death were also performed.\n    \n\n\n          Main outcome measures:\n        \n      \n      Baseline and annual stereoscopic fundus photographs were assessed centrally by masked graders for the development of late AMD, either neovascular AMD or geographic atrophy (GA).\n    \n\n\n          Results:\n        \n      \n      Of the 3791 participants (2462 with bilateral large drusen and 1329 with unilateral late AMD at baseline), 1659 (43.8%) were statin users. The overall analysis, with no matching of propensity scores and no adjustment for death as a competing risk, showed that statin use was not associated with progression to late AMD (hazard ratio [HR], 1.08; 95% confidence interval [CI], 0.83-1.41; P = 0.56). When matched for propensity scores and adjusted for death as a competing risk, the result was not statistically significant (HR, 0.81; 95% CI, 0.55-1.20; P = 0.29). Furthermore, subgroup analyses of persons with or without late AMD at baseline and the various components of late AMD (neovascular AMD, central GA, or any GA) also showed no statistically significant association of statin use with progression to AMD.\n    \n\n\n          Conclusions:\n        \n      \n      Statin use was not statistically significantly associated with progression to late AMD in the AREDS2 participants, and these findings are consistent with findings in the majority of previous studies. Statins have been demonstrated to reduce the risk of cardiovascular disease, but our data do not provide evidence of a beneficial effect on slowing AMD progression."
        },
        {
            "title": "AView: An Image-based Clinical Computational Tool for Intracranial Aneurysm Flow Visualization and Clinical Management.",
            "abstract": "Intracranial aneurysms (IAs) occur in around 3% of the entire population. IA rupture is responsible for the most devastating type of hemorrhagic strokes, with high fatality and disability rates as well as healthcare costs. With increasing detection of unruptured aneurysms, clinicians are routinely faced with the dilemma whether to treat IA patients and how to best treat them. Hemodynamic and morphological characteristics are increasingly considered in aneurysm rupture risk assessment and treatment planning, but currently no computational tools allow routine integration of flow visualization and quantitation of these parameters in clinical workflow. In this paper, we introduce AView, a prototype of a clinician-oriented, integrated computation tool for aneurysm hemodynamics, morphology, and risk and data management to aid in treatment decisions and treatment planning in or near the procedure room. Specifically, we describe how we have designed the AView structure from the end-user's point of view, performed a pilot study and gathered clinical feedback. The positive results demonstrate AView's potential clinical value on enhancing aneurysm treatment decision and treatment planning."
        },
        {
            "title": "Surgery for patients with severe supratentorial intracerebral hemorrhage.",
            "abstract": "Introduction:\n        \n      \n      Little information is available on the efficacy of aggressive treatment such as surgery in improving the outcome of severely affected patients after supratentorial intracerebral hemorrhage (ICH). Our objective was to assess the effect of hematoma removal and ventricular drainage on the mortality of patients with severe primary supratentorial ICH.\n    \n\n\n          Methods:\n        \n      \n      We studied 103 consecutive patients who were admitted to the intensive care unit and diagnosed with primary supratentorial ICH. The impacts of clinical factors on 30-day mortality were assessed, including surgery, Glasgow Coma Scale (GCS) score and pupillary abnormality at admission, hematoma volume, and other related factors.\n    \n\n\n          Results:\n        \n      \n      The 30-day mortality rate was 42%, and the median time between admission and death was 3 days (range: 1 to 27 days). Hematoma removal and ventricular drainage, within the first 24 hours of admission, were performed on 11 and 17 patients, respectively. Two patients who were treated with removal and four with drainage died. A logistic regression model for predicting 30-day mortality was performed. After controlling for GCS score, pupillary abnormality, hydrocephalus, and hematoma volume, hematoma removal was identified as an independent predictor of survival (odds ratio [OR], 0.12; 95% confidence interval [CI], 0.02 to 0.92). Ventricular drainage also tended to decrease mortality rate greatly (OR, 0.31; 95% CI, 0.06 to 1.76). Patients with GCS scores of 3 or 4 were 4.01 times more likely to die (95% CI, 1.13 to 14.26) than those with GCS of at least 5.\n    \n\n\n          Conclusions:\n        \n      \n      Hematoma removal may reduce the mortality rate of patients with severe supratentorial ICH."
        },
        {
            "title": "Multidetector-CT angiography in pulmonary embolism-can image parameters predict clinical outcome?",
            "abstract": "Objective:\n        \n      \n      To assess if pulmonary CT angiography (CTA) can predict outcome in patients with pulmonary embolism (PE).\n    \n\n\n          Methods:\n        \n      \n      Retrospective analysis of CTA studies of patients with PE and documentation of pulmonary artery (PA)/aorta ratio, right ventricular (RV)/left ventricular (LV) ratio, superior vena cava (SVC) diameter, pulmonary obstruction index (POI), ventricular septal bowing (VSB), venous contrast reflux (VCR), pulmonary infarction and pleural effusion. Furthermore, duration of total hospital stay, necessity for/duration of ICU therapy, necessity for mechanical ventilation and mortality were recorded. Comparison was performed by logistic/linear regression analysis with significance at 5%.\n    \n\n\n          Results:\n        \n      \n      152 patients were investigated. Mean duration of hospital stay was 21 ± 24 days. 66 patients were admitted to the ICU; 20 received mechanical ventilation. Mean duration of ICU therapy was 3 ± 8 days. Mortality rate was 8%. Significant positive associations of POI, VCR and pulmonary infarction with necessity for ICU therapy were shown. VCR was significantly associated with necessity for mechanical ventilation and duration of ICU treatment. Pleural effusions were significantly associated with duration of total hospital stay whereas the RV/LV ratio correlated with mortality.\n    \n\n\n          Conclusion:\n        \n      \n      Selected CTA findings showed significant associations with the clinical course of PE and may thus be used as predictive parameters."
        },
        {
            "title": "Penetrating keratoplasty in patients with rheumatoid arthritis.",
            "abstract": "The authors reviewed in a retrospective manner 47 penetrating keratoplasties performed on 23 eyes of 21 patients with rheumatoid arthritis. The indications for the first penetrating keratoplasty were corneal melt in 19 eyes (83%), infectious keratitis in 2 eyes (9%), and corneal scarring after ulceration in 2 eyes (9%). Twelve of the 23 eyes required 24 repeat penetrating keratoplasties. Seventeen of the 23 eyes (74%) had clear grafts at the last follow-up visit, a median of 13.7 months after the last penetrating keratoplasty. The final best corrected visual acuity was greater than or equal to 20/60 in 4 eyes (17%), 20/70 to 20/100 in 1 eye (4%), 20/200 to 20/400 in 7 eyes (30%), counting fingers in 4 eyes (17%), hand motions in 2 eyes (9%), and light perception in 4 eyes (17%). One eye (4%) was enucleated. Anatomic success (absence of phthisis bulbi, enucleation, or conjunctival flap) was achieved in 20 eyes (87%). The survival probability for the 21 patients was only 48% 5 years after the first penetrating keratoplasty. The authors conclude that penetrating keratoplasty is often anatomically successful in patients with rheumatoid arthritis; however, the prognosis is poor for both vision and survival of the patient."
        },
        {
            "title": "Clinical outcomes of cataract surgery in very elderly adults.",
            "abstract": "Objectives:\n        \n      \n      To investigate the clinical outcomes of cataract surgery elderly adults.\n    \n\n\n          Design:\n        \n      \n      Retrospective cohort study.\n    \n\n\n          Setting:\n        \n      \n      Two clustered hospitals.\n    \n\n\n          Participants:\n        \n      \n      Two hundred seven individuals aged 90 and older who underwent cataract surgery for primary senile cataracts.\n    \n\n\n          Measurements:\n        \n      \n      Best-corrected preoperative and postoperative Snellen visual acuity, type of cataract, surgical techniques, preoperative systemic or ocular comorbidities, and intraoperative and postoperative complications were assessed. Improvement of visual acuity was defined as a decrease in logMAR acuity of 0.1. Factors associated with visual outcome within 6 months after surgery were identified using logistic regression modeling. The duration of postoperative survival was calculated.\n    \n\n\n          Results:\n        \n      \n      In the 207 participants (mean age 92.0 ± 2.1), 79.7% achieved visual improvement after cataract surgery. Forty-eight percent (mean age 97.4 ± 2.8) were alive on December 31, 2012. The most common systemic comorbidities were hypertension (66.2%), diabetes mellitus (25.1%), and myocardial infarction (19.8%). Age-related macular degeneration (AMRD) (15.9%), glaucoma (10.6%), and myopic degeneration (5.3%) were the three most common ocular comorbidities. Uncomplicated cataract surgery was performed in 87.0% cases. The most common complications were vitreous loss (8.2%), posterior capsular rupture (7.2%), and zonular rupture (4.8%). Participants with AMRD (P = .001, odds ratio (OR) = 4.77, 95% confidence interval (CI) = 1.86-12.26) and vitreous loss (P = .001, OR = 12.86, 95% CI = 2.71-61.10) were less likely to achieve postoperative visual improvement.\n    \n\n\n          Conclusion:\n        \n      \n      Despite a high prevalence of systemic and ocular comorbidities in very elderly adults, good clinical outcomes of cataract surgery were attainable. ARMD and vitreous loss were associated with a lower chance of postoperative visual improvement."
        },
        {
            "title": "High prevalence of diabetes among Indo-Guyanese adults, Schenectady, New York.",
            "abstract": "Introduction:\n        \n      \n      The Indo-Guyanese population is the largest immigrant minority population in Schenectady, New York. A clinic-based study in Schenectady and surveillance reports from Guyana found high diabetes prevalence and mortality among Guyanese of Indian descent. No community-based study has focused on diabetes among Indo-Guyanese immigrants in the United States. We sought information on the prevalence of diabetes and its complications in Indo-Guyanese adults in Schenectady and compared it with the prevalence among non-Hispanic white adults in Schenectady.\n    \n\n\n          Methods:\n        \n      \n      We administered a cross-sectional health survey at community venues in Schenectady in 2011. We identified diagnosed diabetes and its complications through self-reports by using a reliability-tested questionnaire. The final data set included 313 Indo-Guyanese and 327 non-Hispanic white adults aged 18 years or older. We compared the prevalence of diagnosed diabetes and diabetes complications between Indo-Guyanese and non-Hispanic whites.\n    \n\n\n          Results:\n        \n      \n      Most Indo-Guyanese participants were born in Guyana, whereas most non-Hispanic whites were born in the United States. The crude prevalence of diagnosed diabetes among Indo-Guyanese participants and non-Hispanic whites was 30.3% and 16.1%, respectively. The age-standardized prevalence was 28.7% among Indo-Guyanese participants, significantly higher than that among non-Hispanic whites (14.5%, P < .001). Indo-Guyanese participants who had diabetes had a lower body mass index and were more likely to report poor or fair general health and eye or vision complications than non-Hispanic whites who had diabetes.\n    \n\n\n          Conclusion:\n        \n      \n      Our study confirms the higher prevalence of diabetes in Indo-Guyanese adults in Schenectady. The higher prevalence of complications suggests poor control of diabetes. Excess burden of diabetes in this population calls for further research and public health action."
        },
        {
            "title": "Biomarkers and health-related quality of life in end-stage renal disease: a systematic review.",
            "abstract": "Background and objectives:\n        \n      \n      Health-related quality of life (HRQOL) predicts mortality in ESRD, yet adoption of HRQOL monitoring is not widespread, and regulatory authorities remain predominantly concerned with monitoring traditional biologic parameters. To assist with future efforts to adopt HRQOL monitoring while acknowledging the importance of biomarkers, this study sought to establish which domains of HRQOL are most affected by ESRD and to measure the strength of evidence linking common biomarkers to HRQOL in ESRD.\n    \n\n\n          Design, setting, participants, & measurements:\n        \n      \n      A systematic review was performed to identify studies that measured HRQOL in ESRD. Data were abstracted according to a conceptual model regarding the measurement of HRQOL differences, and HRQOL data were converted to weighted mean effect sizes and correlation coefficients.\n    \n\n\n          Results:\n        \n      \n      The impact of ESRD was largest in the Short Form 36 domains of physical functioning (e.g., role-physical, vitality) and smallest in mental functioning (e.g., mental health, role-emotional). Dialysis adequacy, as measured by Kt/V, was a poor correlate for Short Form 36 scores. Similarly, mineral metabolism (e.g., calcium x phosphorous, parathyroid hormone) and inflammatory (e.g., C-reactive protein, TNF) biomarkers had small effect sizes and correlations with HRQOL. In contrast, hematocrit demonstrated small to moderate relationships with mental and physical HRQOL, and nutritional biomarkers (e.g., albumin, creatinine, body mass index) demonstrated moderate to large relationships.\n    \n\n\n          Conclusions:\n        \n      \n      HRQOL in ESRD is most affected in the physical domains, and nutritional biomarkers are most closely associated with these domains. In contrast, Kt/V, mineral metabolism indices, and inflammatory markers are poor HRQOL correlates."
        },
        {
            "title": "The burden of diseases on disability-free life expectancy in later life.",
            "abstract": "Background:\n        \n      \n      The consequences of diseases in later life have been judged predominantly through mortality, resulting in an emphasis on the fatal rather than the nonfatal disabling conditions. We use a longitudinal study with follow-up at 2, 6, and 10 years to assess the impact of different diseases on both total life expectancy (TLE) and disability-free life expectancy (DFLE).\n    \n\n\n          Methods:\n        \n      \n      The Medical Research Council Cognitive Function and Ageing Study investigators interviewed 13,004 people aged 65 years and older from five U.K. centers starting in 1991. Persons aged 75 years and older were oversampled. Disability (mild, moderate, and severe) was assessed through basic Activities of Daily Living (ADL) and Instrumental ADL (IADL) scales at baseline and at follow-ups at 2, 6, and 10 years. TLE and DFLE were compared for persons with and without each of nine conditions.\n    \n\n\n          Results:\n        \n      \n      At age 65, men had a TLE of 15.3 years of which 12.1 (79%) were free of any disability, whereas women of the same age had an average TLE of 19.4 years, 11.0 years (57%) disability-free. Men (women) aged 65 years without stroke had 4.8 (4.6) more years of TLE and 6.5 (5.8) more years DFLE. Without diabetes, men (women) lived 4.4 (5.6) years longer and had 4.1 (5.1) years disability-free.\n    \n\n\n          Conclusions:\n        \n      \n      More disability-free years were gained than total life years in persons free of stroke, cognitive impairment, arthritis, and/or visual impairment at baseline. This finding suggests that elimination of these conditions would result in a compression of disability."
        },
        {
            "title": "Correlates of motorcycle helmet use among recent graduates of a motorcycle training course.",
            "abstract": "Helmets significantly decrease morbidity and mortality from motorcycle crashes, but many areas of the world lack universal helmet laws. To educate motorcyclists in areas without helmet laws, more knowledge of motorcyclists' helmet beliefs is needed. A web-based survey was therefore designed to assess motorcyclists' attitudes, norms and behaviors towards helmets in a U.S. state with a limited helmet law. Of 445 survey respondents, 68.4% of respondents reported always wearing a helmet. The not-always-helmeted riders were more likely than the always-helmeted to be male; to bave less education; and to have a history of previous motorcycle crashes and injuries. Although both groups had taken rider training classes, fewer of the not-always-helmeted had learned how to ride in a class. The strongest correlates of being not-always-helmeted (vs. always-helmeted) were attitudes that helmets were not protective and impaired sight/hearing; and the normative belief that they would only wear helmets if forced by law. Because attitudes are often more easily changed than normative beliefs, education may increase helmet use. However, less than half of riders in this state with a mandatory education program learned how to ride from a rider education course, and 44% of non-helmeted said they would only wear a helmet if forced by law. Legislation may therefore be a more efficient and effective strategy than education to increase helmet use."
        },
        {
            "title": "A preliminary model-based assessment of the cost-utility of a screening programme for early age-related macular degeneration.",
            "abstract": "Objectives:\n        \n      \n      To estimate the cost-effectiveness of screening for age-related macular degeneration (AMD) by developing a decision analytic model that incorporated and assessed all of the National Screening Committee criteria. A further objective was to identify the major areas of uncertainty in the model, and so inform future research priorities in this disease area.\n    \n\n\n          Data sources:\n        \n      \n      Major databases were searched in March 2004 and updated in January 2005.\n    \n\n\n          Review methods:\n        \n      \n      Systematic literature reviews covered the epidemiology and natural history of AMD, the screening and treatment effectiveness and health-related quality of life relating to AMD. A hybrid cohort-individual sampling model was implemented to describe the range of pathways between the incidence of age-related maculopathy (ARM) and death via clinical presentation and treatment at different stages of the disease. As significant shortfalls in the data available from the literature were apparent, so a range of primary data sources were also used to populate the model. To obtain estimates for the value of parameters deemed to be within an expert's remit, data describing some parameters were elicited from relevant experts. The data identified informed probability distributions describing the uncertainty around the model parameters. To incorporate joint parameter uncertainty (i.e. correlations between parameters), the AMD natural history model was calibrated probabilistically. Randomly sampled sets of input parameters were assigned weights representing the accuracy of their predictions of a set of observed model outputs. The analysis of the AMD screening model estimated the costs, numbers of quality-adjusted life-years (QALYs) and cases of blindness in a general population sample of 50-year-olds over the remainder of their lifetime, for 16 alternative screening options (including no screening). The reference case analysis incorporated current treatment options of laser photocoagulation and photodynamic therapy. Sensitivity analyses describing six alternative sets of intervention strategies, based on horizon scanning of potential future treatments for AMD, were also undertaken.\n    \n\n\n          Results:\n        \n      \n      There remains significant uncertainty about whether any form of screening for AMD is cost-effective. However, annual screening from age 60 years seems to provide the highest mean net benefits, but this is based on a cost-effectiveness estimate that has very poor precision (high levels of uncertainty). The probabilistic sensitivity analysis shows that the 95% credible interval for annual screening from age 60 years ranges from this option dominating the previous option to an incremental cost per QALY of over 0.5 million pounds sterling. Plotting a cost-effectiveness acceptability frontier shows that although annual screening from age 60 years has the highest net benefits at a value of QALY of 30,000 pounds sterling, the associated probability of this option being the most cost-effective option is only around 20%. The sensitivity analyses around potential future treatment options indicate that screening may become more cost-effective with the new treatments.\n    \n\n\n          Conclusions:\n        \n      \n      The conclusions focus on the interpretation of the results from the perspective of defining the major areas of uncertainty, which were defined as disease progression, rates of clinical presentation, screening test and optician effectiveness, treatment effectiveness, and costs of blindness. Future research may be best targeted at assessing how routine data may be used to describe clinical presentation rates of ARM. Other potential studies include a pilot study of the effectiveness of screening and opticians' referral patterns for AMD and a costing study of blindness as a continuum of association with deterioration in vision."
        },
        {
            "title": "[Screening for prevention of colorectal carcinoma. Who, when, how?].",
            "abstract": "Long-term survival of patients with colon carcinoma is largely determined by the timing of the diagnosis. For the identification of early colorectal carcinoma, it is of particular importance to detect and remove local precursor lesions (polyps) by means of effective screening, before they undergo malignant degeneration. The gold standard for such screening continues to be colonoscopy, followed by sigmoidoscopy, which latter, however, leaves large segments of the proximal uninspected. Additional--though less sensitive and more complicated--current screening techniques are the test for occult blood in the stools and the barium Doppler contrast examination, and, possibly in the near future, virtual colonoscopy and genetic testing for tumor DNA in the stools. Detailed screening recommendations are to be found in the guidelines issued by the German Society for Digestive and Metabolic Diseases. A prerequisite for effective prevention of colorectal carcinoma is the provision of information to, and motivation of, both the population and the individual patient, to participate in screening measures."
        },
        {
            "title": "EQ-5D™-derived utility values for different levels of migraine severity from a UK sample of migraineurs.",
            "abstract": "Background:\n        \n      \n      To estimate utility values for different levels of migraine pain severity from a United Kingdom (UK) sample of migraineurs.\n    \n\n\n          Methods:\n        \n      \n      One hundred and six migraineurs completed the EQ-5D to evaluate their health status for mild, moderate and severe levels of migraine pain severity for a recent migraine attack, and for current health defined as health status within seven days post-migraine attack. Statistical tests were used to evaluate differences in mean utility scores by migraine severity.\n    \n\n\n          Results:\n        \n      \n      Utility scores for each health state were significantly different from 1.0 (no problems on any EQ-5D dimension) (p < 0.0001) and one another (p < 0.0001). The lowest mean utility, - 0.20 (95% confidence interval [CI]: -0.27 - -0.13), was for severe migraine pain. The smallest difference in mean utility was between mild and moderate migraine pain (0.13) and the largest difference in mean utility was between current health (without migraine) and severe migraine pain (1.07).\n    \n\n\n          Conclusions:\n        \n      \n      Results indicate that all levels of migraine pain are associated with significantly reduced utility values. As severity worsened, utility decreased and severe migraine pain was considered a health state worse than death. Results can be used in cost-utility models examining the relative economic value of therapeutic strategies for migraine in the UK."
        },
        {
            "title": "Assessment of the effect of visual impairment on mortality through multiple health pathways: structural equation modeling.",
            "abstract": "Purpose:\n        \n      \n      To estimate the direct effects of self-reported visual impairment (VI) on health, disability, and mortality and to estimate the indirect effects of VI on mortality through health and disability mediators.\n    \n\n\n          Methods:\n        \n      \n      The National Health Interview Survey (NHIS) is a population-based annual survey designed to be representative of the U.S. civilian noninstitutionalized population. The National Death Index of 135,581 NHIS adult participants, 18 years of age and older, from 1986 to 1996 provided the mortality linkage through 2002. A generalized linear structural equation model (GSEM) with latent variable was used to estimate the results of a system of equations with various outcomes. Standard errors and test statistics were corrected for weighting, clustering, and stratification.\n    \n\n\n          Results:\n        \n      \n      VI affects mortality, when direct adjustment was made for the covariates. Severe VI increases the hazard rate by a factor of 1.28 (95% CI: 1.07-1.53) compared with no VI, and some VI increases the hazard by a factor of 1.13 (95% CI: 1.07-1.20). VI also affects mortality indirectly through self-rated health and disability. The total effects (direct effects plus mediated effects) on the hazard of mortality of severe VI and some VI relative to no VI are hazard ratio (HR) 1.54 (95% CI: 1.28-1.86) and HR 1.23 (95% CI: 1.16-1.31), respectively.\n    \n\n\n          Conclusions:\n        \n      \n      In addition to the direct link between VI and mortality, the effects of VI on general health and disability contribute to an increased risk of death. Ignoring the latter may lead to an underestimation of the substantive impact of VI on mortality."
        },
        {
            "title": "Contrast-induced nephropathy among Israeli hospitalized patients: incidence, risk factors, length of stay and mortality.",
            "abstract": "Background:\n        \n      \n      Radiological procedures utilizing intravascular contrast media are being widely applied for both diagnostic and therapeutic purposes. This has resulted in the increasing incidence of procedure-related contrast-induced nephropathy. In Israel, data on the incidence of CIN and its consequences are lacking.\n    \n\n\n          Objectives:\n        \n      \n      To describe the epidemiology of CIN among hospitalized patients in the Western Galilee Hospital, Nahariya (northern Israel), and to explore the impact of CIN on mortality and length of stay.\n    \n\n\n          Methods:\n        \n      \n      The study group was a historical cohort of 1111 patients hospitalized during the year 2006 who underwent contrast procedure and whose serum creatinine level was measured before and after the procedure. Data were electronically extracted from different computerized medical databases and merged into a uniform platform using visual basic application.\n    \n\n\n          Results:\n        \n      \n      The occurrence of CIN among hospitalized patients was 4.6%. Different CIN rates were noticed among various high risk subgroups such as patients with renal insufficiency and diabetes mellitus (14.1%-44%). Average in-hospital length of stay was almost twice as long among patients with CIN compared to subjects without this condition. Furthermore, the in-hospital death rate among CIN patients was 10 times higher. A direct association was observed between severity of CIN based on the RIFLE classification and risk of mortality.\n    \n\n\n          Conclusions:\n        \n      \n      Low CIN occurrence was demonstrated in general hospitalized patients (4.6%), and high rates (44%) in selected high risk subgroups of patients (with renal insufficiency or diabetes mellitus). Furthermore, prolonged length of stay and high in-hospital mortality were directly related to CIN severity."
        },
        {
            "title": "Morphometric analysis of optic nerves and retina from an end-stage retinitis pigmentosa patient with an implanted active epiretinal array.",
            "abstract": "Purpose:\n        \n      \n      To characterize optic nerve and retinal changes in a patient with end-stage retinitis pigmentosa (RP) with an implanted active epiretinal array.\n    \n\n\n          Methods:\n        \n      \n      A 74-year-old man with end-stage X-linked RP underwent implantation of an epiretinal array over the macula in the right eye and subsequent stimulation until his death at 5 years and 3 months after implantation. The optic nerves from this study patient, as well as those from two age-matched normal patients and two age-matched RP patients, were morphometrically analyzed against two different sets of criteria and compared. The retina underlying the array in the study patient was also morphometrically analyzed and compared with corresponding regions of the retina in the age-matched RP patients.\n    \n\n\n          Results:\n        \n      \n      Optic nerve total axon counts were significantly lower in the study patient and RP patients than in normal patients. However, there was no significant difference when comparing total axon counts from the optic nerve corresponding to the patient's implanted right eye versus the optic nerves from the RP patients (P = 0.59 and P = 0.61 using the two different criteria). Degenerated axon data quantified damage and did not show increased damage in the optic nerve quadrant that retinotopically corresponded to the site of epiretinal array implantation and stimulation. Except for the tack site, there was no significant difference when comparing the retina underlying the array and the corresponding perimacular regions of two RP patients.\n    \n\n\n          Conclusions:\n        \n      \n      Long-term implantation and electrical stimulation with an epiretinal array did not result in damage that could be appreciated in a morphometric analysis of the optic nerve and retina."
        },
        {
            "title": "Incidence of myocardial infarction, stroke, and death in patients with age-related macular degeneration treated with intravitreal anti-vascular endothelial growth factor therapy.",
            "abstract": "Purpose:\n        \n      \n      To describe the rates of myocardial infarction (MI), stroke, and mortality in patients who have treatment with intravitreal anti-vascular endothelial growth factor (anti-VEGF) injections for age-related macular degeneration (AMD).\n    \n\n\n          Design:\n        \n      \n      A retrospective population linkage study.\n    \n\n\n          Method:\n        \n      \n      We identified patients aged 40 years and above who received treatment with intravitreal anti-VEGF injections for AMD from January 1, 2008 to December 31, 2011 at the Singapore National Eye Centre. We used a national record linkage database to identify patients who developed MI, stroke, and all-cause mortality after the first injection, excluding those with previous MI or stroke at baseline from the respective analysis. We compared rates of MI, stroke, and mortality to that of the total Singapore population.\n    \n\n\n          Results:\n        \n      \n      A total of 1182 individuals had an intravitreal anti-VEGF injection included in this analysis, with the majority receiving bevacizumab (n = 1011). Overall, 19 patients developed MI, 16 developed stroke, and there were 43 mortalities, giving an age-adjusted incidence rate of 350.2 per 100 000 person-years for MI, 299.3 per 100 000 person-years for stroke, and 778.9 per 100 000 person-years for mortality. This is comparable to the weighted incidence rates of the Singapore population (427.1 per 100 000 person-years for MI, 340.4 per 100 000 person-years for stroke, and 921.3 per 100 000 person-years for mortality).\n    \n\n\n          Conclusion:\n        \n      \n      The incidence rate of MI, stroke, and death in this cohort of AMD patients treated with anti-VEGF was low, and was not significantly higher than the age-adjusted incidence rate of these events in the Singapore population."
        },
        {
            "title": "Detection of cardiovascular disease in elite athletes using cardiac magnetic resonance imaging.",
            "abstract": "Purpose:\n        \n      \n      Sudden cardiac death [SCD] in competitive athletes is caused by a diverse set of cardiovascular diseases such as hypertrophic and dilated cardiomyopathy [HCM/DCM], myocarditis, coronary anomalies or even coronary artery disease. In order to identify potential risk factors responsible for SCD, elite athletes underwent cardiac magnetic resonance [CMR] imaging.\n    \n\n\n          Materials and methods:\n        \n      \n      73 male [M] and 22 female [F] athletes (mean age 35.2 ± 11.4 years) underwent CMR imaging. ECG-gated breath-hold cine SSFP sequences were used for the evaluation of wall motion abnormalities and myocardial hypertrophy as well as for quantitative analysis (left and right ventricular [LV, RV] end-diastolic and end-systolic volume [EDV, ESV], stroke volume [SV], ejection fraction [EF] and myocardial mass [MM]). Furthermore, left and right atrial sizes were assessed by planimetry and delayed enhancement imaging was performed 10 minutes after the application of contrast agent. Coronary arteries were depicted using free-breathing Flash-3 D MR angiography.\n    \n\n\n          Results:\n        \n      \n      The quantitative analyses showed eccentric hypertrophy of the left ventricle (remodeling index [MM/LV-EDV]: M 0.75, F 0.665), enlargement of the RV volumes (RV-EDV: M 122.6 ± 19.0 ml/m², F 99.9 ± 7.2 ml/m²) and an increased SV (LV-SV: M 64.7 ± 10.0 ml/m², F 56.5 ± 5.7 ml/m²; RV-SV; M 66.7 ± 10.4 ml/m², F 54.2 ± 7.1 ml/m²). Abnormal findings were detected in 6 athletes (6.3 %) including one benign variant of coronary anomaly and abnormal late gadolinium enhancement in 2 cases. None of the athletes showed wall motion abnormalities or signs of myocardial ischemia.\n    \n\n\n          Conclusion:\n        \n      \n      CMR imaging of endurance athletes revealed abnormal findings in more than 5 % of the athletes. However, the prognostic significance remains unclear. Thus, cardiac MRI cannot be recommended as a routine examination in the care of athletes."
        },
        {
            "title": "Radiologic complete response (rCR) in contrast-enhanced magnetic resonance imaging (CE-MRI) after neoadjuvant chemotherapy for early breast cancer predicts recurrence-free survival but not pathologic complete response (pCR).",
            "abstract": "Background:\n        \n      \n      Patients with early breast cancer (EBC) achieving pathologic complete response (pCR) after neoadjuvant chemotherapy (NACT) have a favorable prognosis. Breast surgery might be avoided in patients in whom the presence of residual tumor can be ruled out with high confidence. Here, we investigated the diagnostic accuracy of contrast-enhanced MRI (CE-MRI) in predicting pCR and long-term outcome after NACT.\n    \n\n\n          Methods:\n        \n      \n      Patients with EBC, including patients with locally advanced disease, who had undergone CE-MRI after NACT, were retrospectively analyzed (n = 246). Three radiologists, blinded to clinicopathologic data, reevaluated all MRI scans regarding to the absence (radiologic complete remission; rCR) or presence (no-rCR) of residual contrast enhancement. Clinical and pathologic responses were compared categorically using Cohen's kappa statistic. The Kaplan-Meier method was used to estimate recurrence-free survival (RFS) and overall survival (OS).\n    \n\n\n          Results:\n        \n      \n      Overall rCR and pCR (no invasive tumor in the breast and axilla (ypT0/is N0)) rates were 45% (111/246) and 29% (71/246), respectively. Only 48% (53/111; 95% CI 38-57%) of rCR corresponded to a pCR (= positive predictive value - PPV). Conversely, in 87% (117/135; 95% CI 79-92%) of patients, residual tumor observed on MRI was pathologically confirmed (= negative predictive value - NPV). Sensitivity to detect a pCR was 75% (53/71; 95% CI 63-84%), while specificity to detect residual tumor and accuracy were 67% (117/175; 95% CI 59-74%) and 69% (170/246; 95% CI 63-75%), respectively. The PPV was significantly lower in hormone-receptor (HR)-positive compared to HR-negative tumors (17/52 = 33% vs. 36/59 = 61%; P = 0.004). The concordance between rCR and pCR was low (Cohen's kappa - 0.1), however in multivariate analysis both assessments were significantly associated with RFS (rCR P = 0.037; pCR P = 0.033) and OS (rCR P = 0.033; pCR P = 0.043).\n    \n\n\n          Conclusion:\n        \n      \n      Preoperative CE-MRI did not accurately predict pCR after NACT for EBC, especially not in HR-positive tumors. However, rCR was strongly associated with favorable RFS and OS."
        },
        {
            "title": "Contrast-induced nephropathy in patients undergoing primary angioplasty for acute myocardial infarction.",
            "abstract": "Objectives:\n        \n      \n      The aim of this research was to assess the incidence, clinical predictors, and outcome of contrast-induced nephropathy (CIN) after primary percutaneous coronary intervention (PCI) for acute myocardial infarction (AMI).\n    \n\n\n          Background:\n        \n      \n      Contrast-induced nephropathy is associated with significant morbidity and mortality after PCI. Patients undergoing primary PCI may be at higher risk of CIN because of hemodynamic instability and unfeasibility of adequate prophylaxis.\n    \n\n\n          Methods:\n        \n      \n      In 208 consecutive AMI patients undergoing primary PCI, we measured serum creatinine concentration (Cr) at baseline and each day for the following three days. Contrast-induced nephropathy was defined as a rise in Cr >0.5 mg/dl.\n    \n\n\n          Results:\n        \n      \n      Overall, CIN occurred in 40 (19%) patients. Of the 160 patients with baseline Cr clearance >/=60 ml/min, only 21 (13%) developed CIN, whereas it occurred in 19 (40%) of those with Cr clearance <60 ml/min (p < 0.0001). In multivariate analysis, age >75 years (odds ratio [OR] 5.28, 95% confidence interval [CI] 1.98 to 14.05; p = 0.0009), anterior infarction (OR 2.17, 95% CI 0.88 to 5.34; p = 0.09), time-to-reperfusion >6 h (OR 2.51, 95% CI 1.01 to 6.16; p = 0.04), contrast agent volume >300 ml (OR 2.80, 95% CI 1.17 to 6.68; p = 0.02) and use of intraaortic balloon (OR 15.51, 95% CI 4.65 to 51.64; p < 0.0001) were independent correlates of CIN. Patients developing CIN had longer hospital stay (13 +/- 7 days vs. 8 +/- 3 days; p < 0.001), more complicated clinical course, and significantly higher mortality rate (31% vs. 0.6%; p < 0.001).\n    \n\n\n          Conclusions:\n        \n      \n      Contrast-induced nephropathy frequently complicates primary PCI, even in patients with normal renal function. It is associated with higher in-hospital complication rate and mortality. Thus, preventive strategies are needed, particularly in high-risk patients."
        },
        {
            "title": "APACHE IV versus PPI for predicting community hospital ICU mortality.",
            "abstract": "Background:\n        \n      \n      Both the Acute Physiology and Chronic Health Evaluation (APACHE) IV and Palliative Performance Index (PPI) are scales used to estimate intensive care unit (ICU) prognosis and mortality.\n    \n\n\n          Objective:\n        \n      \n      To Compare the diagnostic utility of the PPI and APACHE IV and their subsequent implications in predicting ICU mortality at a community hospital.\n    \n\n\n          Design:\n        \n      \n      This was a Prospective Cohort Study.\n    \n\n\n          Setting:\n        \n      \n      The study was conducted at the Community hospital ICU.\n    \n\n\n          Patients:\n        \n      \n      Participants were 211 patients admitted from December 24, 2008 to June 11, 2009.\n    \n\n\n          Measurements:\n        \n      \n      An observer gathered appropriate data and performed the APACHE IV and PPI scales within 24 hours of admission. Results were then analyzed using standard formulae.\n    \n\n\n          Results:\n        \n      \n      The study included 211 participants in total with 211 participants in the PPI group (n = 211) and 162 in the APACHE IV group (n = 162). The APACHE score and PPI were found to be significant for predicting ICU mortality (P value of P < .002 and 99% CI of 13.74 to 20.32, P value of P < .001and 99% CI of 3.70 to 4.61, respectively). APACHE IV demonstrated a sensitivity of 84.6%, specificity of 96.0%, PPV of 64.7%, and NPV of 98.6%. In contrast, the PPI possessed a sensitivity of 69.2%, specificity of 96.0%, PPV of 64.7%, and NPV of 97.8%.\n    \n\n\n          Limitations:\n        \n      \n      Limitations may have occurred with the subjective nature of the PPI and Glasgow Coma Scale (GCS), along with meeting criterion for the APACHE IV.\n    \n\n\n          Conclusion:\n        \n      \n      This prospective cohort study in the ICU of a community hospital demonstrated that both the APACHE IV and PPI were significant tools for predicting ICU mortality. When contrasting the 2 scales, the APACHE IV could more accurately rule in mortality when mortality occurred and rule out mortality when survival occurred."
        },
        {
            "title": "Contrast-enhanced digital mammography.",
            "abstract": "Mammography is the only technology documented to reduce breast cancer mortality. Its sensitivity, however, is 75% to 80% at best and reduced to 30% to 50% in women with dense breasts. MR imaging is a sensitive modality for the detection of breast cancer but cannot be used in all patients. Its sensitivity is due in large part to its ability to detect enhancement of tumor vascularity so cancers can be detected before a mass is present. Contrast-enhanced dual-energy mammography uses the same capability of vascular enhancement and has been demonstrated to be more sensitive than routine mammography."
        },
        {
            "title": "Procalcitonin predicts patients at low risk of death from community-acquired pneumonia across all CRB-65 classes.",
            "abstract": "The aim of the present study was to investigate the prognostic value, in patients with community-acquired pneumonia (CAP), of procalcitonin (PCT) compared with the established inflammatory markers C-reactive protein (CRP) and leukocyte (WBC) count alone or in combination with the CRB-65 (confusion, respiratory rate >or=30 breaths x min(-1), low blood pressure (systolic value <90 mmHg or diastolic value <or=60 mmHg) and age >or=65 yrs) score. In total, 1,671 patients with proven CAP were enrolled in the study. PCT, CRP, WBC and CRB-65 score were all determined on admission and patients were followed-up for 28 days for survival. In contrast to CRP and WBC, PCT levels markedly increased with the severity of CAP, as measured by the CRB-65 score. In 70 patients who died during follow-up, PCT levels on admission were significantly higher compared with levels in survivors. In receiver operating characteristic analysis for survival, the area under the curve (95% confidence interval) for PCT and CRB-65 was comparable (0.80 (0.75-0.84) versus 0.79 (0.74-0.84)), but each significantly higher compared with CRP (0.62 (0.54-0.68)) and WBC (0.61 (0.54-0.68)). PCT identified low-risk patients across CRB classes 0-4. In conclusion, procalcitonin levels on admission predict the severity and outcome of community-acquired pneumonia with a similar prognostic accuracy as the CRB-65 score and a higher prognostic accuracy compared with C-reactive protein and leukocyte count. Procalcitonin levels can provide independent identification of patients at low risk of death within CRB-65 (confusion, respiratory rate >or=30 breaths x min(-1), low blood pressure (systolic value <90 mmHg or diastolic value <or=60 mmHg) and age >or=65 yrs) risk classes."
        },
        {
            "title": "Impact of contrast-induced acute kidney injury definition on clinical outcomes.",
            "abstract": "Background:\n        \n      \n      Contrast-induced acute kidney injury (CIAKI) is a frequent complication after infusion of contrast media in patients undergoing percutaneous coronary intervention. A wide range of CIAKI rates occurs after intervention between 3% and 30%, depending on the definition. The aim of this study was to identify which methodology was more effective at recognizing patients at high risk for in-hospital and out-of-hospital adverse events.\n    \n\n\n          Methods and results:\n        \n      \n      Serum creatinine increases, after contrast agent infusion, were evaluated in 755 consecutive and unselected patients. Incidences of CIAKI diagnosed by 2 common definitions varied from 6.9% (creatinine increase of ≥0.5 mg/dL, CIAKI-0.5) to 15.9% (creatinine increase of ≥25%, CIAKI-25%). Significant differences appeared between the 2 definitions of sensitivity to predict renal failure according to receiver operating characteristic curve analysis (98% for CIAKI-0.5 and 62% for CIAKI-25%), using a cutoff value of postprocedural glomerular filtration rate of 60 mL/min. Both definitions of CIAKI were related to composite adverse events, but CIAKI-0.5 showed a stronger predicting value (odds ratio 2.875 vs 1.802, P = .036). In multivariate linear regression, only CIAKI-0.5 was a predictive variable of death (odds ratio 3.174, 95% CI 1.368-7.361).\n    \n\n\n          Conclusions:\n        \n      \n      An increase in serum creatinine of ≥0.5 mg/dL is more sensitive because it recognizes more selectively those patients with a higher risk of mortality and morbidity. Serum creatinine increases of ≥25% overestimate CIAKI by including many patients without postprocedural relevant deterioration of renal function and affected by a lower risk of adverse events at follow-up."
        },
        {
            "title": "Muscle strength as a predictor of long-term survival in severe congestive heart failure.",
            "abstract": "Aims:\n        \n      \n      The objective of the study was to test the relationship between isolated muscle strength and outcome, and its significance in the context of other exercise variables.\n    \n\n\n          Methods and results:\n        \n      \n      122 consecutive patients (LVEF 21+/-7%) were enrolled in the study. Isokinetic strength testing of the knee extensor and flexor muscles were performed. A subset of 51 patients underwent additional upright bicycle testing with gas exchange analysis. The outcome up to 60 months was defined by event-free survival (group A, n=59) or death (group B, n=34). Patients who had been transplanted were excluded from further analysis. The peak strength of the quadriceps muscle was comparable in both groups (N.S.). In contrast, the index (value adjusted for weight) did reveal significant differences (P<0.04), similar to the peak torque of the knee flexor muscle (P<0.04), whose index was even more significant with regard to differences (P<0.01). Multivariate analysis including muscle strength variables, pVO2 and workload into one model show that the flexor strength index is the only independent variable (x2=9 P<0.003). A cut-off point of 68 Nm x 100/kg in the strength index of the flexor muscles was used to establish a significant difference between groups with regard to outcome (P<0.01). Thus, the isokinetic strength of the knee flexor muscles is related to outcome. Moreover, this parameter is superior to variables such as peakVO2 and workload."
        },
        {
            "title": "Health Service Use and Mortality of the Elderly Blind.",
            "abstract": "## PURPOSE\nTo determine whether blindness in older people is associated with increased health service use and mortality.\n## DESIGN\nRetrospective matched cohort study from July 1, 1999, through June 30, 2010.\n## PARTICIPANTS\nA blind cohort 65 years of age and older from a volunteer blind register and a cohort of age- and gender-matched controls selected randomly from the Western Australian electoral roll.\n## METHODS\nPerson-level linked hospital, emergency department (ED), mental health, and death records for the blind and control cohorts were used. Generalized estimating equations assuming a negative binomial distribution were used to estimate relative rates of hospital admissions, lengths of stay, and mortality after adjusting for sociodemographic variables and comorbidity. Emergency department and mental health service visits also were quantified.\n## MAIN OUTCOME MEASURES\nRelative rates of hospital admissions, lengths of stay, and mortality, as well as crude proportions of ED and mental health service visits.\n## RESULTS\nThe blind cohort comprised 1726 individuals alongside 1726 matched controls; 39% were men, and the mean age was 83 years. Combined, the cohorts accumulated a total of 34 130 hospital admissions amounting to 201 867 bed-days. After adjusting for the principal reason for hospital admission and comorbidity, the blind cohort was admitted to the hospital 11% (95% confidence interval [CI], 6%-17%) more often than the control cohort. The blind cohort also stayed in the hospital longer than the controls, but this effect varied by age. Blind participants 65 to 69 years of age spent 88% more days (95% CI, 27%-178%) in the hospital compared with age-matched controls, whereas there was no difference in length of stay between the cohorts by 80 years of age (rate ratio, 1.10; 95% CI, 0.97-1.25). A larger proportion of the blind cohort visited a hospital ED and accessed mental health services compared with the control cohort.\n## CONCLUSIONS\nHealth service use is increased for the elderly blind compared with age-matched controls after accounting for comorbidity. The elderly blind have more hospital admissions, ED visits, and mental health-related visits. The younger elderly blind stay longer in hospital. However, there was no evidence of worse mortality outcomes after adjusting for comorbidity.\n"
        },
        {
            "title": "A masked prospective evaluation of outcome parameters for cytomegalovirus-related retinal detachment surgery in patients with acquired immune deficiency syndrome.",
            "abstract": "Purpose:\n        \n      \n      The management of cytomegalovirus (CMV)-related rhegmatogenous retinal detachments in patients with acquired immune deficiency syndrome (AIDS) has been the subject of recent attention and controversy because of the high degree of variability in visual outcome, as well as significant differences in the reported incidence of profound postoperative optic atrophy. This study was designed to evaluate the various parameters affecting postoperative visual outcome, and to quantitate the degree of postoperative optic disc pallor.\n    \n\n\n          Methods:\n        \n      \n      The results of 65 consecutive surgeries for CMV-related retinal detachments in 51 patients with AIDS were prospectively studied. Postoperative vision, survival, optic disc pallor, and retinitis extent were analyzed. Serial photographs of optic discs underwent masked evaluation.\n    \n\n\n          Results:\n        \n      \n      Mean postoperative survival was 30 weeks (range, 2-146 weeks). Mean best postoperative visual acuity was 20/66 (range, 20/20-2/200) and mean final postoperative visual acuity was 20/100 (range, 20/25-no light perception). Analysis of visual outcome for eyes with no macular or papillo-macular retinitis showed a best postoperative visual acuity of 20/60 (range, 20/25-2/200) and mean final postoperative visual acuity of 20/80 (range, 20/25-no light perception). Postoperative vision was not affected by the presence of a preoperative macular detachment, with both groups (macula on or off detachments), achieving a best postoperative visual acuity of 20/60 in the absence of macular retinitis. Mild postoperative optic disc pallor was observed in 30% of surgical eyes at the final postoperative visit, and moderate pallor was noted in 13%. The mean degree of optic disc pallor was not different from the degree of optic disc pallor seen in fellow, nonsurgical eyes with CMV retinitis (surgical versus fellow nonsurgical eyes, 29% +/- 23% versus 26% +/- 30%; P = 0.64).\n    \n\n\n          Conclusion:\n        \n      \n      In this largest reported series of reattachment surgery for CMV-related retinal detachments, patients are experiencing increased postoperative survival, good vision, and relative optic nerve health."
        },
        {
            "title": "Donor Diabetes Mellitus Severity and Corneal Transplant Suitability in a US Eye Bank Donor Population.",
            "abstract": "Purpose:\n        \n      \n      To determine whether donor diabetes mellitus (DM) severity is associated with differences in endothelial cell density (ECD) and surgically unsuitable tissue.\n    \n\n\n          Methods:\n        \n      \n      Raw data were obtained from Saving Sight Eye Bank (Kansas City, MO) including 10,454 donated eyes from 5346 eligible donors from July 2014 through May 2017. Donors were grouped into 5 categories by their insulin use and the presence of microvascular end-organ complications. The categories were non-DM (NDM), noninsulin-dependent DM without complications (NIDDMnc), noninsulin-dependent DM with complication (NIDDMc), insulin-dependent DM without complications, and insulin-dependent DM with complication. Outcome variables included ECD and tissue transplant suitability. Mixed effects models were used to adjust for the random effect of repeated measures and fixed effects of donor age, race, lens status, and death to refrigeration and death to preservation times. Interaction effects of DM severity group and donor age and DM severity group and lens status were included in the models.\n    \n\n\n          Results:\n        \n      \n      One thousand six hundred eighty-four (32.1%) donors had a diagnosis of DM. Six hundred fifty-eight donors were in the NIDDMnc group, 225 in the NIDDMc group, 404 in the insulin-dependent DM without complication group, and 397 in the insulin-dependent DM with complication group. Compared with non-DM, donors with DM were older (P < 0.001) and more likely to be pseudophakic (P < 0.001). DM severity groups did not affect adjusted ECD at mean donor age. There was no statistically significant ECD interaction between DM severity group and lens status. There was a statistically significant ECD crossover interaction with NIDDMnc and donor age (P < 0.001). In phakic eyes, NIDDMc was associated with a statistically significantly lower odds of transplant suitability (odds ratio 0.62, P = 0.006).\n    \n\n\n          Conclusions:\n        \n      \n      DM severity does not affect lowering adjusted ECD at mean donor age. DM severity and pseudophakia were not associated with lower adjusted ECD. NIDDMnc was associated with an attenuation of the age-dependent decrease in ECD. NIDDMc was associated with decreased transplant suitability in phakic eyes. Future studies should include age, lens status, and interaction effects in their models of ECD and transplant suitability."
        },
        {
            "title": "Uveal metastasis from breast cancer in 264 patients.",
            "abstract": "Purpose:\n        \n      \n      Breast cancer is an increasingly important health problem in women and is the most common tumor to metastasize to the uvea. This study was designed to evaluate the clinical features, management, and prognosis of patients with uveal metastasis from breast cancer.\n    \n\n\n          Design:\n        \n      \n      Retrospective interventional case series.\n    \n\n\n          Methods:\n        \n      \n      We retrospectively reviewed 264 consecutive patients with uveal metastasis from breast cancer. We assessed the clinical features of the patient and tumor at the time of presentation, management, and prognosis. Kaplan-Meier survival estimates were used to analyze the probability of death as a function of time.\n    \n\n\n          Results:\n        \n      \n      Uveal metastasis was the initial manifestation of breast cancer in seven patients (3%) and the first systemic metastatic site of previously diagnosed breast cancer in 43 (16%). Associated with uveal metastasis, optic disk metastasis was found in 13 patients (5%), eyelid metastasis in one patient (1%), and conjunctival and orbital in one patient (<1%). Of 264 patients with uveal metastasis, 225 (85%) had choroidal metastasis, eight (3%) iris metastasis, two (<1%) ciliary body metastasis, and 29 (11%) had metastasis in multiple uveal sites. In the 264 patients with uveal metastasis, the most common symptom was blurred vision in 197 patients (88%), floaters in 15 (5%), photopsia in 12 (5%), and 19 (7%) were asymptomatic. The uveal metastases were bilateral in 99 patients (38%) and unilateral in 165 (62%). In 55 (56%) of the 99 bilateral cases, a uveal metastasis was found in the asymptomatic fellow eye during follow-up examination. External beam radiotherapy was used in 137 patients with uveal metastasis (52%), providing tumor control in 116 patients (85%) at a mean follow-up of 21 months. Using Kaplan-Meier estimates, survival rates of all patients with uveal metastasis from breast cancer was 65% at 1-year, 34% at 3-year, and 24% at 5-year follow-up.\n    \n\n\n          Conclusions:\n        \n      \n      Patients with uveal metastasis from breast cancer presented to ophthalmologists with visual symptoms in 93% of cases. However, asymptomatic metastases were commonly detected in the fellow eye. Local ocular tumor control was excellent with current therapies. However, systemic prognosis for all patients, including those who had been treated with different management options, was poor with survival rates of 65% at 1-year and 24% at 5-year follow-up."
        },
        {
            "title": "QRS-based assessment of myocardial damage and adverse events associated with cardiac sarcoidosis.",
            "abstract": "Background:\n        \n      \n      Cardiac sarcoidosis (CS) generates myocardial scar and arrhythmogenic substrate. CS diagnosis according to the Japanese Ministry of Health and Welfare guidelines relies, among others, on cardiac magnetic resonance imaging with late gadolinium enhancement (CMR-LGE). However, access to CMR-LGE is limited. The electrocardiography-based Selvester QRS score has been validated for identifying myocardial scar in ischemic/nonischemic cardiomyopathy, but its efficacy has not been tested to evaluate CS.\n    \n\n\n          Objective:\n        \n      \n      The purpose of this study was to examine whether the QRS score can be applied to CS.\n    \n\n\n          Methods:\n        \n      \n      CS-associated myocardial scar was assessed by both CMR-LGE and QRS scoring in patients with extra-CS (n = 59).\n    \n\n\n          Results:\n        \n      \n      Of 59 patients, 35 (59%) were diagnosed with CS according to the Japanese Ministry of Health and Welfare guidelines. QRS-estimated scar mass positively correlated with that quantified by CMR-LGE (signal intensity ≥2SD above the reference; r = 0.68; P < .001). Receiver operating characteristic curves demonstrated optimal cutoffs of 9% CMR-LGE scar and 3-point QRS score to identify patients with CS. The areas under the curves of CMR-LGE and the QRS score were not significantly different (0.83 and 0.78, respectively; P = .27); both methods demonstrated similar diagnostic performance. A QRS score of ≥3 led to a higher incidence of CS-associated adverse events (death/fatal arrhythmia/heart failure hospitalization) than did a QRS score of <3 (35 ± 21 months of follow-up; P = .01). QRS score was an independent predictor of risk in multivariate analysis (P = .03).\n    \n\n\n          Conclusion:\n        \n      \n      The Selvester QRS scoring estimates CS-associated myocardial damage and identifies patients with CS equally well as CMR-LGE. A higher QRS score is also associated with an increased risk of life-threatening events in CS, indicating its potential use as a risk predictor."
        },
        {
            "title": "Long-term results of iodine 125 irradiation of uveal melanoma.",
            "abstract": "The authors report on 64 of the first 65 patients treated with iodine 125. The mean follow-up was 64.9 months. After treatment, 29 patients (45.3%) retained visual acuity of 20/100 or better, and 18 patients (28.1%) retained visual acuity within two lines of visual acuity before irradiation. Eleven patients (17.2%) died of metastasis, and 5 patients (7.8%) had local recurrence. Cataract developed in 29 (45.3%) patients; keratitis developed in only 2 (3.1%) patients, and dry eye developed in none. Neovascular glaucoma developed in 7 (10.9%) patients, and 15 (23.4%) patients had radiation retinopathy. Eleven patients (17.2%) required enucleation for either tumor growth or neovascular glaucoma. These results show the increasing number of radiation complications seen with long-term observation and the frequently seen adverse visual outcome."
        },
        {
            "title": "Cataract surgery complications in nonagenarians.",
            "abstract": "Purpose:\n        \n      \n      To investigate whether nonagenarians relative to octogenarians are at increased risk of ocular complications from cataract surgery in the US Veterans Health Administration (VHA).\n    \n\n\n          Design:\n        \n      \n      A retrospective cohort study.\n    \n\n\n          Participants:\n        \n      \n      A total of 554 nonagenarians and 11 407 octogenarians who received cataract surgery in the VHA.\n    \n\n\n          Methods:\n        \n      \n      Nonagenarians and octogenarians who received 1 cataract surgery without a second surgery within 90 days between October 1, 2005, and September 30, 2007, were identified using the National Patient Care Database (NPCD). Data collected include demographics, preoperative systemic and ocular comorbidities, intraoperative complications, and 90-day postoperative complications. The adjusted odds ratio (OR) of complications in nonagenarians using octogenarians as a reference group was calculated using logistic regression modeling.\n    \n\n\n          Main outcome measures:\n        \n      \n      Intraoperative and postoperative ocular complications within 90 days of cataract surgery in nonagenarians versus octogenarians.\n    \n\n\n          Results:\n        \n      \n      The most common systemic comorbidity for both age groups was diabetes mellitus (DM), and the most common ocular comorbidity for both age groups was age-related macular degeneration (AMD). Octogenarians had a higher prevalence of most systemic comorbidities, and nonagenarians had a higher prevalence of most ocular comorbidities. The most common intraoperative and postoperative complications for both age groups were vitreous loss or posterior capsular tear and posterior capsular opacification. The risk of having any intraoperative or postoperative complication was 13.5% for octogenarians and 13.4% for nonagenarians (P = 0.9001). The OR of having any intraoperative or postoperative complication in nonagenarians with octogenarians as a reference group was 0.94 (95% confidence interval, 0.73-1.22).\n    \n\n\n          Conclusions:\n        \n      \n      Nonagenarians relative to octogenarians are not at increased risk of ocular complications from cataract surgery in the VHA. Further studies are needed to evaluate other outcome parameters, such as visual function and quality of life, in nonagenarians undergoing cataract surgery."
        },
        {
            "title": "Treatment of retinal detachments in patients with the acquired immune deficiency syndrome.",
            "abstract": "Thirty-nine eyes from 31 patients with retinal detachment due to cytomegalovirus (CMV) retinitis were treated by either laser photocoagulation (22 eyes), scleral buckle (9 eyes), pars plana vitrectomy (5 eyes), or no therapy (3 eyes). The success rates for photocoagulation (77.2%), scleral buckle (77.7%), and vitrectomy (with gas or oil, 80%) were similar. The median survival time was 95 days (range, of 7 to 280 days). The extent of detachment, the presence of active disease in either the periphery or the posterior pole, and overall health served to determine what type of therapy was best suited for each patient. Although silicone oil appears to be best for patients with a total retinal detachment and active disease, this small series suggests that conservative modes of therapy such as laser photocoagulation and scleral buckles can be used successfully to treat these patients if there is an absence of active retinitis."
        },
        {
            "title": "Retinopathy and CKD as predictors of all-cause and cardiovascular mortality: National Health and Nutrition Examination Survey (NHANES) 1988-1994.",
            "abstract": "Background:\n        \n      \n      Retinopathy is associated with increased mortality risk in general populations. We evaluated the joint effect of retinopathy and chronic kidney disease (CKD) on mortality in a representative sample of U.S. adults.\n    \n\n\n          Study design:\n        \n      \n      Prospective cohort study.\n    \n\n\n          Setting & participants:\n        \n      \n      7,640 adults from NHANES (National Health and Nutrition Examination Survey) 1988-1994 with mortality linkage through December 31, 2006.\n    \n\n\n          Predictors:\n        \n      \n      CKD, defined as low estimated glomerular filtration rate (<60 mL/min/1.73 m2) or albuminuria (urine protein-creatinine ratio ≥30 mg/g), and retinopathy, defined as the presence of microaneurysms, hemorrhages, exudates, microvascular abnormalities, or other evidence of diabetic retinopathy by fundus photograph.\n    \n\n\n          Outcomes:\n        \n      \n      All-cause and cardiovascular mortality.\n    \n\n\n          Measurements:\n        \n      \n      Multivariable-adjusted Cox proportional hazards.\n    \n\n\n          Results:\n        \n      \n      Overall, 4.6% of participants had retinopathy and 15% had CKD. Mean age was 56 years, 53% were women, and 81% were non-Hispanic whites. The prevalence of retinopathy in patients with CKD was 11%. We identified 2,634 deaths during 14.5 years' follow-up. In multivariable analyses, compared with individuals with neither CKD nor retinopathy, HRs for all-cause mortality were 1.02 (95% CI, 0.75-1.38), 1.52 (95% CI, 1.35-1.72), and 2.39 (95% CI, 1.77-3.22) for individuals with retinopathy only, those with CKD only, and those with both CKD and retinopathy, respectively. Corresponding HRs for cardiovascular mortality were 0.96 (95% CI, 0.50-1.84), 1.72 (95% CI, 1.47-2.00), and 2.96 (95% CI, 2.11-4.15), respectively. There was a significant synergistic interaction between retinopathy and CKD on all-cause mortality (P=0.04).\n    \n\n\n          Limitations:\n        \n      \n      The presence of retinopathy was evaluated only once. Small sample size of some of the subpopulations studied.\n    \n\n\n          Conclusions:\n        \n      \n      In the presence of CKD, retinopathy is a strong predictor of mortality in this adult population."
        },
        {
            "title": "Response and progression-free survival in oropharynx squamous cell carcinoma assessed by pretreatment perfusion CT: comparison with tumor volume measurements.",
            "abstract": "Background and purpose:\n        \n      \n      Perfusion CT (PCT) provides a rapid, reliable, and non-invasive technique for assessing tumor vascularity. The purpose of this study was to assess whether pretreatment dynamic perfusion CT (PCT) may predict response to induction chemotherapy and midterm progression-free survival (PFS) in advanced oropharynx squamous cell carcinoma (SCCA) and to compare the results with those derived by tumor volume measurements.\n    \n\n\n          Materials and methods:\n        \n      \n      Nineteen patients underwent routine contrast-enhanced CT (CECT), pretreatment PCT, and conventional endoscopy. Tumor response was determined according to radiologic (RECIST) criteria. The PCT parameters, tumor volume, radiologic response, and PFS were analyzed with use of Cox-proportional hazards model, receiver operating characteristic (ROC), and Kaplan-Meier analysis.\n    \n\n\n          Results:\n        \n      \n      The baseline blood flow (BF), blood volume (BV), and permeability surface area product (PS) were significantly higher, whereas mean transit time (MTT) was significantly lower in the responders than in the nonresponders (P < or = .002). BV showed 100% sensitivity, MTT and PS had the highest specificity (100%), and BF showed 84.2% sensitivity and 66.7% specificity for prediction of tumor response after induction chemotherapy. The pretreatment tumor volume correlated with PFS in the pooled patients group (r = 0.4; P < .0001), whereas postinduction tumor volume correlated significantly with PFS in the responders and nonresponders (r = 0.22-0.64; P < or = .006). Pretreatment tumor volume (P = .0001) and BF (P = .001) were significant predictors for PFS.\n    \n\n\n          Conclusions:\n        \n      \n      Pretreatment PCT parameters may predict response after induction chemotherapy. Tumor volume and BF values may predict PFS in patients with advanced oropharyngeal SCCA."
        },
        {
            "title": "Visual impairment and mortality in a rural adult population (the Southern Harbin eye study).",
            "abstract": "Purpose:\n        \n      \n      To evaluate the association between visual acuity (VA) and 4-year mortality in an older population-based cohort.\n    \n\n\n          Methods:\n        \n      \n      Five thousand and fifty-seven persons aged 50 to 96 years (91.0% of the eligible population) residents of the Southern Harbin, Heilongjiang Provence, China participated in the study. At baseline (2006), the main ocular diseases were diagnosed from a basic ocular examination including presenting and best-corrected VA. Of the 5,057 participants in the baseline survey, those who died after the study were identified and the death certificate was checked. The physicians in charge of the health of the village population were asked for the presumed cause of death. The rate of death was determined in the follow-up survey in 2010. We evaluated the association of visual impairment (VI) and mortality using multiple logistic regression.\n    \n\n\n          Results:\n        \n      \n      Between the baseline examination and the censoring cutpoint study, a total of 214 subjects (4.2%) were dead. Females with VI were less likely to have died relative to male gender with VI (P<0.05). Compared with participants who reported better presenting VA (VA ≥ 20/60), the risk of mortality was significantly higher for those reporting moderate VI (20/400 ≤ VA < 20/60) (Odds Ratio [OR], 2.1; 95% confidence interval [CI],1-4.1) and those reporting severe VI (VA < 20/400) (OR,3.6; 95% CI, 2.0-6.6). Similar associations were obtained for best-corrected VA in the better eye (OR, 3.1; 95% CI: 1.5-6.4 and 3.9; 95%CI: 2.1-7.2, respectively).\n    \n\n\n          Conclusion:\n        \n      \n      In this Chinese population-based cohort we found that visual impairment predicted a significantly elevated mortality."
        },
        {
            "title": "[Screening in cardiovascular diseases].",
            "abstract": "Cardiovascular disease still ranks number one in the mortality statistics in the industrialized world. In Germany the five most common causes of death are all associated with arteriosclerotic changes of the arterial vasculature. As the treatment often extends over long periods and it can be impossible for patients to work, peripheral arterial occlusive disease (PAOD) constitutes a not inconsiderable economic factor. Thus, screening for arteriosclerotic disease seems to be reasonable, because the potential for influencing arteriosclerotic changes is known to be higher in an early stage of the disease even before symptoms become apparent. Not every case can be cured, but progression can frequently be slowed down. The need for invasive procedures, some of them associated with ionizing radiation, limited the use of imaging of the arterial vasculature for a long time. Noninvasive clinical examinations such as the \"ankle brachial index\" (ABI) can indicate the presence of PAOD, though exact localization of the pathologic changes is not possible except with imaging methods. In contrast to these, MRI is a noninvasive imaging modality that does not involve ionizing radiation but offers high spatial resolution arterial imaging."
        },
        {
            "title": "The impact of visual impairment on health, function and mortality.",
            "abstract": "Background and aims:\n        \n      \n      Our aim was to determine the impact of visual impairment on self-rated health, function and mortality amongst a community-dwelling elderly cohort.\n    \n\n\n          Methods:\n        \n      \n      The study design was prospective and longitudinal, subjects being taken from an age-homogeneous, community-dwelling cohort comprising 452 subjects aged 70 in 1990 and 839 subjects aged 77 in 1998. Comprehensive data were collected by structured interviews and medical examinations carried out during home visits. Data included each subject's demographic and socio-economic profile, medical history, physical findings, functional status and self-rated health status. Visual acuity was measured using a Snellen chart and visual impairment was defined as best-eye corrected visual acuity of 20/40 or worse on Snellen chart testing.\n    \n\n\n          Results:\n        \n      \n      Measured and self-reported visual impairment correlated closely, and were significantly more prevalent amongst subjects with low education and poor financial status. Visually impaired subjects showed significantly greater dependence in ADL and IADL, poor self-rated health, less ability to rely on friends, increased loneliness and, in men aged 77, increased visits to the emergency room and hospital admissions. Visual impairment at age 70 significantly predicted poor self-rated health (p=0.029, OR 2.36, 95% CI 1.09-5.10), dependence in ADL (p=0.007, OR 2.91, 95% CI 1.34-6.33), general tiredness (p=0.037, OR 2.40, 95% CI 1.06-5.44), and mortality, with a two-and-a-half-fold increase in risk of death at seven years (p=0.0017,OR 2.84, 95% CI 1.48-5.46).\n    \n\n\n          Conclusions:\n        \n      \n      Visual impairment in the elderly increases the risk of social, functional and medical decline."
        },
        {
            "title": "Predictors of pneumonia severity in HIV-infected adults admitted to an Urban public hospital.",
            "abstract": "Data on outcomes of community-acquired pneumonia (CAP) in the HIV-infected population are mixed and the perception of worse outcomes in HIV may lead to excess hospitalization. We retrospectively evaluated the utility of the Pneumonia Severity Index, or PORT score, as a prediction rule for mortality in 102 HIV-infected adults hospitalized at an urban public hospital with CAP. Primary outcome was survival at 30 days. Secondary outcomes included survival on discharge, intensive care unit (ICU) admission, length of stay, and readmission within 30 days. The cohort was predominantly male (70%) with a mean age of 45.4 years (standard deviation [SD] ± 7.4). Mean CD4 cell count was 318 cells per microliter; 40 (39%) had CD4 less than 200 cells per microliter. Forty-three percent were on antiretroviral therapy at the time of admission and 31% on prophylactic antibiotics. Twelve patients had bacteremia on admission, predominantly with Streptococcus pneumoniae. Of the 46 patients with admission sputum cultures, 20 yielded an organism, most commonly Haemophilus influenzae and S. pneumoniae. Overall survival in the cohort was high, 96%. Most patients (81%) had a low PORT risk score (class I-III). PORT score predicted 30-day survival (p=0.01) and ICU admission (p=0.03), but antiretroviral use did not. In contrast to a prior study, we did not find that CD4 cell count predicted CAP outcome. Lack of stable housing was not associated with worse outcomes. The PORT score may be a valid tool to predict mortality and need for hospital admission in HIV-infected patients with CAP."
        },
        {
            "title": "Seven chronic conditions: their impact on US adults' activity levels and use of medical services.",
            "abstract": "Objectives:\n        \n      \n      This paper analyzes the impact of seven chronic conditions (three nonfatal: arthritis, visual impairment, hearing impairment; four fatal: ischemic heart disease, chronic obstructive pulmonary disease, diabetes mellitus, malignant neoplasms) on US adults aged 18 and older. Impact refers to how readily a condition prompts activity limitations, physician visits, and hospital stays.\n    \n\n\n          Methods:\n        \n      \n      Data come from three national health surveys and vital statistics. For comparability, a single disease classification scheme was applied, and new rates were estimated. Frequency, impact, and prominence of the target conditions are studied via rates, ratios of rates, and ranks, respectively.\n    \n\n\n          Results:\n        \n      \n      In young adulthood, the nonfatal conditions prompt limitations less readily than do the fatal ones, but by older ages, arthritis and visual impairment have a limiting impact equivalent to that of fatal conditions. Despite high prevalence and limitations, nonfatal conditions stand well below fatal conditions for health services use.\n    \n\n\n          Conclusions:\n        \n      \n      Although statistics on frequency, impact, and prominence all indicate conditions \"importance,\" they give only weak clues about specific service needs of affected persons. The persistent finding that nonfatal conditions do not receive health services care commensurate with their prevalence and impact reflects long-standing imbalanced attention on fatal conditions in research and medical care."
        },
        {
            "title": "Mantle cell lymphoma: prognostic capacity of the Follicular Lymphoma International Prognostic Index.",
            "abstract": "The International Prognostic Index (IPI) is the most commonly used prognostic model for mantle cell lymphoma (MCL). However, the prognostic value of the IPI is limited. The recently published Follicular Lymphoma International Prognostic Index (FLIPI) is built on variables, which are pertinent to MCL. This study was conducted to evaluate the prognostic value of FLIPI in a population-based series of 93 patients with MCL diagnosed in a 7-year period. End points of the study were response to therapy, overall survival, and disease-free survival (DFS) according to the IPI and FLIPI. Applied to the whole series, the FLIPI identified three risk groups with markedly different outcome with 5-year overall survival rates of 65%, 42%, and 8% respectively. Notably, the high-risk group comprised 53% of patients. In contrast, the IPI only allocated 16% of cases to the high-risk group and had a lower overall predictive capacity. When both the FLIPI and IPI were included in a multivariate analysis, only the FLIPI was related to survival. Multivariate analysis of DFS also identified the FLIPI, and not the IPI, as independently significant. Thus, in the present study, the FLIPI was superior as a prognostic model compared with the IPI and can therefore be recommended as a clinical prognostic index for MCL."
        },
        {
            "title": "A prospective pilot study evaluating the 'cardiac decompensation score' in the setting of intraaortic balloon counterpulsation.",
            "abstract": "The study objective was to determine whether the 'cardiac decompensation score' could identify cardiac decompensation in a patient with existing cardiac compromise managed with intraaortic balloon counterpulsation (IABP). A one-group, posttest-only design was utilised to collect observations in 2003 from IABP recipients treated in the intensive care unit of a 450 bed Australian, government funded, public, cardiothoracic, tertiary referral hospital. Twenty-three consecutive IABP recipients were enrolled, four of whom died in ICU (17.4%). All non-survivors exhibited primarily rising scores over the observation period (p<0.001) and had final scores of 25 or higher. In contrast, the maximum score obtained by a survivor at any time was 15. Regardless of survival, scores for the 23 participants were generally decreasing immediately following therapy escalation (p=0.016). Further reflecting these changes in patient support, there was also a trend for scores to move from rising to falling at such treatment escalations (p=0.024). This pilot study indicates the 'cardiac decompensation score' to accurately represent changes in heart function specific to an individual patient. Use of the score in conjunction with IABP may lead to earlier identification of changes occurring in a patient's cardiac function and thus facilitate improved IABP outcomes."
        },
        {
            "title": "Disability trends in the United States population 1966-76: analysis of reported causes.",
            "abstract": "According to data published by the United States National Center for Health Statistics, disability reported among the US population has increased substantially during the years 1966 to 1976. Among younger age groups, the increase in activity limitation involves visual and hearing impairments as well as asthma. In the middle age group (45-64), four causes increased in both sexes (diabetes, musculoskeletal disorders, hypertension, and diseases of the circulatory system other than hypertension and heart conditions); one cause affected men only (heart conditions) and one women only (malignant neoplasms). In the 65 and over age group, diabetes and circulatory diseases (excluding heart conditions and hypertension) increased significantly. Although the US population increased by 10 per cent, the number of persons permanently limited in their activities because of health conditions increased by 37 per cent with a much larger proportion of those disabled claiming to be unable to carry on their main activity. Changes in health survey procedures and changes in standards used by respondents to rate their health status are not believed to account for these findings. Factors which could have contributed to this trend include environmental deterioration and improved social benefits easing retirement and providing better access to the health care system. Planning agencies need to recognize the relationships of the health care system to disability as well as to mortality."
        },
        {
            "title": "Characterization of neuromyelitis optica and neuromyelitis optica spectrum disorder patients with a late onset.",
            "abstract": "Background:\n        \n      \n      Few data are available for patients with a late onset (≥ 50 years) of neuromyelitis optica (LONMO) or neuromyelitis optica spectrum disease (LONMOSD), defined by an optic neuritis/longitudinally extensive transverse myelitis with aquaporin-4 antibodies (AQP4-Ab).\n    \n\n\n          Objective:\n        \n      \n      To characterize LONMO and LONMOSD, and to analyze their predictive factors of disability and death.\n    \n\n\n          Methods:\n        \n      \n      We identified 430 patients from four cohorts of NMO/NMOSD in France, Germany, Turkey and UK. We extracted the late onset patients and analyzed them for predictive factors of disability and death, using the Cox proportional model.\n    \n\n\n          Results:\n        \n      \n      We followed up on 63 patients with LONMO and 45 with LONMOSD during a mean of 4.6 years. This LONMO/LONMOSD cohort was mainly of Caucasian origin (93%), women (80%), seropositive for AQP4-Ab (85%) and from 50 to 82.5 years of age at onset. No progressive course was noted. At last follow-up, the median Expanded Disability Status Scale (EDSS) scores were 5.5 and 6 in the LONMO and LONMOSD groups, respectively. Outcome was mainly characterized by motor disability and relatively good visual function. At last follow-up, 14 patients had died, including seven (50%) due to acute myelitis and six (43%) because of opportunistic infections. The EDSS 4 score was independently predicted by an older age at onset, as a continuous variable after 50 years of age. Death was predicted by two independent factors: an older age at onset and a high annualized relapse rate.\n    \n\n\n          Conclusion:\n        \n      \n      LONMO/LONMOSD is particularly severe, with a high rate of motor impairment and death."
        },
        {
            "title": "Real-time myocardial contrast echocardiography as a useful tool to select candidates for coronary revascularization among patients with end-stage renal disease - a 3-year follow-up study.",
            "abstract": "Purpose:\n        \n      \n      To evaluate a real-time myocardial contrast echocardiography (MCE) as a tool to select candidates for coronary revascularization among patients with ESRD and to assess the rate of revascularization and mortality.\n    \n\n\n          Material/methods:\n        \n      \n      58 ESRD patients were screened for CAD using MCE. We analyzed the rate of coronary revascularization during 3-year follow-up. Patients with and without perfusion disturbances on MCE were compared.\n    \n\n\n          Results:\n        \n      \n      CAD was found in 46.2% patients out of 39 who underwent coronary angiography. 11 (39.3%) patients out of 28 from the group with perfusion defects on MCE underwent revascularization procedure (21.4% - PCI, 17.9% - CABG). No one from the group without perfusion defects had revascularization procedure. Perfusion defect (OR 1.37 CI 1.37-1.86, p=0.022) was related to revascularization in multivariant analysis (OR 12.87, CI 1.86-89.21, p=0.025). There was no difference in mortality between the group which underwent invasive procedures and treated conservatively (p=0.6643). In ROC analysis defects on MCE and CAD on angiography were equally good in anticipating combined end-point (AUC 0.716, CI 95% 0.544-0.851 and AUC 0.747, CI 95% 0.577-0.875, p=0.701) and death (AUC 0.752, CI 95% 0.582-0.878 and AUC 0.729, CI 95% 0.558-0.861, p=0.805).\n    \n\n\n          Conclusions:\n        \n      \n      Our results indicate that MCE is a safe and uncomplicated method which may help along with other methods to select candidates for coronary revascularization among ESRD patients. In our study coronary revascularization procedures were successful but they did not improve patients' survival on 3-year follow-up."
        },
        {
            "title": "Correlates of low physical activity across 46 low- and middle-income countries: A cross-sectional analysis of community-based data.",
            "abstract": "Physical inactivity accounts for 5.5% of all avoidable global deaths. However, a paucity of multinational studies, particularly in low- and middle-income countries (LMICs), has investigated correlates of physical activity (PA). Thus, we assessed the correlates of PA using cross-sectional, community-based data of the World Health Survey including 46 LMICs. PA was assessed by the International Physical Activity Questionnaire (IPAQ) and participants were dichotomized into those who do (≥150min moderate-vigorous PA per week) and do not (<150min=low PA) comply with the World Health Organization (WHO) PA recommendations. Multivariable logistic regression was used to assess the PA correlates. The prevalence of low PA in 206,356 persons (mean age 38.4years; 49.6% males) was 29.2% (95%CI=28.3%-30.0%). In the overall sample, female sex, not married/cohabiting, high education and wealth, unemployment, and urban setting were significant sociodemographic correlates of low PA. In terms of other correlates, inadequate fruit and vegetable intake, subsyndromal depression, worse sleep/energy and cognition, visual impairment, hearing problems and asthma were associated with not meeting the WHO recommendations. There were some variations in the correlates depending on age and sex. Interventions should be developed that operate at multiple levels of influence and take into account age- and gender-related PA patterns in order to assist people in LMICs to comply with the WHO PA recommendations. Researchers, funding bodies, practitioners and policymakers in education, mental and physical health, and urban planning have a critical role to play."
        },
        {
            "title": "Continuous computer simulation analysis of the cost-effectiveness of screening and treating diabetic retinopathy.",
            "abstract": "This paper analyzes the cost-effectiveness of screening and treating diabetic retinopathy (DR) by simulating the disease progress continuously with existing data. A new computer simulation based on Monte Carlo techniques and logistic transformation follows cohorts from diabetes onset until death in five care scenarios. For younger-onset patients, ophthalmic care reduces the prevalence of blindness by 52% or greater while savings in disability facilities and production losses surpass direct costs. For older-onset patients, less favorable results appear. Financial benefits surpass costs for juvenile-onset patients. For other patients, the net costs of ophthalmic care seem lower than in other health care programs."
        },
        {
            "title": "Colorectal cancer screening: a community case-control study of proctosigmoidoscopy, barium enema radiography, and fecal occult blood test efficacy.",
            "abstract": "Objective:\n        \n      \n      To examine the effectiveness of screening proctosigmoidoscopy, barium enema radiography, and the fecal occult blood test (FOBT) in decreasing colorectal cancer mortality in a community setting.\n    \n\n\n          Patients and methods:\n        \n      \n      In this population-based case-control study, cases comprised 218 Rochester, Minn, residents who died of colorectal cancer between 1970 and 1993. Controls were 435 age- and sex-matched residents who did not have a diagnosis of colorectal cancer. Screening proctosigmoidoscopy, barium enema radiography, and FOBT results were documented for the 10 years prior to and including the date of diagnosis of fatal colorectal cancer in cases and for the same period in matched controls. History of general medical examinations and hospitalizations was also recorded.\n    \n\n\n          Results:\n        \n      \n      Within the 10 years prior to diagnosis, the percentages of cases vs controls with at least 1 screening proctosigmoidoscopy were 23 (10.6%) of 218 cases vs 43 (9.9%) of 435 controls; at least 1 screening barium enema radiographic study was done in 12 (5.5%) of 218 vs 25 (5.7%) of 435. Within 3 years prior to diagnosis, the percentages of cases vs controls with at least 1 screening FOBT were 27 (12.4%) of 218 vs 44 (10.1%) of 435. Adjusted odds ratios were 1.04 (95% confidence interval [CI], 0.21-5.13) for proctosigmoidoscopy (distal rectosigmoid cancers only), 0.67 (95% CI, 0.31-1.48) for barium enema radiography, and 0.83 (95% CI, 0.45-1.52) for FOBT over the above time periods.\n    \n\n\n          Conclusion:\n        \n      \n      In this case-control study within a community setting, a colorectal cancer-specific mortality benefit could not be demonstrated for screening by FOBT, proctosigmoidoscopy, or barium enema radiography. Screening frequency was low, which may have contributed to the lack of measurable effects."
        },
        {
            "title": "Long-term Outcomes of Cytomegalovirus Retinitis in the Era of Modern Antiretroviral Therapy: Results from a United States Cohort.",
            "abstract": "Purpose:\n        \n      \n      To describe the long-term outcomes of patients with cytomegalovirus (CMV) retinitis and AIDS in the modern era of combination antiretroviral therapy.\n    \n\n\n          Design:\n        \n      \n      Prospective, observational cohort study.\n    \n\n\n          Participants:\n        \n      \n      Patients with AIDS and CMV retinitis.\n    \n\n\n          Methods:\n        \n      \n      Immune recovery, defined as a CD4+ T-cell count >100 cells/μl for ≥3 months.\n    \n\n\n          Main outcome measures:\n        \n      \n      Mortality, visual impairment (visual acuity <20/40), and blindness (visual acuity ≤20/200) on logarithmic visual acuity charts and loss of visual field on quantitative Goldmann perimetry.\n    \n\n\n          Results:\n        \n      \n      Patients without immune recovery had a mortality of 44.4/100 person-years (PYs) and a median survival of 13.5 months after the diagnosis of CMV retinitis, whereas those with immune recovery had a mortality of 2.7/100 PYs (P < 0.001) and an estimated median survival of 27.0 years after the diagnosis of CMV retinitis. The rates of bilateral visual impairment and blindness were 0.9 and 0.4/100 PYs, respectively, and were similar between those with and without immune recovery. Among those with immune recovery, the rate of visual field loss was approximately 1% of the normal field per year, whereas among those without immune recovery it was approximately 7% of the normal field per year.\n    \n\n\n          Conclusions:\n        \n      \n      Among persons with CMV retinitis and AIDS, if there is immune recovery, long-term survival is likely, whereas if there is no immune recovery, the mortality rate is substantial. Although higher than the rates in the population not infected by human immunodeficiency virus, the rates of bilateral visual impairment and blindness are low, especially when compared with rates in the era before modern antiretroviral therapy."
        },
        {
            "title": "Hearing, mobility, and pain predict mortality: a longitudinal population-based study.",
            "abstract": "Objective:\n        \n      \n      Measures of health-related quality of life (HRQL), including the Health Utilities Index Mark 3 (HUI3) are predictive of mortality. HUI3 includes eight attributes, vision, hearing, speech, ambulation, dexterity, cognition, emotion, and pain and discomfort, with five or six levels per attribute that vary from no to severe disability. This study examined associations between individual HUI3 attributes and mortality.\n    \n\n\n          Study design and setting:\n        \n      \n      Baseline data and 12 years of follow-up data from a closed longitudinal cohort study, the 1994/95 Canadian National Population Health Survey, consisting of 12,375 women and men aged 18 and older. A priori hypotheses were that ambulation, cognition, emotion, and pain would predict mortality. Cox proportional hazards regression models were applied controlling for standard determinants of health and risk factors.\n    \n\n\n          Results:\n        \n      \n      Single-attribute utility scores for ambulation (hazard ratio [HR]=0.10; 0.04-0.22), hearing (HR=0.18; 0.06-0.57), and pain (HR=0.53; 0.29-0.96) were statistically significantly associated with an increased risk of mortality; ambulation and hearing were predictive for the 60+ cohort.\n    \n\n\n          Conclusion:\n        \n      \n      Few studies have identified hearing or pain as risk factors for mortality. This study is innovative because it identifies specific components of HRQL that predict mortality. Further research is needed to understand better the mechanisms through which deficits in hearing and pain affect mortality risks."
        },
        {
            "title": "Effectiveness of the immunofecal occult blood test for colorectal cancer screening in a large population.",
            "abstract": "Background:\n        \n      \n      Guaiac tests are the most widely used tests to detect colorectal cancer (CRC). However, their sensitivity is relatively low and results may be affected by various factors. Immunofecal occult blood test (IFOBT) is specific for human hemoglobin and does not require dietary restrictions.\n    \n\n\n          Aims:\n        \n      \n      The aim of this study was to evaluate the effectiveness of IFOBT for the screening of precancerous lesions and CRC.\n    \n\n\n          Methods:\n        \n      \n      From July 2006 to June 2007, IFOBT was performed on 5,919 adults who received periodic health examinations in our hospital. The positive cases were examined by colonoscopy and a double-contrast barium enema. Diagnosis was confirmed by histopathological analysis.\n    \n\n\n          Results:\n        \n      \n      Positive IFOBT was detected in 314 of 5,919 cases (5.30%). Further examinations were made in 264 IFOBT-positive cases. Of these, 116 cases with colorectal cancer (16 cases) or precancerous lesions (94 cases with colorectal adenomatous polyps and 6 cases with active ulcerative colitis) were detected. The total detection rate of CRC and precancerous lesions was 43.94% (116/264). TNM classification of 16 CRC cases was as follows: TNM I in eight cases (50.00%), TNM II in seven cases (43.75%) and TNM III in one case (6.25%), indicating IFOBT can detect CRC in the early stages.\n    \n\n\n          Conclusion:\n        \n      \n      Regular IFOBT can detect precancerous lesions and CRC in early stages and can thus reduce mortality from CRC."
        },
        {
            "title": "Retrospective evaluation of patients with uveal melanoma treated by stereotactic radiosurgery with and without tumor resection.",
            "abstract": "Importance:\n        \n      \n      The present study intended to analyze the suitability of single-dose stereotactic radiotherapy in the treatment of uveal melanoma that cannot be handled with ruthenium-brachytherapy and therefore is a challenge for ophthalmologists concerning local tumor control, as well as preservation of the eye and visual function.\n    \n\n\n          Objectives:\n        \n      \n      To evaluate local tumor control, eye preservation, visual course, radiation complications, metastases, and death after single-dose stereotactic radiotherapy (SDRT) applied exclusively or combined with tumor resection in uveal melanomas that are neither suitable nor favorably located for ruthenium brachytherapy.\n    \n\n\n          Design:\n        \n      \n      Retrospective, observational case series.\n    \n\n\n          Setting:\n        \n      \n      Primary care center.\n    \n\n\n          Participants:\n        \n      \n      Seventy-eight patients with uveal melanoma were treated.\n    \n\n\n          Intervention:\n        \n      \n      Between June 3, 2003, and March 18, 2008, patients with uveal melanoma received SDRT monotherapy (group 1, 60 patients) or SDRT combined with tumor resection (group 2, 18 patients). Radiotherapy was performed with a tumor-surrounding dose of 25 Gy on a linear accelerator.\n    \n\n\n          Main outcome measures:\n        \n      \n      Local tumor control, eye preservation, visual results, and radiation complications.\n    \n\n\n          Results:\n        \n      \n      Within a median follow-up of 33.7 months (range, 0.13-81.13 months), 6 recurrences occurred in group 1; none recurred in group 2. The Kaplan-Meier estimate for local control was 85% at 3 years in group 1 and 100% in group 2 (P = .22). Eye preservation rate was 77% vs 87% at 3 years (groups 1 and 2, respectively) (P = .82). Visual acuity decreased with a median loss of -18 Snellen lines (group 1) and -22 Snellen lines (group 2). More retinopathies (P = .07), opticopathies (P = .27), and rubeotic glaucomas (P = .10) occurred in group 1. No significant difference was observed in the development of metastases (P = .33). The groups differed in overall survival because of 2 deaths occurring shortly after surgery in group 2 for unexplained reasons (P = .06).\n    \n\n\n          Conclusions and relevance:\n        \n      \n      Survival analysis suggested that SDRT with combined tumor resection might be associated with increased tumor control and fewer radiation complications than SDRT as monotherapy. Both groups had similar eye retention rates and were comparable concerning the decrease in visual function in most eyes. However, the protocol was stopped after 3 unexplainable deaths after surgery."
        },
        {
            "title": "Tactile intervention as a novel technique in improving body stability in healthy elderly and elderly with diabetes.",
            "abstract": "Background:\n        \n      \n      Body sway increases in the elderly because of normal aging and high incidence of disease such as diabetes. Prevalence of sway is greater in the elderly with diabetes because of damage to the central and peripheral nervous systems. Increase in body sway is associated with an elevated risk of falling. Falling is one of the major causes of morbidity and mortality in the elderly. The purpose of this study was to develop a new technique to improve body stability and decrease body sway in the elderly people with or without diabetes.\n    \n\n\n          Subjects and methods:\n        \n      \n      Twenty-two subjects--12 elderly (mean age, 75.5±7.3 years) and 10 age-matched elderly with diabetes (mean age, 72.5±5.3 years)--were recruited for this study. Subjects received tactile feedback as a tingling sensation resulting from electrical stimulation triggered by body sway.\n    \n\n\n          Results:\n        \n      \n      The results showed a significant reduction in body sway in the elderly while standing on foam with eyes open (1.0±0.31 vs. 1.9±0.8; P=0.006) and eyes closed (1.8±0.7 vs. 3.3±1.5; P=0.001). In the group with diabetes, there was a significant reduction in body sway while standing on foam with eyes closed (1.4±0.5 vs. 2.3±0.8; P=0.045) but not with eyes open.\n    \n\n\n          Conclusions:\n        \n      \n      In this small study, this technique offers a new tool for training people with diabetes and elderly people to improve body stability and balance."
        },
        {
            "title": "The natural history of moderate (50% to 79%) internal carotid artery stenosis in symptomatic, nonhemispheric, and asymptomatic patients.",
            "abstract": "Purpose:\n        \n      \n      This study was undertaken to determine the incidence of disease progression of moderate (50% to 79%) internal carotid artery stenosis in patients with symptoms, patients with nonhemispheric symptoms, and symptom-free patients and to define the risk of development of new neurologic events in each group.\n    \n\n\n          Methods:\n        \n      \n      Over a 6-year period, 272 patients with moderate internal carotid artery stenoses were monitored for a mean of 44 months with color-flow duplex scanning (CFS). At the time of the initial scan, 142 patients were symptom free, 87 had experienced transient ischemic attacks, amaurosis fugax, or mild strokes, and 43 had ill-defined nonhemispheric symptoms. The average number of follow-up scans was 2.4 per patient (range 1 to 11).\n    \n\n\n          Results:\n        \n      \n      During follow-up, 23 (26%) of the patients with symptoms, 17 (40%) of the patients with nonhemispheric symptoms, and 30 (21%) of the symptom-free patients had development of additional neurologic symptoms. Life-table comparison of ipsilateral ischemic events showed a significantly (p = 0.03) higher cumulative rate in the symptomatic group (20%) than in the asymptomatic group (7%) at 2 years. Mean annual stroke rates were 6% and 2% in patients in the symptomatic and asymptomatic groups, respectively. None of the patients in the nonhemispheric group had a stroke within 4 years of the initial study. Disease progression occurred in 16% of the patients. In the asymptomatic group, ipsilateral stroke occurred more frequently (p = 0.0001) in patients with disease progression (25%) than in patients with stable lesions (1%). CFS detected disease progression in 19 (79%) of 24 patients before the artery occluded or stroke occurred. In patients with symptoms, stroke was more frequent (p = 0.02) in patients with six or more risk factors (29%) than in those with five or fewer risk factors (7%).\n    \n\n\n          Conclusion:\n        \n      \n      Although the risk of stroke is less in patients with moderate stenosis than it is in patients with severely stenotic lesions, symptom-free patients with advancing disease and patients with symptoms and multiple risk factors are at increased risk for development of neurologic events. These findings support the use of CFS to monitor patients with carotid artery disease and suggest that a more aggressive surgical approach may be indicated in selected patients with moderate carotid artery stenosis."
        },
        {
            "title": "[Fitness to drive under traffic ophthalmological aspects in elderly drivers].",
            "abstract": "The Federal Roads Office (FEDRO), Switzerlands federal authority carries responsibility for the action program “Via Sicura” in order to reduce drastically the number of road traffic fatalities and serious injuries on Swiss roads. The revision of the VZV (Verkehrszulassungsverordnung) included in this program will come in to force on 1.July 2016. On that account the legal medical requirements for driver will be renewed. In particular, the requirements for vision (visual acuity, visual field) will be adjusted to international standards. Due to demographic changes the number of elderly drivers with old age (85 – 90+) with eye associated diseases increases. Therefore, questions concerning traffic ophthalmological problems have to be increasingly considered within traffic medical assessments. The driver's vision in traffic's safety must enable him to perceive relevant information, process information quickly and perform an adequate reaction in time, even if visibility is limited (e. g. due to rain, night, darkness) or in the presence of physical or psychical constraints."
        },
        {
            "title": "The reproducibility of ophthalmic utility values.",
            "abstract": "Purpose:\n        \n      \n      Utility values have been used in the ophthalmic literature to measure the quality of life associated with a health state. By convention, a utility value of 1.0 is associated with perfect health, and a value of 0.0 is associated with death. Construct validity of utility values has been demonstrated, particularly in regard to decreasing utility values as the vision decreases in the better seeing eye, but long-term test-retest reliability has not been demonstrated. The purpose of this study was to demonstrate the test-retest reliability of ophthalmic utility values.\n    \n\n\n          Methods:\n        \n      \n      One hundred fifteen patients with ophthalmic diseases and stable visual acuity underwent time trade-off utility analysis with retesting at various intervals ranging from 1 month to 2 years. The results were analyzed using the Wilcoxon signed rank test. The study was designed to have an 50% power, using a two-sided alpha of 5%, to be able to detect a 10% difference between the test and retest groups.\n    \n\n\n          Results:\n        \n      \n      The mean time from testing to retesting was 0.87 years, with a median time of 1.0 year and range of 1 month to 2 years. The mean utility value in the test group was 0.766 (SD = .21; 95% CI, 0.730-0.802), while the mean utility value in the retest group was 0.763 (SD = .22; 95% CI, 0.724-0.802). The difference between the means of the test-retest groups was not significant (P = .99). The intraclass correlation between the initial and follow-up utility scores was .5246 (P < .00005).\n    \n\n\n          Conclusions:\n        \n      \n      Ophthalmic utility values appear to have good test-retest reliability over prolonged periods of time. This information is important because it gives researchers increased confidence in the validity of basic tools for ophthalmic cost-effective (cost-utility) analyses."
        },
        {
            "title": "Deterioration in quality-of-life of late (10-year) survivors of head and neck cancer.",
            "abstract": "Objectives:\n        \n      \n      To determine 10-year quality-of-life (QOL) in head and neck cancer patients and to examine the potential predictors of late QOL.\n    \n\n\n          Design:\n        \n      \n      Prospective 10-year (QOL) assessment in a cohort of head and neck cancer patients.\n    \n\n\n          Setting:\n        \n      \n      Tertiary referral head and neck cancer centre in Auckland, New Zealand.\n    \n\n\n          Participants:\n        \n      \n      Two hundred patients diagnosed and were treated for head and neck cancer. Exclusion criteria were blindness, learning difficulties or inability to understand or read English.\n    \n\n\n          Main outcome measures:\n        \n      \n      Quality-of-life at 10 years measured by Auckland QOL questionnaire, and analysed for associations with the following co-variates: age, gender; co-morbidities (alcohol intake and smoking), type and stage of disease; treatment modality; and QOL measures.\n    \n\n\n          Results:\n        \n      \n      At 10 years following diagnosis, overall QOL (life satisfaction), decreased significantly by an average of 11% (95% CI: -5, -17) compared with before treatment, and by 15% when compared with years 1 and 2. Pre-treatment QOL significantly predicted late QOL, whilst QOL 1 year after treatment did not. None of the socio-demographic, disease- or treatment-related factors predicted long-term QOL on univariate analysis, but this may be due to the small sample size.\n    \n\n\n          Conclusions:\n        \n      \n      This observed, late drop in the QOL of head and neck cancer patients requires further corroboration and investigation. Due to small sample sizes associated with long-term studies in head and neck cancer cohorts, studies of predictors of long-term QOL will only be likely to succeed if done as multi-centre studies. As there is some evidence to suggest that psychosocial interventions improve the QOL of head and neck cancer patients, it may be appropriate to consider screening for risk of a late deterioration in QOL in order to plan appropriate psycho-social intervention."
        },
        {
            "title": "Glaucoma medications and mortality: a retrospective cohort study.",
            "abstract": "Purpose:\n        \n      \n      This study presents a retrospective, cohort analysis to estimate the 4-year rate of all-cause risk-adjusted mortality for veterans who were dispensed glaucoma medications. The main outcome measures was hazard of death according to glaucoma medication exposure.\n    \n\n\n          Methods:\n        \n      \n      Beneficiaries 40 years and older enrolled in Veterans Health Administration (VHA) with International Disease Classification diagnoses of glaucoma. VHA clinical and pharmacy data sets were linked with a national VHA mortality registry. Patients were identified with a primary diagnosis of glaucoma were then put into one of two groups, glaucoma medication exposure or nonexposure medication group. Four-year survival analysis using the Cox proportional hazard method was adjusted for comorbidities using pertinent demographic characteristics and hierarchical condition categories from the Centers for Medicare and Medicaid Services.\n    \n\n\n          Results:\n        \n      \n      Of 214,971 beneficiaries with glaucoma or suspected glaucoma, 25,148 (11.7%) died during the study period. Compared to unexposed patients with glaucoma, the use of any class of glaucoma medication was associated with a statistically significant 7% reduced hazard of death (adjusted hazard ratio, 0.93; 95% confidence interval: 0.90-0.95). Reduced risk of death was found for multiple topical medication use, but not individual glaucoma agents.\n    \n\n\n          Conclusions:\n        \n      \n      The seemingly inexplicable protective effect of glaucoma agents on all-cause mortality may best be explained by unmeasured confounding variables involved in the clinical decision of whether or not to treat patients with limited life expectancy for glaucoma."
        },
        {
            "title": "Long-term Outcomes After Proton Beam Irradiation in Patients With Large Choroidal Melanomas.",
            "abstract": "Importance:\n        \n      \n      Although radiotherapy has been used more frequently in past decades for the management of large melanomas, long-term efficacy of proton beam irradiation (PBI) of large choroidal melanomas has not been reported.\n    \n\n\n          Objective:\n        \n      \n      To evaluate long-term outcomes in patients who underwent PBI for the treatment of large choroidal melanomas.\n    \n\n\n          Design, setting, and participants:\n        \n      \n      Data were obtained at a single Boston, Massachusetts, academic tertiary referral practice for this retrospective cohort study. In total, 336 patients with large tumors treated over a 13-year period from January 1, 1985, to December 31, 1997, and followed up until the end points were reached or until December 31, 2008, were included. Data analyses were initially completed in February 2017 and finalized in July 2017. Large tumors were those with a height 10 mm or greater or a longest linear diameter greater than 16 mm or a height greater than 8 mm when the optic nerve was involved.\n    \n\n\n          Intervention:\n        \n      \n      Proton beam irradiation (total 70 Gy) delivered in 5 equal fractions.\n    \n\n\n          Main outcomes and measures:\n        \n      \n      The primary outcomes of rates of visual acuity retention, eye retention, tumor recurrence, and melanoma-related mortality were calculated using Kaplan-Meier estimates, and Cox proportional hazards regression analyses were completed to evaluate risk factors for tumor recurrence and melanoma-related mortality.\n    \n\n\n          Results:\n        \n      \n      In this cohort of 336 patients with large tumors, 150 were women and 329 were white; mean (SD) age was 60.0 (14.0) years. Of 178 patients without optic nerve involvement (tumor >1 disc diameter from optic nerve), the mean (SD) largest basal diameter was 18.1 (1.9) mm and mean height was 8.2 (2.7) mm. Optic nerve involvement and tumors greater than 8 mm were observed in 109 patients (32.4% of the cohort). Baseline visual acuity of 20/200 or better was observed in 244 patients (72.6%), and worse than 20/800 in 52 (15.5%). Ten-year rates of visual acuity retention were 8.7% (95% CI, 4.1%-15.6%) for at least 20/200 and 22.4% (95% CI, 15.4%-30.4%) for at least counting fingers. Ten years after PBI therapy, the eye was retained (70.4%; 95% CI, 61.5%-77.6%) and tumor controlled (87.5%; 95% CI, 76.8%-93.5%) in most patients. The 10-year all-cause mortality rate was 60.7% (95% CI, 55.5%-65.9%). Approximately half of the patients died of metastatic uveal melanoma (10-year rate, 48.5%; 95% CI, 43.0%-54.4%).\n    \n\n\n          Conclusions and relevance:\n        \n      \n      This study demonstrates that eye conservation is possible in most cases, with ambulatory vision retained in a small proportion of patients 10 years after PBI. Tumor recurrence rates were low and mortality rates were comparable to those observed after enucleation."
        },
        {
            "title": "Retinopathy predicts coronary heart disease mortality.",
            "abstract": "Background:\n        \n      \n      Retinopathy lesions are fairly common findings in clinic settings and may predict risk of coronary heart disease (CHD).\n    \n\n\n          Objective:\n        \n      \n      To examine whether retinopathy independently predicts a risk of CHD-related mortality in people with and without diabetes.\n    \n\n\n          Methods:\n        \n      \n      In an Australian population-based cohort of people with (n = 199) and without (n = 2768) diabetes (Blue Mountains Eye Study, total n = 2967), the presence and severity of retinopathy was assessed from retinal photographs. 12-Year cumulative CHD deaths were ascertained from Australian National Death Index records.\n    \n\n\n          Results:\n        \n      \n      Over 12 years, 353 participants (11.9%) had incident CHD-related deaths. Retinopathy was present in 57/199 (28.6%) participants with, and in 268/2768 (9.7%) without, diabetes. The presence of retinopathy increased the CHD mortality rate per person-year by an amount (0.005) equivalent to the presence of diabetes itself (12-year CHD mortality rate per person-year of 0.010 in people with neither diabetes nor retinopathy, 0.015 in those with diabetes alone, 0.016 in those with retinopathy alone). After adjusting for cardiovascular risk factors, retinopathy remained an independent predictor of CHD death both in people with diabetes (hazard ratio (HR) = 2.21, 95% CI 1.20 to 4.05) and in those without diabetes (HR = 1.33, 95% CI 1.02 to 1.83). Moderate retinopathy was associated with adjusted HR = 6.68 (95% CI 2.24 to 20.0) in people with diabetes and adjusted HR = 2.29 (95% CI 1.10 to 4.76) in people without diabetes.\n    \n\n\n          Conclusions:\n        \n      \n      A finding of retinopathy in people with or without diabetes may signal increased CHD risk. The increased CHD mortality associated with retinopathy in people without diabetes was equivalent to the presence of diabetes itself."
        },
        {
            "title": "Multivariate prediction of in-hospital mortality associated with surgical procedures.",
            "abstract": "Background:\n        \n      \n      The aims of this prospective multicenter study were to identify variables associated with in-hospital mortality among patients undergoing surgical procedures, to develop a prediction rule, and to statistically validate its reliability.\n    \n\n\n          Methods:\n        \n      \n      Data from 24,654 consecutive informed patients over 15 years of age were collected from 22 surgical centers between January 1989 and December 1990. Using logistic regression analysis separate models were fit for seven surgical disciplines to predict the risk of 30-day in hospital mortality. Variables used to construct the regression models included age, sex, systolic blood pressure, renal dysfunction, hepatic dysfunction, concomitant diseases, severity of surgery, priority of surgery and duration of anesthesia. The performance of the prediction rule was evaluated by computing sensitivity, specificity and predictive values, analyzing the ROC curve and comparing observed with expected deaths.\n    \n\n\n          Results:\n        \n      \n      The significance of the independent variables varied within each model. All models significantly predicted the occurrence of in-hospital mortality. At a 0.5 cuptoint of predicted risk sensitivity of prediction rule was 99.89%, positive predictive value 98.51%, and overall predictive value 98.41%, whereas specificity was 7.92% and negative value slightly higher than 50%. The area under the ROC curve was 0.80 (perfect, 1.0). The correlation between observed and expected deaths was 0.99.\n    \n\n\n          Conclusion:\n        \n      \n      This prediction rule, developed using multicenter data, is characterized by the following advantages: includes only nine variables; can be utilized by seven different surgical disciplines; is highly accurate, and is easily available to clinicals with access to a microcomputer or programmable calculator. This validated multivariate prediction rule would be useful both to calculate the risk of mortality for an individual surgical patient and to contrast observed and expected mortality rates for an institution or a particular clinician."
        },
        {
            "title": "Diagnostic and prognostic accuracy of clinical and laboratory parameters in community-acquired pneumonia.",
            "abstract": "Background:\n        \n      \n      Community-acquired pneumonia (CAP) is the most frequent infection-related cause of death. The reference standard to diagnose CAP is a new infiltrate on chest radiograph in the presence of recently acquired respiratory signs and symptoms. This study aims to evaluate the diagnostic and prognostic accuracy of clinical signs and symptoms and laboratory biomarkers for CAP.\n    \n\n\n          Methods:\n        \n      \n      545 patients with suspected lower respiratory tract infection, admitted to the emergency department of a university hospital were included in a pre-planned post-hoc analysis of two controlled intervention trials. Baseline assessment included history, clinical examination, radiography and measurements of procalcitonin (PCT), highly sensitive C-reactive protein (hsCRP) and leukocyte count.\n    \n\n\n          Results:\n        \n      \n      Of the 545 patients, 373 had CAP, 132 other respiratory tract infections, and 40 other final diagnoses. The AUC of a clinical model including standard clinical signs and symptoms (i.e. fever, cough, sputum production, abnormal chest auscultation and dyspnea) to diagnose CAP was 0.79 [95% CI, 0.75-0.83]. This AUC was significantly improved by including PCT and hsCRP (0.92 [0.89-0.94]; p < 0.001). PCT had a higher diagnostic accuracy (AUC, 0.88 [0.84-0.93]) in differentiating CAP from other diagnoses, as compared to hsCRP (AUC, 0.76 [0.69-0.83]; p < 0.001) and total leukocyte count (AUC, 0.69 [0.62-0.77]; p < 0.001). To predict bacteremia, PCT had a higher AUC (0.85 [0.80-0.91]) as compared to hsCRP (p = 0.01), leukocyte count (p = 0.002) and elevated body temperature (p < 0.001). PCT, in contrast to hsCRP and leukocyte count, increased with increasing severity of CAP, as assessed by the pneumonia severity index (p < 0.001).\n    \n\n\n          Conclusion:\n        \n      \n      PCT, and to a lesser degree hsCRP, improve the accuracy of currently recommended approaches for the diagnosis of CAP, thereby complementing clinical signs and symptoms. PCT is useful in the severity assessment of CAP."
        },
        {
            "title": "Factors associated with lifetime risk of open-angle glaucoma blindness.",
            "abstract": "Purpose:\n        \n      \n      To investigate factors associated with bilateral glaucoma blindness, particularly factors available at the time of diagnosis.\n    \n\n\n          Methods:\n        \n      \n      Retrospective chart review of all patients with primary open-angle glaucoma (POAG) or pseudoexfoliative glaucoma (PEXG) followed at the Department of Ophthalmology or Low Vision Center of Skåne University Hospital, Malmö, Sweden, who died between January 2006 and June 2010. Disease stage at diagnosis was defined by a simplified version of Mills' glaucoma staging system using perimetric mean deviation (MD) to define six stages of severity. Blindness was defined according to WHO criteria. We used logistic regression analysis to examine the association between risk factors and glaucoma blindness.\n    \n\n\n          Results:\n        \n      \n      Four hundred and 23 patients were included; 60% POAG and 40% PEXG. Sixty-four patients (15%) became blind from glaucoma. Blind patients had significantly longer mean duration with diagnosed disease than patients who did not go blind (14.8 years ± 5.8 versus 10.6 years ± 6.5, p < 0.001). The risk of blindness increased with higher intraocular pressure (IOP) (OR 1.08, 95% CI 1.03-1.13) and with each stage of more advanced field loss at time of diagnosis (OR 1.80 95% CI 1.34-2.41). Older age at death was also associated with an increased risk of blindness (OR 1.09 95% CI 1.03-1.14), while age at diagnosis was unimportant. PEXG was not an independent risk factor for blindness.\n    \n\n\n          Conclusions:\n        \n      \n      Higher IOP and worse visual field status at baseline were important risk factors, as was older age at death."
        },
        {
            "title": "Skeletal muscle perfusion in peripheral arterial disease a novel end point for cardiovascular imaging.",
            "abstract": "Peripheral arterial disease (PAD) is characterized by lower limb arterial obstruction due to atherosclerosis. There are over 8 million people with PAD in the U.S at present (1). As a consequence of impaired tissue perfusion, PAD patients can experience pain, diminished exercise capacity, and tissue loss, with some ultimately requiring amputation (2). The presence of PAD is a high risk marker of additional cardiovascular disease as the annual rate of events including myocardial infarction, stroke, and cardiovascular death is 5% to 7% (3). Presently used diagnostic methods include the ankle-brachial index (ABI), pulse volume recordings, duplex ultrasonography, venous plethysmography and angiography by X-ray, computed tomography, or magnetic resonance imaging, all of which have limitations."
        },
        {
            "title": "Entering the ninth decade is not a contraindication for carotid endarterectomy.",
            "abstract": "The role of carotid endarterectomy (CEA) in stroke prevention is now better defined. However, its role in patients older than 79 years of age is controversial. This group of patients has been excluded in most clinical trials. In this study the authors reviewed their experience with CEA patients >79 years old. The records of all patients older than 79 years of age who underwent a CEA in a recent time period from January 1988 to December 1996 were retrospectively reviewed. Forty-one patients (31 men, 10 women) were identified by computer search. The indication for operation included transient ischemic attack in 12 (29.3%), amaurosis fugax in nine (22%), stroke in two (4.9%), and nonhemispheric symptoms in three (7.3%). Fifteen patients (36.6%) were asymptomatic. Medical risk factors included coronary artery disease in 26 (63.4%), hypertension in 22 (53.7%), and smoking in 12 (29.3%). The procedure was performed under EEG monitoring in all patients. General anesthesia was administered in 37 (90%) and regional anesthesia in four (10%). Shunts were used in four (10%) patients. The internal carotid artery was patched in 16 patients (39%). One patient (2.4%) developed a perioperative stroke and only one patient developed perioperative myocardial infarction (MI). None of the patients died within 30 days of surgery. In addition to the one MI case, five patients developed minor complications. The average length of time for stay after CEA was 3.4 days. Patients were followed up for an average of 20.7 months. Six patients died during follow-up. Four of those died from an MI and two from a stroke. The authors conclude that with proper selection of patients, CEA is safe in the octogenarian. Age alone should not be a contraindication for CEA."
        },
        {
            "title": "The diagnostic value of contrast-enhanced ultrasound (CEUS) for assessing hepatocellular carcinoma compared to histopathology; a retrospective single-center analysis of 119 patients1.",
            "abstract": "Background:\n        \n      \n      HCC as the 6th most common tumor entity with the fourth highest mortality and an increasing prevalence especially due to today's lifestyle acquires a high attention in the clinical setting. Beside CECT and CEMRI, CEUS depicts a dynamic, low-risk and radiation free imaging method that finds its use mainly in screening and active surveillance programs.\n    \n\n\n          Purpose:\n        \n      \n      The aim of the retrospective study was to evaluate the diagnostic value of CEUS in correlation to pathologic findings.\n    \n\n\n          Materials and methods:\n        \n      \n      Between 2004 and 2018 a total number of 119 patients were included in this retrospective single-center study. Every patient underwent CEUS in addition to a native B-mode and Color-Doppler scan. After given informed consent SonoVue® (Bracco, Milan, Italy), a second-generation blood-pool agent, was used as contrast medium. Every examination was performed and interpreted by a single experienced radiologist (EFSUMB level 3). A low mechanical index (MI) of <0,2 was chosen to obtain a good imaging quality.\n    \n\n\n          Results:\n        \n      \n      All 119 included patients received CEUS followed by a liver biopsy for inter-modality comparison. In correlation to the pathology results, CEUS showed a diagnostic sensitivity of 96,6%, a specificity of 63,9%, a PPV of 86,7% and a NPV of 88,5% by detecting liver lesions suspicious for HCC. According to the Cohen's Kappa coefficient (k = 0,659) CEUS shows a strong inter-modality agreement in comparison to the histopathological finding.\n    \n\n\n          Conclusion:\n        \n      \n      With a high sensitivity and a strong cross-modality comparability to histopathology, the CEUS is highly effective in the detection of suspicious HCC lesions."
        },
        {
            "title": "Diagnostic accuracy of computed tomography pulmonary angiography with reduced radiation and contrast material dose: a prospective randomized clinical trial.",
            "abstract": "Objective:\n        \n      \n      The objective of the study was to test the diagnostic performance of low-dose computed tomography pulmonary angiography (CTPA) at peak tube voltage of 80 kVp with both reduced radiation and reduced contrast material (CM) dose.\n    \n\n\n          Materials and methods:\n        \n      \n      In this single-center, single-blinded prospective randomized trial, 501 patients with body weights of less than 100 kg with suspected acute pulmonary embolism (PE) were assigned to normal-dose CTPA (100-kVp tube energy and 100-mL CM, 255 patients) and low-dose CTPA (80-kVp tube energy and 75-mL CM, 246 patients). Primary end points were evidence of PE in CTPA and accuracy of CTPA on a composite reference standard. Results were compared by calculating the odds ratio with the 95% confidence interval.\n    \n\n\n          Results:\n        \n      \n      The reference diagnosis was equivocal in 20 of the 501 patients. Diagnosis of CTPA was correct in 240 patients and incorrect in 5 in the normal-dose group. Computed tomography pulmonary angiography was correct in 230 patients and incorrect in 6 in the low-dose group (odds ratio, 1.25; 95% confidence interval, 0.38-4.16; P = 0.77). Sensitivity was 96.9% and 100% and specificity was 98.1% and 97.1% in the normal-dose and low-dose groups, respectively. No PE or PE-related death occurred during the 90-day follow-up. The size-specific dose estimates were 30% lower at 80 kVp (4.8 ± 1.0 mGy) compared with that at 100 kVp (6.8 ± 1.2 mGy; P < 0.001).\n    \n\n\n          Conclusions:\n        \n      \n      The accuracy of low-dose CTPA at 80 kVp with a 30% reduced radiation dose and a 25% lower CM volume is not significantly different from that of normal-dose CTPA at 100 kVp in detecting acute PE in patients weighing less than 100 kg."
        },
        {
            "title": "Impact of Diabetic Retinopathy on Vulnerability of Atherosclerotic Coronary Plaque and Incidence of Acute Coronary Syndrome.",
            "abstract": "Although an association has been reported between the microvascular complications of diabetic patients and their poor prognosis after cardiovascular events related to advanced atherosclerosis, it is not clear whether there is a relation between diabetic retinopathy (DR) and the severity of plaque vulnerability. Fifty-seven diabetic patients with coronary artery disease, classified as non-DR (n = 42) or DR (n = 15), underwent angioscopic observation of at least 1 entire coronary artery. The number of yellow plaques (NYP) through the observed coronary artery was counted and their color grades, defined as 1 (light yellow), 2 (yellow), or 3 (intense yellow), were evaluated. The NYP per vessel and the maximum yellow grade were determined. The association between the presence of DR and incidences of acute coronary syndrome (ACS) was analyzed during the follow-up period (mean 7.1 ± 3.3 years; range, 0.83 to 11.75 years). Mean NYP per vessel and maximum yellow grade were significantly greater in DR than in non-DR patients (2.08 ± 1.01 vs 1.26 ± 0.77, p = 0.002, and 2.40 ± 0.74 vs 1.90 ± 0.82, p = 0.044, respectively). The cumulative incidences of ACS were higher in the DR group (p = 0.004), and the age-adjusted hazard ratio for ACS was 6.943 (95% CI 1.267 to 38.054; p = 0.026) for DR compared with non-DR patients. Our findings indicate that coronary atherosclerosis and plaque vulnerability are more severe in patients with DR. DR as a microvascular complication may be directly linked with macrovascular plaque vulnerability and fatal cardiovascular events such as ACS."
        },
        {
            "title": "The most commonly used disease severity scores are inappropriate for risk stratification of older emergency department sepsis patients: an observational multi-centre study.",
            "abstract": "Background:\n        \n      \n      Sepsis recognition in older emergency department (ED) patients is difficult due to atypical symptom presentation. We therefore investigated whether the prognostic and discriminative performance of the five most commonly used disease severity scores were appropriate for risk stratification of older ED sepsis patients (≥70 years) compared to a younger control group (<70 years).\n    \n\n\n          Methods:\n        \n      \n      This was an observational multi-centre study using an existing database in which ED patients who were hospitalized with a suspected infection were prospectively included. Patients were stratified by age < 70 and ≥70 years. We assessed the association with in-hospital mortality (primary outcome) and the area under the curve (AUC) with receiver operator characteristics of the Predisposition, Infection, Response, Organ dysfunction (PIRO), quick Sequential Organ Failure Assessment (qSOFA), Mortality in ED Sepsis (MEDS), and the Modified and National Early Warning (MEWS and NEWS) scores.\n    \n\n\n          Results:\n        \n      \n      In-hospital mortality was 9.5% ((95%-CI); 7.4-11.5) in the 783 included older patients, and 4.6% (3.6-5.7) in the 1497 included younger patients. In contrast to younger patients, disease severity scores in older patients associated poorly with mortality. The AUCs of all disease severity scores were poor and ranged from 0.56 to 0.64 in older patients, significantly lower than the good AUC range from 0.72 to 0.86 in younger patients. The MEDS had the best AUC (0.64 (0.57-0.71)) in older patients. In older and younger patients, the newly proposed qSOFA score (Sepsis 3.0) had a lower AUC than the PIRO score (sepsis 2.0).\n    \n\n\n          Conclusion:\n        \n      \n      The prognostic and discriminative performance of the five most commonly used disease severity scores was poor and less useful for risk stratification of older ED sepsis patients."
        },
        {
            "title": "Disability and 6-year mortality in elderly population. Role of visual impairment.",
            "abstract": "Background and aims:\n        \n      \n      We investigated the prevalence of visual impairment (VI), its determinants and its association with Activities of Daily Living (ADL), Instrumental Activities of Daily Living (IADL), Gross Mobility (GM) and 6-year mortality in elderly subjects. A cross-sectional survey in a large population randomly selected in 1992 with a 6-year mortality evaluation in Campania, a region in southern Italy.\n    \n\n\n          Methods:\n        \n      \n      A random sample of 1332 elderly subjects aged 65 to 95 years (mean age 74.2 +/- 6.4), selected from the electoral rolls, was interviewed by trained physicians. Self-reported visual function, socio-demographic and clinical characteristics were recorded. Disability was assessed by measuring ability in ADL, IADL and GM.\n    \n\n\n          Results:\n        \n      \n      VI was found to affect 34% of this population, with an age-related increase of mild and severe VI. VI affects ADL, IADL and GM disability. Age, diabetes and low educational level, but not comorbidity or hypertension, proved to be predictors of VI. Mortality increased with severity of VI in 38.1% of subjects with severe functional impairment (p < 0.001). The presence of VI was seen to increase the risk of mortality by 1.40 (95% CI 1.07-1.84), independently of age, sex, comorbidity, diabetes, hypertension or disability.\n    \n\n\n          Conclusions:\n        \n      \n      This study demonstrates an association between visual impairment and disability in an elderly population, and the predictive effect of visual impairment on mortality independently of comorbidity. These results illustrate the need to eradicate avoidable blindness, in order to improve the quality of life and to prolong survival of the elderly."
        },
        {
            "title": "Occupational health of miners at altitude: adverse health effects, toxic exposures, pre-placement screening, acclimatization, and worker surveillance.",
            "abstract": "Context:\n        \n      \n      Mining operations conducted at high altitudes provide health challenges for workers as well as for medical personnel.\n    \n\n\n          Objective:\n        \n      \n      To review the literature regarding adverse health effects and toxic exposures that may be associated with mining operations conducted at altitude and to discuss pre-placement screening, acclimatization issues, and on-site surveillance strategies.\n    \n\n\n          Methods:\n        \n      \n      We used the Ovid ( http://ovidsp.tx.ovid.com ) search engine to conduct a MEDLINE search for \"coal mining\" or \"mining\" and \"altitude sickness\" or \"altitude\" and a second MEDLINE search for \"occupational diseases\" and \"altitude sickness\" or \"altitude.\" The search identified 97 articles of which 76 were relevant. In addition, the references of these 76 articles were manually reviewed for relevant articles. CARDIOVASCULAR EFFECTS: High altitude is associated with increased sympathetic tone that may result in elevated blood pressure, particularly in workers with pre-existing hypertension. Workers with a history of coronary artery disease experience ischemia at lower work rates at high altitude, while those with a history of congestive heart failure have decreased exercise tolerance at high altitude as compared to healthy controls and are at higher risk of suffering an exacerbation of their heart failure. PULMONARY EFFECTS: High altitude is associated with various adverse pulmonary effects, including high-altitude pulmonary edema, pulmonary hypertension, subacute mountain sickness, and chronic mountain sickness. Mining at altitude has been reported to accelerate silicosis and other pneumoconioses. Miners with pre-existing pneumoconioses may experience an exacerbation of their condition at altitude. Persons traveling to high altitude have a higher incidence of Cheyne-Stokes respiration while sleeping than do persons native to high altitude. Obesity increases the risk of pulmonary hypertension, acute mountain sickness, and sleep-disordered breathing. NEUROLOGICAL EFFECTS: The most common adverse neurological effect of high altitude is acute mountain sickness, while the most severe adverse neurological effect is high-altitude cerebral edema. Poor sleep quality and sleep-disordered breathing may contribute to daytime sleepiness and impaired cognitive performance that could potentially result in workplace injuries, particularly in miners who are already at increased risk of suffering unintentional workplace injuries. OPHTHALMOLOGICAL EFFECTS: Adverse ophthalmological effects include increased exposure to ultraviolet light and xerophthalmia, which may be further exacerbated by occupational dust exposure. RENAL EFFECTS: High altitude is associated with a protective effect in patients with renal disease, although it is unknown how this would affect miners with a history of chronic renal disease from exposure to silica and other renal toxicants. HEMATOLOGICAL EFFECTS: Advanced age increases the risk of erythrocytosis and chronic mountain sickness in miners. Thrombotic and thromboembolic events are also more common at high altitude. MUSCULOSKELETAL EFFECTS: Miners are at increased risk for low back pain due to occupational factors, and the easy fatigue at altitude has been reported to further predispose workers to this disorder. TOXIC EXPOSURES: Diesel emissions at altitude contain more carbon monoxide due to increased incomplete combustion of fuel. In addition, a given partial pressure of carbon monoxide at altitude will result in a larger percentage of carboxyhemoglobin at altitude. Miners with a diagnosis of chronic obstructive pulmonary disease may be at higher risk for morbidity from exposure to diesel exhaust at altitude.\n    \n\n\n          Conclusions:\n        \n      \n      Both mining and work at altitude have independently been associated with a number of adverse health effects, although the combined effect of mining activities and high altitude has not been adequately studied. Careful selection of workers, appropriate acclimatization, and limited on-site surveillance can help control most health risks. Further research is necessary to more completely understand the risks of mining at altitude and delineate what characteristics of potential employees put them at risk for altitude-related morbidity or mortality."
        },
        {
            "title": "Demographic, health, cognitive, and sensory variables as predictors of mortality in very old adults.",
            "abstract": "Cognitive and sensorimotor predictors of mortality were examined in the Australian Longitudinal Study of Ageing, controlling for demographic and health variables. A stratified random sample of 1,947 males and females aged 70 and older were interviewed, and 1,500 were assessed on measures of health, memory. verbal ability, processing speed, vision, hearing, and grip strength in 1992 and 1994. Analyses of incident rate ratios for mortality over 4- and 6-year periods were conducted using Cox hierarchical regression analyses. Results showed that poor performance on nearly all cognitive variables was associated with mortality, but many of these effects were explained by measures of self-rated health and disease. Significant decline in hearing and cognitive performance also predicted mortality as did incomplete data at Wave 1. Results suggest that poor cognitive performance and cognitive decline in very old adults reflect both biological aging and disease processes.\n"
        },
        {
            "title": "Automatic detection of carotid arteries in computed tomography angiography: a proof of concept protocol.",
            "abstract": "Atherosclerosis is one of the leading causes of mortality in the western world. Computed tomography angiography (CTA) is the conventional imaging method used for pre-surgery assessment of the blood flow within the carotid vessel. In this paper, we present a proof of concept of a novel, fast and operator independent protocol for the automatic detection (seeding) of the carotid arteries in CTA in the thorax and upper neck region. The dataset is composed of 14 patients' CTA images of the neck region. The performance of this method is compared with manual seeding by four trained operators. Inter-operator variation is also assessed based on the dataset. The minimum, average and maximum coefficient of variation among the operators was (0, 2, 5 %), respectively. The performance of our method is comparable with the state of the art alternative, presenting a detection rate of 75 and 71 % for the lowest and uppermost image levels, respectively. The mean processing time is 167 s per patient versus 386 s for manual seeding. There are no significant differences between the manual and automatic seed positions in the volumes (p = 0.29). A fast, operator independent protocol was developed for the automatic detection of carotid arteries in CTA. The results are encouraging and provide the basis for the creation of automatic detection and analysis tools for carotid arteries."
        },
        {
            "title": "Migraine and Risk of Ocular Motor Cranial Nerve Palsies: A Nationwide Cohort Study.",
            "abstract": "Purpose:\n        \n      \n      To determine whether migraine is associated with an increased risk of developing ocular motor cranial nerve palsies (OMCNP).\n    \n\n\n          Design:\n        \n      \n      Nationwide retrospective cohort study.\n    \n\n\n          Participants:\n        \n      \n      Medical records of patients with migraine who were entered in the National Health Insurance Research Database (NHIRD) between 2005 and 2009 were retrieved from the NHIRD in Taiwan. Two cohorts were selected: patients with migraine (n = 138 907) and propensity score-matched controls (n = 138 907).\n    \n\n\n          Main outcome measures:\n        \n      \n      Cohorts were followed until the end of 2010, death, or occurrence of cranial nerve (CN)3, CN4, or CN6 palsies. A Cox proportional hazards regression model was used to calculate the hazard ratios (HRs) and 95% confidence intervals (CIs), which were used to compare to the risk of developing CN3, CN4, and CN6 palsy between cohorts.\n    \n\n\n          Results:\n        \n      \n      After a mean follow-up period of 3.1 years (range, 1-6 years), the migraine cohort exhibited a greater risk of developing subsequent CN3, CN4, and CN6 palsies compared with the control cohort (HR, 2.67, P < 0.001; HR, 4.23, P < 0.001; HR, 3.37; P < 0.001). This finding was maintained after excluding potential confounders during sensitivity tests. Moreover, the significant association between migraine and OMCNP remained after we adjusted for potential risk factors of microvascular ischemia. However, different migraine subtypes showed no significant differences.\n    \n\n\n          Conclusions:\n        \n      \n      Migraine is an unrecognized risk factor for OMCNP development in adults. Further studies are needed to validate our findings and to delineate the exact pathophysiologic mechanisms linking migraine and OMCNP."
        },
        {
            "title": "Long-term impact of dry eye symptoms on vision-related quality of life after phacoemulsification surgery.",
            "abstract": "Purpose:\n        \n      \n      To observe the long-term changes in dry eye symptoms and vision-related quality of life in age-related cataract patients after phacoemulsification.\n    \n\n\n          Methods:\n        \n      \n      A total of 101 cataract patients after phacoemulsification combined with IOL implantation (Ph-IOL) in one eye were enrolled. Visual acuity, tear film breakup time (BUT), and Schirmer test 1 (ST1) were measured before and 1, 3, and 6 months after surgery. Ocular Surface Disease Index (OSDI) scores were used to evaluate the severity of dry eye symptoms. Utility values were assessed by the time trade-off (TTO), standard gamble for death (SGD), standard gamble for blindness (SGB) and rating scale (RS).\n    \n\n\n          Results:\n        \n      \n      The average LogMAR visual acuity in the operated eye was 1.35 ± 0.50 and increased rapidly after Ph-IOL, approaching a peak at 3 months (0.26 ± 0.15). The BUT and ST1 results decreased abruptly 1 month after surgery and gradually recovered until 6 months. OSDI scores increased significantly after surgery and gradually decreased until 6 months. Utility values evaluated by TTO, SGD, SGB and RS before surgery were 0.67 ± 0.19, 0.75 ± 0.15, 0.67 ± 0.20 and 0.2 ± 0.18, respectively, and increased to 0.91 ± 0.06, 0.98 ± 0.04, 0.92 ± 0.52 and 0.91 ± 0.06, 6 months after. Utility values measured with TTO, SGB or RS correlated significantly (P < 0.05) with visual acuity and OSDI scores pre- and postoperatively.\n    \n\n\n          Conclusions:\n        \n      \n      Dry eye symptoms persist more than 3 months after Ph-IOL. Utility values were negatively influenced by dry eye symptoms."
        },
        {
            "title": "Forecasting the prognosis of choroidal melanoma with an artificial neural network.",
            "abstract": "Purpose:\n        \n      \n      To develop an artificial neural network (ANN) that will forecast the 5-year mortality from choroidal melanoma.\n    \n\n\n          Design:\n        \n      \n      Retrospective, comparative, observational cohort study.\n    \n\n\n          Participants:\n        \n      \n      One hundred fifty-three eyes of 153 consecutive patients with choroidal melanoma (age, 58.4+/-14.6 years) who were treated with ruthenium 106 brachytherapy between 1988 and 1998 at the Department of Ophthalmology, Hadassah University Hospital, Jerusalem, Israel.\n    \n\n\n          Methods:\n        \n      \n      Patients were observed clinically and ultrasonographically (A- and B-mode standardized ultrasonography). Metastatic screening included liver function tests and liver imaging. Backpropagation ANNs composed of 3 or 4 layers of neurons with various types of transfer functions and training protocols were assessed for their ability to predict the 5-year mortality. The ANNs were trained on 77 randomly selected patients and tested on a different set of 76 patients. Artificial neural networks were compared based on their sensitivity, specificity, forecasting accuracy, area under the receiver operating curves, and likelihood ratios (LRs). The best ANN was compared with the results of logistic regression and the performance of an ocular oncologist.\n    \n\n\n          Main outcome:\n        \n      \n      The ability of the ANNs to forecast the 5-year mortality from choroidal melanoma.\n    \n\n\n          Results:\n        \n      \n      Thirty-one patients died during the follow-up period of metastatic choroidal melanoma. The best ANN (one hidden layer of 16 neurons) had 84% forecasting accuracy and an LR of 31.5. The number of hidden neurons significantly influenced the ANNs' performance (P<0.001). The performance of the ANNs was not significantly influenced by the training protocol, the number of hidden layers, or the type of transfer function. In comparison, logistic regression reached 86% forecasting accuracy, with a very low LR (0.8), whereas the human expert forecasting ability was <70% (LR, 1.85).\n    \n\n\n          Conclusions:\n        \n      \n      Artificial neural networks can be used for forecasting the prognosis of choroidal melanoma and may support decision-making in treating this malignancy."
        },
        {
            "title": "Socioeconomic status, systolic blood pressure and intraocular pressure: the Tanjong Pagar Study.",
            "abstract": "Background:\n        \n      \n      Lower socioeconomic status (SES) is associated with higher morbidity and mortality in many countries. Present evidence suggests that glaucoma has similar risk factors to major chronic diseases such as cardiovascular disease. This study investigates the association between SES and intraocular pressure (IOP), an important risk factor for glaucoma.\n    \n\n\n          Methods:\n        \n      \n      The Tanjong Pagar Study was a population-based cross-sectional survey of Chinese people aged 40-79 years, who were randomly selected from the Singapore electoral register. Of the 2000 people selected, 1717 were considered eligible and 1090 were examined in clinic and included in the present study. IOP was measured using applanation tonometry. SES was assessed using a standardised questionnaire; education and income were used as the main explanatory variables. The effect of systolic blood pressure (SBP) was also examined.\n    \n\n\n          Results:\n        \n      \n      Participants with lower levels of education and income had higher mean IOP (both p<0.01). These associations remained after adjusting for age and central corneal thickness, a strong independent predictor. SBP was strongly associated with both SES and IOP (both p<0.01). Adjusting for SBP attenuated the association between SES and IOP.\n    \n\n\n          Conclusion:\n        \n      \n      Participants with lower education and income have a higher mean IOP. This effect may be mediated, in part, by an association of education and income with SBP. This is the first study to suggest that there is a social gradient in the distribution of the only major modifiable risk factor for glaucoma. Increasing similarities exist between the causation models of chronic diseases and that of glaucoma."
        },
        {
            "title": "Stakeholders in outcome measures: review from a clinical perspective.",
            "abstract": "Modern interest in patient-reported outcomes measures (PROMs) in orthopaedics dates back to the mid-1980s. While gradual growth of activity in this area has occurred over the past 25 years, the extent to which this research methodology is applied in clinical practice to improve patient care is unclear. WHERE ARE WE NOW?: Historically, clinical research in orthopaedics has focused on the technical success of treatment, and objective indicators such as mortality, morbidity, and complications. By contrast, the PROMs framework focuses on effects of treatment described in terms of relief of symptoms, restoring functional ability, and improving quality of life. PROMs can be used to study the relative effects of disease, injury, and treatment across different health conditions. WHERE DO WE NEED TO GO?: All clinical research should begin with identifying clear and meaningful research questions so that the resources and efforts required for data collection result in useful data. Different consumers of research data have different perspectives on what comprises meaningful information. Involving stakeholders such as patients, providers, payers, and policy-makers when defining priorities in the larger research endeavor is one way to inform what type of data should be collected in a particular study. HOW DO WE GET THERE?: Widespread collection of outcomes data would potentially aid these stakeholders by identifying best practices, benefits and costs, and important patient or practice characteristics related to outcomes. Several initiatives currently underway may help systematic collection of PROMs, create efficient systems, and foster collaborations to provide support and resources to minimize costs."
        },
        {
            "title": "Clinical and Administrative Data on the Research of Acute Coronary Syndrome in Spain. Minimum Basic Data Set Validity.",
            "abstract": "Introduction and objectives:\n        \n      \n      Health outcomes research is done from clinical registries or administrative databases. The aim of this work was to evaluate the concordance of the Minimum Basic Data Set (MBDS) with the DIOCLES (Descripción de la Cardiopatía Isquémica en el Territorio Español) registry and to analyze the implications of use of the MBDS in the study of acute coronary syndrome in Spain.\n    \n\n\n          Methods:\n        \n      \n      Through indirect identifiers, DIOCLES was linked with MBDS and unique matches were selected. Some of most relevant variables for risk adjustment of in-hospital mortality due to acute myocardial infarction were considered. Kappa coefficient was used to evaluate the concordance; sensitivity, specificity and positive and negative predictive values to measure the validity of the MBDS, and the area under ROC (receiver operating characteristic) curve to calculate its discrimination. The results were compared among hospitals quintiles according to their contribution to DIOCLES. The influence of unmatched episodes on results was assessed by a sensitivity analysis, using looser linking criteria.\n    \n\n\n          Results:\n        \n      \n      Overall, 1539 (60.85%) unique matches were achieved. The prevalence was higher in DIOCLES (acute myocardial infarction: 71.09%; Killip 3-4: 9.17%; cerebrovascular accident: 0.97%; thrombolysis: 8.64%; angioplasty: 61.92% and coronary bypass: 1.75%) than in the MBDS (P < .001). The agreement level observed was almost perfect (κ = 0.863). The MBDS showed a sensitivity of 85.10% and a specificity of 98.31%. Most results were confirmed by using sensitivity analysis (79.95% episodes matched).\n    \n\n\n          Conclusions:\n        \n      \n      The MBDS can be a useful tool for outcomes research of acute coronary syndrome in Spain. The contrast of DIOCLES and MBDS with medical records could verify their validity."
        },
        {
            "title": "Risk of emergent carotid endarterectomy varies by type of presenting symptoms.",
            "abstract": "Background:\n        \n      \n      The timing of carotid revascularization in symptomatic patients is a matter of ongoing debate. Current evidence indicates that carotid endarterectomy (CEA) within 2 weeks of symptoms is superior to delayed treatment. However, there is little evidence on the outcomes of emergent CEA (eCEA). The purpose of this study was to compare outcomes of emergency eCEA vs nonemergent CEA (non-eCEA), stratified by type of presenting symptoms.\n    \n\n\n          Methods:\n        \n      \n      We analyzed the Vascular Targeted-National Surgical Quality Improvement Program dataset from 2011 to 2016. Symptomatic patients were divided into two groups: eCEA and non-eCEA. Univariable and multivariable methods were used to compare patient characteristics and to evaluate stroke, death, myocardial infarction (MI), stroke/death, and stroke/death/MI within 30 days of surgery adjusting for all potential confounders. A further subgroup analysis was done to compare the outcomes of eCEA vs non-eCEA stratified by the type of presenting symptoms (amaurosis, transient ischemic attack [TIA], and stroke).\n    \n\n\n          Results:\n        \n      \n      A total of 9271 patients were identified, of which 10.7% were eCEA vs 89.3% non-eCEA. Comparing eCEA vs non-eCEA, the two groups were similar in age (70.8 vs 70.5), female gender (36.3% vs 36.9%), diabetes (26.2% vs 28.9%), and smoking status (31.9% vs 28.7%; all P > .05). Patients undergoing eCEA were less likely to be hypertensive (76.2% vs 80.2%; P = .025), but more likely to belong to non-white race (51.5% vs 20.5%; P < .001). The eCEA patients were less likely to be on preprocedural medication vs non-eCEA (antiplatelets, 76.8% vs 89.2%; statins, 74.2% vs 79.9%; beta-blockers, 44.6% vs 50.4%; all P < .05). The 30-day outcomes comparing eCEA vs non-eCEA were: stroke, 6.2% vs 3.1%; death, 2% vs 1%; and stroke/death, 6.9% vs 3.7% (all P < .05). After risk adjustment, perioperative stroke (odds ratio [OR], 2.04; 95% confidence interval [CI], 1.36-3.0), stroke/death (OR, 1.66; 95% CI, 1.13-2.45), and stroke/death/MI (OR, 1.58; 95% CI, 1.18-2.23) were higher after eCEA (all P < .01). When stratified by the type of presenting symptom, eCEA vs non-eCEA stroke outcomes were similar in patients who presented with stroke or amaurosis fugax. However, in the subset of patients presenting with TIA, eCEA had much worse outcomes compared with non-eCEA (stroke, 8.3% vs 2.5%; stroke/death, 8.3% vs 3.2%) and had significantly higher odds of stroke (OR, 3.12; 95% CI, 1.71-5.68) and stroke/death (OR, 2.24; 95% CI, 1.25-4.03) in the adjusted analysis (all P < .05).\n    \n\n\n          Conclusions:\n        \n      \n      In patients presenting with stroke, eCEA does not seem to add significant risk compared with non-eCEA. However, patients presenting with TIA might be better served with non-emergent surgery as their risk of stroke is tripled when CEA is performed emergently."
        },
        {
            "title": "Mild retinopathy is a risk factor for cardiovascular mortality in Japanese with and without hypertension: the Ibaraki Prefectural Health Study.",
            "abstract": "Background:\n        \n      \n      It is unclear whether mild hypertensive retinopathy is a risk factor for mortality. This study examined whether mild hypertensive retinopathy could be a risk factor for cardiovascular mortality in subjects with and without hypertension.\n    \n\n\n          Methods and results:\n        \n      \n      In this cohort study, 87 890 individuals (29 917 men and 57 973 women) 40 to 79 years of age in 1993 were followed up until 2008. Retinal photography was classified as normal, grade 1, or grade 2 based on the Keith-Wagener-Barker system. Risk ratios for all-cause and cause-specific mortality for each classification were calculated with Cox proportional hazards regression models. Covariates included age, systolic blood pressure, antihypertensive medication use, and other cardiovascular risk factors. Multivariable hazard ratios for total cardiovascular disease mortality were 1.24 (95% confidence interval [CI], 1.12-1.38) and 1.23 (95% CI, 1.03-1.47) for grades 1 and 2 among men and 1.12 (95% CI, 1.01-1.24) and 1.44 (95% CI, 1.24-1.68) for grades 1 and 2 among women, respectively. Hazard ratios for total stroke mortality were 1.31 (95% CI, 1.13-1.53) and 1.38 (95% CI, 1.08-1.77) for grades 1 and 2 among men and 1.30 (95% CI, 1.12-1.50) and 1.70 (95% CI, 1.36-2.11) for grades 1 and 2 among women, respectively. For both hypertensive and normotensive subjects of each sex, multivariable hazard ratios for all-cause mortality, total cardiovascular mortality, and total stroke mortality were significantly higher for grade 1 or 2 compared with normal.\n    \n\n\n          Conclusions:\n        \n      \n      Mild hypertensive retinopathy is a risk factor for cardiovascular mortality independently of cardiovascular risk factors among men and women with and without hypertension."
        },
        {
            "title": "Colorectal cancer screening: making sense of the different guidelines.",
            "abstract": "Screening for colorectal cancer, as called for by new guidelines from three different groups, should result in a lower mortality rate from this disease. This paper reviews the guidelines' similarities and differences and gives our recommendations for situations in which the data remain incomplete and controversy persists."
        },
        {
            "title": "Showing no spot sign is a strong predictor of independent living after intracerebral haemorrhage.",
            "abstract": "Background:\n        \n      \n      A spot sign on computed tomography angiography (CTA) is a potentially strong predictor of poor outcome on ultra-early radiological imaging. The aim of this study was to assess the spot sign as a predictor of functional outcome at 3 months as well as long-term mortality, with a focus on the ability to identify patients with a spontaneous, acceptable outcome.\n    \n\n\n          Methods:\n        \n      \n      In a prospective, consecutive single-centre registry of acute stroke patients, we investigated patients with spontaneous intracerebral haemorrhage (ICH) admitted within 4.5 h after symptom onset from April 2009 to January 2013. The standard work-up in our centre included CTA for spot sign status, unless a contraindication was present. Modified Rankin Scale (mRS) scores were assessed at 3 months in the outpatient clinic or by telephone interviews. Long-term mortality was assessed by electronic chart follow-up for up to 1,500 days.\n    \n\n\n          Results:\n        \n      \n      Of the 128 patients, 37 (28.9%) had a spot sign on admission CTA. The presence of a spot sign was associated with larger median admission haematoma volume [38.0 ml (IQR 18.0-78.0) vs. 12.0 ml (5.0-24.0); p<0.0001] and higher median National Institutes of Health Stroke Scale score [19 (IQR 12-23) vs. 12 (6-16); p<0.0001]. Three months after stroke, the median functional outcome was considerably better in patients without spot sign [mRS score 3 (IQR 2-4) vs. 6 (4-6); p<0.0001]. The absence of a spot sign showed a sensitivity and specificity for good outcome (mRS scores 0-2) of 0.91 and 0.36, respectively. The presence of a spot sign was, in multivariate models, an independent inverse predictor of good 3-month outcome (OR 0.17; 95% CI: 0.03-0.88) as well as a prominent independent predictor of poor 3-month outcome (mRS scores 5-6; OR 3.40; 95% CI: 1.10-10.5) and death during follow-up (HR 3.04; 95% CI: 1.45-6.34). Patients with a spot sign surviving the acute phase had long-term survival comparable to patients with no spot sign.\n    \n\n\n          Conclusion:\n        \n      \n      The absence or presence of a spot sign is a reliable ultra-early predictor of long-term mortality and functional outcome in patients with spontaneous ICH."
        },
        {
            "title": "Breast cancer screening controversies.",
            "abstract": "Background:\n        \n      \n      The Cochrane Collaborative, a respected independent review body, recently published a meta-analysis of the effectiveness of screening mammography in decreasing breast cancer mortality. Based on the results of two controlled trials they judged to be of medium validity, they concluded that screening mammography was unjustified. In contrast, the US Preventive Services Task Force recently updated their screening recommendations, and based on a meta-analysis of the same randomized controlled trials, they recommended screening mammography for all women starting at age 40 years. Additionally the Canadian Task Force on Preventive Health Care no longer recommends breast self-examination (BSE). This article reviews the controversies regarding breast cancer screening.\n    \n\n\n          Methods:\n        \n      \n      We performed a systematic review of the literature using keywords and cross-referencing articles. We also used automated data from the Breast Cancer Screening Program at Group Health Cooperative to determine the sensitivity of the clinical breast examination (CBE) at our institution. For the latter we included all cancers diagnosed within 1 year of a screening examination and then determined which of those had been found by CBE.\n    \n\n\n          Results:\n        \n      \n      Although most screening studies have shown that mammography decreases breast cancer death, there are controversies about the validity of some of the randomized controlled screening mammography trials. These controversies have led to different conclusions about the efficacy of screening mammography. Evidence is limited about the optimal interval for screening mammography. No studies have directly tested the efficacy of the CBE in decreasing breast cancer mortality. At Group Health Cooperative, 8% of all diagnosed breast cancers were found by the CBE alone (negative mammogram). Whether this 8% incremental increase in case finding leads to decreased breast cancer deaths is unknown. There is good evidence that training women to perform BSE does not increase breast cancer diagnoses or decrease breast cancer deaths.\n    \n\n\n          Conclusion:\n        \n      \n      There are limitations to randomized controlled trials and meta-analyses. The balance of the evidence still favors screening mammography in women aged 40 years and older at least every 2 years. The independent incremental benefit of the CBE, when added to mammography, in decreasing breast cancer mortality is unknown. Population-based education and training to do BSE are unlikely to lead to decreased breast cancer deaths. Many women find their own breast cancers, so women need to pay attention to symptoms or changes in their breasts."
        },
        {
            "title": "Factors Influencing Transitions Between Frailty States in Elderly Adults: The Progetto Veneto Anziani Longitudinal Study.",
            "abstract": "Objectives:\n        \n      \n      To investigate frailty state transitions in a cohort of older Italian adults to identify factors exacerbating or improving frailty conditions.\n    \n\n\n          Design:\n        \n      \n      Population-based longitudinal study with mean follow-up of 4.4 years.\n    \n\n\n          Setting:\n        \n      \n      Community.\n    \n\n\n          Participants:\n        \n      \n      Individuals enrolled in the Progetto Veneto Anziani (Pro.V.A.) (N = 2,925; n = 1,179 male, n = 1,746 female; mean age 74.4 ± 7.3).\n    \n\n\n          Measurements:\n        \n      \n      Frailty was identified at baseline and follow-up based on the presence of at least three Fried criteria; prefrailty was defined as the presence of one or two Fried criteria. Anthropometric, socioeconomic, and clinical characteristics were assessed at baseline in a personal interview and clinical examination using validated scales and medical history.\n    \n\n\n          Results:\n        \n      \n      During the study period, 1,114 (38.1%) subjects retained their baseline frailty status, 1,066 (36.4%) had a transition in frailty status, and the remainder of the sample died. Older age, female sex, obesity, cardiovascular disease, osteoarthritis, smoking, loss of vision, low levels of self-sufficiency and physical performance, cognitive impairment, hypovitaminosis D, hyperuricemia, and polypharmacy were associated with increasing frailty and greater mortality. Conversely, overweight, low to moderate drinking, high educational level, and living alone were associated with decreasing frailty.\n    \n\n\n          Conclusions:\n        \n      \n      Frailty was confirmed as a dynamic syndrome, with socioeconomic and clinical factors that could be targets of preventive actions influencing transitions to better or worse frailty status."
        },
        {
            "title": "Balthazar computed tomography severity index is superior to Ranson criteria and APACHE II scoring system in predicting acute pancreatitis outcome.",
            "abstract": "Aim:\n        \n      \n      Acute pancreatitis (AP) is a process with variable involvement of regional tissues or organ systems. Multifactorial scales included the Ranson, Acute Physiology and Chronic Health Evaluation (APACHE II) systems and Balthazar computed tomography severity index (CTSI). The purpose of this review study was to assess the accuracy of CTSI, Ranson score, and APACHE II score in course and outcome prediction of AP.\n    \n\n\n          Methods:\n        \n      \n      We reviewed 121 patients who underwent helical CT within 48 h after onset of symptoms of a first episode of AP between 1999 and 2003. Fourteen inappropriate subjects were excluded; we reviewed the 107 contrast-enhanced CT images to calculate the CTSI. We also reviewed their Ranson and APACHE II score. In addition, complications, duration of hospitalization, mortality rate, and other pathology history also were our comparison parameters.\n    \n\n\n          Results:\n        \n      \n      We classified 85 patients (79%) as having mild AP (CTSI <5) and 22 patients (21%) as having severe AP (CTSI > or =5). In mild group, the mean APACHE II score and Ranson score was 8.6+/-1.9 and 2.4+/-1.2, and those of severe group was 10.2+/-2.1 and 3.1+/-0.8, respectively. The most common complication was pseudocyst and abscess and it presented in 21 (20%) patients and their CTSI was 5.9+/-1.4. A CTSI > or =5 significantly correlated with death, complication present, and prolonged length of stay. Patients with a CTSI > or =5 were 15 times to die than those CTSI <5, and the prolonged length of stay and complications present were 17 times and 8 times than that in CTSI <5, respectively.\n    \n\n\n          Conclusion:\n        \n      \n      CTSI is a useful tool in assessing the severity and outcome of AP and the CTSI > or =5 is an index in our study. Although Ranson score and APACHE II score also are choices to be the predictors for complications, mortality and the length of stay of AP, the sensitivity of them are lower than CTSI."
        },
        {
            "title": "Donor and tissue profile of a community eye bank in Eastern India.",
            "abstract": "Purpose:\n        \n      \n      The purpose of this study is to analyze the donor and tissue profile of a community eye bank in Eastern India.\n    \n\n\n          Materials and methods:\n        \n      \n      Eye bank records were analyzed for the period July 2007-June 2011. Variables analyzed included donor demographics (age, gender, and ethnicity), donor cause of death, consent for recovery, death-to-preservation interval, preservation-to-utilization interval, endothelial cell density (ECD), corneal suitability for transplantation, and corneal tissue utilization.\n    \n\n\n          Results:\n        \n      \n      During this study period, 743 corneal tissues were retrieved from 373 donors (male:female = 263:110). The mean age of donors was 52 ± 21 years (range: 3-95 years). The most common donor age group was 41-50 and 71-80 years. Most of the donors belonged to one religious faith (99%). The most common causes of death were cardiorespiratory failure (34%) followed by road traffic accident (30%). Majority donors were motivated (n = 320; 86%), and remaining (n = 53; 14%) were voluntary. Most of the consents were given by sons or daughters of the deceased (45%) followed by siblings (18%). Mean death-to-preservation interval was 3.9 ± 1.9 h. Mean preservation-to-utilization interval was 56.0 ± 24.4 h. The mean ECD of donor corneal tissue was 2857 ± 551 cells/mm 2 and the median value was 2898 cells/mm 2 . Of harvested corneas 556 (75%) corneal tissues were utilized. The most common causes of nonutilization were septicemia in donor (n = 56; 30%) and poor quality of tissue (n = 55; 30%).\n    \n\n\n          Conclusions:\n        \n      \n      Although, there is significant corneal tissue utilization, there is a need for increased awareness among people in order to augment voluntary donations."
        },
        {
            "title": "Long-term follow-up of Ru-106/Rh-106 brachytherapy for posterior uveal melanoma.",
            "abstract": "Background:\n        \n      \n      Currently available information about survival, local tumor control rates, secondary enucleation rates, and visual acuity following Ru-106/Rh-106 applicator therapy for choroidal and ciliochoroidal melanomas is limited in terms of duration of follow-up among surviving patients.\n    \n\n\n          Methods:\n        \n      \n      The authors performed a retrospective descriptive study of the rates of survival, local treatment failure, secondary enucleation, and visual acuity decrease in 140 patients (141 eyes) with choroidal or ciliochoroidal melanoma treated by Ru-106/Rh-106 applicator radiotherapy between 1964 and 1976. Median follow-up duration among surviving patients in this series was 17.3 years (mean 18.6 years). The Kaplan-Meier method was used to estimate cumulative survival rates and event rate curves. Multivariate Cox proportional hazards modeling was used to identify prognostic clinical variables associated with the various evaluated outcomes.\n    \n\n\n          Results:\n        \n      \n      The 15-year survival rate based on all causes of death was 48.0% (standard error=4.4%), and that based on confirmed and suspected metastatic uveal melanoma was 66.7% (standard error=4.5%). The cumulative 15-year rates of local treatment failure and secondary enucleation were 36.8% (standard error=4.4%) and 34.4% (standard error=4.5%) respectively. The cumulative 10-year rates of visual acuity loss to less than 20/200 and no light perception were 62.8% (standard error=4.4%) and 40.6% (standard error=5.2%) respectively. Prognostic factors associated with death from confirmed and suspected metastatic melanoma were greater tumor diameter and anterior tumor location. Greater tumor diameter was associated with local treatment failure. Baseline visual acuity equal to or worse than 20/200 was associated with profound visual acuity loss.\n    \n\n\n          Conclusions:\n        \n      \n      Although a high proportion of treated eyes eventually lost a great deal of vision, and although many treated eyes ultimately underwent secondary enucleation, a substantial number of patients treated by plaque radiotherapy in this series survived for well over 10 years and retained the tumor-containing eye with a visual decrease of varying severity."
        },
        {
            "title": "Rapid detection of coronary artery stenoses with real-time perfusion echocardiography during regadenoson stress.",
            "abstract": "Background:\n        \n      \n      Real-time myocardial contrast echocardiography permits the detection of myocardial perfusion abnormalities during stress echocardiography, which may improve the accuracy of the test in detecting coronary artery stenoses. We hypothesized that this technique could be used after a bolus injection of the selective A2A receptor agonist regadenoson to rapidly and safely detect coronary artery stenoses.\n    \n\n\n          Methods and results:\n        \n      \n      In 100 patients referred for quantitative coronary angiography, real-time myocardial contrast echocardiography was performed during a continuous intravenous infusion of 3% Definity at baseline and at 2-minute intervals for up to 6 minutes after a regadenoson bolus injection (400 μg). Myocardial perfusion was assessed by examination of myocardial contrast replenishment after brief high mechanical index impulses. A perfusion defect was defined as a delay (>2 seconds) in myocardial contrast replenishment in 2 contiguous segments. Wall motion was also analyzed. The overall sensitivity/specificity/accuracy for myocardial perfusion analysis in detecting a >50% diameter stenosis was 80%/74%/78%, whereas for wall motion analysis it was 60%/72%/66% (P<0.001 for differences in sensitivity). Sensitivity for myocardial perfusion analysis was highest on images obtained during the first 2 minutes after regadenoson bolus (P<0.001 compared with wall motion), whereas wall motion sensitivity was highest at the 4-to-6-minute period after the bolus. No significant side effects occurred after regadenoson bolus injection.\n    \n\n\n          Conclusions:\n        \n      \n      Regadenoson real-time myocardial contrast echocardiography appears to be a feasible, safe, and rapid noninvasive method for the detection of significant coronary artery stenoses."
        },
        {
            "title": "Validation of National Cardiovascular Data Registry risk models for mortality, bleeding and acute kidney injury in interventional cardiology at a German Heart Center.",
            "abstract": "Background and purpose:\n        \n      \n      The National Cardiovascular Data Registry (NCDR) risk scores for mortality, bleeding and acute kidney injury (AKI) are accurate outcome predictors of coronary catheterization procedures in North American populations. However, their application in German clinical practice remained elusive and we thus aimed to verify their use.\n    \n\n\n          Methods:\n        \n      \n      NCDR scores for mortality, bleeding and AKI and corresponding clinical outcomes were retrospectively assessed in patients undergoing catheterization for ST-segment elevation myocardial infarction (STEMI), non-ST-segment elevation myocardial infarction (NSTEMI) or for elective coronary procedures at a German Heart Center from 2014 to 2017. Risk model performance was assessed using receiver-operating-characteristic curves (discrimination) and graphical analysis/logistic regression (calibration).\n    \n\n\n          Results:\n        \n      \n      A total of 1637 patients were included, procedures were performed for STEMI (565 patients, 34.5%), NSTEMI (572 patients, 34.9%) and elective purposes (500 patients, 30.5%); 6% (13% of STEMI and 5% of NSTEMI patients) presented in cardiogenic shock and 3% with resuscitated cardiac arrest. Radial access was used in 38% of procedures and cross-over was necessary in 5%; PCI was performed in 60% of procedures. In-hospital mortality was 6.3% (STEMI 14.5%; NSTEMI 3.7%; elective 0%) and major bleedings occurred in 5.6% (STEMI 10.6%; NSTEMI 5.4%; elective 0.2%); AKI was detected in 18.1% of patients (STEMI 23.7%; NSTEMI 27.3%; elective 1.4%), amounting to KDIGO stage I/II/III in 11.5%/3.5%/3.2%. NCDR risk models discriminated very well for mortality [AUC 0.93 with 95% confidence interval (CI) 0.91-0.95] and well for major bleeding (AUC 0.82, CI 0.78-0.86) and any AKI (AUC 0.83, CI 0.81-0.86). Discrimination in the subgroup of patients with PCI was comparable (mortality: AUC 0.90; major bleeding: AUC 0.78; any AKI: AUC 0.79). However, calibration showed considerable underestimation of mortality and AKI in high-risk patients, while major bleeding was consistently overestimated (Hosmer-Lemeshow p < 0.02 for all outcomes).\n    \n\n\n          Conclusions:\n        \n      \n      The NCDR risk models showed excellent performance in discriminating high-risk from low-risk patients in contemporary German interventional cardiology. Model calibration for adverse event probability prediction, however, is limited and demands recalibration, especially in high-risk patients."
        },
        {
            "title": "Surrogate Imaging Biomarkers of Response of Colorectal Liver Metastases After Salvage Radioembolization Using 90Y-Loaded Resin Microspheres.",
            "abstract": "Objective:\n        \n      \n      The purpose of the present study is to evaluate Response Evaluation Criteria in Solid Tumors (RECIST) version 1.1, tumor attenuation criteria, Choi criteria, and European Organization for Research and Treatment of Cancer (EORTC) PET criteria as measures of response and subsequent predictors of liver progression-free survival (PFS) after radioembolization (RE) of colorectal liver metastases (CLM). The study also assesses interobserver variability for measuring tumor attenuation using a single 2D ROI on a simple PACS workstation.\n    \n\n\n          Materials and methods:\n        \n      \n      We performed a retrospective review of the clinical RE database at our institution, to identify patients treated in the salvage setting for CLM between December 2009 and March 2013. Response was evaluated on FDG PET scans, with the use of EORTC PET criteria, and on portal venous phase CT scans, with the use of RECIST 1.1, tumor attenuation criteria, and Choi criteria. Two independent blinded observers measured tumor attenuation using a single 2D ROI. The intraclass correlation coefficient (ICC) for interobserver variability was assessed. Kaplan-Meier methodology was used to calculate liver PFS, and the log-rank test was used to assess the response criteria as predictors of liver PFS.\n    \n\n\n          Results:\n        \n      \n      A total of 25 patients with 46 target tumors were enrolled in the study. The ICC was 0.95 at baseline and 0.98 at response evaluation. Among the 25 patients, more responders (i.e., partial response) were identified on the basis of EORTC PET criteria (n = 14), Choi criteria (n = 15), and tumor attenuation criteria (n = 13) than on the basis of RECIST 1.1 (n = 2). The median liver PFS was 3.0 months (95% CI, 2.1-4.0 months). Response identified on the basis of EORTC PET criteria (p < 0.001), Choi criteria (p < 0.001), or tumor attenuation criteria (p = 0.01) predicted liver PFS; however, response identified by RECIST 1.1 did not (p = 0.1).\n    \n\n\n          Conclusion:\n        \n      \n      RECIST 1.1 has poor sensitivity for detecting metabolic responses classified by EORTC PET criteria. EORTC PET criteria, Choi criteria, and tumor attenuation criteria appear to be equally reliable surrogate imaging biomarkers of liver PFS after RE in patients with CLM."
        },
        {
            "title": "Visual acuity change and mortality in older adults.",
            "abstract": "## PURPOSE\nSeveral studies indicate an increased mortality rate in older adults who have visual impairment, but few have attempted to address a potential causal mechanism. The goals of this study are to determine whether visual acuity loss increases the risk of dying and to examine whether depressive symptoms act as a mediator in this relationship.\n## METHODS\nData were derived from the 2520 older adults who participated in the Salisbury Eye Evaluation project, a population-based prospective 8-year cohort study. Presenting binocular visual acuity was measured with the Early Treatment Diabetic Retinopathy Study [ETDRS] eye chart and depressive symptoms with the General Health Questionnaire Part D subscale. Mortality data were collected by staff follow-up. Analyses were performed with the Cox proportional hazards regression.\n## RESULTS\nWorse baseline acuity was associated with a higher mortality rate (hazard ratio [HR] = 1.05; 95% confidence interval [CI], 1.01-1.09). Also, those who gained two or more lines of visual acuity over 2 years had a lower adjusted risk of dying (HR = 0.47; 95% CI, 0.23-0.95). An interaction was detected, in that women who lost > or =3 lines of visual acuity over a 2-year period had a higher adjusted risk of dying (HR = 3.97; 95% CI, 2.21-7.15), whereas men did not (HR = 1.32; 95% CI, 0.66-2.63). Depressive symptoms did not mediate these relationships.\n## CONCLUSIONS\nIf the relationship between visual acuity and mortality is indeed causal, it most likely acts via numerous pathways through a variety of intervening variables. The identification of these intervening variables could give additional targets for intervention if acuity cannot be restored.\n"
        },
        {
            "title": "Frequent retinal ganglion cell damage after acute optic neuritis.",
            "abstract": "Background:\n        \n      \n      To identify the extent of ganglion cell damage after first-time optic neuritis (ON) using the inter-ocular difference between affected and fellow eyes, and whether this approach is able to detect more patients suffering from ganglion cell damage than using absolute values.\n    \n\n\n          Methods:\n        \n      \n      Thirty-four patients with first-time unilateral ON were followed for a median 413 days. Patients underwent optical coherence tomography testing to determine ganglion cell plus inner plexiform layer thickness (GCIP). Ganglion cell loss was quantified as GCIP difference between ON-affected and fellow eyes (inter-GCIP) and was compared against measurements from 93 healthy controls (HC). Visual function was assessed with high contrast visual acuity; and standard automated perimetry-derived measures of mean deviation and foveal threshold.\n    \n\n\n          Results:\n        \n      \n      At clinical presentation after median 19 days from symptom onset, 47.1% of patients showed early GCIP thinning in the ON-affected eye based on inter-GCIP. At the last visit acute ON was associated with 16.1 ± 10.0 µm GCIP thinning compared to fellow eyes (p = 3.669e-06). Based on inter-GCIP, 84.9% of ON patients sustained GCIP thinning in their affected eye at the last visit, whereas using absolute values only 71.0% of patients suffered from GCIP thinning (p = 0.002076). Only 32.3% of these patients had abnormal visual function. The best predictor of GCIP thinning as a measure of ON severity at the last visit was worse visual field mean deviation at clinical presentation.\n    \n\n\n          Conclusion:\n        \n      \n      Inter-ocular GCIP identifies significantly more eyes suffering damage from ON than absolute GCIP, visual fields or visual acuity loss. Effective interventional options are needed to prevent ganglion cell loss."
        },
        {
            "title": "In whom do cancer survivors trust online and offline?",
            "abstract": "Background:\n        \n      \n      In order to design effective educational intervention for cancer survivors, it is necessary to identify most-trusted sources for health-related information and the amount of attention paid to each source.\n    \n\n\n          Objective:\n        \n      \n      The objective of our study was to explore the sources of health information used by cancer survivors according to their access to the internet and levels of trust in and attention to those information sources.\n    \n\n\n          Materials and methods:\n        \n      \n      We analyzed sources of health information among cancer survivors using selected questions adapted from the 2012 Health Information National Trends Survey (HINTS).\n    \n\n\n          Results:\n        \n      \n      Of 357 participants, 239 (67%) had internet access (online survivors) while 118 (33%) did not (offline survivors). Online survivors were younger (p<0.001), more educated (p<0.001), more non-Hispanic whites (p<0.001), had higher income (p<0.001), had more populated households (p<0.001) and better quality of life (p<0.001) compared to offline survivors. Prevalence of some disabilities was higher among offline survivors including serious difficulties with walking or climbing stairs (p<0.001), being blind or having severe visual impairment (p=0.001), problems with making decisions (p<0.001), doing errands alone (p=0.001) and dressing or bathing (p=0.001). After adjusting for socio- demographic status, cancer survivors who were non-Hispanic whites (OR=3.49, p<0.01), younger (OR=4.10, p<0.01), more educated (OR=2.29, p=0.02), with greater income (OR=4.43, p<0.01), and with very good to excellent quality of life (OR=2.60, p=0.01) had higher probability of having access to the internet, while those living in Midwest were less likely to have access (OR=0.177, p<0.01). Doctors (95.5%) were the most and radio (27.8%) was the least trusted health related information source among all cancer survivors. Online survivors trusted internet much more compared to those without access (p<0.001) while offline cancer survivors trusted health-related information from religious groups and radio more than those with internet access (p<0.001 and p=0.008). Cancer survivors paid the most attention to health information on newsletters (63.8%) and internet (60.2%) and the least to radio (19.6%). More online survivors paid attention to internet than those without access (68.5% vs 39.1%, p<0.001) while more offline survivors paid attention to radio compared to those with access (26.8% vs 16.5%, p=0.03).\n    \n\n\n          Conclusions:\n        \n      \n      Our findings emphasize the importance of improving the access and empowering the different sources of information. Considering that the internet and web technologies are continuing to develop, more attention should be paid to improve access to the internet, provide guidance and maintain the quality of accredited health information websites. Those without internet access should continue to receive health-related information via their most trusted sources."
        },
        {
            "title": "Cognition and mortality from the major causes of death: the Health and Lifestyle Survey.",
            "abstract": "Objective:\n        \n      \n      To investigate the influence of reaction time and cognition on the risk of death from cause-specific mortality and to examine whether any association found remains after adjustment for available socioeconomic, lifestyle, and health factors.\n    \n\n\n          Methods:\n        \n      \n      Participants were from the UK Health and Lifestyle Survey. The sample consisted of 6424 community dwelling individuals aged between 18 and 97 years at baseline (1984/1985). Sociodemographic, lifestyle, health, and physiological information was collected alongside cognitive testing which included simple (SRT) and choice (CRT) reaction time, a short-term memory test, and a test of visual-spatial reasoning. Participants have been followed for 21 years for cause-specific mortality.\n    \n\n\n          Results:\n        \n      \n      Slower and more variable reaction times and poorer cognitive performance were associated with a higher risk of death from cardiovascular disease, stroke, and respiratory disease after controlling for age and sex. Slight attenuation was noted after adjustments for all covariates. However, only CRT mean remained significantly associated with death from respiratory disease. No associations were found for coronary heart disease, lung cancer, and all nonlung cancers. Significant cognition-mortality associations were mostly obtained in those aged over 60 years. The possibility of reverse causality was partly excluded by reanalysing the data after omitting individuals who died within 5 years of cognitive testing.\n    \n\n\n          Conclusions:\n        \n      \n      Slower and more variable reaction times and poorer cognitive performance were related to an increased risk of mortality from cardiovascular disease, stroke, and respiratory disease. The possibility of reverse causality requires further testing."
        },
        {
            "title": "Direct and indirect effects of visual impairment on mortality risk in older persons.",
            "abstract": "Objective:\n        \n      \n      To investigate pathways from visual impairment to increased all-cause mortality in older persons.\n    \n\n\n          Methods:\n        \n      \n      The Blue Mountains Eye Study examined 3654 persons 49 years and older (82.4% response) during 1992-1994 and after 5 and 10 years. Australian National Death Index data confirmed deaths until 2005. Visual impairment was defined as presenting, correctable, and noncorrectable, using better-eye visual acuity. Associations between visual impairment and mortality risk were estimated using Cox regression and structural equation modeling.\n    \n\n\n          Results:\n        \n      \n      After 13 years, 1273 participants had died. Adjusting for mortality risk markers, higher mortality was associated with noncorrectable visual impairment (hazard ratio [HR], 1.35; 95% confidence interval [CI], 1.04-1.75). This association was stronger for ages younger than 75 years (HR, 2.58; 95% CI, 1.42-4.69). Structural equation modeling revealed greater effects of noncorrectable visual impairment on mortality risk (HR, 5.25; 95% CI, 1.97-14.01 for baseline ages <75 years), with both direct (HR, 2.16; 95% CI, 1.11-4.23) and indirect (HR, 2.43; 95% CI, 1.17-5.03) effects. Of mortality risk markers examined, only disability in walking demonstrated a significant indirect pathway for the link between visual impairment and mortality.\n    \n\n\n          Conclusions:\n        \n      \n      Visual impairment predicted mortality by both direct and indirect pathways, particularly for persons younger than 75 years with noncorrectable visual impairment. Disability in walking, which can substantially influence general health, represented a major indirect pathway."
        },
        {
            "title": "Outcomes among valvular heart disease patients experiencing ischemic stroke or transient ischemic attack in Olmsted County, Minnesota.",
            "abstract": "Objective:\n        \n      \n      To estimate the rates and predictors of survival and recurrence among residents of Olmsted County, Minnesota, who received an Initial diagnosis based on 2-dimensional color Doppler echocardiography of moderate or severe mitral or aortic stenosis or regurgitation and who experienced a first ischemic stroke, transient ischemic attack (TIA), or amaurosis fugax.\n    \n\n\n          Patients and methods:\n        \n      \n      At the Mayo Clinic in Rochester, Minn, we used the resources of the Rochester Epidemiology Project to identify Individuals who met the criteria for inclusion in the study and to verify exclusion criteria. The study included all residents of Olmsted County, Minnesota, who experienced a first Ischemic stroke, TIA, or amaurosis fugax within 30 days of or subsequent to receiving a first-time 2-dimensional color Doppler echocardlography-based diagnosis of moderate or severe mitral or aortic stenosis or regurgitation between January 1, 1985, and December 31, 1992. The Kaplan-Meier product-limit method was used to estimate the rates of subsequent stroke and death after the ischemic stroke, TIA, or amaurosis fugax. The Cox proportional hazards model was used to assess the effect of several potential risk factors on subsequent stroke occurrence and death.\n    \n\n\n          Results:\n        \n      \n      For the 125 patients in the study, the Kaplan-Meier estimates of the risk of death and the risk of stroke at 2-year follow-up were 38.6% (95% confidence interval [CI], 29.9%-47.5%) and 18.5% (95% CI, 10.0%-27.0%), respectively. Compared with the general population, death rates were significantly Increased (standardized mortality ratio = 1.75; 95% CI, 1.38-2.19; P < .001) but rates of subsequent stroke occurrence were not (standardized morbidity ratio = 1.20; 95% CI, 0.75-1.84; P = .40). After adjustment for age, sex, and cardiac comorbidity, neither the type nor severity of valvular heart disease was an independent determinant of survival or subsequent stroke occurrence.\n    \n\n\n          Conclusions:\n        \n      \n      Patients with mitral or aortic valvular heart disease who experience Ischemic stroke, TIA, or amaurosis fugax have Increased rates of death, but not recurrent stroke, compared with expected rates. Other cardiovascular risk factors are more important determinants of survival In these patients than the type or echocardiographic severity of the valvular heart disease."
        },
        {
            "title": "The Effect of Opioid Dependence on Firearm Injury Treatment Outcomes: A Nationwide Analysis.",
            "abstract": "Background:\n        \n      \n      Both the opioid and gun violence epidemics are recurrent public health issues in the United States. We sought to determine the effect of opioid dependence on gunshot injury treatment outcomes.\n    \n\n\n          Materials and methods:\n        \n      \n      Using the 2016 National Readmission Database, patients were included if they had a principal diagnosis of firearm injury. Opioid dependence was identified using appropriate International Classification of Diseases, 10th Revision, Clinical Modification codes. The primary outcome was 30-day all-cause readmission. Secondary outcomes were in-hospital and 1-year mortality, resource utilization, and most common reasons for admission and readmission. Confounders were adjusted for using multivariate regression analysis.\n    \n\n\n          Results:\n        \n      \n      A total of 31,303 patients were included, 695 of whom were opioid dependent. Opioid-dependent patients were more likely to be young (35.1 y, range: 33.4-36.7 y) and male (89.9%) compared with patients without opioid dependence. Opioid dependence was associated with higher 30-day readmission rates (adjusted odds ratio [aOR]: 1.67, 95% confidence interval [CI]: 1.12-2.50, P = 0.01). However, opioid dependence was associated with lower in-hospital (aOR: 0.16, CI: 0.07-0.38, P < 0.01) and 1-year (aOR: 0.15, CI: 0.06-0.38, P < 0.01) mortality, longer mean length of stay (adjusted mean difference [aMD]: 2.09 d, CI: 0.43-3.76, P = 0.03), and total hospitalization costs (aMD: $6,318, CI: $ 257-$12,380, P = 0.04). Both groups had similar total hospitalization charges (aMD: $$10,491, CI: -$12,618-$33,600, P-value = 0.37).\n    \n\n\n          Conclusions:\n        \n      \n      Opioid dependence leads to higher rates of 30-day readmission and resource utilization among patients with firearm injuries. However, the in-hospital and 1-year mortality rates are lower among patients with opioid dependence secondary to lower injury acuity."
        },
        {
            "title": "A cross-sectional study of barriers to cervical cancer screening uptake in Ghana: An application of the health belief model.",
            "abstract": "Background:\n        \n      \n      The high incidence (32.9, age-standardized per 100,000) and mortality (23.0, age-standardized per 100,000) of cervical cancer (CC) in Ghana have been largely attributed to low screening uptake (0.8%). Although the low cost (Visual inspection with acetic acid) screening services available at various local health facilities screening uptake is meager.\n    \n\n\n          Objective:\n        \n      \n      The purpose of the study is to determine the barriers influencing CC screening among women in the Ashanti Region of Ghana using the health belief model.\n    \n\n\n          Methods:\n        \n      \n      A analytical cross-sectional study design was conducted between January and March 2019 at Kenyase, the Ashanti Region of Ghana. The study employed self-administered questionnaires were used to collect data from 200 women. Descriptive statistics were used to examine the differences in interest and non-interest in participating in CC screening on barriers affecting CC screening. Multivariable logistic regression was used to determine factors affecting CC screening at a significance level of p<0.05.\n    \n\n\n          Results:\n        \n      \n      Unemployed women were less likely to have an interest in CC screening than those who were employed (adjustes odds ratio (aOR) = 0.005, 95%CI:0.001-0.041, p = 0.005). Women who were highly educated were 122 times very likely to be interested in CC screening than those with no or low formal education (aOR = 121.915 95%CI: 14.096-1054.469, p<0.001) and those who were unmarried were less likely to be interested in CC screening than those with those who were married (aOR = 0.124, 95%CI: 0.024-0.647, p = 0.013). Also, perceived threat, perceived benefits, perceived barriers and cues for action showed significant differences with interest in participating in screening with a P-values <0.003. The association was different for long waiting time, prioritizing early morning and late evening screening which showed no significant difference (P-value > 0.003).\n    \n\n\n          Conclusions:\n        \n      \n      Married women, unemployed and those with no formal education are less likely to participate in CC screening. The study details significant barriers to cervical cancer screening uptake in Ghana. It is recommended that the Ghana health services should develop appropriate, culturally tailored educational materials to inform individuals with no formal education through health campaigns in schools, churches and communities to enhance CC screening uptake."
        },
        {
            "title": "Effects of early contrast-enhanced computed tomography on clinical course and complications in patients with acute pancreatitis.",
            "abstract": "Purpose:\n        \n      \n      To investigate the effect of an early contrast-enhanced computed tomography (CECT) on clinical course and complications of acute pancreatitis (AP).\n    \n\n\n          Material and methods:\n        \n      \n      58 patients with AP who had at least one CECT examination were analyzed retrospectively. Laboratory as well as clinical data, and results from the assessment of disease severity (CT severity index (CTSI) and its modified (MCTSI) version) were analyzed. The primary endpoint was the development of severe complications, defined as death, respiratory failure, acute renal failure, and the need for invasive interventions. Patients were divided into two groups: an early group (CECT within the first 48 h after the onset of symptoms, n = 32) and a late group (CECT > 48 h after the onset of symptoms, n = 26). Multivariate regression analysis was performed to identify risk factors for severe complications.\n    \n\n\n          Results:\n        \n      \n      There were no statistically significant differences between both groups concerning baseline characteristics, CTSI, and MCTSI. Complications occurred more often in the early CECT group (p = 0.008). Multivariate logistic regression analysis identified an early CECT and a severe MCTSI as independent risk factors for the occurrence of severe complications (p = 0.02 and p = 0.002, respectively).\n    \n\n\n          Conclusion:\n        \n      \n      CECT performed within the first 48 h after the onset of symptoms is associated with an unfavorable outcome in AP."
        },
        {
            "title": "Prospective study of high-sensitivity C-reactive protein as a determinant of mortality: results from the MONICA/KORA Augsburg Cohort Study, 1984-1998.",
            "abstract": "Background:\n        \n      \n      C-reactive protein (CRP), an exquisitely sensitive systemic marker of inflammation, has emerged as an independent predictor of cardiovascular diseases (CVD). Because other chronic diseases are also associated with an inflammatory response, we sought to assess the association of high-sensitivity CRP (hsCRP) with total and cause-specific mortality in a large cohort of middle-aged men.\n    \n\n\n          Methods:\n        \n      \n      We measured hsCRP at baseline in 3620 middle-aged men, randomly drawn from 3 samples of the general population in the Augsburg area (Southern 0Germany) in 1984-85, 1989-90, and 1994-95. Outcome was defined as all deaths, fatal CVD, fatal coronary heart disease (CHD) including sudden cardiac deaths, and cancer deaths.\n    \n\n\n          Results:\n        \n      \n      During an average follow-up of 7.1 years, 408 deaths occurred (CVD 196, CHD 129, cancer 127). In multivariable Cox regression analysis, subjects with hsCRP >3 mg/L at baseline showed an almost 2-fold increased risk to die vs those with hsCRP <1 mg/L [hazard ratio (HR) 1.88, 95% CI 1.41-2.52]. HRs were 2.15 (95% CI 1.39-3.34) for fatal CVD, 1.74 (1.04-2.92) for fatal CHD, and 1.65 (1.01-2.68) for cancer mortality. In contrast, neither total nor HDL cholesterol significantly predicted all-cause or cancer mortality, and cholesterol had only modest effects on CVD mortality.\n    \n\n\n          Conclusions:\n        \n      \n      Our results suggest that increased circulating hsCRP concentrations are associated with an increased risk of death from several widespread chronic diseases. Persistently increased hsCRP is a sensitive and valuable nonspecific indicator of an ongoing disease process that deserves serious and careful medical attention."
        },
        {
            "title": "Mixed effects logistic regression models for longitudinal ordinal functional response data with multiple-cause drop-out from the longitudinal study of aging.",
            "abstract": "In the context of analyzing ordinal functional limitation responses from the Longitudinal Study of Aging, we investigate the association between current functional limitation and previous year's limitation and its modification by physical activity and multiple causes of drop-out. We accommodate the longitudinal nature of the multiple causes of informative drop-out (death and unknown loss-to-follow-up) with a mixed effects logistic model. Under the proposed model with a random intercept and slope, the ordinal functional outcome and multiple discrete time survival profiles share a common random effect structure. This shared parameter selection model assumes that the multiple causes of drop-out are conditionally independent of the functional limitation outcome given the underlying random effect representing an individual's trajectory of general health status across time. Although it is not possible to fully assess the adequacy of this assumption, we assess the robustness of the approach by varying the assumptions underlying the proposed model, such as the random effects distribution and the drop-out component. It appears that between-subject differences in initial functional limitation are strongly associated with future functional limitation and that this association is stronger for those who do not have physical activity regardless of the random effects and informative drop-out specifications. In contrast, the association between current functional limitation and previous trajectory of functional status within an individual is weaker and more sensitive to changes in the random effects and drop-out assumptions."
        },
        {
            "title": "Mortality and incidence of renal replacement therapy in people with type 1 diabetes mellitus--a three decade long prospective observational study in the Lainz T1DM cohort.",
            "abstract": "Context and objective:\n        \n      \n      We investigated long term mortality, requirement for renal replacement therapy (RRT), and incidence of other late diabetic complications in an observational cohort study of 641 people with type 1 diabetes (T1DM).\n    \n\n\n          Design:\n        \n      \n      Prospective observational cohort study.\n    \n\n\n          Setting:\n        \n      \n      The study was conducted at a Tertiary Diabetes Centre in Vienna, Austria.\n    \n\n\n          Patients:\n        \n      \n      A cohort with all people with T1DM (n = 641, 47% females, 30 ± 11 years) attending their annual diabetes review was created in 1983-1984. Biomedical data were collected.\n    \n\n\n          Main outcome measures:\n        \n      \n      In 2013 we investigated mortality rates and incidence rates of RRT by record linkage with national registries and incidence of other major diabetes complications by questionnaire.\n    \n\n\n          Results:\n        \n      \n      156 (24%) patients died [mortality rate: 922 (95%CI: 778-1066) per 100 000 person years]. Fifty-five (8.6%) received RRT [incidence rate: 335 (95%CI: 246-423) per 100 000 person years]. The 380 questionnaires (78% return rate) recorded cardiac events, strokes, limb amputations, and/or blindness, affecting 21.8% of survivors. Mortality and incidence of RRT increased in each quartile of baseline HbA1c, with the lowest rates in the quartile with HbA1c ≤ 6.5% (48 mmol/mol) (P < .05).\n    \n\n\n          Conclusions:\n        \n      \n      In people with established type 1 diabetes who were observed for almost three decades, the overall mortality was 24% and the incidence of renal replacement therapy was 8.6%, with a 21.8% combined incidence rate of the other hard endpoints in the surviving people. A clear linear relationship between early glycemic control and the later development of end stage renal disease and mortality has been found."
        },
        {
            "title": "Clinical course and outcome of disseminated intravascular coagulation diagnosed by Japanese Association for Acute Medicine criteria. Comparison between sepsis and trauma.",
            "abstract": "The Japanese Association for Acute Medicine (JAAM) disseminated intravascular coagulation (DIC) study group recently announced new diagnostic criteria for DIC. These criteria have been prospectively validated and demonstrated to progress to overt DIC as defined by the International Society on Thrombosis and Haemostasis (ISTH). Although an underlying condition is essential for the development of DIC, it has never been clarified if patients with different underlying disorders have a similar course. Among 329 patients with DIC diagnosed by the JAAM criteria, those with underlying sepsis (n = 98) or trauma (n = 95) were compared. The 28-day mortality rate was significantly higher in sepsis patients than trauma patients (34.7% vs. 10.5%, p < 0.0001). Within three days of fulfilling the JAAM criteria, sepsis patients had a lower platelet count, higher prothrombin time ratio, higher systemic inflammatory response syndrome score, and higher Sequential Organ Failure Assessment score compared with trauma patients. On day 3, a significantly higher percentage of trauma patients than sepsis patients showed improvement of DIC (64.2% vs. 30.6%, p < 0.001). These differences were mainly due to patients with lower JAAM DIC scores. More than 50% of the JAAM DIC patients with sepsis who died within 28 days could not be detected by ISTH DIC criteria during the initial three days. In contrast, most trauma patients who died within 28 days had DIC simultaneously diagnosed by JAAM and ISTH criteria, except for those with brain death. These findings suggest that coagulation abnormalities, organ dysfunction, and the outcome of JAAM DIC differ between patients with sepsis and trauma."
        },
        {
            "title": "Emotional well-being does not predict survival in head and neck cancer patients: a Radiation Therapy Oncology Group study.",
            "abstract": "Background:\n        \n      \n      The objective of the current study was to examine whether emotional well-being predicted survival in a large sample of patients with head and neck cancer who were participating in multicenter clinical trials.\n    \n\n\n          Methods:\n        \n      \n      Participants were enrolled in 2 Radiation Oncology Group (RTOG) clinical trials (RTOG 9003 and RTOG 9111) and completed a baseline measure of quality of life (the Functional Assessment of Cancer Therapy-General [FACT-G]), which included an Emotional Well-Being subscale. The outcome measure was overall survival. Main statistical analyses included overall survival rates, which were estimated by using the Kaplan-Meier method with univariate comparisons analyzed using the log-rank test. A multivariate Cox proportional hazards model was used to determine whether emotional well-being had prognostic impact on survival after accounting for tumor-related and sociodemographic variables. Additional exploratory analyses examined possible subgroup effects.\n    \n\n\n          Results:\n        \n      \n      No statistically significant univariate or multivariate effects were observed for emotional well-being, and there were no effects limited to subgroups. These results stand in sharp contrast to the prognostic value of a variety of demographic and clinical variables.\n    \n\n\n          Conclusions:\n        \n      \n      The current results add to the weight of the evidence that emotional functioning is not an independent predictor of survival in cancer patients. The study had the advantage of a large number of deaths to be explained in a sample with the uniformity of treatment and quality of care that is required in clinical trials."
        },
        {
            "title": "[Epidemiology of acute cerebrovascular disease in Lleida from 1996 to 1997. Predictive factors of mortality at short and medium term].",
            "abstract": "Introduction:\n        \n      \n      Given the great clinical relevance of the cerebrovascular disease, the incidence, nosology, vascular risk factors and factors predicting short and medium-term survival after stroke were evaluated in Lleida (Spain).\n    \n\n\n          Patients and methods:\n        \n      \n      Five hundred forty-five consecutive patients with an acute stroke admitted to the Hospital Universitario Arnau de Vilanova during the period 1996-1997 were evaluated. A descriptive epidemiological study and a multivariate logistic regression analysis of predictive factors of mortality at 1-month and 1-year after the stroke were made. The latter provided a clinical scoring system for predicting survival.\n    \n\n\n          Results:\n        \n      \n      The incidence rate of stroke was 138.3/100,000 inhabitants. Significant risk factors were hypertension and peripheral vasculopathy. There were 80.1% of ischemic and 19.9% of hemorrhagic strokes (p < 0.001). A Glasgow scale < or = 7, hemianopsia and hemorraghic stroke were significant predictors of 1-month mortality, whereas age > or = 70 years, diabetes, atrial fibrillation and limb weakness decreased survival at 1 year.\n    \n\n\n          Conclusions:\n        \n      \n      The incidence rate of stroke in Lleida is low respect to other studies in Spain. Simple clinical measures may help to establish a prognosis at short and medium-term."
        },
        {
            "title": "In-hospital outcomes comparison of transfemoral vs transapical transcatheter aortic valve replacement in propensity-matched cohorts with severe aortic stenosis.",
            "abstract": "Background:\n        \n      \n      Transcatheter aortic valve replacement (TAVR) is the preferred option for high-risk patients with severe aortic stenosis. The preferred access for TAVR is transfemoral (TF). Alternatives include the transapical (TA), trans-subclavian (TS), and direct aortic (TAo) approaches.\n    \n\n\n          Hypothesis:\n        \n      \n      The TF approach is associated with lower in-hospital outcomes as well as shorter length of stay and lower cost of hospitalization.\n    \n\n\n          Methods:\n        \n      \n      The National Inpatient Sample database from 2012 through 2014 was used to obtain the TAVR study population. International Classification of Diseases, 9th Revision, Clinical Modification procedure codes were utilized to identify the 2 groups. In-hospital outcomes were compared in propensity-score-matched (1:3) cohorts, in which we took TA-TAVR as a control.\n    \n\n\n          Results:\n        \n      \n      A total of 8210 (weighted N = 41 050) patients were identified. Of these, 1622 (weighted N = 8110) patients underwent TA-TAVR and 6588 (weighted N = 32 940) patients underwent TF-TAVR. In-hospital mortality was lower with TF-TAVR (4% vs 5.4%; P = 0.0355), along with a shorter length of stay (7.7 vs 9.7 days; P < 0.0001) and lower median hospitalization cost ($64 216 vs $74 735; P < 0.0001). Secondary outcomes of acute renal failure, transfusion, cardiogenic shock, and composite of all complications were lower with TF-TAVR.\n    \n\n\n          Conclusions:\n        \n      \n      TF-TAVR is safer and associated with lower in-hospital outcomes compared with TA-TAVR and should be the preferred approach. As TAVR is gaining popularity in intermediate- and low-risk patients, we must not lose sight of the serious mortality and secondary outcomes associated with TA-TAVR access."
        },
        {
            "title": "A randomized controlled evaluation of a psychosocial intervention in adults with chronic lung disease.",
            "abstract": "The effect of a stress management program on morbidity and psychosocial and physical function in patients with chronic lung disease was assessed. Adults attending either a VA pulmonary clinic or university hospital pulmonary rehabilitation clinic who met criteria for obstructive or restrictive pulmonary disease were randomly assigned to receive the intervention or to a control group. The intervention was provided by a nurse and included one to three teaching sessions, reading material, audiotapes, and telephone follow-up. The program focused on stress management techniques such as cognitive restructuring, progressive relaxation, breathing exercises, and visual imagery. The 45 experimental subjects were similar to the 49 controls with respect to baseline characteristics. Experimental and control subjects had similar rates of mortality, hospital days, bed-disability days, restricted-activity days, and physician visits during the 12-month follow-up. There were no differences between the two groups in physical or psychosocial function at six months or in levels of stressful life changes, social supports, and self-esteem at six and 12 months. Intervention recipients had better function at 12 months, suggesting a possible benefit of the intervention."
        },
        {
            "title": "[Gender-related risk of non-fatal stroke, myocardial infarction and blindness in the type 2 diabetic patients depend on the type of treatment].",
            "abstract": "The aim of present observational cross-sectional study was the estimation of the gender-related risk of non-fatal stroke, myocardial infarction and blindness in type 2 diabetics (T2D) depending on treatment in Ukraine. In Dec 2005 the data from 11 Ukrainian regions for all living T2D already included into the nationwide population-based diabetes register (start of creation Jan 2001) was extracted. Male/female relative risks (RRs) for non-fatal stroke, myocardial infarction and blindness were calculated in treatment groups: Diet only (DO) 7273/15901; oral antidiabetic drugs (OAD) 15109/ 33913; insulin (I) 5529/12462 male/female respectively. 95% confidence intervals were estimated using logarithmic transformation, chi-square test was used to compare the differences between groups. The higher prevalence of stroke among men was in those patients treated with OAD or DO:RRs = 1.38 (1.26-1.51) and 1.48 (1.25-1.76), respectively. The gender differences were absent among those treated with I:RR = 0.98 (0.87-1.10). An excess male risk of myocardial infarction was in persons treated with I:RR = 1.37 (1.21-1.55), though smaller than in those treated without I - male/female RRs: 2.18 (1.97-2.41) and 2.37 (1.99-2.86), respectively for OAD and DO (P<0.001 in both cases). The significantly increased female prevalence of blindness was in patients treated with I:RR = 0.80 (0.70-0.92). The latest is in accordance with our earlier results, which have demonstrated the raised prevalence of both low sight and proliferative retinopathy among the insulin-treated diabetic women. No sex-related differences in blindness prevalence were revealed in patients treated with OAD or DO. The gender risks of not-fatal stroke, myocardial infarction and blindness in T2D are significantly different depending on the fact of insulin treatment. Further research could clarify if it depends upon insulin-related hyperandrogenisation in T2D women."
        },
        {
            "title": "Should predictive scores based on vital signs be used in the same way as those based on laboratory data? A hypothesis generating retrospective evaluation of in-hospital mortality by four different scoring systems.",
            "abstract": "Background:\n        \n      \n      few studies have compared the discrimination of predictive scores of in-hospital mortality that used vital signs with those using laboratory results in different patient populations.\n    \n\n\n          Methods:\n        \n      \n      a hypothesis generating retrospective observational cohort study. A score that only used vital signs was compared with three other scores that used laboratory changes in 44,985 medical and 20,432 surgical patients.\n    \n\n\n          Results:\n        \n      \n      the discrimination of the score based only on vital signs was highest for the prediction of in-hospital death within 24h. In contrast the, albeit lower, discrimination of scores based only on laboratory data remained constant for the prediction of death up to 30 days after hospital admission. Moreover, the discrimination of scores based only on laboratory data was higher in surgical than in medical patients.\n    \n\n\n          Conclusion:\n        \n      \n      in acutely ill medical patients a vital sign based score appears to predict mortality within 24h better than scores using laboratory data. This may be because in acutely ill patients vital sign changes indicate how well a patient is responding to a current insult. In contrast, for patients without acute illness laboratory data may be a more valuable indication of the patient's capacity to respond to insults in the future."
        },
        {
            "title": "Outcome in Dilated Cardiomyopathy Related to the Extent, Location, and Pattern of Late Gadolinium Enhancement.",
            "abstract": "Objectives:\n        \n      \n      This study sought to investigate the association between the extent, location, and pattern of late gadolinium enhancement (LGE) and outcome in a large dilated cardiomyopathy (DCM) cohort.\n    \n\n\n          Background:\n        \n      \n      The relationship between LGE and prognosis in DCM is incompletely understood.\n    \n\n\n          Methods:\n        \n      \n      The authors examined the association between LGE and all-cause mortality and a sudden cardiac death (SCD) composite based on the extent, location, and pattern of LGE in DCM.\n    \n\n\n          Results:\n        \n      \n      Of 874 patients (588 men, median age 52 years) followed for a median of 4.9 years, 300 (34.3%) had nonischemic LGE. Estimated adjusted hazard ratios for patients with an LGE extent of 0 to 2.55%, 2.55% to 5.10%, and >5.10%, respectively, were 1.59 (95% confidence interval [CI]: 0.99 to 2.55), 1.56 (95% CI: 0.96 to 2.54), and 2.31 (95% CI: 1.50 to 3.55) for all-cause mortality, and 2.79 (95% CI: 1.42 to 5.49), 3.86 (95% CI: 2.09 to 7.13), and 4.87 (95% CI: 2.78 to 8.53) for the SCD endpoint. There was a marked nonlinear relationship between LGE extent and outcome such that even small amounts of LGE predicted a substantial increase in risk. The presence of septal LGE was associated with increased mortality, but SCD was most associated with the combined presence of septal and free-wall LGE. Predictive models using LGE presence and location were superior to models based on LGE extent or pattern.\n    \n\n\n          Conclusions:\n        \n      \n      In DCM, the presence of septal LGE is associated with a large increase in the risk of death and SCD events, even when the extent is small. SCD risk is greatest with concomitant septal and free-wall LGE. The incremental value of LGE extent beyond small amounts and LGE pattern is limited."
        },
        {
            "title": "Qualitative and quantitative CECT features for differentiating renal primitive neuroectodermal tumor from the renal cell carcinoma and its subtypes.",
            "abstract": "Objective::\n        \n      \n      To identify important qualitative and quantitative clinical and imaging features that could potentially differentiate renal primitiveneuroectodermal tumor (PNET) from various subtypes of renalcell carcinoma (RCC).\n    \n\n\n          Methods::\n        \n      \n      We retrospectively reviewed 164 patients, 143 with pathologically proven RCC and 21 with pathologically proven renal PNET. Univariate analysis of each parameter was performed. In order to differentiate renal PNET from RCC subtypes and overall RCC as a group, we generated ROC curves and determined cutoff values for mean attenuation of the lesion, mass to aorta attenuation ratio and mass to renal parenchyma attenuation ratio in the nephrographic phase.\n    \n\n\n          Results::\n        \n      \n      Univariate analysis revealed 11 significant parameters for differentiating renal PNET from clear cell RCC (age, p = <0.001; size, p =< 0.001; endophytic growth pattern, p < 0.001;margin of lesion, p =< 0.001; septa within the lesion, p =< 0.001; renal vein invasion, p =< 0.001; inferior vena cava involvement, p = 0.014; enhancement of lesion less than the renal parenchyma, p = 0.008; attenuation of the lesion, p = 0.002; mass to aorta attenuation ratio, p =< 0.001; and mass to renal parenchyma attenuation ratio, p =< 0.001). Univariate analysis also revealed seven significant parameters for differentiating renal PNET from papillary RCC. For differentiating renal PNET from overall RCCs as a group, when 77.3 Hounsfield unit was used as cutoff value in nephrographic phase, the sensitivity and specificity were 71.83 and 76.92 % respectively. For differentiating renal PNET from overall RCCs as a group, when 0.57 was used as cutoff for mass to aorta enhancement ratio in nephrographic phase, the sensitivity and specificity were 80.28 and 84.62 % respectively.\n    \n\n\n          Conclusion::\n        \n      \n      Specific qualitative and quantitative features can potentially differentiate renal PNET from various subtypes of RCC.\n    \n\n\n          Advances in knowledge::\n        \n      \n      The study underscores the utility of combined demographic and CT findings to potentially differentiate renal PNET from the much commoner renal neoplasm, i.e. RCC. It has management implications as if RCC is suspected, surgeons proceed with resection without need for confirmatory biopsy. On the contrary, a suspected renal PNET should proceed with biopsy followed by chemoradiotherapy, thus obviating the unnecessary morbidity and mortality."
        },
        {
            "title": "Urinary Neutrophil Gelatinase-Associated Lipocalin predicts the severity of contrast-induced acute kidney injury in chronic kidney disease patients undergoing elective coronary procedures.",
            "abstract": "Background:\n        \n      \n      Contrast-induced acute kidney injury (CI-AKI) particularly in high risk patients with chronic kidney disease (CKD), increases morbidity and mortality. Neutrophil gelatinase-associated lipocalin (NGAL) is a protein excreted by the kidney during AKI. There are no urine (u) NGAL data as an early CI-AKI marker in CKD patients undergoing coronary procedures.\n    \n\n\n          Methods:\n        \n      \n      This prospective study enrolled 130 patients with estimated glomerular filtration rate (eGFR) < 60 ml/min/1.73 m² undergoing elective coronary procedures. Serial urine samples, obtained at baseline and 3, 6, 12, 18, and 24 h post contrast administration were analyzed by NGAL ELISA kit. AKI was defined as an increase in serum creatinine (SCr) of ≥ 0.3 mg/dl or ≥ 1.5 times baseline SCr within 48 h per 2012 KDIGO guidelines. Receiver operator characteristic curve analyses identified optimal uNGAL and delta of uNGAL values for diagnosing CI-AKI.\n    \n\n\n          Results:\n        \n      \n      The uNGAL was significantly and inverse correlated with eGFR (R = 0.25, P < 0.005). CI-AKI developed in 16/130 (12.31%) patients: 13 and 3 in CI-AKI stages I and II, respectively. uNGAL and delta of uNGAL were significantly higher in the CI-AKI group when compared with the No CI-AKI group (P < 0.05). The best uNGAL cut-off for optimal sensitivity 94%, specificity 78%, and area under the curve 0.84 for predicting CI-AKI was 117 ng/mL at 6 h, respectively. Corresponding values for predicting CI-AKI stage II were 100%, 87% and 0.9 when using an uNGAL of 264 ng/mL at 6 h.\n    \n\n\n          Conclusions:\n        \n      \n      Monitoring of uNGAL levels not only provide the early detecting CI-AKI but also predict the severity of CI-AKI in CKD patients undergoing elective coronary procedures."
        },
        {
            "title": "Association of ocular disease and mortality in a diabetic population.",
            "abstract": "Objective:\n        \n      \n      To investigate the association of ocular disease with all-cause and cause-specific mortality in a diabetic population.\n    \n\n\n          Design:\n        \n      \n      Geographically defined population-based cohort study.\n    \n\n\n          Setting:\n        \n      \n      An 11-county area in Wisconsin.\n    \n\n\n          Study population:\n        \n      \n      Participants were all younger-onset diabetic persons (diagnosed as having diabetes at <30 years of age and taking insulin) and a random sample of older-onset diabetic persons (diagnosed as having diabetes at > or =30 years of age). Diabetic retinopathy, macular edema, visual acuity, and cataract were measured using standardized protocols at baseline examinations from 1980 to 1982, in which 996 younger-onset and 1370 older-onset persons participated. Participants were followed up for 16 years.\n    \n\n\n          Main outcome measure:\n        \n      \n      All-cause and cause-specific mortality as determined from death certificates.\n    \n\n\n          Results:\n        \n      \n      In the younger-onset group, after controlling for age and sex, retinopathy severity, macular edema, cataract, history of cataract surgery, and history of glaucoma at baseline were associated with all-cause and ischemic heart disease mortality. In the older-onset group, after controlling for age and sex, retinopathy and visual impairment were related to all-cause, ischemic heart disease, and stroke mortality. No ocular variable under study was related to cancer mortality in the older-onset group. After controlling for systemic risk factors, visual impairment was associated with all-cause and ischemic heart disease mortality in the younger-onset group. In the older-onset group, retinopathy severity was related to all-cause and stroke mortality, and visual impairment was related to all-cause, ischemic heart disease, and stroke mortality.\n    \n\n\n          Conclusions:\n        \n      \n      Presence of more severe retinopathy or visual impairment in diabetic patients is a risk indicator for increased risk of ischemic heart disease death. Presence of these ocular conditions may identify individuals who should be under care for cardiovascular disease."
        },
        {
            "title": "Vision impairment in tuberculous meningitis: predictors and prognosis.",
            "abstract": "Background:\n        \n      \n      Vision impairment is a devastating complication of tuberculous meningitis. In the present study we evaluated the predictors and prognostic significance of vision impairment in tuberculous meningitis.\n    \n\n\n          Methods:\n        \n      \n      In this study, 101 adult patients with tuberculous meningitis were evaluated for vision status and physical disability and were followed up for 6 months. Contrast enhanced magnetic resonance imaging (MRI) was performed at baseline and 6 months.\n    \n\n\n          Result:\n        \n      \n      Out of 101 patients, 74 patients had normal vision and 27 patients had low vision or blindness at enrollment. Thirteen patients died during the study period. Out of 88 patients who survived at 6 months, 68 patients had good vision, 11 patients had low vision and 9 patients had blindness. Predictors of vision deterioration were papilledema, cranial nerve palsies, raised cerebrospinal fluid protein (>1g/L), and presence of optochiasmatic arachnoiditis in MRI. Predictors of blindness, at 6 months, were found to be papilledema, vision acuity < or =6/18, cranial nerve palsies, tuberculous meningitis stage II or III, raised cerebrospinal fluid protein (>1g/L), optochiasmatic arachnoiditis, and optochiasmal tuberculoma. At 6 months, 27 patients had death or severe disability. Predictors of death or severe disability at 6 months were vision acuity < or =6/18, cranial nerve deficits, hemiparesis, clinical stage II or III, and presence of infarct in MRI.\n    \n\n\n          Conclusion:\n        \n      \n      Vision impairment occurred in one-fourth of patients with tuberculous meningitis. Principal causes of vision loss were optochiasmatic arachnoiditis and optochiasmal tuberculoma. Impaired vision predicted death or severe disability."
        },
        {
            "title": "Xanthine oxidase activity in patients with sepsis.",
            "abstract": "Objective:\n        \n      \n      Determine the relation of xanthine oxidase (XO) activity and the outcome of septic patients and its relation to oxidative damage and clinical parameters of sepsis severity.\n    \n\n\n          Design and methods:\n        \n      \n      Patients admitted over a 6-month period were enrolled. Patients were assigned to groups according to the diagnosis of sepsis (n=8), severe sepsis (n=28) or septic shock (n=36). Blood samples were collected to the determination of thiobarbituric acid reactive species (TBARS), protein carbonyls and XO activity.\n    \n\n\n          Results:\n        \n      \n      None of the studied oxidative parameters determined at the time of diagnosis were related to sepsis severity. XO activity, but not oxidative damage parameters, at the time of sepsis diagnosis was significantly higher in non-survival septic patients. In contrast, 24 h after sepsis diagnosis, XO activity was lower in non-survivors septic patients.\n    \n\n\n          Conclusions:\n        \n      \n      XO activity was increased in non-survivors patients and the variations in XO activity could be used for outcome prediction."
        },
        {
            "title": "Visual impairment and morbidity in community-residing adults: the national health interview survey 1986-1996.",
            "abstract": "Purpose:\n        \n      \n      To examine the association between visual impairment (VI) and morbidity.\n    \n\n\n          Methods:\n        \n      \n      Using pooled, annual population-based household interview survey data (n = 140,366) from the 1986-1996 National Health Interview Survey, covariate-adjusted, gender and age group specific logistic regression analyses were used to examine associations between VI and five morbidity indicators: restricted activity days, bed rest days, doctor visits, hospitalizations, and self-rated health.\n    \n\n\n          Results:\n        \n      \n      After controlling for educational status, race and the number of reported non-ocular health conditions, fair or poor health status (compared to excellent, very good, or good health status) was generally more strongly associated with severe, bilateral VI (range of odds ratios [OR's]: 2.14-7.24) than with some VI (OR's: 1.45-2.21). Severe, bilateral VI was also associated with more frequent doctor and hospital visits among adults 18-64 years of age (range of OR's: 1.69-3.34), and restricted activity and bed rest days among males 45 years and older (range of OR's: 1.95-3.69).\n    \n\n\n          Conclusions:\n        \n      \n      The present findings, in conjunction with other studies documenting the impact of VI on morbidity outcomes, indicate that an increased focus on the provision of eye care services will be necessary to address the growing burden of VI in aging societies."
        },
        {
            "title": "A Canadian Cost-Utility Analysis of 2 Trabecular Microbypass Stents at Time of Cataract Surgery in Patients with Mild to Moderate Open-Angle Glaucoma.",
            "abstract": "Purpose:\n        \n      \n      To assess, from the Canadian public payer perspective, the cost-utility of implanting iStent Inject trabecular bypass stent (TBS) devices in conjunction with cataract surgery versus cataract surgery alone in patients with open-angle glaucoma (OAG) and visually significant cataract.\n    \n\n\n          Design:\n        \n      \n      Cost-utility analysis using efficacy and safety results of pivotal randomized clinical trial.\n    \n\n\n          Participants:\n        \n      \n      Modeled cohort of patients with OAG (83.1% with mild disease, 16.9% with moderate disease) and visually significant cataract.\n    \n\n\n          Methods:\n        \n      \n      Open-angle glaucoma treatment costs and effects were projected over a 15-year time horizon using a Markov model with Hodapp-Parrish-Anderson glaucoma stages (mild, moderate, advanced, severe or blind) and death as health states. Patients in the mild or moderate OAG health states received implantation of iStent Inject during cataract surgery versus cataract surgery alone. On worsening of visual field defect and optic disc damage, patients could receive selective laser trabeculoplasty and trabeculectomy. We measured treatment effect as reduction in intraocular pressure (IOP) and mean medication use and estimated transition probabilities based on efficacy-adjusted visual field mean deviation decline per month. Healthcare resource utilization and utility scores were obtained from the literature. Cost inputs (2017 Canadian dollars [C$]) were derived using the Ontario Health Insurance Plan, expert opinion, medication claims datasets, and Ontario Drug Benefit Formulary medication consumption costs. We conducted deterministic and probabilistic sensitivity analyses to examine the impact of alternative model input values on results.\n    \n\n\n          Main outcome measures:\n        \n      \n      Incremental cost per quality-adjusted life year (QALY) gained.\n    \n\n\n          Results:\n        \n      \n      Compared with cataract surgery alone, TBS plus cataract surgery showed a 99% probability of being more effective (+0.023 QALYs; 95% confidence interval [CI], 0.004 to 0.044) and a 73.7% probability of being cost-saving (net cost, -C$389.00; 95% CI, -C$1712.00 to C$850.70). In 95% of all simulations, TBS plus cataract surgery showed a cost per QALY of C$62 366 or less. Results were robust in additional sensitivity and scenario analyses.\n    \n\n\n          Conclusions:\n        \n      \n      iStent Inject TBS implantation during cataract surgery seems to be cost effective for reducing IOP in patients with mild to moderate OAG versus cataract surgery alone."
        },
        {
            "title": "Risk factors in the development of ocular surface epithelial dysplasia.",
            "abstract": "Background:\n        \n      \n      Ocular surface epithelial dysplasia involves a spectrum of diseases ranging from only minor eye irritation to blindness and potentially death.\n    \n\n\n          Methods:\n        \n      \n      A case-control study involving 60 patients with ocular surface epithelial dysplasia treated between 1972 and 1991 and 60 age- and sex-matched individuals was conducted to compare relative ultraviolet light exposures over their lifetimes. A standardized self-administered ultraviolet exposure questionnaire was used for assessment.\n    \n\n\n          Results:\n        \n      \n      Risk factors identified include phenotypic features such as fair skin (odds ratio [OR], 5.4; 95% confidence interval [CI], 1.1, 25.6), pale iris (OR, 1.8; 95%; CI, 0.9, 3.8), and propensity to sunburn (OR, 3.8; 95% CI, 0.7, 19.7), history of previous skin cancers removed (OR, 15; 95% CI, 2.0, 113.6), and being outdoors more than 50% of time in the first 6 years of life while living 30 degrees or less from the equator (OR, 7.5; 95% CI, 1.8, 30.6).\n    \n\n\n          Conclusion:\n        \n      \n      These risk factors suggest that ocular surface epithelial dysplasia is an ultraviolet light-related disease."
        },
        {
            "title": "The Associations between Peripheral Artery Disease and Physical Outcome Measures in Men and Women with Chronic Kidney Disease.",
            "abstract": "Background:\n        \n      \n      Peripheral artery disease (PAD) is highly prevalent and associated with significant morbidity and mortality, but sex-based differences are incompletely understood. We sought to define the associations between PAD and physical outcome measures and to determine if these associations differed by sex in the Chronic Renal Insufficiency Cohort.\n    \n\n\n          Methods:\n        \n      \n      Among 3,543 participants, we assessed the cross-sectional relationship between PAD severity defined by ankle-brachial index; and (1) physical activity (metabolic equivalent [MET]-hr/wk), (2) walking pace (slow versus medium and/or fast), and (3) physical function (12-item Short Form Health Survey [SF-12]) at baseline.\n    \n\n\n          Results:\n        \n      \n      In a multivariable linear regression model, PAD severity was not associated with physical activity defined by total MET-hr per wk in men or women (P = 0.432). However, PAD severity was significantly associated with walking activity (P = 0.037), although this relationship did not differ by sex (P = 0.130). Similarly, PAD severity was significantly associated with walking pace (P < 0.001), although this relationship did not differ by sex (P = 0.086). In contrast, there was an independent association between PAD severity and SF-12 (P = 0.018), with a significant interaction by sex (P < 0.001).\n    \n\n\n          Conclusions:\n        \n      \n      These data suggest that tools used to evaluate the functional consequences of PAD should focus on walking activity and walking pace, as well as physical function, where sex-specific associations should be accounted for."
        },
        {
            "title": "Visual impairment, age-related cataract, and mortality.",
            "abstract": "Objective:\n        \n      \n      To explore associations between visual impairment, cataract, and mortality in older persons after adjusting for other factors associated with mortality.\n    \n\n\n          Methods:\n        \n      \n      A population cohort of 3654 persons aged 49 years or older (82.4% of eligible residents in the Blue Mountains region, west of Sydney, Australia), were examined at the Blue Mountains Eye Study baseline period (1992-1994) and followed up 5 years later (1997-1999). Australian National Death Index data were used to confirm persons who had died since baseline. Associations between mortality and presence of visual impairment and cataract at baseline were assessed using the Cox proportional hazards regression model, controlling for age, sex, demographic and socioeconomic status, medical history, and health risk behaviors.\n    \n\n\n          Results:\n        \n      \n      By June 30, 1999, 604 participants (16.5%) had died. The age- and sex-standardized 7-year cumulative mortality rate was 26% among persons with any visual impairment and 16% in persons without visual impairment. After adjusting for factors found significantly associated with mortality, including age, male sex, low self-rated health, low socioeconomic status, systemic medical conditions, and negative health risk behaviors, the presence at baseline of any visual impairment was independently associated with increased mortality risk (risk ratio [RR], 1.7; 95% confidence interval, 1.2-2.3). The presence of age-related cataract, either nuclear (RR, 1.5), cortical (RR, 1.3), or posterior subcapsular cataract (RR, 1.5), was also significantly associated with increased mortality risk. These associations remained statistically significant when visual impairment and each type of cataract were included simultaneously in the multivariate Cox model.\n    \n\n\n          Conclusion:\n        \n      \n      Visual impairment and age-related cataract may be independent risk factors for increased mortality in older persons."
        },
        {
            "title": "Mortality in patients treated with intravitreal bevacizumab for age-related macular degeneration.",
            "abstract": "Background:\n        \n      \n      The aim of this study is to analyze mortality in patients treated with bevacizumab for wet AMD.\n    \n\n\n          Methods:\n        \n      \n      We conducted a retrospective case-control study between patients who received intravitreal injections of bevacizumab as the sole treatment for exudative AMD between September 2008 and October 2014 (n = 5385) and age and gender matched controls (n = 10,756). All individuals included in the study were reviewed for sociodemographic data and comorbidities. Survival analysis was performed using adjusted Cox regression, using relevant adjusted variables.\n    \n\n\n          Results:\n        \n      \n      During follow-up (maximum: 73 months), 1063 (19.7%) individuals after bevacizumab died compared with 1298 (12.1%) in the control group (P < .001). After adjusted Cox survival regression, mortality differed significantly between the groups, Odds ratio = 1.69, (95% C.I. 1.54-1.84), P < .001.\n    \n\n\n          Conclusions:\n        \n      \n      We found an increased long-term mortality in individuals with wet AMD treated with bevacizumab compared to a same age and gender group without wet AMD."
        },
        {
            "title": "Comparative evaluation of the modified CT severity index and CT severity index in assessing severity of acute pancreatitis.",
            "abstract": "Objective:\n        \n      \n      The purpose of this study was to compare the modified CT severity index (MCTSI) with the CT severity index (CTSI) regarding assessment of severity parameters in acute pancreatitis (AP). Both CT indexes were also compared with the Acute Physiology, Age, and Chronic Health Evaluation (APACHE II) index.\n    \n\n\n          Materials and methods:\n        \n      \n      Of 397 consecutive cases of AP, 196 (49%) patients underwent contrast-enhanced CT (n = 175) or MRI (n = 21) within 1 week of onset of symptoms. Two radiologists independently scored both CT indexes. Severity parameters included mortality, organ failure, pancreatic infection, admission to and length of ICU stay, length of hospital stay, need for intervention, and clinical severity of pancreatitis. Discrimination analysis and kappa statistics were performed.\n    \n\n\n          Results:\n        \n      \n      Although for both CT indexes a significant relationship was observed between the score and each severity parameter (p < 0.0001), no significant differences were seen between the CT indexes. Compared with the APACHE II index, both CT indexes more accurately correlated with the need for intervention (CTSI, p = 0.006; MCTSI, p = 0.01) and pancreatic infection (CTSI, p = 0.04; MCTSI, p = 0.06) and more accurately diagnosed clinically severe disease (area under the curve, 0.87; 95% CI, 0.82-0.92). Interobserver agreement was excellent for both indexes: for CTSI, 0.85 (95% CI, 0.80-0.90) and for MCTSI, 0.90 (95% CI, 0.85-0.95).\n    \n\n\n          Conclusion:\n        \n      \n      No significant differences were noted between the CTSI and the MCTSI in evaluating the severity of AP. Compared with APACHE II, both CT indexes more accurately diagnose clinically severe disease and better correlate with the need for intervention and pancreatic infection."
        },
        {
            "title": "Microsurgical management of tuberculum sellae meningiomas by the frontolateral approach: surgical technique and visual outcome.",
            "abstract": "Objectives:\n        \n      \n      The aim of this study was to evaluate visual outcome in patients with tuberculum sellae meningioma (TSM) treated microsurgically using the frontolateral or fronto-orbital approach and optic canal unroofing to resect tumor involvement of the optic canal.\n    \n\n\n          Methods:\n        \n      \n      Data from 67 patients with TSMs who underwent microsurgical treatment by a frontolateral approach (n=44) or fronto-orbital approach (n=23) between January 2002 and December 2008 were retrospectively collected and analyzed. Change in visual function was evaluated as the main outcome.\n    \n\n\n          Results:\n        \n      \n      Total tumor resection was achieved in 62 of 67 cases (92.4%). Postoperative, visual acuity was improved in 87 eyes (64.9%) and unchanged in 39 eyes (29.1%), and the optic nerve was therefore preserved in 126 of 134 eyes (94.0%). Visual field deficits were improved or stable in 65 eyes, no patient experienced worsening of vision in both eyes. There was no mortality in our series.\n    \n\n\n          Conclusions:\n        \n      \n      The frontolateral approach with microsurgical dissection of the Sylvian fissure provides quick access to TSMs, which can be resected safely and totally. Visual function is improved and neurological and ophthalmological morbidity is minimal. Optic nerve decompression by intradural clinoidectomy and optic canal unroofing is likely to increase the rate of reducing or eliminating preoperative visual symptoms."
        },
        {
            "title": "Correction of Selection Bias in Survey Data: Is the Statistical Cure Worse Than the Bias?",
            "abstract": "In previous articles in the American Journal of Epidemiology (Am J Epidemiol. 2013;177(5):431-442) and American Journal of Public Health (Am J Public Health. 2013;103(10):1895-1901), Masters et al. reported age-specific hazard ratios for the contrasts in mortality rates between obesity categories. They corrected the observed hazard ratios for selection bias caused by what they postulated was the nonrepresentativeness of the participants in the National Health Interview Study that increased with age, obesity, and ill health. However, it is possible that their regression approach to remove the alleged bias has not produced, and in general cannot produce, sensible hazard ratio estimates. First, one must consider how many nonparticipants there might have been in each category of obesity and of age at entry and how much higher the mortality rates would have to be in nonparticipants than in participants in these same categories. What plausible set of numerical values would convert the (\"biased\") decreasing-with-age hazard ratios seen in the data into the (\"unbiased\") increasing-with-age ratios that they computed? Can these values be encapsulated in (and can sensible values be recovered from) 1 additional internal variable in a regression model? Second, one must examine the age pattern of the hazard ratios that have been adjusted for selection. Without the correction, the hazard ratios are attenuated with increasing age. With it, the hazard ratios at older ages are considerably higher, but those at younger ages are well below 1. Third, one must test whether the regression approach suggested by Masters et al. would correct the nonrepresentativeness that increased with age and ill health that I introduced into real and hypothetical data sets. I found that the approach did not recover the hazard ratio patterns present in the unselected data sets: The corrections overshot the target at older ages and undershot it at lower ages."
        },
        {
            "title": "HIV-associated tuberculosis: relationship between disease severity and the sensitivity of new sputum-based and urine-based diagnostic assays.",
            "abstract": "Background:\n        \n      \n      Reducing mortality from HIV-associated tuberculosis (TB) requires diagnostic tools that are rapid and have high sensitivity among patients with poor prognosis. We determined the relationship between disease severity and the sensitivity of new sputum-based and urine-based diagnostic assays.\n    \n\n\n          Methods:\n        \n      \n      Consecutive ambulatory patients enrolling for antiretroviral treatment in South Africa were screened for TB regardless of symptoms using diagnostic assays prospectively applied to sputum (fluorescence smear microscopy, Xpert MTB/RIF and liquid culture (reference standard)) and retrospectively applied to stored urine samples (Determine TB-LAM and Xpert MTB/RIF). Assay sensitivities were calculated stratified according to pre-defined indices of disease severity: CD4 count, symptom intensity, serum C-reactive protein (CRP), hemoglobin concentration and vital status at 90 days.\n    \n\n\n          Results:\n        \n      \n      Sputum culture-positive TB was diagnosed in 15% (89/602) of patients screened and data from 86 patients were analyzed (median CD4 count, 131 cells/μL) including 6 (7%) who died. The sensitivity of sputum microscopy was 26.7% overall and varied relatively little with disease severity. In marked contrast, the sensitivities of urine-based and sputum-based diagnosis using Determine TB-LAM and Xpert MTB/RIF assays were substantially greater in sub-groups with poorer prognosis. Rapid diagnosis from sputum and/or urine samples was possible in >80% of patients in sub-groups with poor prognosis as defined by either CD4 counts <100 cells/μL, advanced symptoms, CRP concentrations >200 mg/L or hemoglobin <8.0 g/dl. Retrospective testing of urine samples with Determine TB-LAM correctly identified all those with TB who died.\n    \n\n\n          Conclusions:\n        \n      \n      The sensitivities of Xpert MTB/RIF and Determine TB-LAM for HIV-associated TB were highest among HIV-infected patients with the most advanced disease and poorest prognostic characteristics. These data provide strong justification for large-scale intervention studies that assess the impact on survival of screening using these new sputum-based and urine-based diagnostic approaches."
        },
        {
            "title": "A mathematical model for determining age-specific diabetes incidence and prevalence using body mass index.",
            "abstract": "Purpose:\n        \n      \n      Few models have been developed specifically for the epidemiology of diabetes. Diabetes incidence is critical in predicting diabetes prevalence. However, reliable estimates of disease incidence rates are difficult to obtain. The aim of this study was to propose a mathematical framework for predicting diabetes prevalence using incidence rates estimated within the model using body mass index (BMI) data.\n    \n\n\n          Methods:\n        \n      \n      A generic mechanistic model was proposed considering birth, death, migration, aging, and diabetes incidence dynamics. Diabetes incidence rates were determined within the model using their relationships with BMI represented by the Hill equation. The Hill equation parameters were estimated by fitting the model to National Health and Nutrition Examination Survey (NHANES) 1999-2010 data and used to predict diabetes prevalence pertaining to each NHANES survey year. The prevalences were also predicted using diabetes incidence rates calculated from the NHANES data themselves. The model was used to estimate death rate parameters and to quantify sensitivities of prevalence to each population dynamic.\n    \n\n\n          Results:\n        \n      \n      The model using incidence rate estimates from the Hill equations successfully predicted diabetes prevalence of younger, middle-aged, and older adults (prediction error, 20.0%, 9.64%, and 7.58% respectively). Diabetes prevalence was positively associated with diabetes incidence in every age group, but the associations among younger adults were stronger. In contrast, diabetes prevalence was more sensitive to death rates in older adults than younger adults. Both diabetes incidence and prevalence were strongly sensitive to BMI at younger ages, but sensitivity gradually declined as age progressed. Younger and middle aged adults diagnosed with diabetes had at least a two-fold greater risk of death than their nondiabetic counterparts. Nondiabetic older adults were found to be under slightly higher death risk (0.079) than those diagnosed with diabetes (0.073).\n    \n\n\n          Conclusions:\n        \n      \n      The proposed model predicts diagnosed diabetes incidence and prevalence reasonably well using the link between BMI and diabetes development risk. Ethnic group and gender-specific model parameter estimates could further improve predictions. Model prediction accuracy and applicability need to be comprehensively evaluated with independent data sets."
        },
        {
            "title": "Recalibration and validation of the SCORE risk chart in the Australian population: the AusSCORE chart.",
            "abstract": "Background:\n        \n      \n      Development of a validated risk prediction model for future cardiovascular disease (CVD) in Australians is a high priority for cardiovascular health strategies.\n    \n\n\n          Design:\n        \n      \n      Recalibration of the SCORE (Systematic COronary Risk Evaluation) risk chart based on Australian national mortality data and average major CVD risk factor levels.\n    \n\n\n          Methods:\n        \n      \n      Australian national mortality data (2003-2005) were used to estimate 10-year cumulative CVD mortality rates for people aged 40-74 years. Average age-specific and sex-specific levels of systolic blood pressure, total cholesterol and prevalence of current smoking were generated from data obtained in eight Australian large-scale population-based surveys undertaken from the late 1980s. The SCORE risk chart was then recalibrated by applying hazard ratios for 10-year CVD mortality obtained in the SCORE project. Discrimination and calibration of the recalibrated model was evaluated and compared with that of the original SCORE and Framingham equations in the Blue Mountains Eye Study in Australia using Harrell's c and Hosmer-Lemeshow chi statistics, respectively.\n    \n\n\n          Results:\n        \n      \n      An Australian risk prediction chart for CVD mortality was derived. Among 1998 Blue Mountains Eye Study participants aged 49-74 years with neither CVD nor diabetes at baseline, the Harrell's c statistics for the Australian risk prediction chart for CVD mortality were 0.76 (95% confidence interval: 0.69-0.84) and 0.71 (confidence interval: 0.62-0.80) in men and women, respectively. The corresponding Hosmer-Lemeshow chi statistics, the measure of calibration, were 2.32 (P = 0.68) and 7.43 (P = 0.11), which were superior to both the SCORE and Framingham equations.\n    \n\n\n          Conclusion:\n        \n      \n      This new tool provides a valid and reliable method to predict risk of CVD mortality in the general Australian population."
        },
        {
            "title": "Effects of Ebola Virus Disease education on student health professionals.",
            "abstract": "Background and purpose:\n        \n      \n      Ebola Virus Disease (EVD) is a severe, often fatal illness. Studies have shown that healthcare professionals lack an in-depth knowledge of EVD. Countries in Europe, Asia, and Africa are beginning to emphasize the need to train healthcare professionals about EVD, but the United States still lacks formal training for healthcare students. There is little research about the effectiveness of EVD training to support this study. The purpose of this study was to examine the knowledge-base and attitudes of healthcare students concerning EVD.\n    \n\n\n          Educational activity and setting:\n        \n      \n      Two-hundred sixty-nine participants (including pharmacy students, physician assistant students, and nursing students) completed a pre- and post-survey. The survey measured both knowledge and perceptions. The post-survey was administered after the intervention to measure change. The intervention was comprised of a pre-recorded lecture about EVD transmission, prevention, and treatment.\n    \n\n\n          Findings:\n        \n      \n      All groups displayed significant changes in knowledge and perception, specifically in the areas of EVD transmission, prevention, and treatment. Pharmacy students' attitudes increased significantly over their baseline score for all three attitude questions, whereas there were no significant changes in attitude to EVD among nurses. Physician assistant students' attitudes changed regarding the topic of isolation.\n    \n\n\n          Discussion:\n        \n      \n      Education on EVD in pharmacy schools may provide beneficial results for students' knowledge, and it may also help schools provide evidence to meet current standards for accreditation.\n    \n\n\n          Summary:\n        \n      \n      This educational intervention represents an effective format that could be a useful tool to help enhance or augment knowledge for healthcare workers. This could lead to better care for patients."
        },
        {
            "title": "Plasma fibrinogen and other cardiovascular disease risk factors and cataract.",
            "abstract": "Purpose:\n        \n      \n      To determine whether associations exist between cataract and established cardiovascular risk factors (other than smoking) - hypertension, body mass index, serum lipids and plasma fibrinogen.\n    \n\n\n          Methods:\n        \n      \n      The Blue Mountains Eye Study is a large (n=3654) population-based cross-sectional study conducted among people aged 49-97 years residing in the Blue Mountains, a region west of Sydney, Australia. Risk factor data were collected using standardised clinical procedures. Lens photographs were taken and graded for presence and severity of cortical, nuclear, and posterior subcapsular cataracts.\n    \n\n\n          Results:\n        \n      \n      Cortical cataract was associated with a history of myocardial infarction, higher plasma fibrinogen, and higher serum cholesterol. Nuclear cataract was associated with a higher platelet count but hypertension was associated with lower prevalence of nuclear cataract. Posterior subcapsular cataract was associated with higher plasma fibrinogen and lower body mass index. Some of these associations appeared to be stronger in women than in men: fibrinogen and cortical cataract and body mass index and posterior subcapsular cataract.\n    \n\n\n          Conclusions:\n        \n      \n      Several risk factors for cardiovascular disease are associated with presence of cataract, perhaps explaining the observation in several studies that people with cataract have increased mortality rates. The possibility of strong associations between plasma fibrinogen and cataract merits further epidemiological and laboratory research."
        },
        {
            "title": "Diabetic Retinopathy -Incidence And Risk Factors In A Community Setting- A Longitudinal Study.",
            "abstract": "Aim:\n        \n      \n      To evaluate the natural history of diabetic retinopathy (DR) in diabetic patients and to assess long term risk for other chronic diseases associated with DR.\n    \n\n\n          Methods:\n        \n      \n      Retrospective, community-based study. Diabetics who underwent their first fundoscopic examination during 2000-2002, and had at least one follow- up examination by the end of 2007 were included. The primary outcome was the development of DR (proliferative diabetic retinopathy (PDR), non PDR (NPDR) or macular edema. Patients were followed for another 9 years for documentation of new diagnosis of related diseases.\n    \n\n\n          Results:\n        \n      \n      516 patients' (1,032 eyes) records were included and were followed first for an average of 4.15 ± 1.27 years. During follow-up, 28 (2.7%) of the total 1,032 eyes examined were diagnosed with PDR. An additional 194 (18.8%) eyes were diagnosed with new NPDR. The cumulative incidence of NPDR was 310/1,032 (30.0%). All the patients who developed PDR had prior NDPR. By the end of the 9 years extended follow up, patients with NPDR had a greater risk for developing chronic renal failure HR = 1.71 (1.14-2.56), ischemic heart disease HR = 1.57 (1.17-2.09), and had an increased mortality rate HR = 1.26 (1.02-1.57) Conclusion: DR is associated with a higher rate of diabetes complications. Patients with DR should be followed more closely. Key points During a mean follow-up of 4.5 years, the cumulative incidence of diabetic retinopathy in a community cohort was 18.8%. NDPR (non-proliferative diabetic retinopathy) is a predictor of PDR (proliferative diabetic retinopathy). In a real life setting NPDR is a marker of a poorer prognosis. Patients with NDPR should be monitored more closely."
        },
        {
            "title": "Dual sensory impairment in older age.",
            "abstract": "Objective:\n        \n      \n      Hearing and visual impairments are commonly viewed separately in research and service provision, but they often occur together as dual sensory impairment or DSI in older populations. This article examines the frequency and effects of DSI in older age and notes limitations in the evidence.\n    \n\n\n          Methods:\n        \n      \n      Search of electronic databases of published papers.\n    \n\n\n          Results:\n        \n      \n      DSI diminishes communication and well-being and can cause social isolation, depression, reduced independence, mortality, and cognitive impairment.\n    \n\n\n          Discussion:\n        \n      \n      Although intuitively DSI may be expected to have additional impacts over single sensory impairment, research findings are inconclusive. Services and supports required by people with DSI are simply a combination of those required by people with single vision and hearing loss, taking account of the unique communication difficulties posed by DSI."
        },
        {
            "title": "Quality of life assessment in the collaborative ocular melanoma study: design and methods. COMS-QOLS Report No. 1. COMS Quality of Life Study Group.",
            "abstract": "The Collaborative Ocular Melanoma Study (COMS) is a set of randomized clinical trials sponsored by the National Eye Institute of the National Institutes of Health. The COMS is being conducted to evaluate the role of radiotherapy in the treatment of patients with choroidal melanoma. Primary choroidal melanoma can enlarge or metastasize and eventually cause death in a significant percentage of cases. The primary COMS trial is designed to determine whether enucleation (removal of the eye) or radiotherapy without removal of the eye provides the patient with the longest remaining lifespan. More than 40 clinical centers in the United States and Canada are participating in the COMS. The objective of the COMS is to assess the effect of treatment upon 5-year and 10-year survival and the reduction or elimination of the disease process in patients randomly assigned to receive either radiation or enucleation. An ancillary component of the COMS, referred to as the COMS-QOLS, was designed to measure the impact of disease and its treatment on quality of life. The two treatment approaches being investigated, enucleation and radiation therapy, are likely to have different psychological and physiological effects on the patients receiving them. The COMS-QOLS assessments include the SF-36 Health Survey, the Activities of Daily Vision Scale (ADVS), the Visual Functioning Questionnaire (VFQ), and the Hospital Anxiety and Depression Scale (HADS). Patients are interviewed at selected intervals during follow-up; in addition, 200 patients will be interviewed before randomization and have repeat interviews at six months and annually after randomization. The patient's quality of life after treatment will become an important consideration in determining the best form of therapy, particularly in the event that no survival difference between treatment groups is found."
        },
        {
            "title": "'A living death': a qualitative assessment of quality of life among women with trichiasis in rural Niger.",
            "abstract": "Background:\n        \n      \n      Prior to blindness, trachoma is thought to profoundly affect women's abilities to lead normal lives, but supporting evidence is lacking. To better understand the effects of trichiasis, we asked women to define quality of life, how trichiasis affects this idea and their perceptions of eyelid surgery.\n    \n\n\n          Methods:\n        \n      \n      Operated and unoperated women were purposively selected for in-depth interviews. These were audio-recorded and transcribed, and codes were identified and applied to the transcripts. Overarching themes, commonalities and differences were identified and matched to quotations.\n    \n\n\n          Results:\n        \n      \n      Twenty-three women were interviewed. Quality of life was defined as health, security, family, social status and religious participation. Trichiasis caused severe pain and loss of health, leading to loss of security. This affected social, economic and religious activities and caused burden on their families. Surgery improved quality of life, even in cases of surgical failure or recurrent disease.\n    \n\n\n          Conclusions:\n        \n      \n      Trichiasis disables most women, even those reporting fewer or less-severe symptoms. While women in rural Niger often live in extreme poverty, trichiasis exacerbates the situation, making women unable to work and undermining their social status. It adds to family burden, as women lose the ability to meaningfully contribute to the household and require additional family resources for their care."
        },
        {
            "title": "Cost-effectiveness of glaucoma interventions in Barbados and Ghana.",
            "abstract": "Purpose:\n        \n      \n      More than 90% of blindness worldwide exists in the developing world, but information on the social and economic burden and the cost-effectiveness of treatment in these settings is often limited or nonexistent. We demonstrate the use of computer modeling to simulate the current and future epidemiology, outcomes, and treatment of primary open-angle glaucoma in high-incidence populations of the developing world.\n    \n\n\n          Methods:\n        \n      \n      A previously validated vision model was modified to simulate the incidence progression and social and economic outcomes of glaucoma in Barbados, which was the source of epidemiology data, and Ghana, which has similar propensity for glaucoma but lower socioeconomic development. We then assessed the cost-effectiveness of hypothetical case-finding and treatment scenarios, including U.S. guideline-level care and one-time laser surgery.\n    \n\n\n          Results:\n        \n      \n      Barbados incurs relatively greater social and economic burden from glaucoma than Ghana. In Barbados, population screening followed by U.S. guideline levels of care appears to be highly cost-effective. Because of a younger population with higher mortality at younger ages, glaucoma appears to cause less visual impairment and blindness in Ghana than in Barbados, resulting in lower per capita disability and productivity losses. Population screening or guideline-level treatment scenarios were generally not cost-effective in Ghana, but treating self-referring patients with a hypothetical one-time laser surgery was highly cost-effective relative to World Health Organization willingness to pay thresholds.\n    \n\n\n          Conclusions:\n        \n      \n      The social and economic burden of glaucoma is higher in developed nations because of increased life expectancy, an older population age profile, and higher per capita gross domestic product. Similarly, lower mortality rates and higher per capita gross domestic product increase the relative cost-effectiveness of screening and treatment interventions intended to mitigate glaucoma burden."
        },
        {
            "title": "Preserved autonomic regulation in patients undergoing transcatheter aortic valve implantation (TAVI): a prospective, comparative study.",
            "abstract": "Heart rate and blood pressure variability as well as baroreflex sensitivity (BRS) lead to additional insights on the patients' prognosis after cardiovascular events. The following study was performed to assess the differences in the postoperative recovery of the autonomic regulation after transcatheter aortic valve implantation (TAVI) and surgical aortic valve replacement (SAVR). Fifty-eight consecutive patients were enrolled in a prospective study; 24 underwent TAVI and 34 SAVR. BRS was calculated according to the Dual Sequence Method, heart rate variability (HRV) was evaluated using standard linear as well as nonlinear parameters. HRV and BRS parameters were reduced after surgery in patients with SAVR only (meanNN: p<0.001, sdNN: p<0.05, Shannon: p<0.01, BRS: p<0.01), while these indexes were preserved in patients after TAVI. Simultaneously, an increased complexity of blood pressure (BP) in SAVR patients (fwShannon: p<0.001, fwRenyi4: p<0.001), but not in TAVI patients was recorded. In this study we were able to demonstrate for the first time that, in contrast to patients undergoing conventional open surgery, there are fewer alterations of the cardiovascular autonomic system in patients with TAVI."
        },
        {
            "title": "Carotid-cavernous and orbital arteriovenous fistulas: ocular features, diagnostic and hemodynamic considerations in relation to visual impairment and morbidity.",
            "abstract": "The author investigated 101 cases with direct dural carotid-cavernous and orbital arteriovenous fistulas (CCF). The characteristic clinical findings, such as specific epibulbar arterialized loops, are described and the differential diagnosis of the striking diagnostic triad (exophthalmos, the above-mentioned loops and glaucoma) is discussed, together with the exclusion criteria for other causes of red eyes, episcleral measurements and blood flow. The results of various diagnostic procedures, such as ultrasonography, Doppler hematotachography and color Doppler of the orbit and carotid systems, magnetic resonance imaging and angiography, and of conservative treatment and embolization processes are dealt with successively. The classification of different types of carotid-cavernous fistulas is presented,(1-3) together with the clinical signs in relation to morbidity and mortality during or after conservative or intervention therapies. The importance of patient follow-up, in the clinic as well as with Doppler methods, is emphasized in order to differentiate a progressive or diminished clinical condition caused by spontaneous thrombosis in the healing process or more arteriovenous flow. A 'decision tree' for use in daily practice is provided. In this study, of the 101 cases in which the localization was diagnosed by angiography, 42 were direct (30 traumatic, 12 spontaneous), 31 were dural (3 traumatic, 28 spontaneous) and 10 were orbital CCFs. In 18 other cases, usually dural or orbital shunts, angiography was not performed. For the management of 42 direct fistulas, conservative treatment was used in 12 cases (7 with success; 58%) and balloon embolization was performed in 18 cases (17 with success; 94.5%); the other cases were treated by direct or indirect surgery. Of the 48 (spontaneous and traumatic) dural fistulas, 39 were treated conservatively (32 recovered or were much improved: 82%, of the total cases, 67%). All seven cases in which embolization was performed were cured and/or much improved. In two cases, one fistula was conservatively treated while one was embolized at another location, both with success. Of the 10 orbital arteriovenous shunts showing signs of dural fistulas, the features disappeared in 8 cases, although after a much longer follow-up period than for the typical dural carotid-cavernous sinus fistulas; in one patient, direct surgery was performed successfully and in one patient the original, non-progressive, orbital features could still be observed."
        },
        {
            "title": "Accuracy of CT chest without oral contrast for ruling out esophageal perforation using fluoroscopic esophagography as reference standard: a retrospective study.",
            "abstract": "Purpose:\n        \n      \n      Esophageal perforation has a high mortality rate. Fluoroscopic esophagography (FE) is the procedure of choice for diagnosing esophageal perforation. However, FE can be difficult to perform in seriously ill patients.\n    \n\n\n          Methods:\n        \n      \n      We retrospectively reviewed charts and scans of all patients who had undergone thoracic CT (TCT) without oral contrast and FE for suspicion of esophageal perforation at our hospital between October, 2010 and December, 2015. Scans were interpreted by a single consultant radiologist having > 5 years of relevant experience. Statistical analysis was performed using SPSS version 20. Sensitivity, specificity, positive predictive value (PPV) and negative predictive value (NPV) of TCT were computed using FE as reference standard.\n    \n\n\n          Results:\n        \n      \n      Of 122 subjects, 106 (83%) were male and their median age was 42 [inter-quartile range (IQR) 29-53] years. Esophageal perforation was evident on FE in 15 (8%) cases. Sensitivity, specificity, PPV and NPV of TCT for detecting esophageal perforation were 100, 54.6, 23.4 and 100%, respectively. When TCT was negative (n = 107), an alternative diagnosis was evident in 65 cases.\n    \n\n\n          Conclusion:\n        \n      \n      Thoracic computed tomography (TCT) had 100% sensitivity and negative predictive value for excluding esophageal perforation. FE may be omitted in patients who have no evidence of mediastinal collection, pneumomediastinum or esophageal wall defect on TCT. However, in the presence of any of these features, FE is still necessary to confirm or exclude the presence of an esophageal perforation."
        },
        {
            "title": "Elderly licensure laws and motor vehicle fatalities.",
            "abstract": "Context:\n        \n      \n      Little is known about how state-level driver licensure laws, such as in-person renewal, vision tests, road tests, and the frequency of license renewal relate to the older driver traffic fatality rate.\n    \n\n\n          Objective:\n        \n      \n      To determine whether state driver's license renewal policies are associated with the fatality rate among elderly drivers.\n    \n\n\n          Design, setting, and population:\n        \n      \n      Retrospective, longitudinal study conducted January 1990 through December 2000 of all fatal crashes in the contiguous United States identified in the Fatality Analysis Reporting System, which involved either an older (ages 65-74 years, 75-84 years, and > or =85 years) or middle-aged (ages 25-64 years) driver. Two regression approaches were used to study the effect of state laws mandating in-person renewal, vision tests, road tests, and frequency of license renewal on driver fatalities, controlling for state-level factors including the number of licensed elderly drivers, primary and secondary seatbelt laws, maximum speed limit laws, blood alcohol level of 0.08, and administrative license revocation drinking and driving laws, per capita income, and unemployment rate. The first regression approach examined only elderly driver fatalities and the second approach examined daytime elderly driver fatalities and used daytime fatalities among middle-aged drivers as a general control for unobserved variation across states and over time.\n    \n\n\n          Main outcome measures:\n        \n      \n      Older driver fatalities and older and middle-aged daytime driver fatalities.\n    \n\n\n          Results:\n        \n      \n      Among individuals aged 85 years or older, there were a total of 4605 driver fatalities and 4179 daytime driver fatalities during the study period. For this age cohort, after controlling for middle-aged daytime driver deaths, states with in-person license renewal were associated with a lower driver fatality rate (incident rate ratio [RR], 0.83; 95% confidence interval [CI], 0.72-0.96). This was the only policy related to older drivers that was significantly associated with a lower fatality risk across both regression models. Thus, state-mandated vision tests, road tests, more frequent license renewal, and in-person renewal (for individuals aged 65-74 years and 75-84 years) were not found to be independently associated with the fatality rate among older drivers in the 2 models.\n    \n\n\n          Conclusions:\n        \n      \n      In-person license renewal was related to a significantly lower fatality rate among the oldest old drivers. More stringent state licensure policies such as vision tests, road tests, and more frequent license renewal cycles were not independently associated with additional benefits."
        },
        {
            "title": "Improving the efficacy of mammography screening: the potential and challenge of developing new computer-aided detection approaches.",
            "abstract": "In this editorial article, we share our visions of how to improve the efficacy of breast cancer screening using quantitative image feature analysis methods with the medical imaging research community. Although earlier cancer detection and treatment can help reduce the mortality rates of patients with breast cancer, the most existing long-term (or lifetime) risk models do not have clinically acceptable discriminatory power at the individual level to select women for current screening. Because of such a clinical dilemma and low efficacy of breast cancer screening, it is of great importance to establish a new and more effective personalized breast cancer screening paradigm. The new computer-aided detection (CAD) approach can be applied in two application fields: analyzing negative mammograms can provide useful information to assess cancer risk, and implementing case-based cancer risk scores can develop an adaptive CAD cueing method to yield higher sensitivity and lower false-positive detection rates."
        },
        {
            "title": "Interleukin-18 production and pulmonary function in COPD.",
            "abstract": "Interleukin (IL)-18 production and pulmonary function were evaluated in patients with chronic obstructive pulmonary disease (COPD) in order to determine the role of IL-18 in COPD. Immunohistochemical techniques were used to examine IL-18 production in the lungs of patients with very severe COPD (Global Initiative for Chronic Obstructive Lung Disease (GOLD) stage IV, n = 16), smokers (n = 27) and nonsmokers (n = 23). Serum cytokine levels and pulmonary function were analysed in patients with GOLD stage I-IV COPD (n = 62), smokers (n = 34) and nonsmokers (n = 47). Persistent and severe small airway inflammation was observed in the lungs of ex-smokers with very severe COPD. IL-18 proteins were strongly expressed in alveolar macrophages, CD8+ T-cells, and both the bronchiolar and alveolar epithelia in the lungs of COPD patients. Serum levels of IL-18 in COPD patients and smokers were significantly higher than those in nonsmokers. Moreover, serum levels of IL-18 in patients with GOLD stage III and IV COPD were significantly higher than in smokers and nonsmokers. There was a significant negative correlation between serum IL-18 level and the predicted forced expiratory volume in one second in patients with COPD. In contrast, serum levels of IL-4, IL-13 and interferon-gamma were not significantly increased in any of the three groups. In conclusion, overproduction of interleukin-18 in the lungs may be involved in the pathogenesis of chronic obstructive pulmonary disease."
        },
        {
            "title": "Volumetric extent of resection and residual contrast enhancement on initial surgery as predictors of outcome in adult patients with hemispheric anaplastic astrocytoma.",
            "abstract": "Object:\n        \n      \n      To investigate the prognostic significance of the volumetrically assessed extent of resection on time to tumor progression (TTP), overall survival (OS), and tumor recurrence patterns, the authors retrospectively analyzed preoperative and postoperative tumor volumes in 102 adult patients from the time of the initial resection of a hemispheric anaplastic astrocytoma (AA).\n    \n\n\n          Methods:\n        \n      \n      The quantification of tumor volumes was based on a previously described method involving computerized analysis of magnetic resonance (MR) images. Analysis of contrast-enhancing tumor volumes on T1-weighted MR images was conducted for 67 patients who had contrast-enhancing tumors. Measurements of T2 hyperintensity were obtained for all 102 patients in the study. The presence or absence of preresection enhancement, actual volume of this enhancement, and the percentage of preoperative enhancement as it relates to the total T2 tumor volume did not have a statistically significant relationship to TTP or OS. In addition to age, the volume of residual disease measured on T2-weighted MR images was the most significant predictor of TTP (p < 0.001), and residual contrast-enhancing tumor volume was the most significant predictor of OS (p = 0.003) on multivariate analysis. In contrast to low-grade gliomas, there was no statistically significant relationship between the extent of resection and histological characteristics at the time of recurrence, that is, tumor Grade III compared with Grade IV.\n    \n\n\n          Conclusions:\n        \n      \n      Data from this retrospective analysis of a histologically uniform group of hemispheric AAs treated in the MR imaging era suggest that residual tumor volumes, as documented on postoperative imaging studies, may be a prognostic factor for TTP and OS for this patient population."
        },
        {
            "title": "Analyzing the severity of accidents on the German Autobahn.",
            "abstract": "We study the severity of accidents on the German Autobahn in the state of North Rhine-Westphalia using data for the years 2009 until 2011. We use a multinomial logit model to identify statistically relevant factors explaining the severity of the most severe injury, which is classified into the four classes fatal, severe injury, light injury and property damage. Furthermore, to account for unobserved heterogeneity we use a random parameter model. We study the effect of a number of factors including traffic information, road conditions, type of accidents, speed limits, presence of intelligent traffic control systems, age and gender of the driver and location of the accident. Our findings are in line with studies in different settings and indicate that accidents during daylight and at interchanges or construction sites are less severe in general. Accidents caused by the collision with roadside objects, involving pedestrians and motorcycles, or caused by bad sight conditions tend to be more severe. We discuss the measures of the 2011 German traffic safety programm in the light of our results."
        },
        {
            "title": "Three-year outcomes of individualized ranibizumab treatment in patients with diabetic macular edema: the RESTORE extension study.",
            "abstract": "Objective:\n        \n      \n      To evaluate long-term efficacy and safety profiles during 3 years of individualized ranibizumab treatment in patients with visual impairment due to diabetic macular edema (DME).\n    \n\n\n          Design:\n        \n      \n      Phase IIIb, multicenter, 12-month, randomized core study and 24-month open-label extension study.\n    \n\n\n          Participants:\n        \n      \n      Of the 303 patients who completed the randomized RESTORE 12-month core study, 240 entered the extension study.\n    \n\n\n          Methods:\n        \n      \n      In the extension study, patients were eligible to receive individualized ranibizumab treatment as of month 12 guided by best-corrected visual acuity (BCVA) and disease progression criteria at the investigators' discretion. Concomitant laser treatment was allowed according to the Early Treatment Diabetic Retinopathy Study guidelines. Based on the treatments received in the core study, the extension study groups were referred to as prior ranibizumab, prior ranibizumab + laser, and laser.\n    \n\n\n          Main outcome measures:\n        \n      \n      Change in BCVA and incidence of ocular and nonocular adverse events (AEs) over 3 years.\n    \n\n\n          Results:\n        \n      \n      Overall, 208 patients (86.7%) completed the extension study. In patients treated with ranibizumab during the core study, consecutive individualized ranibizumab treatment during the extension study led to an overall maintenance of BCVA and central retinal subfield thickness (CRST) observed at month 12 over the 2-year extension study (+8.0 letters, -142.1 μm [prior ranibizumab] and +6.7 letters, -145.9 μm [prior ranibizumab + laser] from baseline at month 36) with a median of 6.0 injections (mean, 6.8 injections; prior ranibizumab) and 4.0 (mean, 6.0 injections; prior ranibizumab + laser). In the prior laser group, a progressive BCVA improvement (+6.0 letters) and CRST reduction (-142.7 μm) at month 36 were observed after allowing ranibizumab during the extension study, with a median of 4.0 injections (mean, 6.5 injections) from months 12 to 35. Patients in all 3 treatment groups received a mean of <3 injections in the final year. No cases of endophthalmitis, retinal tear, or retinal detachment were reported. The most frequently reported ocular and nonocular adverse effects over 3 years were cataract (16.3%) and nasopharyngitis (23.3%). Eight deaths were reported during the extension study, but none were suspected to be related to the study drug/procedure.\n    \n\n\n          Conclusions:\n        \n      \n      Ranibizumab was effective in improving and maintaining BCVA and CRST outcomes with a progressively declining number of injections over 3 years of individualized dosing. Ranibizumab was generally well tolerated with no new safety concerns over 3 years."
        },
        {
            "title": "Royal Society of Tropical Medicine and Hygiene meeting at Manson House, London 17 January 2002. Cervical cancer in developing countries.",
            "abstract": "The public health importance of cervical cancer is now increasingly appreciated as a means to improve the general health of women in many developing countries. Developing countries account for 80% of the world burden, mostly due to the lack of effective control programmes. Infection with oncogenic types of human papillomaviruses (HPV) has been established as the central cause for cervical cancer. Thus, vaccination against HPV is a potentially useful strategy for prevention, but this may take several years to become a reality. Currently, early detection and treatment is the most effective approach to control cervical cancer. Cervical cancer may be controlled through improving awareness and accessibility to diagnostic and treatment services. Cytology-based screening is beyond the capacity of health services in many developing countries, hence, alternative methods to cytology are being investigated. Visual inspection of the cervix after application of 3-5% acetic acid (VIA) seems to be a promising screening test, with a similar sensitivity to that of cytology, but lower specificity. Currently, it is being evaluated for its cost-effectiveness in reducing cervical cancer incidence and mortality in randomized trials. Information from the ongoing studies will be valuable for evolving cervical cancer control policies and programmes in low-resource settings."
        },
        {
            "title": "Evaluation of sepsis induced cardiac dysfunction as a predictor of mortality.",
            "abstract": "Background:\n        \n      \n      Sepsis is characterized by life threatening organ dysfunction with dysregulated immune response. Cardiac dysfunction seen in sepsis is unique as it is reversible within 7-10 days. Initial study by Parker et al. in 1984, showed, paradoxically lower ejection fraction in survivors of septic shock. Subsequent meta-analysis did not support that survivors had lower ejection fraction. Aim of our study was to assess the sepsis induced cardiac dysfunction by 2D echocardiography and Troponin I.\n    \n\n\n          Methods:\n        \n      \n      After obtaining institutional ethical committee approval (ref 125/2016), a prospective observational study was done in an university medical college from February 2016 to April 2016. Inclusion criteria were patients diagnosed with sepsis by new sepsis definition. Pregnant patients and patients with poor echo window were excluded. Echocardiographic assessment was done within 48 h of diagnosis of sepsis by standard methods. Primary outcome was ICU mortality and secondary outcome was ICU length of stay. Statistical analysis was done using STATA™ (Version14, College station TX).\n    \n\n\n          Results:\n        \n      \n      Fifty eight patients were screened, ten were excluded due to poor echo window. Baseline characteristics were similar in survivors and non survivors, except APACHE II, SOFA age and cumulative fluid balance. Echocardiographic parameters, mitral annular plane systolic excursion (MAPSE), E/e' and LV systolic function assessed by visual gestalt method were found to be statistically significant. Parameters found significant in bivariate analysis were used as a covariate in logistic regression. APACHE II and MAPSE were significant co-variates in logistic regression with ROC (0.95) and calibration was satisfactory (chi2(df8),1.98, p = 0.98).\n    \n\n\n          Conclusions:\n        \n      \n      Sepsis induced cardiac dysfunction assessed by echocardiography showed measurement of MAPSE when combined with APACHE II was a good predictor of mortality. Among the echocardiographic parameters MAPSE alone was a good predictor of mortality. Results of this study need further validation from larger study."
        },
        {
            "title": "Diagnostic profiles of patients with late-onset Creutzfeldt-Jakob disease differ from those of younger Creutzfeldt-Jakob patients: a historical cohort study using data from the German National Reference Center.",
            "abstract": "In contrast to other neurodegenerative diseases, sporadic Creutzfeldt-Jakob disease (sCJD) is rarely diagnosed in patients older than 75 years. Data describing the characteristics of sCJD in the very old are rare and inconclusive. Therefore, a historical cohort study was designed to evaluate clinical, cerebrospinal fluid (CSF), electroencephalography (EEG), and magnetic resonance imaging (MRI) features of this group. Patients older than 75 years identified via the German surveillance program from 2001 to 2012 (n = 73) were compared to a random subsample of sCJD patients younger than 75 (n = 73) from the same time period using an historical cohort design. Older patients showed a faster disease progression represented by an earlier point of diagnosis and a shorter survival time (p < 0.001). In the early stages of disease, older patients presented slightly more often with dementia (p = 0.127) or dysarthria (p = 0.238), whereas disorders of the extrapyramidal (p = 0.056) and visual system (p = 0.015) were more common in the younger group. Atypical MRI profiles such as MRI lesions restricted to one hemisphere (p < 0.001) or cortical lesions only (p = 0.258) were found more frequently in patients older than 75 years, whereas typical cortical and basal ganglia hyperintensities were more common in the younger group (p = 0.001). We demonstrated for the first time that patients with late-onset sCJD differ from younger sCJD patients with respect to MRI profiles and initial clinical presentation, but not among CSF markers. Misclassification of Creutzfeldt-Jakob disease cases in patients older than 75 years seems likely due to atypical clinical and radiological presentation. This might contribute to lower sCJD incidence rates in this age group."
        },
        {
            "title": "Outcome of corneal grafting with donor tissue from eyes with primary choroidal melanomas. A retrospective cohort comparison.",
            "abstract": "Objectives:\n        \n      \n      To determine if melanomas have occurred in the recipients of corneas from donor eyes with primary choroidal melanomas and to determine the success of corneal grafting with tissue taken from eyes with primary choroidal melanomas.\n    \n\n\n          Design:\n        \n      \n      Retrospective cohort comparison and follow-up patient questionnaire.\n    \n\n\n          Setting:\n        \n      \n      A tertiary medical center in Rochester, Minn.\n    \n\n\n          Patients:\n        \n      \n      In patients who received corneal transplants, we reviewed 47 consecutive corneas transplanted from donor eyes enucleated for choroidal melanomas and compared them with 47 corneal grafts from donor eyes without melanomas matched for recipient age (+/- 10 years), date of operation (+/- 12 months), corneal storage time (+/- 24 hours), and operation type.\n    \n\n\n          Results:\n        \n      \n      No melanomas occurred in either group over a mean follow-up of 5.4 years (range, 0.4 to 15 years). There was no significant difference between the two groups in corneal thickness and endothelial cell loss at 2 months and 1 year after transplantation and in the probability of a rejection episode.\n    \n\n\n          Conclusions:\n        \n      \n      There is no evidence of tumor transmission by transplantation of corneas from donor eyes with primary choroidal melanomas. Corneas transplanted from donor eyes with primary choroidal melanomas have similar outcomes to corneas transplanted from donor eyes without melanomas."
        },
        {
            "title": "Hong Kong Chinese women's lay beliefs about cervical cancer causation and prevention.",
            "abstract": "Background:\n        \n      \n      This study aimed to seek insights into Chinese women's lay beliefs about cervical cancer causal attributions and prevention.\n    \n\n\n          Materials and methods:\n        \n      \n      Twenty-three new immigrant adult women from Mainland China and thirty-five Hong Kong adult women underwent semi-structured in-depth interviews. Interviews were audio taped, transcribed and analyzed using a Grounded Theory approach.\n    \n\n\n          Results:\n        \n      \n      This study generated three foci: causal beliefs about cervical cancer, perceived risk of cervical cancer, and beliefs about cervical cancer prevention. Personal risky practices, contaminated food and environment pollution were perceived as the primary causes of cervical cancer. New immigrant women more likely attributed cervical cancer to external factors. Most participants perceived cervical cancer as an important common fatal female cancer with increased risk/prevalence. Many participants, particularly new immigrant women participants, expressed helplessness about cervical cancer prevention due to lack of knowledge of prevention, it being perceived as beyond individual control. Many new immigrant participants had never undergone regular cervical screening while almost all Hong Kong participants had done so.\n    \n\n\n          Conclusions:\n        \n      \n      Some Chinese women hold pessimistic beliefs about cervical cancer prevention with inadequate knowledge about risk factors. Future cervical cancer prevention programs should provide more information and include capacity building to increase Chinese women's knowledge and self-efficacy towards cervical cancer prevention."
        },
        {
            "title": "Surgical results of large and giant pituitary adenomas with special consideration of ophthalmologic outcomes.",
            "abstract": "Objective:\n        \n      \n      To analyze functioning and nonfunctioning pituitary adenomas (PAs)>3 cm, with special emphasis on preoperative and postoperative visual functions.\n    \n\n\n          Methods:\n        \n      \n      The cases consisted of 49 women and 54 men with mean age of 43.2 years (range 19-66 years). All cases had a macroadenoma >3 cm in diameter. The transsphenoidal approach was performed in 117 procedures, and the transcranial approach was performed in 8 procedures. Radical tumor excision was achieved in 50 of 103 patients. Postoperative evaluation was done in 88 patients. Preoperative and postoperative visual acuity, visual field, and ocular fundi and their relationship with the pattern and duration of the symptoms and the size of the tumor were evaluated.\n    \n\n\n          Results:\n        \n      \n      Normalization of visual acuity was obtained in 71.5% of patients, improvement occurred in 13.6%, symptoms persisted in 13.6%, and symptoms worsened in 1%. Postoperative improvement of visual field defects (VFDs) was observed in 74.1% of patients, and visual impairment score improved postoperatively in 92% of patients. Patients operated on <6 months before the onset of vision loss had better and more sustained visual improvement. One patient died, and 15.5% of patients experienced surgery-related complications.\n    \n\n\n          Conclusions:\n        \n      \n      This study shows that patients with severe visual impairment may have remarkable improvement if surgical decompression is done early. The transsphenoidal approach should be performed to correct the patient's visual impairment and to relieve the pressure on the optic apparatus caused by macroadenoma of any size."
        },
        {
            "title": "Global burden of eye and vision disease as reflected in the Cochrane Database of Systematic Reviews.",
            "abstract": "Importance:\n        \n      \n      Eye and vision disease burden should help guide ophthalmologic research prioritization. The Global Burden of Disease (GBD) Study 2010 compiled data from 1990 to 2010 on 291 diseases and injuries, 1160 disease and injury sequelae, and 67 risk factors in 187 countries. The Cochrane Database of Systematic Reviews (CDSR) is a resource for systematic reviews in health care, with peer-reviewed systematic reviews that are published by Cochrane Review Groups.\n    \n\n\n          Objective:\n        \n      \n      To determine whether systematic review and protocol topics in the CDSR reflect disease burden, measured by disability-adjusted life-years (DALYs), from the GBD 2010 project. This is one of a series of projects mapping GBD 2010 medical field disease burdens to corresponding systematic reviews in the CDSR.\n    \n\n\n          Design and setting:\n        \n      \n      Two investigators independently assessed 8 ophthalmologic conditions in the CDSR for systematic review and protocol representation according to subject content. The 8 diseases were matched to their respective DALYs from the GBD 2010 project.\n    \n\n\n          Main outcomes and measures:\n        \n      \n      Cochrane Database of Systematic Reviews systematic review and protocol representation and percentage of total 2010 DALYs.\n    \n\n\n          Results:\n        \n      \n      All 8 ophthalmologic conditions were represented by at least 1 systematic review in the CDSR. A total of 91.4% of systematic reviews and protocols focused on these conditions were from the Cochrane Eyes and Vision Group. Comparing the number of reviews and protocols with disability, only cataract was well matched; glaucoma, macular degeneration, and other vision loss were overrepresented. In comparison, trachoma, onchocerciasis, vitamin A deficiency, and refraction and accommodation disorders were underrepresented.\n    \n\n\n          Conclusions and relevance:\n        \n      \n      These results prompt further investigation into why certain diseases are overrepresented or underrepresented in the CDSR relative to their DALY. With regard to ophthalmologic conditions, this study encourages that certain conditions get more focus to create a better representation of what is causing the most disability and mortality within this research database. These results provide high-quality and transparent data to inform future prioritization decisions."
        },
        {
            "title": "[Detection of bone metastasis of prostate cancer - comparison of whole-body MRI and bone scintigraphy].",
            "abstract": "Purpose:\n        \n      \n      Prostate cancer continues to be the third leading cancer-related mortality of western men. Early diagnosis of bone metastasis is important for the therapy regime and for assessing the prognosis. The standard method is bone scintigraphy. Whole-body MRI proved to be more sensitive for early detection of skeletal metastasis. However, studies of homogenous tumor entities are not available. The aim of the study was to compare bone scintigraphy and whole-body MRI regarding the detection of bone metastasis of prostate cancer.\n    \n\n\n          Materials and methods:\n        \n      \n      14 patients with histologically confirmed prostate cancer and a bone scintigraphy as well as whole-body MRI within one month were included. The mean age was 68 years. Scintigraphy was performed using the planar whole-body technique (ventral and dorsal projections). Suspect areas were enlarged. Whole-body MRI was conducted using native T 1w and STIR sequences in the coronary plane of the whole body, sagittal imaging of spine and breath-hold STIR and T 1w-Flash-2D sequences of ribs and chest. Bone scintigraphy and whole-body MRI were evaluated retrospectively by experienced radiologists in a consensus reading on a lesion-based level.\n    \n\n\n          Results:\n        \n      \n      Whole-body MRI detected significantly more bone metastasis (p = 0.024). 96.4 % of the demonstrated skeletal metastases in bone scintigraphy were founded in whole-body MRI while only 58.6 % of the depicted metastases in MRI were able to be located in scintigraphy. There was no significant difference regarding bone metastasis greater than one centimeter (p = 0.082) in contrast to metastasis less than one centimeter (p = 0.035). Small osteoblastic metastases showed a considerably higher contrast in T 1w sequences than in STIR imaging. Further advantages of whole-body MRI were additional information about extra-osseous tumor infiltration and their complications, for example stenosis of spinal canal or vertebral body fractures, found in 42.9 % of patients.\n    \n\n\n          Conclusion:\n        \n      \n      Whole-body MRI using native STIR and T 1w sequences is superior to bone scintigraphy for the detection of small bone metastasis of prostate cancer. Simultaneous clarification of associated complications demonstrates further advantages."
        },
        {
            "title": "Prognostic value of high-dose dipyridamole stress myocardial contrast perfusion echocardiography.",
            "abstract": "Background:\n        \n      \n      The addition of myocardial perfusion (MP) imaging during dipyridamole real-time contrast echocardiography improves the sensitivity to detect coronary artery disease, but its prognostic value to predict hard cardiac events in large numbers of patients with known or suspected coronary artery disease remains unknown.\n    \n\n\n          Methods and results:\n        \n      \n      We studied 1252 patients with the use of dipyridamole real-time contrast echocardiography and followed them for a median of 25 months. The prognostic value of MP imaging regarding death and nonfatal myocardial infarction was determined and related to wall motion (WM), clinical risk factors, and rest ejection fraction by the use of Cox proportional-hazards models, C index, and risk reclassification analysis. A total of 59 hard events (4.7%) occurred during the follow-up (24 deaths, 35 myocardial infarctions). The 2-year event-free survival was 97.9% in patients with normal MP and WM, 91.9% with isolated reversible MP defects but normal WM, and 67.4% with both reversible MP and WM abnormalities (P<0.001). By multivariate analysis the independent predictors of events were age (hazard ratio 1.05, 95% confidence interval [CI], 1.02-1.08), sex (hazard ratio, 2.36; 95% CI, 1.32-4.23), reversible MP defects (hazard ratio, 3.88; 95% CI, 1.83-8.21), and reversible WM abnormalities with reversible MP defects (hazard ratio, 4.51; 95% CI, 2.25-9.07). Reversible MP defects added incremental predictive value and reclassification benefit over WM response and clinical factors (P=0.001).\n    \n\n\n          Conclusions:\n        \n      \n      MP imaging using real-time perfusion echocardiography during dipyridamole real-time contrast echocardiography provides independent, incremental prognostic information regarding hard cardiac events in patients with known or suspected coronary artery disease. Patients with normal MP responses have better outcome than patients with normal WM; patients with both reversible WM and MP abnormalities have the worst outcome."
        },
        {
            "title": "Race-related differences among elderly urban residents: a cohort study, 1975-1984.",
            "abstract": "A population-based cohort of 1,598 urban residents, aged 65 years and over, was studied in 1975, and 645 survivors were re-interviewed in their places of residence in 1984. Since 25.6 percent of the subjects were Black, it was possible to examine race-related changes in health, function, and socioeconomic status over nine years, as well as differences in rates of institutionalization and mortality. Aging urban Blacks continue to experience major social disadvantages, especially in education and income. After age 74, although Blacks probably experience more favorable mortality rates and less institutionalization, they consider themselves less healthy and are more likely to develop diabetes, hypertension, and glaucoma. Although Blacks rate their own mental health lower, this difference is not supported by other measures. Functionally, elderly Whites are more likely to be dependent in certain activities of daily living. The findings are consistent with the previously observed mortality crossover; predictors of mortality are identified but do not differ by race. Lower institutionalization rates among older Blacks may be partly explained by different living patterns, poverty, and a higher proportion of males among surviving Blacks."
        },
        {
            "title": "Changes in visual acuity in a population over a 15-year period: the Beaver Dam Eye Study.",
            "abstract": "Purpose:\n        \n      \n      To describe the change in visual acuity in a 15-year period.\n    \n\n\n          Design:\n        \n      \n      Population-based study.\n    \n\n\n          Methods:\n        \n      \n      setting: Beaver Dam, Wisconsin. participants: 4068 persons 43 to 86 years of age at the time of a baseline examination in 1988 to 1990, and with follow-up examinations every five years thereafter. observation procedures: Best-corrected visual acuity after refraction, assessed by a modification of the ETDRS protocol. main outcome measure: Doubling of the visual angle; incidence of visual impairment.\n    \n\n\n          Results:\n        \n      \n      Eight percent of the population developed impaired vision (20/40 or worse), 0.8% developed severe visual impairment (20/200 or worse), 7% had doubling of the visual angle, and 2% had improved vision. People 75 years of age or older at baseline were more likely to develop impaired vision (odds ratio [OR] 12.8, 95% confidence interval [CI] 9.6 to 17.1, P < .001), doubling of the visual angle (OR 7.8, 95% CI 5.6 to 10.7, P < .001), and severe visual impairment (OR 20.6, 95% CI 9.5 to 44.8, P<0.001) compared with people younger than 75 years of age.\n    \n\n\n          Conclusions:\n        \n      \n      These data provide population-based estimates of the cumulative 15-year incidence of loss of vision over a wide spectrum of ages. In people 75 years of age or older the cumulative incidence of visual impairment accounting for the competing risk of death is 25%, of which 4% is severe, indicating a public health problem of considerable proportions as the US population in this age is expected to increase by 55% from 18 million in the year 2005 to 28 million by the year 2025."
        },
        {
            "title": "Validation of the JEN frailty index in the National Long-Term Care Survey community population: identifying functionally impaired older adults from claims data.",
            "abstract": "Background:\n        \n      \n      Use of a claims-based index to identify persons with physical function impairment and at risk for long-term institutionalization would facilitate population health and comparative effectiveness research. The JEN Frailty Index [JFI] is comprised of diagnosis domains representing impairments and multimorbid clusters with high long-term institutionalization [LTI] risk. We test the index's discrimination of activities-of-daily-living [ADL] dependency and 1-year LTI and mortality in a nationally representative sample of over 12,000 Medicare beneficiaries, and compare long-term community survival stratified by ADL and JFI.\n    \n\n\n          Methods:\n        \n      \n      2004 U.S. National Long-Term Care Survey data were linked to Medicare, Minimum Data Set, Veterans Health Administration files and vital statistics. ADL dependencies, JFI score, age and sex were measured at baseline survey. ADL and JFI groups were cross-tabulated generating likelihood ratios and classification statistics. Logistic regression compared discrimination (areas under receiver operating characteristic curves), multivariable calibration and accuracy of the JFI and, separately, ADLs, in predicting 1-year outcomes. Hall-Wellner bands facilitated contrasts of JFI- and ADL-stratified 5-year community survival.\n    \n\n\n          Results:\n        \n      \n      Likelihood ratios rose evenly across JFI risk categories. Areas under the curves of functional dependency at ≥3 and ≥ 2 for JFI, age and sex models were 0.807 [95% c.i.: 0.795, 0.819] and 0.812 [0.801, 0.822], respectively. The area under the LTI curve for JFI and age (0.781 [0.747, 0.815]) discriminated less well than the ADL-based model (0.829 [0.799, 0.860]). Community survival separated by JFI strata was comparable to ADL strata.\n    \n\n\n          Conclusions:\n        \n      \n      The JEN Frailty Index with demographic covariates is a valid claims-based measure of concurrent activities-of-daily-living impairments and future long-term institutionalization risk in older populations lacking functional information."
        },
        {
            "title": "Pseudo-exfoliation and mortality.",
            "abstract": "The hypothesis that ocular pseudo-exfoliation syndrome is part of a generalized disorder has been tested by suggesting that subjects having this syndrome would have increased mortality. However, no association was found between presence of ocular pseudo-exfoliation syndrome and mortality."
        },
        {
            "title": "Overweight adults may have the lowest mortality--do they have the best health?",
            "abstract": "Numerous recent studies have found that overweight adults experience lower overall mortality than those who are underweight, normal-weight, or obese. These highly publicized findings imply that overweight may be the optimal weight category for overall health via its association with longevity-a conclusion with important public health implications. In this study, the authors examined the association between body mass index (BMI; (weight (kg)/height (m)(2))) and 3 markers of health risks using a nationally representative sample of US adults aged 20-80 years (n = 9,255) from the National Health and Nutrition Examination Survey (2005-2008). Generalized additive models, a type of semiparametric regression model, were used to examine the relations between BMI and biomarkers of inflammation, metabolic function, and cardiovascular function (C-reactive protein, hemoglobin A(1c), and high density lipoprotein cholesterol, respectively). The association between BMI and each biomarker was monotonic, with higher BMI being consistently associated with worse health risk profiles at all ages, in contrast to the U-shaped relation between BMI and mortality. Prior results suggesting that the overweight BMI category corresponds to the lowest risk of mortality may not be generalizable to indicators of health risk."
        },
        {
            "title": "Prognostic value of T1-mapping in TAVR patients: extra-cellular volume as a possible predictor for peri- and post-TAVR adverse events.",
            "abstract": "The benefit of a transcatheter aortic valve replacement (TAVR) can differ in patients, and therapy bears severe risks. High-degree aortic stenosis can lead to cardiac damage such as diffuse myocardial fibrosis, evaluable by extra-cellular volume (ECV) in CMR. Therefore, fibrosis might be a possible risk factor for unfavorable outcome after TAVR. We sought to assess the prognostic value of T1-mapping and ECV to predict adverse events during and after TAVR. The study population consisted of patients undergoing clinically indicated TAVR by performing additional CMR with native and contrast-enhanced T1-mapping sequences for additional evaluation of ECV. Study endpoints were congestive heart failure (CHF) and TAVR-associated conduction abnormalities defined as new onset of left bundle branch block (LBBB), AV-Block or implantation of a pacemaker. 94 patients were examined and followed. Median follow up time was 187 days (IQR 79-357 days). ECV was increased (>30 %) in 38 patients (40 %). There was no significant correlation between ECV and death, Hazard ratio (HR) 0.847 (95 % CI 0.335; 2.14), p = 0.72. ECV in patients with subsequent CHF was higher than in those without an event (33.5 ± 4.6 and 29.1 ± 4.1 %, respectively), but the difference just did not reach the level of significance HR 2.16 (95 % CI 0.969; 4.84), p = 0.06. Patients with post-TAVR conduction abnormality (LBBB, AV-block or pacemaker implantation) had statistically relevant lower ECV values compared to those without an event. Patients with an event had a mean ECV of 28.1 ± 3.16 %; patients without an event had a mean ECV of 29.8 ± 4.53, HR 0.56 (95 % CI 0.32; 0.96), p = 0.036. In this study, elevated myocardial ECV is a predictor of CHF by trend; CMR may be helpful in identifying patients with a high risk for post-TAVR cardiac decompensation benefitting from an intensified post-interventional surveillance. Patients with post-TAVR conductions abnormalities have a significantly decreased ECV. Nevertheless, it remains unclear which precise molecular tissue alteration is the protective factor or risk factor in this case."
        },
        {
            "title": "A scoring-system for angiographic findings in nonocclusive mesenteric ischemia (NOMI): correlation with clinical risk factors and its predictive value.",
            "abstract": "Purpose:\n        \n      \n      This study was designed to evaluate the clinical value of a standardized angiographic scoring system in patients with nonocclusive mesenteric ischemia (NOMI).\n    \n\n\n          Methods:\n        \n      \n      Sixty-three consecutive patients (mean age: 73 ± 8 years) with suspect of NOMI after cardiac or major thoracic vessel surgery underwent catheter angiography of the superior mesenteric artery. Images were assessed by two experienced radiologists on consensus basis using a scoring system consisting of five categories, namely vessel morphology, reflux of contrast medium into the aorta, contrasting and distension of the intestine, as well as the time to portal vein filling. These were correlated to previously published risk factors of NOMI and outcome data.\n    \n\n\n          Results:\n        \n      \n      The most significant correlation was found between the vessel morphology and death (p < 0.001) as well as reflux of contrast medium into the aorta and death (p = 0.005). Significant correlation was found between delayed portal vein filling and preoperative statin administration (p = 0.011), previous stroke (p = 0.033), and renal insufficiency (p = 0.043). Reflux of contrast medium correlated significantly with serum lactate >10 mmol/L (p = 0.046). The overall angiographic score correlated with death (p = 0.017) and renal insufficiency (p = 0.02). The ROC-analysis revealed that a score of ≥3.5 allows for identifying patients with increased perioperative mortality with a sensitivity of 85.7 % and a specificity of 49 %. With the use of a simplified score (vessel morphology, reflux of contrast medium into the aorta, and time to portal vein filling), specificity was increased to 71.4 %.\n    \n\n\n          Conclusions:\n        \n      \n      The applied scoring system allows standardized interpretation of angiographic findings in NOMI patients. Beyond that the score seems to correlate well with risk factors of NOMI and outcome."
        },
        {
            "title": "Contrast-enhanced ultrasonography of the prostate with Sonazoid.",
            "abstract": "Objective:\n        \n      \n      The diagnosis of prostate cancer is based on the results of ultrasonography-guided needle biopsy of the prostate, but cancer foci are often not visible in conventional transrectal ultrasonography. Sonazoid is a new microbubble contrast agent. The purpose of our study was to compare areas of contrast material enhancement in the prostate at ultrasonography with whole-mount radical prostatectomy specimens to determine if the use of Sonazoid improves the detection rate of prostate cancer.\n    \n\n\n          Methods:\n        \n      \n      Fifty patients with biopsy-proven cancer of the prostate who were scheduled to undergo radical prostatectomy were recruited for this study. The day before the operation, each patient was evaluated with ultrasonography at baseline and again during intravenous infusion of Sonazoid. A map of ultrasonography findings was created prospectively at the time of imaging. Following radical prostatectomy, independent mapping of the pathologic results was performed and the maps were compared.\n    \n\n\n          Results:\n        \n      \n      Ultrasonography evaluation at baseline demonstrated that at least one focus of cancer was identified in 20 of the 50 subjects (40.0%). Meanwhile at least one cancer focus was enhanced in 31 of the 50 patients (62.0%) when Sonazoid was used. The combination of baseline grayscale imaging and contrast-enhanced imaging allowed identification of at least one focus of cancer in 40 patients (80.0%). Contrast-enhanced ultrasonography can improve sensitivity, especially for the detection of large cancer, peripheral zone cancer and highly malignant cancer.\n    \n\n\n          Conclusions:\n        \n      \n      Our study has demonstrated significantly improved detection of prostate cancer with the combination of baseline grayscale imaging and contrast-enhanced imaging compared with conventional ultrasonography techniques only, and this technique may be applicable to targeted biopsy."
        },
        {
            "title": "The role of computed tomographic scan in ongoing triage of operative hepatic trauma: A Western Trauma Association multicenter retrospective study.",
            "abstract": "Background:\n        \n      \n      A subset of patients explored for abdominal injury have persistent hepatic bleeding on postoperative computed tomography (CT) and/or angiography, either not identified or not manageable at initial laparotomy. To identify patients at risk for ongoing hemorrhage and guide triage to angiography, we investigated the relationship of early postoperative CT scan with outcomes in operative hepatic trauma.\n    \n\n\n          Methods:\n        \n      \n      This is a retrospective review of 528 patients with hepatic injury taken to laparotomy without imaging within 6 hours of arrival to six trauma centers from 2007 to 2013, coordinated through the Western Trauma Association multicenter trials group.\n    \n\n\n          Results:\n        \n      \n      A total of 528 patients were identified, with a mean age of 31 years, 82% male, and 37% blunt injury; mean (SD) Injury Severity Score (ISS) was 27 (16) and base deficit was -9 (6); in-hospital mortality was 26%. Seventy-three patients died during initial exploration. Of 455 early survivors, 123 (27%) had a postoperative contrast CT scan within 24 hours of laparotomy. CT patients had more common blunt injury, higher ISS, and lower base deficit than those who did not undergo CT. CT identified hepatic contrast extravasation or pseudoaneurysm in 10 patients (8%). Hepatic bleeding on CT was 83% sensitive and 75% specific (likelihood ratio, 3.3) for later positive angiography; negative CT finding was 96% sensitive and 83% specific (likelihood ratio, 5.7) for later negative or not performed angiography. Despite occurring in a more severely injured cohort, performance of early postoperative CT was associated with reduced mortality (odds ratio, 0.16) in multivariate analysis. Blunt mechanism was also a multivariate predictor of mortality (odds ratio, 3.0).\n    \n\n\n          Conclusion:\n        \n      \n      Early postoperative CT scan after laparotomy for hepatic trauma identifies clinically relevant ongoing bleeding and is sufficiently sensitive and specific to guide triage to angiography. Contrast CT should be considered in the management algorithm for hepatic trauma, particularly in the setting of blunt injury. Further study should identify optimal patient selection criteria and CT scan timing in this population.\n    \n\n\n          Level of evidence:\n        \n      \n      Care management/therapeutic study, level IV; epidemiologic/prognostic study, level III."
        },
        {
            "title": "Accurate, practical and cost-effective assessment of carotid stenosis in the UK.",
            "abstract": "Objectives:\n        \n      \n      To determine whether less invasive imaging tests [ultrasound (US), magnetic resonance angiography (MRA), computed tomographic angiography (CTA) and contrast-enhanced MRA (CEMRA)], alone or combined, could replace intra-arterial angiography (IAA), what effect this would have on strokes and deaths, endarterectomies performed and costs, and whether less invasive tests were cost-effective.\n    \n\n\n          Data sources:\n        \n      \n      Electronic databases covering the years 1980-2003 inclusive, updated to April 2004. Key journals from 1990 to the end of 2002.\n    \n\n\n          Review methods:\n        \n      \n      The authors constituted a panel of experts in stroke, imaging, vascular surgery, statistics and health economic modelling. The accuracy of less invasive carotid imaging was systematically reviewed using Standards for Reporting of Diagnostic Accuracy (STARD) methodology, supplemented by individual patient data from UK primary research and audit studies. A systematic review of the costs of less invasive tests, outpatient clinics, endarterectomy and stroke was performed, along with a microcosting exercise. A model of the process of care following a transient ischaemic attack (TIA)/minor stroke was developed, populated with data from stroke epidemiology studies in the UK, effects of medical and surgical interventions, outcomes, quality of life and costs. A survey of UK stroke prevention clinics provided typical timings. Twenty-two different carotid imaging strategies were evaluated for short- and long-term outcomes, quality-adjusted life-years (QALYs) and net benefit.\n    \n\n\n          Results:\n        \n      \n      In 41 included studies (2404 patients, median age 60-65 years), most data were available on 70-99% stenosis. CEMRA was the most accurate [sensitivity 0.94, 95% confidence interval (CI) 0.88 to 0.97; specificity 0.93, 95% CI 0.89 to 0.96], compared with US, MRA and CTA, which were all similar (e.g. for US: sensitivity 0.89, 95% CI 0.85 to 0.92; specificity 0.84, 95% CI 0.77 to 0.89). Data for 50-69% stenoses and on combinations of tests were too sparse to be reliable. There was heterogeneity between studies for all imaging modalities except for CTA. The individual patient data (2416 patients) showed that the literature overestimated test accuracy in routine practice and that, in general, tests perform with higher sensitivity and specificity in asymptomatic than in symptomatic arteries. In the cost-effectiveness model, on current UK timings, strategies allowed more patients to reach endarterectomy very quickly, and where those with 50-69% stenosis would be offered surgery in addition to those with 70-99%, prevented most strokes and produced greatest net benefit. This included most strategies with US as first or repeat test, and not those with IAA. However, the model was sensitive to less invasive test accuracy, cost and timing of endarterectomy. In patients investigated late after TIA, test accuracy is crucial and CEMRA should be used before surgery.\n    \n\n\n          Conclusions:\n        \n      \n      In the UK, less invasive tests can be used in place of IAA if radiologists trained in carotid imaging are available. Imaging should be carefully audited. Stroke prevention clinics should reduce waiting times at all stages to improve speed of access to endarterectomy. In patients presenting late after TIA, test accuracy is very important and US results should be confirmed by CEMRA, as patients with 50-69% stenosis are less likely to benefit. More data are required to define the accuracy of the less invasive tests, with improvements made in the data collection methods used and how data are presented. Consideration should also be given to the use of new technologies and randomised trials."
        },
        {
            "title": "Effect of Coronary Revascularization on the Prognostic Value of Stress Myocardial Contrast Wall Motion and Perfusion Imaging.",
            "abstract": "Background:\n        \n      \n      The assessment of myocardial perfusion (MP) and wall motion (WM) using contrast dipyridamole echocardiography (cSE-WMP) improves the sensitivity to detect coronary artery disease and the stratification of cardiac events, but its long-term value for fatal and nonfatal ischemic cardiac events, also with respect to patients undergoing revascularization or not, remains to be determined.\n    \n\n\n          Methods and results:\n        \n      \n      One-thousand three-hundred and twenty-nine patients with suspect or known CAD who underwent cSE-WMP were followed for a median 5.5 years. The independent prognostic value of cSE-WMP regarding cardiac death or nonfatal myocardial infarction was related to stress WM and MP, rest ejection fraction, clinical risk factors, and medications. Patients revascularized after cSE-WMP were separately analyzed to determine whether the procedure influenced outcome and whether this depends on cSE-WMP results. A total of 125 cardiac fatal and nonfatal ischemic events (9.4%) occurred during the follow-up (61 deaths, 64 myocardial infarctions). The 5-year event rate with normal MP and WM was 5.9%, 9.9% with isolated MP defects (normal WM), and 15.5% with both MP and WM abnormalities. In patients not undergoing revascularization (n=1111), reversible MP defects added discrimination value over WM response and clinical factors/medication data (P=0.001), while in the cohort undergoing revascularization (n=218), cSE-WMP results did not influence outcome.\n    \n\n\n          Conclusions:\n        \n      \n      cSE-WMP, with both contrast MP and WM assessments, provides independent, incremental prognostic information regarding ischemic cardiac events at 5 years in patients with known or suspected coronary artery disease. Revascularization reduces cardiac events after an abnormal cSE-WMP, resulting in outcomes not different from those in patients with normal cSE-WMP."
        },
        {
            "title": "Amaurosis fugax: is it innocuous?",
            "abstract": "A 10 year retrospective study of 103 patients with amaurosis fugax was done. Sixty-two patients with symptoms of amaurosis fugax underwent arteriography, which demonstrated ulcerated carotid plaque in 36 and hemodynamically significant stenoses (greater than 75% diameter reduction) in 26. These 62 patients underwent carotid endarterectomy. The other 41 patients who had proven ulcerated plaque (33 patients) or hemodynamic stenoses (eight patients) were not treated surgically and served as a control series. No strokes or deaths occurred in the immediate postoperative period. Follow-up of the 62 operated patients extending to 10 years (mean 4.2 years), revealed one (1.6%) patient with recurrent amaurosis fugax symptoms, two (3.2%) with transient ischemic attacks, and one (1.6%) with a stroke in the operated hemisphere. In the nonoperated group, despite aspirin or warfarin treatment, four (9.7%) patients had ongoing amaurosis fugax symptoms, and two (4.8%) developed transient ischemic attacks that led to carotid endarterectomy. One (2.4%) other patient developed sudden, permanent monocular blindness, and two (4.8%) suffered hemispheric strokes, one of which was fatal. The cumulative morbidity (ongoing ocular or transient ischemic attack symptoms, perioperative and late stroke) in the operated group was 6.4% (four patients), while the cumulative morbidity in the nonoperated group was significantly higher at 21.9% (nine patients) (p = 0.02). When patients present with symptoms of amaurosis fugax and have demonstrable carotid bifurcation disease, carotid endarterectomy is recommended. Amaurosis fugax should be regarded as a harbinger of monocular blindness and stroke."
        },
        {
            "title": "Quantitative global plaque characteristics from coronary computed tomography angiography for the prediction of future cardiac mortality during long-term follow-up.",
            "abstract": "Aims:\n        \n      \n      Adverse plaque characteristics determined by coronary computed tomography angiography (CTA) have been associated with future cardiac events. Our aim was to investigate whether quantitative global per-patient plaque characteristics from coronary CTA can predict subsequent cardiac death during long-term follow-up.\n    \n\n\n          Methods and results:\n        \n      \n      Out of 2748 patients without prior history of coronary artery disease undergoing CTA with dual-source CT, 32 patients suffered cardiac death (mean follow-up of 5 ± 2 years). These patients were matched to 32 controls by age, gender, risk factors, and symptoms (total 64 patients, 59% male, age 69 ± 10 years). Coronary CTA data sets were analysed by semi-automated software to quantify plaque characteristics over the entire coronary tree, including total plaque volume, volumes of non-calcified plaque (NCP), low-density non-calcified plaque (LD-NCP, attenuation <30 Hounsfield units), calcified plaque (CP), and corresponding burden (plaque volume × 100%/vessel volume), as well as stenosis and contrast density difference (CDD, maximum percent difference in luminal attenuation/cross-sectional area compared to proximal cross-section). In patients who died from cardiac cause, NCP, LD-NCP, CP and total plaque volumes, quantitative stenosis, and CDD were significantly increased compared to controls (P < 0.025 for all). NCP > 146 mm³ [hazards ratio (HR) 2.24; 1.09-4.58; P = 0.027], LD-NCP > 10.6 mm³ (HR 2.26; 1.11-4.63; P = 0.025), total plaque volume > 179 mm³ (HR 2.30; 1.12-4.71; P = 0.022), and CDD > 35% in any vessel (HR 2.85;1.4-5.9; P = 0.005) were associated with increased risk of future cardiac death, when adjusted for segment involvement score.\n    \n\n\n          Conclusion:\n        \n      \n      Among quantitative global plaque characteristics, total, non-calcified, and low-density plaque volumes as well as CDD predict cardiac death in long-term follow-up."
        },
        {
            "title": "Perfusion computed tomography in predicting treatment response of advanced esophageal squamous cell carcinomas.",
            "abstract": "Background:\n        \n      \n      The purpose of this study was to prospectively evaluate the predictive value of perfusion computed tomography (CT) for response of local advanced esophageal carcinoma to radiotherapy and chemotherapy.\n    \n\n\n          Materials and methods:\n        \n      \n      Before any treatment, forty-three local advanced esophageal squamous cell carcinomas were prospectively evaluated by perfusion scan with 16-row CT from June 2009 to January 2012. Perfusion parameters, including perfusion (BF), peak enhanced density (PED), blood volume (BV), and time to peak (TTP) were measured using Philips perfusion software. Seventeen cases received definitive radiotherapy and 26 received concurrent chemo-radiotherapy. The response was evaluated by CT scan and esophagography. Differences in perfusion parameters between responders and non-responders were analyzed, and ROCs were used to assess predictive value of the baseline parameters for treatment response.\n    \n\n\n          Results:\n        \n      \n      There were 25 responders (R) and 18 non-responders (NR). Responders showed significantly higher BF (R:34.1 ml/100 g/min vs NR: 25.0 ml/100 g/min, p=0.001), BV (23.2 ml/100g vs 18.3 ml/100g, p=0.009) and PED (32.5 HU vs 28.32 HU, P=0.003) than non-responders. But the baseline TTP (R: 38.2 s vs NR: 44.10 s, p=0.172) had no difference in the two groups. For baseline BF, a threshold of 36.1 ml/100 g/min achieved a sensitivity of 56%, and a specificity of 94.4% for detection of clinical responders from non-responders.\n    \n\n\n          Conclusions:\n        \n      \n      The results suggest that the perfusion CT can provide some helpful information for identifying tumors that may respond to radio-chemotherapy."
        },
        {
            "title": "Driver's visual attention as a function of driving experience and visibility. Using a driving simulator to explore drivers' eye movements in day, night and rain driving.",
            "abstract": "Road crashes are the main cause of death of young people in the developed world. The reasons that cause traffic crashes are numerous; however, most researchers agree that a lack of driving experience is one of the major contributing factors. In addition it has been demonstrated that environmental factors such as driving during night and rain increases the risk of a crash. Both of these factors may be related to drivers' visual search strategies that become more efficient with increased experience. In the present study we recorded the eye movements of driving instructors and learner drivers while they drove three virtual routes that included day, night and rain routes in a driving simulator. The results showed that driving instructors had an increased sampling rate, shorter processing time and broader scanning of the road than learner drivers. This broader scanning of the road could be possibly explained by the mirror inspection pattern which revealed that driving instructors fixated more on the side mirrors than learner drivers. Also it was found that poor visibility conditions, especially rain, decrease the effectiveness of drivers' visual search. The lack of interaction between driving experience and visibility suggests that some aspects of visual search are affected by general rather than situation specific driving experience. The present findings support the effect of driving experience in modifying eye movement strategies. The high accident risk of night and rain driving could be partly explained by the decrement in visual search strategies during these conditions. Finally it is argued that the use of driving simulators can provide valuable insights regarding driving safety."
        },
        {
            "title": "Predictors and Outcomes of Postcontrast Acute Kidney Injury after Endovascular Renal Artery Intervention.",
            "abstract": "Purpose:\n        \n      \n      To determine incidence, predictors, and clinical outcomes of postcontrast acute kidney injury (PC-AKI) following renal artery stent placement for atherosclerotic renal artery stenosis.\n    \n\n\n          Materials and methods:\n        \n      \n      This retrospective study reviewed 1,052 patients who underwent renal artery stent placement for atherosclerotic renal artery stenosis; 437 patients with follow-up data were included. Mean age was 73.6 years ± 8.3. PC-AKI was defined as absolute serum creatinine increase ≥ 0.3 mg/dL or percentage increase in serum creatinine ≥ 50% within 48 hours of intervention. Logistic regression analysis was performed to identify risk factors for PC-AKI. The cumulative proportion of patients who died or went on to hemodialysis was determined using Kaplan-Meier survival analysis.\n    \n\n\n          Results:\n        \n      \n      Mean follow-up was 71.1 months ± 68.4. PC-AKI developed in 26 patients (5.9%). Patients with PC-AKI had significantly higher levels of baseline proteinuria compared with patients without PC-AKI (odds ratio = 1.38; 95% confidence interval, 1.11-1.72; P = .004). Hydration before intervention, chronic kidney disease stage, baseline glomerular filtration rate, statin medications, contrast volume, and iodine load were not associated with higher rates of PC-AKI. Dialysis-free survival and mortality rates were not significantly different between patients with and without PC-AKI (P = .50 and P = .17, respectively).\n    \n\n\n          Conclusions:\n        \n      \n      Elevated baseline proteinuria was the only predictor for PC-AKI in patients undergoing renal artery stent placement. Patients who developed PC-AKI were not at greater risk for hemodialysis or death."
        },
        {
            "title": "Sensory Impairment, Functional Balance and Physical Activity With All-Cause Mortality.",
            "abstract": "Objective:\n        \n      \n      No study has comprehensively examined the independent and combined effects of sensory impairment, physical activity and balance on mortality risk, which was this study's purpose.\n    \n\n\n          Methods:\n        \n      \n      Data from the population-based 2003-2004 National Health and Nutrition Examination Survey (NHANES) was used, with follow-up through 2011. Physical activity was assessed via accelerometry. Balance was assessed via the Romberg test. Peripheral neuropathy was assessed objectively using a standard monofilament. Visual impairment was objectively assessed using an autorefractor. Hearing impairment was assessed via self-report. A 5-level index variable (higher score is worse) was calculated based on the participant's degree of sensory impairment, dysfunctional balance and physical inactivity.\n    \n\n\n          Results:\n        \n      \n      Among the 1658 participants (age 40-85 yrs), 228 died during the median follow-up period of 92 months. Hearing (Hazard Ratio [HR] = 1.18; P = .40), vision (HR = 1.17; P = .58) and peripheral neuropathy (HR = 1.06; P = .71) were not independently associated with all-cause mortality, but physical activity (HR = 0.97; P = .01) and functional balance (HR = 0.59; P = .03) were. Compared with those with an index score of 0, the HR (95% CI) for those with an index score of 1 to 3, respectively, were 1.20 (0.46-3.13), 2.63 (1.08-6.40) and 2.88 (1.36-6.06).\n    \n\n\n          Conclusions:\n        \n      \n      Physical activity and functional balance are independent contributors to survival."
        },
        {
            "title": "Cone photoreceptor abnormalities correlate with vision loss in patients with Stargardt disease.",
            "abstract": "PURPOSE. To study the relationship between macular cone structure, fundus autofluorescence (AF), and visual function in patients with Stargardt disease (STGD). METHODS. High-resolution images of the macula were obtained with adaptive optics scanning laser ophthalmoscopy (AOSLO) and spectral domain optical coherence tomography in 12 patients with STGD and 27 age-matched healthy subjects. Measures of retinal structure and AF were correlated with visual function, including best-corrected visual acuity, color vision, kinetic and static perimetry, fundus-guided microperimetry, and full-field electroretinography. Mutation analysis of the ABCA4 gene was completed in all patients. RESULTS. Patients were 15 to 55 years old, and visual acuity ranged from 20/25-20/320. Central scotomas were present in all patients, although the fovea was spared in three patients. The earliest cone spacing abnormalities were observed in regions of homogeneous AF, normal visual function, and normal outer retinal structure. Outer retinal structure and AF were most normal near the optic disc. Longitudinal studies showed progressive increases in AF followed by reduced AF associated with losses of visual sensitivity, outer retinal layers, and cones. At least one disease-causing mutation in the ABCA4 gene was identified in 11 of 12 patients studied; 1 of 12 patients showed no disease-causing ABCA4 mutations. CONCLUSIONS. AOSLO imaging demonstrated abnormal cone spacing in regions of abnormal fundus AF and reduced visual function. These findings provide support for a model of disease progression in which lipofuscin accumulation results in homogeneously increased AF with cone spacing abnormalities, followed by heterogeneously increased AF with cone loss, then reduced AF with cone and RPE cell death."
        },
        {
            "title": "Diagnostic testing following fecal occult blood screening in the elderly.",
            "abstract": "Background:\n        \n      \n      Screening with a fecal occult blood test (FOBT) has been shown to reduce colorectal cancer mortality in controlled trials. Recently, Medicare approved payment for FOBT screening. We evaluated the pattern of diagnostic testing following the initial FOBT in elderly Medicare beneficiaries. Such follow-up testing would in the long run influence both the cost and the benefit of widespread use of FOBT.\n    \n\n\n          Methods:\n        \n      \n      Using Medicare's National Claims History System, we identified 24 246 Americans 65 years old or older who received FOBT at physician visits between January 1 and April 30, 1995. Prior to FOBT, these people had no evidence of any conditions for which FOBT might be used diagnostically. We examined relevant diagnostic testing in this cohort during the subsequent 8 months and determined what proportion of those received an evaluation recommended by the American College of Physicians.\n    \n\n\n          Results:\n        \n      \n      For every 1000 Medicare beneficiaries who received FOBT, 93 (95% confidence interval = 89-96 per 1000) had positive findings and relevant testing in the subsequent 8 months. Of these, 34% had the recommended evaluation of either colonoscopy or flexible sigmoidoscopy with an air-contrast barium enema. Another 34% received a partial colonic evaluation with either flexible sigmoidoscopy or a barium enema. The remaining 32% received other gastrointestinal (GI) testing without evaluation of the colonic lumen: computed tomography or magnetic resonance imaging of the abdomen (15%), upper GI series (10%), carcinoembryonic antigen (7%), and upper endoscopy (2%). Restricting the analysis to testing performed within 2 months of the initial FOBT yielded similar results.\n    \n\n\n          Conclusion:\n        \n      \n      Following FOBT, many Medicare beneficiaries get further diagnostic testing, but only a small proportion receives the recommended evaluation. With this pattern of practice, population screening is likely to be more costly and less effective than estimated from controlled trials."
        },
        {
            "title": "Preprocedural N-terminal pro-brain natriuretic peptide (NT-proBNP) is similar to the Mehran contrast-induced nephropathy (CIN) score in predicting CIN following elective coronary angiography.",
            "abstract": "Background:\n        \n      \n      N-terminal pro-brain natriuretic peptide (NT-proBNP) has been associated with important risk factors for contrast-induced nephropathy (CIN). However, few studies have investigated the predictive value of NT-proBNP itself. This study investigated whether levels of preprocedural NT-proBNP could predict CIN after elective coronary angiography as effectively as the Mehran CIN score.\n    \n\n\n          Methods and results:\n        \n      \n      We retrospectively observed 2248 patients who underwent elective coronary angiography. The predictive value of preprocedural NT-proBNP for CIN was assessed by receiver operating characteristic and multivariable logistic regression analysis. The 50 patients (2.2%) who developed CIN had higher Mehran risk scores (9.5 ± 5.1 versus 4.8 ± 3.8), and higher preprocedural levels of NT-proBNP (5320 ± 7423 versus 1078 ± 2548 pg/mL, P<0.001). Receiver operating characteristic analysis revealed that NT-proBNP was not significantly different from the Mehran CIN score in predicting CIN (C=0.7657 versus C=0.7729, P=0.8431). An NT-proBNP cutoff value of 682 pg/mL predicted CIN with 78% sensitivity and 70% specificity. Multivariable analysis suggested that, after adjustment for other risk factors, NT-proBNP >682 pg/mL was significantly associated with CIN (odds ratio: 4.007, 95% CI: 1.950 to 8.234; P<0.001) and risk of death (hazard ratio: 2.53; 95% CI: 1.49 to 4.30; P=0.0006).\n    \n\n\n          Conclusions:\n        \n      \n      Preprocedural NT-proBNP >682 pg/mL was significantly associated with the risk of CIN and death. NT-proBNP, like the Mehran CIN score, may be another useful and rapid screening tool for CIN and death risk assessment, identifying subjects who need therapeutic measures to prevent CIN."
        },
        {
            "title": "Noninvasive Assessment of Portal Hypertension Using Spectral Computed Tomography.",
            "abstract": "Background:\n        \n      \n      Early diagnosis of portal hypertension is imperative for timely treatment to reduce the mortality rate. However, there is still no adequate method to noninvasively and accurately assess the portal hypertension in routine clinical practice.\n    \n\n\n          Purpose:\n        \n      \n      We aimed to evaluate the accuracy of parameters measured using dual energy spectral computed tomography (LightSpeed CT750 HD) in assessing portal venous pressure in patients with liver cirrhosis.\n    \n\n\n          Study:\n        \n      \n      Forty-five patients with liver cirrhosis who underwent percutaneous transhepatic portal vein puncture as part of their treatment for liver disease were enrolled in this study. Measurement of direct portal venous pressure was performed preoperatively. All patients underwent dual energy spectral computed tomography within 3 days before their operations.\n    \n\n\n          Results:\n        \n      \n      The iodine concentrations of portal vein and hepatic parenchyma during the portal venous phase and the alanine aminotransferase level were found to be independently correlated with the direct portal venous pressure according to stepwise multivariate linear regression analysis (P<0.001, 0.004, and 0.024, respectively). In a receiver operating characteristic analysis, the area under the receiver operating characteristic of iodine concentrations of the portal vein (ICPV) for identifying clinically significant portal hypertension (≥10 mm Hg) was significantly higher than that of iodine concentrations of hepatic parenchyma (ICliver) and the alanine aminotransferase level (0.944, 0.825, and 0.301, respectively). The threshold ICPV of 58.27 yielded a sensitivity of 93.8%, specificity of 69.2%, positive predictive value of 88.2%, and negative predictive value of 81.8%, respectively.\n    \n\n\n          Conclusions:\n        \n      \n      ICPV values may be a useful tool in noninvasively assessing the portal venous pressure and identifying clinically significant portal hypertension in liver cirrhosis."
        },
        {
            "title": "Perception of bedside teaching within the palliative care setting-views from patients, students and staff members.",
            "abstract": "Background:\n        \n      \n      Bedside teaching is an essential part of medical education. However, within the setting of palliative care at a university hospital, different needs and interests may collide. On the one hand students need to be prepared for the care for critically ill patients yet on the other, patients require particular tranquility and protection. An analysis of potential harm and benefits resulting from bedside teaching in palliative care is a crucial prerequisite for the organization of bedside teaching in this sensitive setting.\n    \n\n\n          Methods:\n        \n      \n      We performed a qualitative study researching the perception and challenges of bedside teaching on the palliative care ward at the Rostock University Medical Center. To that extent, elective courses \"Intensive Practical Training in Palliative Care\" were held during the summer and winter terms of 2016 and 2016/2017, respectively. Students and team members answered a self-developed questionnaire on the perception of bedside teaching on the palliative care ward. Patients were lead through semi structured interviews later analyzed according to the thematic framework approach.\n    \n\n\n          Results:\n        \n      \n      A group of 21 students in their clinical years, 20 patients and 19 members of the palliative care team participated in this study. The experience of working with patients in the palliative care setting was very valuable for almost all students. Most patients enjoyed the presence of students on the ward. However, some missed clear cutoff criteria for termination. Students mostly felt comfortable with palliative care patients and did not request professional help for coping with experienced aspects of dying and death. In contrast, members of the palliative care team though were concerned about patients' safety and comfort and requested strict guidance of students during the course.\n    \n\n\n          Conclusions:\n        \n      \n      Bedside teaching within the palliative care setting classified a valuable tool for specialized palliative care skills. However, in order to protect the critically ill, students need structured guidance and strict behavioral instructions for presence at the bed side."
        },
        {
            "title": "Survival outcomes of breast cancer patients who receive neoadjuvant chemotherapy: association with dynamic contrast-enhanced MR imaging with computer-aided evaluation.",
            "abstract": "Purpose:\n        \n      \n      To retrospectively evaluate whether dynamic contrast agent-enhanced (DCE) magnetic resonance (MR) imaging parameters assessed by a computer-aided evaluation program are associated with recurrence-free and overall survival in breast cancer patients who received neoadjuvant chemotherapy (NAC).\n    \n\n\n          Materials and methods:\n        \n      \n      This study was institutional review board approved and informed consent was waived. Between January 2007 and December 2009, 187 consecutive women (mean age, 46.6 years; range, 24-78 years) who had undergone NAC, DCE MR imaging before and after NAC, and surgery for invasive breast cancers (mean size, 5.0 cm; range, 2.0-14.8 cm on surgical histologic analysis) were identified. The tumor size, volume, and kinetic parameters (persistent, plateau, or washout components) were measured with a computer-aided evaluation program on DCE MR images before and after NAC, and their percentage changes were calculated. The Cox proportional hazards model was used to determine the association between DCE MR imaging parameters and recurrence-free survival and overall survival after controlling for clinical-pathologic variables.\n    \n\n\n          Results:\n        \n      \n      There were 50 events, including 38 recurrences (29 distant, six local, and three both) and 12 deaths, at a mean follow-up of 47.4 months. At multivariate analysis, a smaller reduction in tumor volume (recurrence-free survival hazard ratio, 5.75; 95% confidence interval: 1.14, 8.64; and overall survival hazard ratio, 2.12; 95% confidence interval: 1.08, 5.69) and a smaller reduction in washout component (recurrence-free survival hazard ratio, 1.15; 95% CI: 1.06, 1.55; and overall survival hazard ratio, 1.26; 95% confidence interval: 1.03, 1.52) after NAC were independent significant variables for worse recurrence-free survival and overall survival.\n    \n\n\n          Conclusion:\n        \n      \n      Smaller reduction in tumor volume and a smaller reduction in washout component on DCE MR images assessed with computer-aided evaluation after NAC were independent parameters of worse recurrence-free survival and overall survival in breast cancer patients who received NAC."
        },
        {
            "title": "Quality of life associated with visual loss: a time tradeoff utility analysis comparison with medical health states.",
            "abstract": "Purpose:\n        \n      \n      To assess the visual utility values of patients with ocular disease and to compare these values with those of patients with systemic health states\n    \n\n\n          Design:\n        \n      \n      Cross-sectional utility value assessment.\n    \n\n\n          Methods:\n        \n      \n      Consecutive patients with ophthalmic diseases were interviewed in a one-on-one fashion using a standardized time tradeoff utility value assessment form. These values were compared with utility values for systemic health states present in the literature.\n    \n\n\n          Intervention:\n        \n      \n      None.\n    \n\n\n          Main outcome measure:\n        \n      \n      Time tradeoff utility value on a scale ranging from 1.0 (perfect visual health) to 0.0 (death). The ophthalmic patient groups were stratified into 4 visual groups dependent on the visual acuity in the better-seeing eye. The groups were as follows: group 1, 20/20 to 20/25; group 2, 20/30 to 20/50; group 3, 20/60 to 20/100; group 4, 20/200 to no light perception.\n    \n\n\n          Results:\n        \n      \n      A total of 500 subjects were enrolled in the study. The mean utility values for the visually stratified groups were: group 1, 0.88; group 2, 0.81; group 3, 0.72; group 4, 0.61. Comparable respective systemic health state utility values for each of the ophthalmic groups were: diabetes mellitus, status after kidney transplantation, moderate stroke, and moderately severe stroke.\n    \n\n\n          Conclusions:\n        \n      \n      Visual loss is associated with a substantial and measurable diminution in quality of life. This diminution in quality of life can be directly compared with that induced by systemic health states."
        },
        {
            "title": "Surgical treatment of iris and ciliary body melanoma: follow-up of a 25-year series of patients.",
            "abstract": "Purpose:\n        \n      \n      To evaluate outcome of surgical resection of iris and irido-ciliary melanomas.\n    \n\n\n          Method:\n        \n      \n      Retrospective analysis of all cases treated in Denmark 1975-1999 with clinical follow-up in 2002 and death certificate analysis in 2008. A quality of life questionnaire was completed at follow-up.\n    \n\n\n          Results:\n        \n      \n      A total of 53 patients were identified. Of these, 47 were examined at follow-up. Median observation time was 7.15 years (range 0.3-27.4 years). Five patients had died of nonmelanoma causes, and one could not be reached because of immigration. None of the patient had melanoma metastases, and none had died of melanoma-related causes. Only one patient had a local recurrence, which was successfully treated by cryotherapy. The quality of life-related questions demonstrated that most patients (40) suffered from photophobia, and eight patients had changed their driving habits, not driving at night time. However, none had changed job as a consequence of the surgical treatment. Only two patients were emotionally affected by the diagnosis of iris melanoma.\n    \n\n\n          Conclusion:\n        \n      \n      Resection of small iris and irido-ciliary melanomas is a safe and efficient procedure, provided that strict diagnostic and surgical procedures are followed and the preoperative intraocular pressure is normal."
        },
        {
            "title": "Organ dysfunction and muscular disability in myotonic dystrophy type 1.",
            "abstract": "Myotonic dystrophy type 1 (DM1) is a multisystemic disorder characterized by muscle weakness and multiple organ impairment, especially the eyes, lung, and heart. We conducted the current study to analyze the prevalence and intercorrelation among these disorders and their respective relationships with muscular disability. We assessed medical history, anthropometric data, lung volumes, arterial and venous blood samples, surface 12-lead electrocardiogram, echocardiography, ophthalmologic examination, and muscular impairment rating scale (MIRS) in 106 patients (48 male and 58 female) with DM1, aged 43.7 ± 12.8 years. Obesity, hypertriglyceridemia, and diabetes were found in respectively 25.6%, 47.6%, and 17.1% of patients. Disabling cataract was found in 43.4%, and was independently predicted by age and MIRS. Restrictive lung disease was noted in 34%, and was predicted by MIRS, CTG repeat expansion, and body mass index. Conduction disorders were found in 30.2% of patients and were predicted by left ventricular ejection fraction, MIRS, and CTG repeat expansion.We found significant relationships between cataract, restrictive lung disease, and conduction disorders: patients with cataract and those with conduction disorders exhibited more severe restrictive lung disease than the other patients. Conversely, the relative risk of restrictive lung disease was 2.42 (1% confidence interval [CI], 1.06-5.51) in patients with cataract and 2.54 (1% CI, 1.26-5.07) in patients with conduction disorders. Multivariate analysis revealed that MIRS was the only independent predictor for conduction disorders and restrictive lung disease. MIRS ≥3 and MIRS ≥4 were the best simple cutoff values to predict, respectively, lung and cardiac involvements.To conclude, muscular disability, ophthalmologic, and cardiac and pulmonary involvement are strongly correlated. Particular attention should be given to these entities in patients with distal or proximal muscular weakness."
        },
        {
            "title": "Synergistic effects of cognitive impairment on physical disability in all-cause mortality among men aged 80 years and over: Results from longitudinal older veterans study.",
            "abstract": "Objective:\n        \n      \n      We evaluated effects of the interrelationship between physical disability and cognitive impairment on long-term mortality of men aged 80 years and older living in a retirement community in Taiwan.\n    \n\n\n          Methods:\n        \n      \n      This prospective cohort study enrolled older men aged 80 and older living in a Veterans Care Home. Those with confirmed diagnosis of dementia were excluded. All participants received comprehensive geriatric assessment, including sociodemographic data, Charlson's Comorbidity Index (CCI), geriatric syndromes, activities of daily living (ADL) using the Barthel index and cognitive function using the Mini-Mental State Examination (MMSE). Subjects were categorized into normal cognitive function, mild cognitive deterioration, and moderate-to-severe cognitive impairment and were further stratified by physical disability status. Kaplan-Meier log-rank test was used for survival analysis. After adjusting for sociodemographic characteristics and geriatric syndromes, Cox proportional hazards model was constructed to examine associations between cognitive function, disability and increased mortality risk.\n    \n\n\n          Results:\n        \n      \n      Among 305 male subjects aged 85.1 ± 4.1 years, 89 subjects died during follow-up (mean follow-up: 1.87 ± 0.90 years). Kaplan-Meier unadjusted analysis showed reduced survival probability associated with moderate-to-severe cognitive status and physical disability. Mortality risk increased significantly only for physically disabled subjects with simultaneous mild cognitive deterioration (adjusted HR 1.951, 95% CI 1.036-3.673, p = 0.038) or moderate-to-severe cognitive impairment (aHR 2.722, 95% CI 1.430-5.181, p = 0.002) after adjusting for age, BMI, education levels, smoking status, polypharmacy, visual and hearing impairment, urinary incontinence, fall history, depressive symptoms and CCI. Mortality risk was not increased among physically independent subjects with or without cognitive impairment, and physically disabled subjects with intact cognition.\n    \n\n\n          Conclusions:\n        \n      \n      Physical disability is a major risk factor for all-cause mortality among men aged 80 years and older, and risk increased synergistically when cognitive impairment was present. Cognitive impairment alone without physical disability did not increase mortality risk in this population."
        },
        {
            "title": "Blood pressure control in type II diabetics with diabetic retinopathy.",
            "abstract": "Background:\n        \n      \n      Large clinical trials have emphasized that blood pressure control provides a major clinical benefit in reducing the risk of blindness in patients with diabetic retinopathy.\n    \n\n\n          Methods:\n        \n      \n      This audit was carried out to assess the quality of care for hypertension in 100 consecutive type II diabetics with diabetic retinopathy.\n    \n\n\n          Results:\n        \n      \n      The target blood pressure of 140/80 mmHg was achieved only in 38% of the patients. We also observed that 65% of the patients requiring diabetic macular laser treatment in this cohort had suboptimal control. The factors associated with suboptimal blood pressure control are identified and discussed.\n    \n\n\n          Conclusion:\n        \n      \n      Despite the unequivocal fact that lowering BP significantly reduces morbidity and mortality in diabetics, the majority of patients are not treated to a goal BP."
        },
        {
            "title": "Age differential effects of severity of visual impairment on mortality among older adults in China.",
            "abstract": "We use a population-based longitudinal survey in China from 2002 to 2005 to examine age differentials in the association between severity of visual impairment and mortality risk in older adults. Controlling for numerous factors and baseline health, a substantial age difference is found. Young-old women and men aged 65 to 79 with severe visual impairments have 161% (hazard ratio = 2.61) and 52% (hazard ratio = 1.52) higher risk of death respectively as compared to their unimpaired counterparts. Mild impairment does not increase mortality risk among young-old adults, while both mild and severe impairment increase mortality risk by 33% and 32% for women and 24% and 34% for men among the oldest-old as a whole when all factors are controlled for. We conclude that visual impairment is an independent predictor of mortality and severe visual impairment likely plays a stronger role in determining mortality risk among young-old adults than among the oldest-old."
        },
        {
            "title": "Recurrent ovarian cancer: use of contrast-enhanced CT and PET/CT to accurately localize tumor recurrence and to predict patients' survival.",
            "abstract": "Purpose:\n        \n      \n      To compare accuracy and interobserver variability in the detection and localization of recurrent ovarian cancer with contrast material-enhanced (CE) computed tomography (CT) and positron emission tomography (PET)/CT and determine whether imaging findings can be used to predict survival.\n    \n\n\n          Materials and methods:\n        \n      \n      Waiving informed consent, the institutional review board approved this HIPAA-compliant, retrospective study of 35 women (median age, 54.4 years) with histopathologically proven recurrent ovarian carcinoma who underwent CE CT and PET/CT before exploratory surgery. All CE CT and PET/CT scans were independently analyzed. Tumor presence, number of lesions, and the size and maximum standardized uptake value (SUV(max)) of the largest lesion were recorded for patient and region. Surgical histopathologic findings constituted the reference standard. Areas under the receiver operating characteristic curves (AUCs), κ statistics, and hazard ratios were calculated.\n    \n\n\n          Results:\n        \n      \n      Readers' AUCs in detection of recurrence for region were 0.85 (95% confidence interval [CI]: 0.81, 0.90) and 0.78 (95% CI: 0.72, 0.83) for CE CT and 0.84 (95% CI: 0.79, 0.89) and 0.74 (95% CI: 0.67, 0.81) for PET/CT (P = .76); 12 patients died. At PET/CT, size, number, and SUV(max) of peritoneal deposits were significantly associated with poor survival for readers 1 and 2 (P ≤ .01and ≤ .05, respectively), as were long- and short-axis diameters, number, and SUV(max) of distant lymph nodes for reader 1 (P ≤ .001). With CE CT, size (reader 1) and number (readers 1 and 3) of peritoneal deposits were significantly associated with poor survival (P ≤ .01), as were long- and short-axis diameters and number of distant lymph nodes for reader 1 (P ≤ .01). Interobserver agreement ranged from fair (patient, κ = 0.30) to moderate (region, κ = 0.55) for CE CT and fair (patient, κ = 0.24) to substantial (region, κ = 0.63) for PET/CT.\n    \n\n\n          Conclusion:\n        \n      \n      Preliminary data suggest that CE CT and PET/CT may have similar accuracy in detection of recurrent ovarian cancer. Tumor size, number, and SUV(max) may have potential as prognostic biomarkers for patients with recurrent ovarian cancer."
        },
        {
            "title": "Prevalence and prognostic significance of left ventricular myocardial late gadolinium enhancement in severe aortic stenosis.",
            "abstract": "Background:\n        \n      \n      Myocardial fibrosis occurs in aortic stenosis (AS) as part of the hypertrophic response. It can be detected by LGE, which is associated with an adverse prognosis in the form of increased mortality and morbidity.\n    \n\n\n          Objectives:\n        \n      \n      To assess the prevalence of LGE patterns using cardiac magnetic resonance (CMR) in severe AS patients and to study its prognostic significance.\n    \n\n\n          Methods:\n        \n      \n      Patients enrolled into the study from June 2012 to November 2014. All the patients underwent CMR and various patterns of LGE studied. These patients if symptomatic were advised AVR and others were managed conservatively. All patients were followed up and watched for outcomes like mortality, heart failure/hospitalization for cardiovascular cause, fall in left ventricular ejection fraction (LVEF) ≥20% and arrhythmia.\n    \n\n\n          Results:\n        \n      \n      A total of 109 patients (mean age-57.7±12.5yrs) underwent CMR with 63 males. These patients were followed up for a mean of 13 months. Among 38 patients who underwent AVR, 6 died (5-cardiovascular cause, 1-non cardiovascular). 71 patients were managed conservatively out of which 18 died (17-cardiovascular cause, 1-non cardiovascular cause). LGE patterns were seen in 46 patients (43%); mid myocardial enhancement was seen in 31.1% of cases (33 patients). No LGE pattern was seen in 57%(63 patients). Basal and mid regions were maximally involved with mid myocardial enhancement in 66% & 68.3% respectively. LV ejection fraction (p=0.002), peak aortic systolic velocity (p=0.01) and peak aortic systolic gradient (p=0.02) were the main predictors of LGE. Main predictors of primary outcome were NYHA class [OR- 13.4(2.8-26.1), p≤0.001], age- 62± 9.6yrs(p=0.001), EF simpson-50.9±13%(p≤ 0.001), LGE[OR 2.8 (1.27-6.47),p=0.01], number of segments involved [2.37±2.1,P≤0.001] & CMR LV mass (151.73±32gms, p=0.007). LGE predicted heart failure/hospitalization for cardiovascular cause [OR- 3.8(1.2-11.9), p=0.01] and fall in LVEF [OR- 5.8(1.5-22.5), p=0.005]. Patients with LGE had 2.87 times risk of adverse outcomes and patients with more than 3 segment LGE involvement had again increased chances for adverse outcomes.\n    \n\n\n          Conclusions:\n        \n      \n      LGE was detected by CMR in 43% of patients with severe AS. It predicted recurrent heart failure, hospitalization for cardiovascular cause and fall in LV ejection fraction. Our study has laid a path to larger prospective studies with long term follow up to assess the prognostic impact of CMR in patients with severe AS."
        },
        {
            "title": "Is this elderly patient dehydrated? Diagnostic accuracy of hydration assessment using physical signs, urine, and saliva markers.",
            "abstract": "Objectives:\n        \n      \n      Dehydration in older adults contributes to increased morbidity and mortality during hospitalization. As such, early diagnosis of dehydration may improve patient outcome and reduce the burden on healthcare. This prospective study investigated the diagnostic accuracy of routinely used physical signs, and noninvasive markers of hydration in urine and saliva.\n    \n\n\n          Design:\n        \n      \n      Prospective diagnostic accuracy study.\n    \n\n\n          Setting:\n        \n      \n      Hospital acute medical care unit and emergency department.\n    \n\n\n          Participants:\n        \n      \n      One hundred thirty older adults [59 males, 71 females, mean (standard deviation) age = 78 (9) years].\n    \n\n\n          Measurements:\n        \n      \n      Participants with any primary diagnosis underwent a hydration assessment within 30 minutes of admittance to hospital. Hydration assessment comprised 7 physical signs of dehydration [tachycardia (>100 bpm), low systolic blood pressure (<100 mm Hg), dry mucous membrane, dry axilla, poor skin turgor, sunken eyes, and long capillary refill time (>2 seconds)], urine color, urine specific gravity, saliva flow rate, and saliva osmolality. Plasma osmolality and the blood urea nitrogen to creatinine ratio were assessed as reference standards of hydration with 21% of participants classified with water-loss dehydration (plasma osmolality >295 mOsm/kg), 19% classified with water-and-solute-loss dehydration (blood urea nitrogen to creatinine ratio >20), and 60% classified as euhydrated.\n    \n\n\n          Results:\n        \n      \n      All physical signs showed poor sensitivity (0%-44%) for detecting either form of dehydration, with only low systolic blood pressure demonstrating potential utility for aiding the diagnosis of water-and-solute-loss dehydration [diagnostic odds ratio (OR) = 14.7]. Neither urine color, urine specific gravity, nor saliva flow rate could discriminate hydration status (area under the receiver operating characteristic curve = 0.49-0.57, P > .05). In contrast, saliva osmolality demonstrated moderate diagnostic accuracy (area under the receiver operating characteristic curve = 0.76, P < .001) to distinguish both dehydration types (70% sensitivity, 68% specificity, OR = 5.0 (95% confidence interval 1.7-15.1) for water-loss dehydration, and 78% sensitivity, 72% specificity, OR = 8.9 (95% confidence interval 2.5-30.7) for water-and-solute-loss dehydration).\n    \n\n\n          Conclusions:\n        \n      \n      With the exception of low systolic blood pressure, which could aid in the specific diagnosis of water-and-solute-loss dehydration, physical signs and urine markers show little utility to determine if an elderly patient is dehydrated. Saliva osmolality demonstrated superior diagnostic accuracy compared with physical signs and urine markers, and may have utility for the assessment of both water-loss and water-and-solute-loss dehydration in older individuals. It is particularly noteworthy that saliva osmolality was able to detect water-and-solute-loss dehydration, for which a measurement of plasma osmolality would have no diagnostic utility."
        },
        {
            "title": "Periorbital lesions in severely burned patients.",
            "abstract": "Purpose:\n        \n      \n      This study aimed to characterize the injuries involving periorbital region in our severely burned patients.\n    \n\n\n          Method:\n        \n      \n      A 2 years retrospective study was conducted with a total of 210 severe burns admissions. Periorbital burn injuries (all produced in association with facial injuries) were encountered in 126 patients, representing the study group that was further analyzed for multiple parameters: demographics, mechanism of injury, TBSA (total body surface area), burn depth, inhalation injury, need for intubation and mechanical ventilation. The presence and severity of ocular injuries were also evaluated.\n    \n\n\n          Results:\n        \n      \n      Analyzing our study group (n=126), we observed the presence of multiple negative prognosis factors: elderly patients, extensive burns, deep burns affecting functional areas, unfavorable mechanism (electric, chemical or explosions), inhalation injuries, need for intubation and mechanical ventilation, leading to severe morbidity and high mortality level. Ocular injuries were encountered in 37 patients (30 primary and 7 secondary lesions). The predominance of primary ocular lesions is explained trough high severity burns encountered in our patients with high mortality and lack of long-term clinical observations.\n    \n\n\n          Conclusion:\n        \n      \n      The clinical outcome for periorbital burn injuries depends on patient characteristics, etiology, burn extension and depth, associated lesions, infectious risk and the quality of the treatment applied. Presence of ocular injuries in various severity degrees impose an adequate evaluation and specialized treatment, being associated with important morbidity. In severely burned patients, it is mandatory to apply preventive measures to avoid ocular complications. If exposure keratopathy is detected, prompt ophthalmologic treatment is essential to avoid functional impairment including loss of vision. Abbreviations: TBSA = total body surface area, MSOF = multisystem organ failure, OCS = orbital compartment syndrome, AION = anterior ischemic optic neuropathy."
        },
        {
            "title": "Frailty in end-stage renal disease: comparing patient, caregiver, and clinician perspectives.",
            "abstract": "Background:\n        \n      \n      Frailty is associated with poor outcomes for patients on dialysis and is traditionally measured using tools that assess physical impairment. Alternate measurement tools highlight cognitive and functional domains, requiring clinician, patient, and/or caregiver input. In this study, we compared frailty measures for incident dialysis patients that incorporate patient, clinician, and caregiver perspectives with an aim to contrast the measured prevalence of frailty using tools derived from different conceptual frameworks.\n    \n\n\n          Methods:\n        \n      \n      A prospective cohort study of incident dialysis patients was conducted between February 2014 and June 2015. Frailty was assessed at dialysis onset using: 1) modified definition of Fried Phenotype (Dialysis Morbidity Mortality Study definition, DMMS); 2) Clinical Frailty Scale (CFS); 3) Frailty Assessment Care Planning Tool (provides CFS grading, FACT-CFS); and 4) Frailty Index (FI). Measures were compared via correlation and sensitivity/specificity analyses.\n    \n\n\n          Results:\n        \n      \n      A total of 98 patients participated (mean age of 61 ± 14 years). Participants were primarily Caucasian (91%), male (58%), and the majority started on hemodialysis (83%). The median score for both the CFS and FACT-CFS was 4 (interquartile range of 3-5). The mean FI score was 0.31 (standard deviation ± 0.16). The DMMS identified 78% of patients as frail. The FACT-CFS demonstrated highest correlation (r = 0.71) with the FI, while the DMMS was most sensitive (97%, 100%) and a CFS ≥ 5 most specific (100%, 77%) at corresponding FI cutoff values (>0.21, >0.45).\n    \n\n\n          Conclusions:\n        \n      \n      Frailty assessments of incident dialysis patients that include clinician, caregiver and patient perspectives have moderate to strong correlation with the FI. At specified FI cutoff values, the FACT-CFS and DMMS are highly sensitive measures of frailty. The CFS and FACT-CFS may represent viable alternative screening tools in dialysis patients."
        },
        {
            "title": "Right ventricular dysfunction: an independent and incremental predictor of cardiac deaths late after acute myocardial infarction.",
            "abstract": "Prognostic implication of right ventricular dysfunction and infarction scar in the chronic phase of the myocardial infarction has been little analyzed. In 299 consecutive patients (age 63 ± 11 years) with >3 months old myocardial infarction, we quantified right and left ventricular volumes and ejection fractions by cine cardiac magnetic resonance, and right and left ventricular scar tissue by late gadolinium enhancement. During follow-up (median, 2.4 years) cardiac events (cardiac-related deaths or appropriate intra-cardiac defibrillator shocks) occurred in 21 patients. Right ventricular systolic dysfunction (ejection fraction lower the reference mean values-2 SD) was present in 67 patients (22 %), right ventricular late gadolinium enhancement was observed in 15 patients (5 %). After adjustment for left ventricular end-diastolic volume, wall motion score index, and global extent of late gadolinium enhancement, right ventricular dysfunction was an independent and incremental predictor of cardiac events (p = 0.0053), while right ventricular scar tissue extent was not. Right ventricular dysfunction is an independent and incremental predictor of cardiac events also in the chronic phase of the myocardial infarction. In these patients, right ventricular dysfunction does not necessarily mean right ventricular infarction scar, but likely reflects the effects of hemodynamic and biohumoral factors."
        },
        {
            "title": "59 eyes with endogenous endophthalmitis- causes, outcomes and mortality in a Danish population between 2000 and 2016.",
            "abstract": "Background:\n        \n      \n      To study the epidemiology of patients with endogenous endophthalmitis in Denmark.\n    \n\n\n          Material and methods:\n        \n      \n      Retrospective and prospective case series of 59 eyes in patients with endogenous endophthalmitis in Denmark between 2000 and 2016.\n    \n\n\n          Results:\n        \n      \n      The age of the patients ranged from 28 to 90 years with a median of 66 years. Sixty-two percent of the eyes had a final VA (visual acuity) ≤ 0.1 while 8% had a final VA ≥ 1.0. Positive cultures were obtained in 51% of the cases from the blood and in 43% from the vitreous. Streptococcus species and Staphylococcus aureus were the most commonly identified microorganisms. The sources of endogenous endophthalmitis were diverse and were not identified in 36% of the patients. Diabetes (36%) was the most predisposing medical illness. A total of 15% of the patients died within the first year after surgery for endophthalmitis and half of the patients died during follow up. The mortality of patients was 22.6 times higher compared to a Danish background population. Culture positive patients had a higher mortality compared to culture negative patients.\n    \n\n\n          Conclusions:\n        \n      \n      Endogenous endophthalmitis is a heterogeneous condition which is reflected in the age, the visual outcome and the mortality of the patients. The epidemiology of the disease is very different in Scandinavia compared to Asia. The visual prognosis remains grave and the majority of the eyes lose useful vision."
        },
        {
            "title": "Retinal vessel caliber among people with acquired immunodeficiency syndrome: relationships with disease-associated factors and mortality.",
            "abstract": "Purpose:\n        \n      \n      To evaluate relationships between retinal vessel caliber, AIDS-related factors, and mortality.\n    \n\n\n          Design:\n        \n      \n      Longitudinal, observational cohort study.\n    \n\n\n          Methods:\n        \n      \n      We evaluated data for participants without ocular opportunistic infections at initial examination (baseline) in the Longitudinal Studies of the Ocular Complications of AIDS (1998-2008). Semi-automated evaluation of fundus photographs (1 eye/participant) determined central retinal artery equivalent (CRAE), central retinal vein equivalent (CRVE), and arteriole-to-venule ratio (AVR) at baseline. Multiple linear regression models, using forward selection, identified independent relationships between indices and various host- and disease-related variables.\n    \n\n\n          Results:\n        \n      \n      Included were 1250 participants. Mean follow-up for determination of mortality was 6.1 years. Smaller CRAE was related to increased age (P < .001) and hypertension (P < .001); larger CRAE was related to lower hematocrit (P = .002). Larger CRAE and CRVE were associated with black race (P < .001). Larger CRVE was related to smoking (P = .004); smaller CRVE was related to age (P < .001) and higher mean corpuscular volume (P = .001). We observed the following relationships with AIDS-associated factors: smaller CRAE and larger CRVE with history of highly active antiretroviral therapy (HAART; P < .001); and larger CRAE with lower CD4+ T lymphocyte count (P = .04). We did not identify independent relationships with human immunodeficiency virus RNA blood levels. There was a 12% (95% CI, 2%-21%) increase in mortality risk per quartile of decreasing AVR (P = .02).\n    \n\n\n          Conclusions:\n        \n      \n      Variations in retinal vascular caliber are associated with AIDS-specific factors and are markers for increased mortality risk. Relationships are consistent with the hypothesis that the vasculature is altered by known atherogenic effects of chronic HAART or the prolonged inflammatory state associated with AIDS."
        },
        {
            "title": "Relationship of reported clinical features of pre-eclampsia and postpartum haemorrhage to demographic and other variables.",
            "abstract": "Background:\n        \n      \n      Maternal death and disability remain significant problems in developing countries and are predominantly caused by preeclampsia and postpartum haemorrhage. The diagnostic criteria for preeclampsia and postpartum haemorrhage require medical technologies not readily available in underdeveloped areas.\n    \n\n\n          Objective:\n        \n      \n      To determine the correlates of pre-eclampsia and postpartum haemorrhage using symptoms in a rural setting.\n    \n\n\n          Methods:\n        \n      \n      This was a cross-sectional study in which 577 women from the Kwahu South District of the Eastern Region of Ghana completed questionnaires that sought for signs and symptoms of pre-eclampsia and postpartum haemorrhage in their current or prior pregnancies. The study was conducted over a period of two months, symptoms of pre-eclampsia assessed included headache, visual disturbance, urination, breathing, leg swelling and seizures. For postpartum haemorrhage, the following features were assessed: placenta delivery, length of labour, difficult delivery of placenta, lacerations associated with delivery, size of newborn, headache, visual disturbance and amount of vaginal bleeding.\n    \n\n\n          Results:\n        \n      \n      There was a significant association between education and the number of signs and symptoms of preeclampsia, (Χ² =9.059, =0.018; OR no education vs >7 years=6.8). Mothers with no education were about seven times more likely to have all six signs and symptoms of preeclampsia than those with seven or more years of education. There was no significant association between education and postpartum haemorrhage, (Χ² = 1.835, = 0.400). However, the OR of 1.59, indicated an inverse association between the two variables.\n    \n\n\n          Conclusion:\n        \n      \n      The high number of symptoms associated with preeclampsia among women with no formal education strongly supports the need for educational outreach and basic prenatal care in rural Ghana."
        },
        {
            "title": "Severity of concurrent visual and hearing impairment and mortality: the 1986-1994 National Health Interview Survey.",
            "abstract": "Purpose:\n        \n      \n      Visual impairment and, to a lesser extent, hearing impairment are independent predictors of reduced survival in selected studies of community-residing adults. To date, the association of severity of concurrent impairment and mortality has not been examined.\n    \n\n\n          Method:\n        \n      \n      The National Health Interview Survey is a continuous, multistage, area probability survey of the U.S. civilian noninstitutionalized population. Mortality linkage with the National Death Index of 116,796 adult participants from 1986 to 1994 with complete impairment data was performed through 1997.\n    \n\n\n          Results:\n        \n      \n      Findings indicate that moderate to severe concurrent hearing and visual impairment in women is associated with significantly increased risk of mortality. More modest mortality associations are evident for men and for adults with less severe impairments, irrespective of gender.\n    \n\n\n          Discussion:\n        \n      \n      Prevention of severe visual and hearing impairment should be a national public health priority, especially given the aging of the U.S. population."
        },
        {
            "title": "Prognostic value of CT angiography for major adverse cardiac events in patients with acute chest pain from the emergency department: 2-year outcomes of the ROMICAT trial.",
            "abstract": "Objectives:\n        \n      \n      The aim of this study was to determine the 2-year prognostic value of cardiac computed tomography (CT) for predicting major adverse cardiac events (MACE) in patients presenting to the emergency department (ED) with acute chest pain.\n    \n\n\n          Background:\n        \n      \n      CT has high potential for early triage of acute chest pain patients. However, there is a paucity of data regarding the prognostic value of CT in this ED cohort.\n    \n\n\n          Methods:\n        \n      \n      We followed 368 patients from the ROMICAT (Rule Out Myocardial Infarction Using Computer Assisted Tomography) trial (age 53 ± 12 years; 61% male) who presented to the ED with acute chest pain, negative initial troponin, and a nonischemic electrocardiogram for 2 years. Contrast-enhanced 64-slice CT was obtained during index hospitalization, and caregivers and patients remained blinded to the results. CT was assessed for the presence of plaque, stenosis (>50% luminal narrowing), and left ventricular regional wall motion abnormalities (RWMA). The primary endpoint was MACE, defined as composite cardiac death, nonfatal myocardial infarction, or coronary revascularization.\n    \n\n\n          Results:\n        \n      \n      Follow-up was completed in 333 patients (90.5%) with a median follow-up period of 23 months. At the end of the follow-up period, 25 patients (6.8%) experienced 35 MACE (no cardiac deaths, 12 myocardial infarctions, and 23 revascularizations). Cumulative probability of 2-year MACE increased across CT strata for coronary artery disease (CAD) (no CAD 0%; nonobstructive CAD 4.6%; obstructive CAD 30.3%; log-rank p < 0.0001) and across combined CT strata for CAD and RWMA (no stenosis or RWMA 0.9%; 1 feature-either RWMA [15.0%] or stenosis [10.1%], both stenosis and RWMA 62.4%; log-rank p < 0.0001). The c statistic for predicting MACE was 0.61 for clinical Thrombolysis In Myocardial Infarction risk score and improved to 0.84 by adding CT CAD data and improved further to 0.91 by adding RWMA (both p < 0.0001).\n    \n\n\n          Conclusions:\n        \n      \n      CT coronary and functional features predict MACE and have incremental prognostic value beyond clinical risk score in ED patients with acute chest pain. The absence of CAD on CT provides a 2-year MACE-free warranty period, whereas coronary stenosis with RWMA is associated with the highest risk of MACE."
        },
        {
            "title": "Neoadjuvant proton beam irradiation vs. adjuvant ruthenium brachytherapy in transscleral resection of uveal melanoma.",
            "abstract": "Background:\n        \n      \n      Uveal melanoma is the most common primary ocular malignancy in adults in the USA and Europe. The optimal treatment of large uveal melanoma is still under debate. Radiation therapy has its limitation due its eye-threatening secondary complications and is therefore often combined with surgical excision of the tumor.\n    \n\n\n          Methods:\n        \n      \n      In a retrospective interventional review, we evaluated in total 242 patients with uveal melanoma that underwent transscleral tumor resection with a predefined protocol, either with adjuvant ruthenium brachytherapy (Ru-106 group, n 136,), or with neoadjuvant proton beam therapy (PBT group, n 106). Kaplan-Meier estimates with log-rank test were used to show survival curves and a multivariable Cox regression model was used to calculate adjusted rate ratios.\n    \n\n\n          Results:\n        \n      \n      Local tumor recurrence rates after 3 and 5 years were 4% (95% CI 1.2-17.8%) and 9.1% (95% CI 2.9-27.3%), respectively, in the PBT group and 24.6% (95% CI 15.8-37.1%) and 27.5 (95% CI 17.8-41.1%), respectively, in the Ru-106 group. This leads to an overall recurrence rate almost 4 times higher in the Ru-106 group compared to the PBT group. After adjusting for the a priori confounders and the tumor distance to optic disc and ciliary body infiltration, the adjusted risk of tumor recurrence was 8 times (RR 7.69 (2.22-26.06), p < 0.001) higher in the Ru-106 group as compared to the PBT group. Three- and 5-year metastatic rates were 23.2% (95% CI 5.6-37.1%) and 31.8% (95% CI 20.7-46.8%), respectively, in the PBT group and 13.2% (95% CI 6.8-24.9%) and 30.3% (95% CI 18.3-47.5%), respectively, in the Ru-106 group. There was no statistically significant difference in the overall metastasis rate between the two groups even after adjusting for possible confounders.\n    \n\n\n          Conclusion:\n        \n      \n      Transscleral resection of large uveal melanomas combined with neoadjuvant proton beam therapy leads to a lower local tumor recurrence rate compared to transscleral tumor resection with adjuvant ruthenium brachytherapy. There was no statistically significant difference in the occurrence of rubeosis iridis, neovascular glaucoma, and in the need for enucleation later on."
        },
        {
            "title": "Intravitreal aflibercept for macular edema following branch retinal vein occlusion: the 24-week results of the VIBRANT study.",
            "abstract": "Purpose:\n        \n      \n      To compare the efficacy and safety of intravitreal aflibercept injection (IAI) with macular grid laser photocoagulation for the treatment of macular edema after branch retinal vein occlusion (BRVO).\n    \n\n\n          Design:\n        \n      \n      The VIBRANT study was a double-masked, active-controlled, randomized, phase III trial.\n    \n\n\n          Participants:\n        \n      \n      Treatment-naïve eyes with macular edema after BRVO were included in the study if the occlusion occurred within 12 months and best-corrected visual acuity (BCVA) was between ≤73 and ≥24 Early Treatment Diabetic Retinopathy Study (ETDRS) letters (20/40-20/320 Snellen equivalent).\n    \n\n\n          Methods:\n        \n      \n      Eyes (1 eye per patient) received either IAI 2 mg every 4 weeks (n=91) from baseline to week 20 or grid laser (n=92) at baseline with a single grid laser rescue treatment, if needed, from weeks 12 through 20.\n    \n\n\n          Main outcome measures:\n        \n      \n      The primary outcome measure was the proportion of eyes that gained ≥15 ETDRS letters from baseline BCVA at week 24. Secondary end points included mean change from baseline BCVA and central retinal thickness (CRT) at week 24.\n    \n\n\n          Results:\n        \n      \n      The proportion of eyes that gained ≥15 ETDRS letters from baseline at week 24 was 52.7% in the IAI group compared with 26.7% in the laser group (P=0.0003). The mean improvement from baseline BCVA at week 24 was 17.0 ETDRS letters in the IAI group and 6.9 ETDRS letters in the laser group (P<0.0001). The mean reduction in CRT from baseline at week 24 was 280.5 μm in the IAI group and 128.0 μm in the laser group (P<0.0001). Traumatic cataract in an IAI patient was the only ocular serious adverse event (SAE) that occurred. There were no cases of intraocular inflammation or endophthalmitis. The incidence of nonocular SAEs was 8.8% in the IAI group and 9.8% in the laser group. One Anti-Platelet Trialists' Collaboration-defined event of nonfatal stroke (1.1%) and 1 death (1.1%) due to pneumonia occurred during the 24 weeks of the study, both in patients in the laser group.\n    \n\n\n          Conclusions:\n        \n      \n      Monthly IAI provided significantly greater visual benefit and reduction in CRT at 24 weeks than grid laser photocoagulation in eyes with macular edema after BRVO."
        },
        {
            "title": "Health screening in older women.",
            "abstract": "Health screening is an important aspect of health promotion and disease prevention in women over 65 years of age. Screening efforts should address conditions that cause significant morbidity and mortality in this age group. In addition to screening for cardiovascular disease, cerebrovascular disease and cancer, primary care physicians should identify risk factors unique to an aging population. These factors include hearing and vision loss, dysmobility or functional impairment, osteoporosis, cognitive and affective disorders, urinary incontinence and domestic violence. Although screening for many conditions cannot be proved to merit an \"A\" recommendation (indicating conclusive proof of benefit), special attention to these factors can decrease morbidity and improve quality of life in aging women."
        },
        {
            "title": "Fatal and non-fatal adverse events of glucocorticoid therapy for Graves' orbitopathy: a questionnaire survey among members of the European Thyroid Association.",
            "abstract": "Objective:\n        \n      \n      The objective of this study was to investigate the side effects of glucocorticoid (GC) therapy observed by European thyroidologists during the treatment of Graves' orbitopathy (GO).\n    \n\n\n          Design:\n        \n      \n      A questionnaire-based survey among members of the European Thyroid Association (ETA) who treat GO.\n    \n\n\n          Results:\n        \n      \n      A response was obtained from 128 ETA members of which 115 used GC therapy for GO. The majority of respondents (83/115, 72%) used intravenous (i.v.) GC, with a relatively wide variety of therapeutic regimens. The cumulative dose of methylprednisolone ranged between 0.5 and 12 g (median 4.5 g) for i.v.GC and between 1.0 and 4.9 g (median 2.4 g) for oral GC. Adverse events were often reported during oral GCs (26/32, 81%); most side effects were non-severe, but ten respondents reported severe adverse events (hepatic, cardiovascular, and cerebrovascular complications), including two fatal cases, both receiving a total of 2.3 g prednisone. Adverse events were less common in i.v.GC (32/83 respondents, 39%), but mostly consisted of severe events, including seven fatal cases. All but one fatal event occurred in cumulative i.v.GC doses (>8 g) higher than those currently recommended.\n    \n\n\n          Conclusions:\n        \n      \n      GCs are preferentially administered i.v. for the treatment of GO in Europe. Both oral and i.v.GC may be associated with severe adverse effects, including fatal cases, which are more frequently reported in daily or alternate day i.v.GC. IvGC therapy should be undertaken in centers with appropriate expertise. Patients should be carefully examined for risk factors before treatment and monitored for side effects, which may be asymptomatic, both during and after treatment."
        },
        {
            "title": "Usefulness of stress gated technetium-99m single photon emission computed tomographic myocardial perfusion imaging for the prediction of cardiac death in patients with moderate to severe left ventricular systolic dysfunction and suspected coronary artery disease.",
            "abstract": "Although stress gated technetium-99m single-photon emission computed tomographic (SPECT) myocardial perfusion imaging (MPI) is useful in differentiating ischemic from nonischemic cardiomyopathy, its prognostic usefulness in this patient population is not well understood. Consecutive unique patients with suspected coronary artery disease who, for clinical indications, underwent technetium-99m rest and stress MPI demonstrating ejection fractions ≤40% by gated SPECT imaging were retrospectively identified. In addition to prescan variables, previously defined cutoffs for gated SPECT parameters using visual and standard 17-segment semiquantitative scoring were applied and related to the occurrence of cardiac death up to 5 years after MPI. Of the 475 patients fulfilling criteria for study inclusion, follow-up was complete in 444 (93%) over 3.7 ± 1.6 years. Of 393 patients without subsequent early (≤60 days) coronary revascularization, cardiac death occurred in 64 (16%). The summed stress score, an MPI measure of the extent and severity of coronary artery disease that also accounts for the ischemic burden, was the gated SPECT parameter most related to cardiac death with Kaplan-Meier 5-year cardiac death-free survival of 85.6% and 67.3% in patients with summed stress scores ≤8 and >8, respectively (p <0.001). In multivariate Cox regression analysis, a summed stress score >8 independently contributed to cardiac death (adjusted hazard ratio 2.20, 95% confidence interval 1.34 to 3.61), and its addition to the model significantly increased the global chi-square value over prescan variables (from 32.46 to 41.67, p = 0.002). In conclusion, stress MPI data from gated technetium-99m SPECT scans are useful for the prediction of cardiac death in patients with moderate to severe left ventricular systolic dysfunction in whom there is suspicion of underlying coronary artery disease."
        },
        {
            "title": "Health management in cancer survivors: Findings from a population-based prospective cohort study-the Yamagata Study (Takahata).",
            "abstract": "The number of cancer survivors is increasing; however, optimal health management of cancer survivors remains unclear due to limited knowledge. To elucidate the risk of non-communicable diseases, and the effect of lifestyle habits on risk of non-communicable diseases, we compared cancer survivors and those who never had cancer (non-cancer controls) using a population-based prospective cohort study. The baseline survey of 2292 participants was carried out from 2004 to 2006, and the follow-up survey of 2124 participants was carried out in 2011. We compared the baseline characteristics and the risk of non-communicable diseases between cancer survivors and non-cancer controls. Analyzed participants included 124 cancer survivors (men/women, 57/67), and 2168 non-cancer controls (939/1229). Several lifestyle factors and nutritional intake significantly differed between survivors and non-cancer controls, although smoking status did not differ between the groups (P = 0.30). Univariate logistic regression analysis showed increased risk of death (odds ratio [OR], 3.64; 95% confidence interval [CI], 2.19-6.05) and heart disease (OR, 2.60; 95% CI, 1.06-6.39) in cancer survivors. Increased risk of heart disease was also significant (OR, 2.95; 95% CI, 1.05-8.26; P = 0.04) in the multivariate analysis of the smoking-related cancer subgroup. Current smoking significantly increased risk of death (OR, 2.42; 95% CI, 1.13-5.18). Specific management should be implemented for cancer survivors. More intense management against smoking is necessary, as continued smoking in cancer survivors may increase the risk of second primary cancer. Moreover, cancer survivors are at a high risk of heart disease; thus, additional care should be taken."
        },
        {
            "title": "Unmet healthcare needs of elderly people in Korea.",
            "abstract": "Background:\n        \n      \n      Elderly people often have more complicated healthcare needs than younger adults due to additional functional decline, physical illness, and psychosocial needs. Unmet healthcare needs increase illness severity, complications, and mortality. Despite this, research on the unmet healthcare needs of elderly people is limited in Korea. This study analysed the effect of functional deterioration related to aging on unmet healthcare needs based on the Korea Health Panel Study.\n    \n\n\n          Methods:\n        \n      \n      This cross-sectional study used data from the 2011-2013 survey of 8666 baseline participants aged 65 years and older. Unmet healthcare needs were calculated using a complex weighted sample design. Group differences in categorical variables were analysed using the Rao-Scott Chi-square test. Using logistic regression analysis, the association between unmet healthcare needs and aging factors was analysed.\n    \n\n\n          Results:\n        \n      \n      The prevalence of unmet healthcare needs in Korean elderly was 17.4%. Among them, the leading reason was economic hardship (9.2%). Adjusting for sex, age, socioeconomic characteristics, and health-related characteristics, the group with depression syndrome was 1.45 times more likely to have unmet healthcare needs than that without depression syndrome (95% CI = 1.13-1.88). The group with visual impairment was 1.48 times more likely to have unmet healthcare needs than that without it (95% CI = 1.22-1.79). The group with hearing impairment was 1.40 times more likely to have unmet healthcare needs than that without it (95% CI = 1.15-1.72). The group with memory impairment was 1.74 times more likely to have unmet healthcare needs than that without it (95% CI = 1.28-2.36).\n    \n\n\n          Conclusions:\n        \n      \n      The unmet medical needs of the elderly are more diverse than those of younger adults. This is because not only socioeconomic and health-related factors but also aging factors that are important to the health of the elderly are included. All factors were linked organically; therefore, integrated care is needed to improve healthcare among the elderly. To resolve these unmet healthcare needs, it is necessary to reorganize the healthcare system in Korea to include preventive and rehabilitative services that address chronic diseases in an aged society and promote life-long health promotion."
        },
        {
            "title": "Evaluation of Outcomes After Endoscopic Endonasal Surgery for Large and Giant Pituitary Macroadenoma: A Retrospective Review of 39 Consecutive Patients.",
            "abstract": "Background:\n        \n      \n      The endoscopic endonasal approach for pituitary neoplasms has shown similar efficacy compared with the microscopic approach. However, outcomes and complication rates with larger macroadenomas is not as well documented. This study addresses the efficacy and outcome of the fully endoscopic endonasal approach for large and giant pituitary adenomas.\n    \n\n\n          Methods:\n        \n      \n      Endoscopic endonasal resection was performed in 39 patients with large (>3 cm) or giant (>4 cm) pituitary macroadenomas. Outcomes were assessed using formal visual examinations, endocrine status, and neurologic examinations. Statistical analyses of multiple variables were addressed for correlation to visual, endocrine, and neurologic outcomes.\n    \n\n\n          Results:\n        \n      \n      Gross total resection of the pituitary macroadenoma was achieved in 22 of 39 (56.4%) patients based on postoperative magnetic resonance imaging. Higher Knosp grade was associated with near-total resection or subtotal resection (P = 0.0004). All patients had improved or stable visual symptoms. Time to diagnosis, preoperative visual deficit, and tumor size were not significant predictors of visual outcome. Of patients, 34 (87.1%) had a \"good\" endocrine outcome, whereas 5 did not. Among the 5 patients who did not have a good outcome, 1 had new hypopituitarism, and 4 required increased dosages of pharmacologic therapy. All patients with recurrent tumors had stable visual and good endocrine outcomes. Postoperative cerebrospinal fluid leak occurred in 4 patients; lumbar drainage resolved the leak in 3, and reoperation was performed in 1 patient. There were no new cranial nerve deficits, new neurologic deficits, or mortality.\n    \n\n\n          Conclusions:\n        \n      \n      Endoscopic endonasal resection of large and giant pituitary macroadenomas is safe and efficient. Postoperative complications, including cerebrospinal fluid leak, are low. Surgical efficacy of the fully endoscopic endonasal approach for large and giant macroadenomas makes the technique a preferable option in this subset of patients."
        },
        {
            "title": "Association between mortality and cognitive change over 7 years in a large representative sample of UK residents.",
            "abstract": "Objective:\n        \n      \n      To examine the association between change in reaction time and cognitive performance over 7 years and the risk of death from all causes and some specific causes after controlling for known risk factors.\n    \n\n\n          Methods:\n        \n      \n      The sample comprised members of the Health and Lifestyle Survey (HALS) of community-dwelling adults in England, Scotland, and Wales. Baseline testing (HALS1), involving 9003 people, took place in 1985 and 1986. Sociodemographic, lifestyle, health, and physiological information was collected. Cognitive functioning was measured using tests of simple and choice reaction time, a short memory test, and a test of visual-spatial reasoning. Follow-up testing (HALS2) took place in 1991 and 1992, when 5352 members of the study were administered the same questionnaires, physiological examinations, and cognitive tests. The sample has been followed for mortality up to June 2005.\n    \n\n\n          Results:\n        \n      \n      After controlling for age, gender, and the relevant baseline cognitive test scores, greater declines between HALS1 and HALS2 on simple reaction time mean and variability, choice reaction time mean and variability, memory and visual-spatial reasoning were associated with significantly increased risks of death from all causes, all cardiovascular diseases (CVDs), coronary heart disease (CHD), stroke, and respiratory disease. These associations were only slightly attenuated after adjusting for occupational social class, educational, smoking, alcohol consumption, physical activity, body mass index, blood pressure, and lung function.\n    \n\n\n          Conclusions:\n        \n      \n      Decline in performance of reaction times and simple cognitive tasks across a 7-year period was associated with an increased risk of death from all causes, all CVDs, CHD, stroke, and respiratory disease up to 13 years later, even after adjustment for known risk factors."
        },
        {
            "title": "Extent of thoracic aortic atheroma burden and long-term mortality after cardiothoracic surgery: a computed tomography study.",
            "abstract": "Objectives:\n        \n      \n      We hypothesized that the extent of aortic atheroma of the entire thoracic aorta, determined by pre-operative multidetector-row computed tomographic angiography (MDCTA), is associated with long-term mortality following nonaortic cardiothoracic surgery.\n    \n\n\n          Background:\n        \n      \n      In patients evaluated for cardiothoracic surgery, presence of severe aortic atheroma is associated with adverse short- and long-term post-operative outcome. However, the relationship between aortic plaque burden and mortality remains unknown.\n    \n\n\n          Methods:\n        \n      \n      We reviewed clinical and imaging data from all patients who underwent electrocardiographic-gated contrast-enhanced MDCTA prior to coronary bypass or valvular heart surgery at our institution between 2002 and 2008. MDCTA studies were analyzed for thickness and circumferential extent of aortic atheroma in 5 segments of the thoracic aorta. A semiquantitative total plaque-burden score (TPBS) was calculated by assigning a score of 1 to 3 to plaque thickness and to circumferential plaque extent. When combined, this resulted in a score of 0 to 6 for each of the 5 segments and, hence, an overall score from 0 to 30. The primary end point was all-cause mortality during long-term follow-up.\n    \n\n\n          Results:\n        \n      \n      A total of 862 patients (71% men, 67.8 years) were included and followed over a mean period of 25 ± 16 months. The mean TPBS was 8.6 (SD: ±6.0). The TPBS was a statistically significant predictor of mortality (p < 0.0001) while controlling for baseline demographics, cardiovascular risk factors, and type of surgery including reoperative status. The estimated hazard ratio for TPBS was 1.08 (95% confidence interval: 1.045 to 1.12). Other independent predictors of mortality were glomerular filtration rate (p = 0.015), type of surgery (p = 0.007), and peripheral artery disease (p = 0.03).\n    \n\n\n          Conclusions:\n        \n      \n      Extent of thoracic aortic atheroma burden is independently associated with increased long-term mortality in patients following cardiothoracic surgery. Although our data do not provide definitive evidence, they suggest a relationship to the systemic atherosclerotic disease process and, therefore, have important implications for secondary prevention in post-operative rehabilitation programs."
        },
        {
            "title": "Myocardial viability after primary coronary angioplasty: low-dose dobutamine stress echocardiography versus myocardial contrast echocardiography.",
            "abstract": "Background:\n        \n      \n      Successful reperfusion therapy in patients with acute myocardial infarction (AMI) improves survival. Indeed, after AMI myocardial dysfunction may be reversible (hibernating or stunned myocardium). Low-dose dobutamine stress echocardiography (LDDSE) provides us with the possibility of evaluating viable myocardial segments, while myocardial contrast echocardiography (MCE) allows the study of the microcirculation in the same myocardial areas. The aim of our study was to compare LDDSE and MCE, in the prediction of the recovery of segments in patients with AMI who were submitted to primary coronary angioplasty (PTCA).\n    \n\n\n          Methods:\n        \n      \n      We studied 14 patients with AMI. Both LDDSE and MCE with Levovist were performed after primary PTCA. The viability gold standard was a recovery of contractility detected at echocardiography 2 months later.\n    \n\n\n          Results:\n        \n      \n      For LDDSE, the sensitivity was 91%, the specificity 71% and the positive and negative predictive values were 93 and 64% respectively. For MCE, the sensitivity was 94%, the specificity 44%, the positive predictive value 89%, and the negative predictive value 59%. Two tests agreed in 81% of the cases. Stress echocardiography and contrast echocardiography agreed in 81% of cases.\n    \n\n\n          Conclusions:\n        \n      \n      LDDSE has a very good positive accuracy, it has an acceptable negative predictive value and is relatively cheap. On the other hand, MCE has a good positive accuracy, but a low negative accuracy and carries a high cost. The integration of these two tests, which are too expensive in clinical practice, could improve our comprehension of the post-PTCA pathophysiology."
        },
        {
            "title": "Prediction of cardiovascular mortality in patients with ST-elevation myocardial infarction after primary percutaneous coronary intervention.",
            "abstract": "Objectives:\n        \n      \n      We analyzed a large patient group to develop a clinical risk score that could be applied to patients after primary percutaneous coronary intervention (PCI).\n    \n\n\n          Methods:\n        \n      \n      We reviewed 2529 consecutive patients treated with primary PCI for ST-elevation myocardial infarction between 2003 and 2008. All clinical, angiographic and follow-up data were retrospectively collected. Independent predictors of in-hospital cardiovascular mortality were determined by multivariate Cox regression analysis in all study patients.\n    \n\n\n          Results:\n        \n      \n      Five variables (Killip class 2/3, unsuccessful procedure, contrast-induced nephropathy, diabetes mellitus, and age >70 years) were selected from the initial multivariate model. Each of them was weighted with 1 point according to their respective odds ratio for in-hospital mortality and then total risk score was calculated for each patient with a range of 0-5 points. For simplicity, four strata of risk were defined (low risk, score 0; intermediate risk, score 1; high risk, score 2 and very high risk, score > or =3). Each risk strata had a strong association with in-hospital cardiovascular mortality (P<0.001 for trend). Moreover, among survivors after an in-hospital period, our risk score continued to be a powerful predictor of long-term mortality (P<0.001 for trend).\n    \n\n\n          Conclusion:\n        \n      \n      In patients treated with primary PCI, a risk score, which was developed from five risk factors readily available after intervention, may be useful to predict in-hospital and long-term cardiovascular mortality."
        },
        {
            "title": "Acute pancreatitis: the role of imaging in diagnosis and management.",
            "abstract": "Acute pancreatitis is one of the more commonly encountered aetiologies in the emergency setting and its incidence is rising. Presentations range from a mild-self limiting condition which usually responds to conservative management to one with significant morbidity and mortality in its most severe forms. While clinical criteria are necessary to make the initial diagnosis, contrast-enhanced CT is the mainstay of imaging and has a vital role in assessing the extent and evolution of the disease and its associated complications. The purpose of this article is to summarise the natural course of acute severe pancreatitis, clarify confusing nomenclature, demonstrate the morphological stages in conjunction with radiological scoring systems and illustrate the complications. We will review and illustrate the increasing and significant role interventional radiology has in the management of these patients, which are often life-saving and surgery-sparing."
        },
        {
            "title": "The neuro-ophthalmology of head trauma.",
            "abstract": "Traumatic brain injury (TBI) is a major cause of morbidity and mortality. Concussion, a form of mild TBI, might be associated with long-term neurological symptoms. The effects of TBI and concussion are not restricted to cognition and balance. TBI can also affect multiple aspects of vision; mild TBI frequently leads to disruptions in visual functioning, while moderate or severe TBI often causes structural lesions. In patients with mild TBI, there might be abnormalities in saccades, pursuit, convergence, accommodation, and vestibulo-ocular reflex. Moderate and severe TBI might additionally lead to ocular motor palsies, optic neuropathies, and orbital pathologies. Vision-based testing is vital in the management of all forms of TBI and provides a sensitive approach for sideline or post-injury concussion screening. One sideline test, the King-Devick test, uses rapid number naming and has been tested in multiple athlete cohorts."
        },
        {
            "title": "Dying at life׳s beginning: Experiences of parents and health professionals in Switzerland when an 'in utero ' diagnosis incompatible with life is made.",
            "abstract": "Objective:\n        \n      \n      The disclosure of a diagnosis during pregnancy of a fetal malformation, which is incompatible with life, normally comes completely unexpectedly to the parents. Although a body of international literature has considered the topic, most of it comes from the United States and little has been generated from Europe. This study aims to illuminate the contemporary treatment associated with such diagnoses, regardless of whether parents decide to terminate or continue the pregnancy.\n    \n\n\n          Design:\n        \n      \n      a qualitative design was used with data collected by semi-structured interviews and subjected to a thematic analysis.\n    \n\n\n          Setting:\n        \n      \n      the research was conducted in the German speaking areas of Switzerland with data collected from participants in places of their choice.\n    \n\n\n          Participants:\n        \n      \n      61 interviews were conducted with 32 parents and 29 health professionals.\n    \n\n\n          Findings:\n        \n      \n      the theme of 'temporality' identified four main time points from the professionals: diagnosis, decision, birth/death, and afterwards. However, in contrast to these, six major themes in this study, primarily generated from parents and extended from receiving the diagnosis until the interview, were identified: shock, choices and dilemmas, taking responsibility, still being pregnant, forming a relationship with the baby, letting go. Although there was concurrence on many aspects of care at the point of contact, parents expressed major issues as gaps between the points of contact.\n    \n\n\n          Conclusions:\n        \n      \n      care varied regionally but was as sensitive as possible, attempting to give parents the space to accept their loss but fulfil legal requirements. A gap exists between diagnosis and decision with parents feeling pressured to make decisions regarding continuing or terminating their pregnancies although health professionals' testimonies indicated otherwise. A major gap manifested following the decision with no palliative care packages offered. During the birth/death of the baby, care was sensitive but another gap manifested following discharge from hospital."
        },
        {
            "title": "Underutilized strategies in traffic safety: Results of a nationally representative survey.",
            "abstract": "Objective: Numerous strategies proven to be effective in reducing crash fatalities have been underutilized in the United States, including sobriety checkpoints; automated enforcement; lower blood alcohol concentration (BAC) limits; primary enforcement of safety belt and motorcycle helmet use laws; alcohol ignition interlock installations; drugged driving screening; lowered residential speed limits; and roundabout installations. If these strategies are implemented widely in every state, traffic fatalities could be reduced by at least 50%. A barrier to implementation is the perception by officials that the public is against them. The purpose of this study was to determine which of these underutilized measures would be favorable to the American public given that they are educated on the research of their effectiveness.Methods: A representative survey of 2,000 U.S. drivers was conducted in October 2018 with 30 questions about these underutilized strategies using the National Opinion Research Center's (NORC) AmeriSpeak® survey instrument. Our objective was to gauge the public's opinion of these strategies when they are aware of the research on their effectiveness.Results: Respondents were given a summary of the research on the effectiveness of these strategies and then asked whether they were in favor of them in their communities; 64.7% of the respondents were in favor of conducting sobriety checkpoints at least monthly; 68.2% were in favor of police using passive alcohol sensors at sobriety checkpoints; 60.3% of respondents were in favor of using speed and red light cameras for automated enforcement; 70.1% were in favor of a law that required all cars to have seat belt reminders that continuously chime until the seat belt is buckled, including for rear seat passengers; and 62.5% were in favor of raising the fine in their state for not using a seat belt from $25 to $100. Other results indicated public support for these strategies.Conclusions: The results indicate that when drivers in the United States are given facts about certain strategies to reduce crash fatalities, the majority are in favor of the underutilized strategies. This information could be useful to legislators and highway safety officials in their decisions to implement these strategies."
        },
        {
            "title": "Role of contrast-enhanced dobutamine stress echocardiography in predicting outcome in patients with known or suspected coronary artery disease.",
            "abstract": "Although the application of intravenous contrast agents during stress echocardiography has been shown to improve diagnostic accuracy for detecting coronary artery disease, less information exists regarding its prognostic value. The aim of this study was to determine the role of contrast-enhanced dobutamine stress echocardiography (DSE) for predicting future cardiac events in patients with coronary artery disease (CAD). We studied 893 patients (mean age: 66, 581 men) with known or suspected CAD undergoing contrast-enhanced DSE. Positivity was defined as new/worsened wall motion abnormality or fixed abnormality during stress. All patients were followed for 15 +/- 10 months to evaluate hard cardiac events (cardiac death and nonfatal myocardial infarction) and total cardiac events (hard cardiac events, congestive heart failure, unstable angina, and late revascularization). Three patients were lost to follow-up, and 128 patients developed cardiac events, including 21 hard cardiac events. The 3-year event free survival rate was significantly lower in patients with positive DSE results than in those with negative DSE results. Stepwise Cox multivariate analysis revealed that positivity of DSE (P < 0.0001, Hazard ratio (HR): 2.48) and peak wall motion score index (WMSI) >1.5 (P < 0.0001, HR: 2.41) were independent predictors for total cardiac events. Considering hard cardiac events, the independent predictors were peak WMSI > 1.5 (P < 0.0001, HR: 6.65) and age > 70 years (P < 0.005, HR: 3.27). We conclude that contrast-enhanced DSE provides important prognostic information for future cardiac events."
        },
        {
            "title": "Early detection in lung cancer. Case finding and screening.",
            "abstract": "There is general agreement that the most effective approach to lung cancer is primary prevention--stop smoking. Richards has proposed the MVROCST--the Monosyllabic Verbal Response Office Cancer Screening Test: \"Do you smoke?\" If \"yes,\" intervene. If \"no,\" move on. Ample evidence exists that a clear message from a physician to a patient about the importance of stopping smoking makes a difference. In contrast to the maze of arguments and data on early detection, this is something that each physician clearly can and should do. A reduced risk for lung cancer may begin as early as 5 years after cessation of cigarette use. Huuskonen has proposed conceptualizing screening as a coordinated intervention with the goal of identifying populations at risk and working to modify that risk. Primary prevention should be central to any efforts to reduce mortality from lung cancer, and attention to this area needs to increase despite the difficulties and frustration. Despite declining percentages of smokers in the population as a whole, it is estimated that more than 3000 teenagers become regular smokers each day in the United States. In this environment, the question of whether to recommend a CXR or sputum for early detection is not going to disappear in the near future. The NCI has recognized the persistent and important nature of this debate and is currently funding the Prostate, Lung, Colon and Ovary Cancer Screening Trial. This is a large and powerful randomized study of men and women aged 60 to 74. The lung cancer arm is designed to look at the usefulness of a yearly CXR intervention in reducing cancer-specific mortality. The overall power of the study (based on national mortality data) is 0.99 for a 15% reduction in lung cancer mortality and 0.89 for a 10% reduction, with differentially better sensitivity in men than women. The study is currently in progress at multiple sites and will be completed over the next 12 to 14 years. In the meantime, what is the right approach? It is useful in considering this question to return to the concepts of early detection, screening, and case finding. 1. Early detection in lung cancer remains a concept of uncertain applicability because of the unknowns and variability in the natural history of the disease. The available, accessible, and acceptable detection tools appear to be inadequate by current evidence. This is not a static field, however, and new work in the area of biomarkers carries promise for significantly more sensitive and specific techniques. Tockman and colleagues conclude that early detection is conceptually sound, although not currently practical, and further research may expand the role of intervention. In the end, a judgement on early detection in lung cancer must be linked to the proposed setting--screening or case finding. 2. Screening, defined as the application of a test to the general population to define disease risk further with the implied benefit of improved treatment and outcome, cannot be recommended for lung cancer. This is the perspective of the major organizations cited previously, and it is based on admittedly imperfect but nonetheless convincing data. 3. Case finding, the situation of the patient who seeks care and is available for informed discussion and negotiation on possible testing, is a potentially different situation.(ABSTRACT TRUNCATED)"
        },
        {
            "title": "Scope insensitivity in contingent valuation studies of health care services: should we ask twice?",
            "abstract": "The main purpose of the present study was to test for outcome scope insensitivity. Respondents were initially asked to value one of two severe health states by way of a time-trade-off (TTO) exercise. Subsequent to the TTO exercise all respondents were asked to value an intervention, which offered a reduction in risk of falling into the health state they had evaluated. All respondents were subsequent to this initial CV exercise asked to value the same risk reduction, but in this case the outcome was death. Although our study passes the internal scope test, there is not a high degree of sensitivity to outcome. As many as 68% of respondents stated an identical maximum WTP in first and second CV valuation exercise implying that they value the interventions equally despite the fact that the health state presented in the initial CV question was deemed far better than death according to the TTO responses given by the same respondents. In contrast, the external scope test (comparison of response to initial CV across study arms) fared much better."
        },
        {
            "title": "The prognosis of amaurosis fugax and hemispheric transient ischemic attacks.",
            "abstract": "Background:\n        \n      \n      The natural history of amaurosis fugax and of hemispheric transient ischemic attacks (TIAs) may be different. We analysed the ischemic risk factors, carotid status and prognosis with respect to future ischemic events in a cohort of patients who presented with either of these transient ischemic episodes.\n    \n\n\n          Methods:\n        \n      \n      The charts of patients who presented to our institution between February 1983 and April 1995 with amaurosis fugax or hemispheric TIAs were reviewed. Patients under the age of 45 years with a history of migraine or previous carotid surgery were excluded. Follow-up by a clinical visit or telephone interview was performed. Information was obtained regarding demographic features, presenting symptoms, ischemic risk factors, carotid status (as measured by duplex ultrasonography), type of medical treatment prescribed and occurrence of subsequent ischemic events. Outcome ischemic events were graded as major (myocardial infarction, cerebrovascular accident or death due to either of these) or minor (recurrent amaurosis fugax or hemispheric TIA).\n    \n\n\n          Results:\n        \n      \n      A total of 141 patients were followed for a mean of 47 months. Risk factors were more prevalent in patients with hemispheric TIAs than in those with amaurosis fugax. Most patients had a low degree of carotid stenosis. There was no statistically significant difference in the occurrence of major outcome events between the two groups. Kaplan-Meier survival curves were similar for the two groups.\n    \n\n\n          Interpretation:\n        \n      \n      Amaurosis fugax and hemispheric TIA both carry a risk for future ischemic events. However, we did not find a statistically significant difference in prognosis between the two groups."
        },
        {
            "title": "Radiation therapy for primary carcinoma of the eyelid: tumor control and visual function.",
            "abstract": "Background and purpose:\n        \n      \n      Surgical excision remains the standard and most reliable curative treatment for eyelid carcinoma, but frequently causes functional and cosmetic impairment of the eyelid. We therefore investigated the efficacy and safety of radiation therapy in eyelid carcinoma.\n    \n\n\n          Patients and methods:\n        \n      \n      Twenty-three patients with primary carcinoma of the eyelid underwent radiation therapy. Sebaceous carcinoma was histologically confirmed in 16 patients, squamous cell carcinoma in 6, and basal cell carcinoma in 1. A total dose of 50-66.6 Gy (median, 60 Gy) was delivered to tumor sites in 18-37 fractions (median, 30 fractions).\n    \n\n\n          Results:\n        \n      \n      All but 3 of the 23 patients had survived at a median follow-up period of 49 months. The overall survival and local progression-free rates were 87% and 93% at 2 years, and 80% and 93% at 5 years, respectively. Although radiation-induced cataracts developed in 3 patients, visual acuity in the other patients was relatively well preserved. There were no other therapy-related toxicities of grade 3 or greater.\n    \n\n\n          Conclusion:\n        \n      \n      Radiation therapy is safe and effective for patients with primary carcinoma of the eyelid. It appears to contribute to prolonged survival as a result of good tumor control, and it also facilitates functional and cosmetic preservation of the eyelid."
        },
        {
            "title": "Active bleeding in acute subarachnoid hemorrhage observed by multiphase dynamic-enhanced CT.",
            "abstract": "Background and purpose:\n        \n      \n      Acute SAH is reportedly associated with rebleeding from aneurysms, and recent advances in imaging technology allow us to visualize active bleeding in SAH cases. This study aimed to retrospectively investigate the incidence and characteristics of active bleeding in patients with spontaneous SAH by using multiphase dynamic-enhanced CT.\n    \n\n\n          Materials and methods:\n        \n      \n      We retrospectively surveyed a series of patients with SAH who underwent CTP with 18-phase dynamic enhancement and confirmed the presence of extravasated contrast medium in the source image. We compared clinical features between 2 groups of patients with and without extravasation.\n    \n\n\n          Results:\n        \n      \n      Active bleeding was observed with increasing enhancement in 25.5% (13/51) of patients. All patients with extravasation were in Claassen grade 3 or 4 and WFNS grades 3, 4, or 5. The other group without extravasation included patients in all grades. A significant difference was observed in Claassen grade, WFNS grade, and increase of hematomas in follow-up CT (P < .05, for each) between the 2 groups. All CTP results of patients with extravasation were obtained within 2 hours of the onset of symptoms of SAH (P < .05). There was no significant difference in mortality at 14 days between the 2 groups (P = .128).\n    \n\n\n          Conclusions:\n        \n      \n      A high incidence of active bleeding (25.5%) was detected by multiphase dynamic-enhanced CT in patients with acute SAH. These results indicate that an awareness of active bleeding in patients with SAH has the potential to affect the treatment strategy."
        },
        {
            "title": "Automated whole-breast ultrasound: advancing the performance of breast cancer screening.",
            "abstract": "Screening has been successful in the early detection of some cancers, including cervical, colon, and breast. However, the success in mortality reduction by screening mammography has been limited in women with mammographically dense tissue. Magnetic resonance imaging has been used with success in the screening of high-risk women, but it is expensive. Also, its use in a screening program requires a contrast medium that has not been tested in humans or animals for long-term safety in humans or animals for repeated biannual or annual injections. Ultrasound also has the potential to be an ideal screening tool because it is relatively inexpensive and requires no injected contrast or ionizing radiation. However, the relatively poor conspicuity of some cancers by hand scanning and the considerable radiologist time necessary limit its use. Automated whole-breast ultrasound (AWBU) allows the radiologist to read the images quickly, at a convenient time, while being free from doing the scan. Two-dimensional AWBU uses a cine loop of axial images, with <1 mm image spacing, which are read on a high-resolution monitor to improve the conspicuity of small cancers. A blinded study of this system combined with screening mammography showed that adding AWBU both doubles overall cancer detection and triples the 1 cm-or-less invasive cancers found in dense-breasted women. As expected, mammography had a significantly greater detection rate of ductal carcinoma in situ than AWBU. As yet no clinical studies of other AWBU systems have been published."
        },
        {
            "title": "ADMA predicts major adverse renal events in patients with mild renal impairment and/or diabetes mellitus undergoing coronary angiography.",
            "abstract": "Asymmetric dimethylarginine (ADMA) is a competitive inhibitor of the nitric oxide (NO)-synthase and a biomarker of endothelial dysfunction (ED). ED plays an important role in the pathogenesis of contrast-induced nephropathy (CIN). The aim of our study was to evaluate serum ADMA concentration as a biomarker of an acute renal damage during the follow-up of 90 days after contrast medium (CM) application.Blood samples were obtained from 330 consecutive patients with diabetes mellitus or mild renal impairment immediately before, 24 and 48 hours after the CM application for coronary angiography. The patients were followed for 90 days. The composite endpoints were major adverse renal events (MARE) defined as occurrence of death, initiation of dialysis, or a doubling of serum creatinine concentration.Overall, ADMA concentration in plasma increased after CM application, although, there was no differences between ADMA levels in patients with and without CIN. ADMA concentration 24 hours after the CM application was predictive for dialysis with a specificity of 0.889 and sensitivity of 0.653 at values higher than 0.71 μmol/L (area under the curve: 0.854, 95% confidential interval: 0.767-0.941, P < 0.001). This association remained significant in multivariate Cox regression models adjusted for relevant factors of long-term renal outcome. 24 hours after the CM application, ADMA concentration in plasma was predictive for MARE with a specificity of 0.833 and sensitivity of 0.636 at a value of more than 0.70 μmol/L (area under the curve: 0.750, 95% confidence interval: 0.602-0.897, P = 0.004). Multivariate logistic regression analysis confirmed that ADMA and anemia were significant predictors of MARE. Further analysis revealed that increased ADMA concentration in plasma was highly significant predictor of MARE in patients with CIN. Moreover, patients with CIN and MARE had the highest plasma ADMA levels 24 hours after CM exposure in our study cohort. The impact of ADMA on MARE was independent of such known CIN risk factors as anemia, pre-existing renal failure, pre-existing heart failure, and diabetes.ADMA concentration in plasma is a promising novel biomarker of major contrast-induced nephropathy-associated events 90 days after contrast media exposure."
        },
        {
            "title": "Long-term lung cancer survivors have permanently decreased quality of life after surgery.",
            "abstract": "Background:\n        \n      \n      Retrospective evaluation of the long-term health-related quality of life (HRQoL) among survivors after non-small-cell lung cancer (NSCLC) surgery.\n    \n\n\n          Patients and methods:\n        \n      \n      A total of 586 patients underwent surgery for NSCLC in Helsinki University Central Hospital between January 2000 and June 2009. Two validated quality-of-life questionnaires, the 15D and the EORTC QLQ-C30 with its lung cancer-specific module, QLQ-LC13, were sent to the 276 patients alive in June 2011. Response rate was 83.3%. Results of the 15D were compared with those of an age- and gender-standardized general population.\n    \n\n\n          Results:\n        \n      \n      Median follow-up was 5 years. Compared with a general population, our patients had a significantly lower 15D total score, representing their total HRQoL and scores for dimensions of mobility, breathing, usual activities, depression, distress, and vitality. The patients, however, scored significantly higher on vision, hearing, and mental function.\n    \n\n\n          Conclusions:\n        \n      \n      NSCLC survivors may suffer postoperatively from permanently reduced long-term HRQoL compared to an age- and gender-matched general population. This is essential patient information as more patients are surviving longer."
        },
        {
            "title": "Visualization and dynamics of multidimensional health-related quality-of-life-adjusted overall survival: a new analytic approach.",
            "abstract": "Background:\n        \n      \n      Vesnarinone Trial (VesT) was a three-armed, placebo-controlled, randomized clinical trial designed to study the effects of 30 mg or 60 mg/day vesnarinone. Certain contradictory results involving patient health-related quality-of-life (HRQOL) and overall survival (OS) have made a definitive and unified conclusion difficult.\n    \n\n\n          Methods:\n        \n      \n      To reconcile these findings, we have focused on the HRQOL-adjusted OS, commonly known as quality-adjusted life years (QALYs). Currently, analyses of QALYs incorporate a single HRQOL subscale. However, the VesT HRQOL instrument had two subscales: physical (PHYS) and emotional (EMOT). We have developed new ways to visualize and compare EMOT- and PHYS-adjusted OS.\n    \n\n\n          Results:\n        \n      \n      In each VesT arm, there was an increased probability of superior EMOT-adjusted OS, compared to PHYS-adjusted OS. The magnitude of these findings was comparable across trial arms. Despite inferior survival and superior EMOT and PHYS scores, the 60-mg/day arm presents similar EMOT- and PHYS-adjusted OS compared to the placebo arm.\n    \n\n\n          Conclusions:\n        \n      \n      We have provided a fresh perspective on the complex interactions between multiple HRQOL dimensions and OS. These novel methods address the burgeoning need for robust information on the interplay between OS and HRQOL from a patient, clinical care and public policy perspective."
        },
        {
            "title": "Awareness Regarding Eye Donation in an Urban Slum Population: A Community-Based Survey.",
            "abstract": "Objectives:\n        \n      \n      Our objective was to assess the awareness of eye donation in an urban slum population and willingness to donate eyes after death.\n    \n\n\n          Materials and methods:\n        \n      \n      A cross-sectional, population-based study was undertaken in 20 urban slum clusters of the Indian capital, New Delhi. A total of 2004 individuals aged 18 years and older were recruited. After written, informed consent was obtained, knowledge regarding eye donation was assessed through a predesigned close-ended questionnaire. The questionnaire was framed so as to understand the sociodemographic factors influencing the willingness to donate and the awareness of eye donation in this distinct population.\n    \n\n\n          Results:\n        \n      \n      The mean age of the recruited individuals was 36.53 ± 13.68 years. Age did not have any significant effect on awareness regarding eye donation. We observed that 34.3% of the study population had no knowledge of eye donation and that 7.78% of the study population had excellent knowledge. Education seemed to be an important determining factor regarding knowledge of eye donation. Multivariable logistic regression demonstrated better awareness among the Hindu population (81.1%) and those belonging to a higher caste (P < .05). The younger age group (those 18-30 years old) showed significant willingness to donate their eyes versus older age groups (P < .001). In our study population, male participants (P = .006), those classified as literate (P < .001), and those classified as Hindu (P < .001) were more willing to pledge their eyes for donation.\n    \n\n\n          Conclusions:\n        \n      \n      Although there is substantial awareness about eye donation, willingness to pledge eyes was very low in the urban slum population. Additional efforts are needed to translate this awareness into actual eye donation in the urban poor population."
        },
        {
            "title": "Risk factors for falls in Iranian older adults: a case-control study.",
            "abstract": "Falls are an important cause of morbidity and mortality in older adults. Identifying potential risk factors would provide a considerable public health benefit. The objective of this retrospective study was to determine the risk factors for falling among Iranian older adults. Two hundred eighty community-dwelling elders, with and without a history of falls, participated in the study. Elders aged 60 or over referred to retirement centres completed a multi-section questionnaire on demographic information, behavioural, environmental, and medical factors of fall from May to September 2018. Data analysis was performed with descriptive statistics and logistic regression using the Stata version 14 software. Sedentary activity level (OR: 2.14; 95% CI: 1.85, 3.23), hearing loss (OR: 2.17; 95% CI: 1.23, 3.83), vertigo or dizziness (OR: 2.24; 95% CI: 1.02, 4.91) and visual impairment (OR: 1.63; 95% CI: 1.01, 2.67) were important predictors of falls. No significant associations were observed between falls with demographic factors and medication. This study indicates several modifiable risk factors may be associated with falls that affect the health of older adults. Appropriate interventions are necessary to reduce modifiable risk factors of falls of high-risk elders."
        },
        {
            "title": "Cost-Effectiveness of Screening for Intermediate Age-Related Macular Degeneration during Diabetic Retinopathy Screening.",
            "abstract": "Purpose:\n        \n      \n      To determine whether screening for age-related macular degeneration (AMD) during a diabetic retinopathy (DR) screening program would be cost effective in Hong Kong.\n    \n\n\n          Design:\n        \n      \n      We compared and evaluated the impacts of screening, grading, and vitamin treatment for intermediate AMD compared with no screening using a Markov model. It was based on the natural history of AMD in a cohort with a mean age of 62 years, followed up until 100 years of age or death.\n    \n\n\n          Participants:\n        \n      \n      Subjects attending a DR screening program were recruited.\n    \n\n\n          Method:\n        \n      \n      A cost-effectiveness analysis was undertaken from a public provider perspective. It included grading for AMD using the photographs obtained for DR screening and treatment with vitamin therapy for those with intermediate AMD. The measures of effectiveness were obtained largely from a local study, but the transition probabilities and utility values were from overseas data. Costs were all from local sources. The main assumptions and estimates were tested in sensitivity analyses.\n    \n\n\n          Main outcome measures:\n        \n      \n      The outcome was cost per quality-adjusted life year (QALY) gained. Both costs and benefits were discounted at 3%. All costs are reported in United States dollars ($).\n    \n\n\n          Results:\n        \n      \n      The cost per QALY gained through screening for AMD and vitamin treatment for appropriate cases was $12,712 after discounting. This would be considered highly cost effective based on the World Health Organization's threshold of willingness to pay (WTP) for a QALY, that is, less than the annual per capita gross domestic product of $29,889. Because of uncertainty regarding the utility value for those with advanced AMD, we also tested an extreme, conservative value for utility under which screening remained cost effective. One-way sensitivity analyses revealed that, besides utility values, the cost per QALY was most sensitive to the progression rate from intermediate to advanced AMD. The cost-effectiveness acceptability curve showed a WTP for a QALY of $29,000 or more has a more than 86% probability of being cost effective compared with no screening.\n    \n\n\n          Conclusions:\n        \n      \n      Our analysis demonstrated that AMD screening carried out simultaneously with DR screening for patients with diabetes would be cost effective in a Hong Kong public healthcare setting."
        },
        {
            "title": "Prognostic factors in methanol poisoning.",
            "abstract": "The aim of this study was to assess the clinical and laboratory factors in methanol poisoned patients to determine the prognosis of their toxicity. This survey was done as a prospective cross-sectional study in methanol-poisoned patients in Loghman-Hakim hospital poison center during 9 months from October 1999-June 2000. During this time 25 methanol-poisoned patients were admitted. The mortality rate was 12 (48%). Amongst survivors, three (23%) of the patients developed blindness due to their poisoning and the other 10 (77%) fully recovered without any complication. The mortality rate in comatose patients was nine (90%) while in non-comatose patients it was three (20%) (P<0.001). There was a significant difference in mean pH in the first arterial blood gas of patients who subsequently died (6.82+/-0.03) and survivors (7.15+/-0.06) (P<0.001, M-W). The mean time interval between poisoning and ED presentation in deceased patients were (46+/-15.7) hours, in survived with sequelae were (16.7+/-6.7) and in survived without sequelae were (10.3+/-7.2) hours (P<0.002, K-W). We found no significant difference between the survivors versus the patients who died regarding methanol. Simultaneous presence of ethanol and opium affected the outcome of the treatment for methanol intoxication favourably and unfavourably, respectively. In our study, poor prognosis was associated with pH<7, coma on admission and >24 hours delay from intake to admission."
        },
        {
            "title": "Role of double-contrast barium enema in colorectal cancer screening based on fecal occult blood.",
            "abstract": "Aims and background:\n        \n      \n      Screening for colorectal cancer by fecal occult-blood testing has been shown to be effective in reducing colorectal cancer mortality. Total colonoscopy is the test of choice for the assessment of fecal occult blood-positive subjects. Double-contrast barium enema is commonly employed to study the rest of the colon when colonoscopy is incomplete. The present study evaluated the contribution of double-contrast barium enema in detecting neoplastic lesions of the colon in fecal occult-blood-positive subjects with incomplete colonoscopy.\n    \n\n\n          Methods:\n        \n      \n      In the frame of a screening program for colorectal cancer in the Florence District, a new immunochemical fecal occult-blood test replaced the classic guaiac fecal occult-blood test in 1993. Subjects with a positive fecal occult-blood test were invited to undergo total colonoscopy. Incomplete colonoscopy prompted double-contrast barium enema. Type and rate of neoplastic lesions detected by endoscopy or double-contrast barium enema as single methods or combined were evaluated.\n    \n\n\n          Results:\n        \n      \n      A total of 38,829 subjects underwent fecal occult-blood testing in the period 1993-2000. Overall, 1,542 were positive. Assessment was refused by 235 subjects. Out of 1,307 subjects accepting assessment, total colonoscopy was attempted in 1,294: of these, it was not possible in 343 cases, and double-contrast barium enema was advised and performed in 261 subjects. Colorectal cancer was detected in 115 subjects, single or multiple adenomas in 323, hyperplastic polyps in 58, inflammatory, hamartomatous or not histologically confirmed polyps in 38, and other benign non-polypoid findings or no abnormality in 773. There were significant differences between the rates of detected colorectal cancers or adenomas of total and incomplete colonoscopy. There were also significant differences between incomplete colonoscopy and the combination of incomplete colonoscopy and double-contrast barium enema as regards rates of detected colorectal cancer, and between total colonoscopy and the combination of incomplete colonoscopy with double-contrast barium enema as regards rates of detected adenomas. Double-contrast barium enema associated to incomplete colonoscopy was responsible for an increase in detection rates of cancer or adenoma of 2.3/1000 or 3.8/1000, respectively.\n    \n\n\n          Conclusions:\n        \n      \n      Double-contrast barium enema was useful in detecting colorectal cancer beyond the range reached by incomplete colonoscopy, whereas our data confirmed a lower sensitivity of double-contrast barium enema for polyps. The diagnostic contribution observed in the present survey confirms the opportunity of performing double-contrast barium enema as a routine adjunct to incomplete colonoscopy. Nevertheless, in order to maximize the detection rate of adenomas, the rate of total colonoscopy should be kept as high as possible."
        },
        {
            "title": "Enhancement Characteristics of the Computed Tomography Pulmonary Angiography Test Bolus Curve and Its Use in Predicting Right Ventricular Dysfunction and Mortality in Patients With Acute Pulmonary Embolism.",
            "abstract": "Purpose:\n        \n      \n      The purpose of the study was to evaluate the relationship between computed tomography pulmonary angiography (CTPA) test bolus curve data and mortality in patients with pulmonary embolism (PE) in comparison with conventional methods of right ventricular (RV) dysfunction.\n    \n\n\n          Materials and methods:\n        \n      \n      The study was approved by our institutional review board and is HIPAA-compliant. We retrospectively evaluated consecutive CTPA studies performed with a test bolus technique in a 2-year period. A time-density curve was derived from each test bolus. For comparison, left ventricular (LV) and RV dimensions (area, diameter) and PE load score (Qanadli method) were measured using CT data. A cardiologist blinded to the clinical and other imaging data reviewed a subset of the corresponding echocardiographic images to assess for RV dysfunction. Demographic data, mode of treatment, and patient outcome information were gathered using electronic medical records. Test bolus and anatomic data were correlated with PE-related mortality.\n    \n\n\n          Results:\n        \n      \n      A total of 71 patients (34 men and 37 women, average age 54.4 y) who had a CTPA performed using a test bolus technique were diagnosed with acute PE. Factors that significantly correlated with PE-related mortality on univariate analysis were: age above 60 years (odds ratio 19.1, P = 0.05), RV/LV diameter >1.5 (odds ratio 48.8, P < 0.001), RV/LV area >1 (odds ratio 8.6, P = 0.06), bolus curve upslope time >6 seconds (odds ratio 23.3, P = 0.04), 50% downslope time >6 seconds (odds ratio 20, P = 0.01), and embolus load score >15 (odds ratio 25, P = 0.03). The predictive value of upslope time (Exp(B) 1.65, P = 0.05), RV/LV diameter (Exp(B) 43.8, P = 0.01), and RV/LV area (Exp(B) 16.7, P = 0.01) were confirmed to be statistically significant in multivariate analyses.\n    \n\n\n          Conclusions:\n        \n      \n      Data from the CTPA timing bolus curve provide prognostic value similar to the best conventional methods for predicting PE-related mortality."
        },
        {
            "title": "Metabolic syndrome predicts long-term mortality in subjects without established diabetes mellitus in asymptomatic Korean population: A propensity score matching analysis from the Korea Initiatives on Coronary Artery Calcification (KOICA) registry.",
            "abstract": "Despite the different features of diabetes mellitus (DM) in Asian populations compared with Western populations, the impact of metabolic syndrome (MetS) on long-term mortality according to DM status has not yet been elucidated in the Asian population.After performing 1:1 propensity score matching (PSM) using clinical variables including age, gender, smoking, and individual MetS components between DM and non-DM subjects from the data of the Korea Initiatives on Coronary Artery Calcification registry, mortality was evaluated according to DM and MetS in 14,956 asymptomatic Korean subjects.The mean follow-up duration was 53.1 months (interquartile range: 33-80). The overall prevalence of MetS was 60%. DM subjects had higher mortality compared with non-DM subjects (1.2% vs 0.7%, respectively; P = 0.001); the cumulative mortality by Kaplan-Meier analysis was higher in DM subjects than in non-DM subjects (log-rank P = 0.001). DM increased the risk of mortality in PSM participants (hazard ratio [HR] 1.74; P = 0.001). In non-DM subjects, MetS (HR 2.32) and one of its components, central obesity (HR 1.97), were associated with an increased risk of mortality (both P < 0.05). In contrast, there was no significant difference in the risk of mortality according to MetS or its components in DM subjects. After adjusting for confounding risk factors, it was shown that MetS independently increased the risk of mortality in non-DM subjects.Compared with non-DM subjects, DM subjects have an increased risk of long-term mortality among PSM participants. MetS appears to have an independent impact on mortality in subjects without established DM among the asymptomatic Korean population. Our results may not be applicable to the whole subjects with MetS because the PSM using MetS components was performed between subjects with and without DM which was very high risk for adverse clinical events."
        },
        {
            "title": "[Value of N-terminal pro brain natriuretic peptide in predicting acute kidney injury in patients with acute decompensated chronic heart failure].",
            "abstract": "Aim:\n        \n      \n      To investigate the prognostic value of serum N-terminal pro-brain natriuretic peptide (NT-proBNP) in the development of acute kidney injury (AKI) in patients with acute decompensated chronic heart failure (ADCHF).\n    \n\n\n          Subjects and methods:\n        \n      \n      Eighty-three patients (55 (66%) men and 28 (34%) women; mean age, 65±11 years) with ADCHF were examined. AKI was diagnosed and classified according to the 2012 Kidney Disease Improving Global Outcomes Clinical Practice guidelines. To rule out contrast-induced AKI, the investigation enrolled only patients in whom radiopague agents had not been injected 7 days before and during hospitalization. Enzyme immunoassay was used to determine serum NT-proBNP concentrations in all the patients upon hospital admission.\n    \n\n\n          Results:\n        \n      \n      AKI was diagnosed in 18 (22%) patients, 13 (16%) had Stage I, 4 (5%) had Stage II, and 1 (1%) had Stage III. The serum concentration of NT-proBNP was significantly higher in patients with AKI than that in the other patients [1512.1 (981.0; 2246.2) and 861.8 (499.0; 1383.6) pg/ml (p=0.008). The rise in NT-proBNP concentrations of more than 942 pg/ml was established to be associated with a considerable increase in the risk of AKI (relative risk (RR) was 4.3; 95% confidence interval (CI), 1.27-14.90; p=0.02). RОС analysis indicated that a NT-proBNP level of >942 pg/ml allows prediction of AKI with a sensitivity of 78% (52; 94) and a specificity of 55% (44; 69) (AUC=0.70; p=0.006). Four (5%) patients died in hospital. NT-proBNP levels in all the dead were greater than 942 pg/ml. Two of the 4 deceased patients had AKI.\n    \n\n\n          Conclusion:\n        \n      \n      A high level of NT-proBNP in a patient with ADCHF during hospitalization can serve as a biomarker for high risk of AKI and for high mortality rates."
        },
        {
            "title": "Management of chronic upper abdominal pain in cancer: transdiscal blockade of the splanchnic nerves.",
            "abstract": "Background:\n        \n      \n      The use of celiac plexus block to relieve the intractable pain caused by upper abdominal malignancies is well established. However, its effects are inconsistent for many reasons, mainly because of structural anatomic distortion as a consequence for the malignancy. The splanchnic nerve blockade (SNB) seems to be a useful alternative to the celiac plexus block in upper abdominal pain relief.\n    \n\n\n          Materials and methods:\n        \n      \n      The pain of 109 patients with unresectable upper abdominal or lower esophageal neoplasms was managed by posterior transdiscal SNBs guided by computed tomography at the Instituto Nacional de Cancerología in Mexico City from January 2004 to June 2007. The study evaluated SNB efficacy with regard to pain relief, its adverse effects/complications, and patient satisfaction.\n    \n\n\n          Results:\n        \n      \n      Splanchnic nerve blockade efficacy with regard to pain relief was exhibited by a marked decrease in the visual analog score and in opioid consumption, with preprocedural mean values dropping from 6.1 ± 2.4 and 102.4 mg/d of morphine to 2.7 ± 2.4 and 53.3 mg/d at the first postprocedural visit, respectively. These results persisted during the 1-year follow-up period or until death. Minor adverse effects (moderate diarrhea and mild hypotension) were frequent (n = 64 and n = 47, respectively), and severe complications occurred in 1 patient with a transient paraparesis (n = 1). No procedure-related mortality was observed.\n    \n\n\n          Conclusions:\n        \n      \n      Splanchnic nerve blockade via a transdiscal approach is a technique that provides analgesia and the alleviation of the secondary undesirable effects of analgesic drugs resulting from the decrease of morphine consumption in patients with upper abdominal malignancies. In experienced teams, the reliability of its analgesic effect is high, with a low rate of severe complications."
        },
        {
            "title": "[Corneal arcus and life expectancy].",
            "abstract": "In 1964-66, the authors completed the comprehensive medical screening of 1412 persons. Apart from the indicators of health state they also recorded their social and cultural parameters. The diagnoses they registered included AC, which has been covered in literature in rather contradicting ways. They found no data concerning survival; as analysing such a correlation is only possible within the frame-work of a several-decade follow-up study. By the end of the follow-up stage (31:12:1994), after 30 years, 1375 persons had died. Their death certificates and--if there were any--necropsy records have been processed and thoroughly analysed. They examined the occurrence of AC, life duration and survival probability--all in correlation with age, gender, constitution, certain diseases (hypertonia, ostheoarthrosis) and diagnoses at death (ischaemic heart diseases, acute myocardiac infarction, cerebrovascular diseases). They point it out that the occurrence of AC is significantly higher among males, but it increases in strong correlation with age in both sexes. Those who had AC were found to be older at the time of death, but it doesn't mean that AC correlates with better life expectancy--it means that AC occurs at older age. The survival probability of men over 75 was better than that of women. On the whole, AC is unfavourable concerning life expectancy, but the later it occurs, the less it can be used as an indicator of life expectancy. It was found that greater average weight correlated with longer average life duration, while among females the more a person weighed, the less frequent AC became. It was true for each weight group that those with AC had worse life expectancy. Altogether those with no AC were found to suffer from hypertonia significantly more frequently. The life expectancy of those with both AC and hypertonia, however, was always worse than those with hypertonia only, regardless of age and the type of hypertonia. Generally women are in a more favourable position, but in the 'serious' and 'very serious' hypertonia groups there is practically no difference in the survival of the two sexes. The authors have also found that AC has a significant negative prognostical value concerning survival and correlation with ostheoarthrosis, ischaemic heart diseases and cerebrovascular diseases. The correlation of AC with acute myocardiac infarction could not be proved convincingly."
        },
        {
            "title": "Islet cell liver metastases: assessment of volumetric early response with functional MR imaging after transarterial chemoembolization.",
            "abstract": "Purpose:\n        \n      \n      To assess early response to transarterial chemoembolization by using volumetric functional magnetic resonance (MR) imaging in patients with islet cell liver metastases (ICLMs).\n    \n\n\n          Materials and methods:\n        \n      \n      This retrospective institutional review board-approved HIPAA-compliant study included 215 ICLMs in 26 patients (15 men, 11 women; mean age, 59.7 years; age range, 37-79 years). Volumetric measurements were performed by an experienced radiologist on diffusion-weighted and contrast material-enhanced MR images at baseline and 1-month follow-up. Measurements included mean change (three-dimensional [3D] mean apparent diffusion coefficient [ADC], 3D mean enhancement) and percentage of tumor with change above a predetermined threshold (3D threshold ADC, 3D threshold enhancement). Response by volumetric measurements at 1-month follow-up was compared with Response Evaluation Criteria in Solid Tumors (RECIST) at 6-month follow-up. Lesions that had complete or partial response were considered responders, while those with stable or progressive disease were considered nonresponders. Statistical analysis included the t test, receiver operating characteristic (ROC) curve analysis, and logistic regression analysis.\n    \n\n\n          Results:\n        \n      \n      RECIST criteria at 6-month follow-up indicated 78 (36.3%) lesions responded, while 137 (63.7%) did not. The increase in 3D mean ADC was significantly higher in responders than in nonresponders (median, 26.2% vs 10.9%; P<.001). The 3D threshold ADC was 71.1% in responders and 47.6% in nonresponders (P<.001). Decrease in 3D mean arterial enhancement (AE) was significantly higher in responders than in nonresponders (median, 40.5% vs 18.0%; P<.001). Decrease in 3D mean venous enhancement (VE) was significantly higher in responders than in nonresponders (median, 28.0% vs 10.0%; P<.001). The 3D threshold VE and 3D threshold AE did not differ between responders and nonresponders. In unadjusted logistic regression analyses, 3D mean ADC and 3D threshold ADC had the highest odds ratio (1.02 and 1.03, respectively) and the largest area under the ROC curve (0.698 and 0.695, respectively).\n    \n\n\n          Conclusion:\n        \n      \n      Volumetric functional MR imaging could be used to predict early response of hepatic ICLMs to therapy and to distinguish between responders and nonresponders."
        },
        {
            "title": "[Long-term results of iridocyclectomy for iris tumours].",
            "abstract": "Background:\n        \n      \n      The aim of this study was to evaluate the long-term survival rate and functional results after iridocyclectomy.\n    \n\n\n          Patients and method:\n        \n      \n      Between 1980 and 2002 39 patients (26 female and 13 male) ranging in age from 20 to 79 years (median m = 58 years) underwent iridocyclectomy for a tumour of periphery iris by means of a lamellar technique or by trepanating. Follow-up time ranged from 3 months to 24 years (m = 11.2 years).\n    \n\n\n          Results:\n        \n      \n      In 21 cases (54 %) there was a malignant tumour including 20 melanomas (mostly spindle-cell and mixed-cell melanomas) and one filiae of a bronchial carcinoma. There was a variety of histopathological entities in the 18 benign lesions (46 %). Naevi were the most frequent. The outcome was satisfactory: 57 % of the patients kept a visual acuity of > 0.5. Three eyes had to be enucleated. The rate of recurrence was 10 % (4 cases). The Kaplan-Meyer estimate for the 10-year-survival of the patients with a malignant iris tumour was 77 %. Two patients died of metastic melanoma following spindle-cell and mixed-cell melanoma.\n    \n\n\n          Conclusion:\n        \n      \n      The long-term functional results after Iridocyclectomy are good, whereas complications and recurrences are rare. The 10-year-survival is high. Over a long period iridocyclectomy is a recommendable surgical procedure for removal of progredient tumours of the anterior uvea."
        },
        {
            "title": "Cytomegalovirus optic neuritis: characteristics, therapy and survival.",
            "abstract": "Little is known about the natural history of cytomegalovirus (CMV) optic neuritis in the acquired immunodeficiency syndrome. We analyzed the clinical course of CMV optic neuritis in 30 consecutive subjects (35 eyes), and compared the survival of patients with CMV optic neuritis to that of a group having CMV retinitis alone, with both groups matched for ganciclovir therapy. Four untreated eyes had a median final visual acuity of no light perception. The median final visual acuity was 20/100 in treated subjects with a mean follow-up of 6.6 months. Following ganciclovir treatment, 2 eyes showed visual improvement, 17 eyes had unchanged visual acuity, and 12 eyes had marked drop in acuity. Relapse occurred in 4 subjects maintained on single-dose ganciclovir, and was controlled on double-dose ganciclovir. Survival was similar in the group of CMV retinitis alone versus the group of CMV optic neuritis with retinitis. Early recognition and therapy of CMV optic neuritis protects against irreversible visual loss. CMV optic neuritis does not carry a worse prognosis for survival than CMV retinitis alone."
        },
        {
            "title": "Effect of the metabolic syndrome on organ damage and mortality in patients with systemic lupus erythematosus: a longitudinal analysis.",
            "abstract": "Objectives:\n        \n      \n      To study the effect of the metabolic syndrome (MetS) on organ damage and mortality in patients with SLE.\n    \n\n\n          Methods:\n        \n      \n      Consecutive patients who fulfilled ≥4 ACR criteria for SLE were assessed for the MetS in October 2010. The MetS was defined by the updated joint consensus criteria, using the Asian criteria for central obesity. Longitudinal data on organ damage and mortality were retrieved. The association between MetS and new damage and mortality was studied by logistic regression.\n    \n\n\n          Results:\n        \n      \n      A total of 577 SLE patients were followed (93% women; age 41.2±13.4 years; SLE duration 9.3±7.2 years) and 85 (14.7%) patients qualified the MetS. After a follow-up of 66.3±1.8 month, new organ damage and vascular events developed in 128(22%) and 23(4.0%) patients, respectively. Thirty-nine (6.8%) patients succumbed. Patients with the MetS, compared to those without, had significantly more SLICC damage score accrual (0.70±1.0 vs 0.26±0.6; p<0.001), new vascular events (11% vs 2.8%; p=0.001), all-cause (14% vs 5.5%; p=0.003) and vascular (7.1% vs 0.2%; p<0.001) mortality. Logistic regression revealed that the MetS was significantly associated with new damage in the renal (OR 5.48[2.06-14.6]; p=0.001) and endocrine system (OR 38.0[4.50-321]; p=0.001), adjusted for age, sex, SLE duration, ever smoking, antiphospholipid antibodies and the new use of glucocorticoids or hydroxychloroquine since recruitment. Moreover, the presence of the MetS also significantly increased the risk of new vascular events (OR 3.38[1.31-8.74];p=0.01) and vascular mortality (OR 28.3[3.24-247]; p=0.002) after adjustment for the same covariates.\n    \n\n\n          Conclusion:\n        \n      \n      In this longitudinal study, the MetS is significantly associated with new organ damage, vascular events and mortality in patients with SLE."
        },
        {
            "title": "Proton irradiation for peripapillary and parapapillary melanomas.",
            "abstract": "Objective:\n        \n      \n      To examine ocular outcomes and survival after proton irradiation in patients with peripapillary and parapapillary melanomas ineligible for the Collaborative Ocular Melanoma Study.\n    \n\n\n          Methods:\n        \n      \n      A total of 573 patients who received proton irradiation from January 4, 1985, through December 24, 1997, for tumors located within 1 disc diameter of the optic nerve, and therefore ineligible for the Collaborative Ocular Melanoma Study, were evaluated. Cumulative rates of vision loss in the treated eye, eye loss, melanoma-related mortality, and tumor recurrence were estimated using the Kaplan-Meier method.\n    \n\n\n          Results:\n        \n      \n      Most (53.4%) tumors abutted the optic disc; median distance from the tumor to the macula was 0.5 disc diameters. By 5 years after proton therapy, radiation papillopathy had developed in 56.8% and maculopathy in 60.4% of patients. Of 450 patients with a baseline visual acuity of 20/200 or better in the treated eye, vision was retained in 54.9% at 2 years after irradiation. This decreased to 20.3% by 5 years after treatment, although 56.2% had visual acuity of counting fingers or better. Five- and 10-year rates of local recurrence were 3.3% and 6.0%, respectively. Enucleation rates were 13.3% at 5 years and 17.1% at 10 years after treatment. Melanoma-related mortality rates were similar to those in our larger cohort of patients (24.0% at 15 years).\n    \n\n\n          Conclusions:\n        \n      \n      Proton irradiation should be considered for treating patients with tumors contiguous to the optic disc. Although visual acuity is compromised, some preservation is possible (counting fingers or better in many patients). Eye conservation is likely, with low rates of tumor recurrence and no increased risk of metastasis."
        },
        {
            "title": "Loss of gray-white matter discrimination as an early CT sign of brain ischemia/hypoxia in victims of asphyxial cardiac arrest.",
            "abstract": "Brain CT obtained from cardiac arrest (CA) victims immediately after resuscitation may be useful in predicting their outcomes. Most data have been derived from CA victims of cardiac etiology, however, CT signs of brain ischemia/hypoxia have rarely been studied in victims of asphyxial CA. Loss of gray-white matter discrimination (GWMD) at the basal ganglia seems to be the most reliable early CT sign of brain ischemia/hypoxia; a retrospective study was conducted to clarify its incidence, prognostic significance, and temporal profile in resuscitated victims of CA by food asphyxiation. Brain CT scans of each victim were interpreted by two blinded observers. During a 5-year period, 39 resuscitated victims of CA by food asphyxiation underwent brain CT. Thirty-one (79%) showed loss of GWMD, none of whom survived to discharge. Among the other eight victims with seemingly intact brain CT, five (63%) survived to discharge. Loss of GWMD predicted fatality with sensitivity of 100% and specificity of 63%. The interobserver concordance was 82% with kappa coefficient of 0.56. Loss of GWMD developed almost invariably when the asphyxiation-return of spontaneous circulation (ROSC) interval exceeded 10 min. There were five victims with asphyxiation-ROSC interval ≤ 10 min, all of whom survived to discharge. In contrast, none of the 34 victims with the interval >10 min survived to discharge. Loss of GWMD may develop in a relatively time-dependent manner and may be a reliable radiographic indicator of poor outcome in resuscitated victims of asphyxial CA."
        },
        {
            "title": "Sensory Impairment and All-Cause Mortality Among the Oldest-Old: Findings from the Chinese Longitudinal Healthy Longevity Survey (CLHLS).",
            "abstract": "Objectives:\n        \n      \n      To investigate the association between sensory impairment and all-cause mortality among the oldest-old (aged 80 and older) in China.\n    \n\n\n          Design:\n        \n      \n      Prospective cohort study.\n    \n\n\n          Setting:\n        \n      \n      Community-based setting in 22 provinces of China.\n    \n\n\n          Participants:\n        \n      \n      A total of 8788 older adults aged 80 and over at baseline with complete hearing and vision function data were included as the study population.\n    \n\n\n          Measurements:\n        \n      \n      Sensory impairment was categorized as no sensory impairment, hearing impairment (HI) only, vision impairment (VI) only and dual sensory impairment (DSI) according to hearing and vision function. Deaths were identified through interviews by close family members. Cox proportion hazards regression models were used to examine the association of sensory impairment with mortality, adjusting for socio-demographic data, life style factors and health status.\n    \n\n\n          Results:\n        \n      \n      The mean age was 92.3 ± 7.6 years old, and 60.1% of participants were female. Among 8788 participants, 9.8% were recognized as DSI, 9.7% were HI only and 10.4% with VI only. Comparing with participants with no sensory impairment, those with VI only (HR=1.10, 95% CI=1.01-1.20) and DSI (HR=1.21, 95% CI=1.09-1.35) were significantly associated with higher risk of all-cause mortality in the fully adjusted model.\n    \n\n\n          Conclusion:\n        \n      \n      Our results demonstrated that VI only and DSI were significantly associated with higher risk of mortality among Chinese older adults aged 80 and over. The finding advocated that it is necessary to identify and manage sensory impairments for the advanced ages to reduce mortality risks."
        },
        {
            "title": "Potential human and economic cost-savings attributable to vision testing policies for driver license renewal, 1989-1991.",
            "abstract": "Purpose:\n        \n      \n      This study assessed the impact of vision-related relicensing policies on traffic fatalities in the United States. There is a limited empirical basis for state vision testing policies for relicensing. Furthermore, it is uncertain whether contemporary vision standards for driver licensing achieve their implicit goal of protecting the public's health, or inappropriately restrict the mobility of competent drivers.\n    \n\n\n          Methods:\n        \n      \n      The 48 contiguous states and the District of Columbia were the \"subjects\" in this investigation. During the study period (1989 to 1991), 10 states did not require vision testing for driver license renewal. Multiple regression modeling was used to assess the impact of vision-related relicensing policies on traffic safety and to estimate the number of avoidable vehicle occupant fatalities and corresponding economic costs associated with traffic crashes involving older drivers (> or = 60 years). The primary data source for this investigation was the Fatal Accident Reporting System (FARS) database.\n    \n\n\n          Results:\n        \n      \n      Vision-related relicensing policies were significantly associated (p < 0.05) with lower vehicle occupant fatality rates of older drivers. According to the final regression model, approximately 222 fewer vehicle occupant fatalities (-12.2%) associated with older drivers would be expected for the 3-year period if mandatory vision testing policies had been in effect in 8 of the 10 states without such policies. Conservatively, those avoidable deaths represent an estimated $31 million in avoidable economic costs.\n    \n\n\n          Conclusions:\n        \n      \n      State-level mandatory vision testing for relicensure may enhance traffic safety and reduce the economic burden of fatal crashes. Vision testing requirements should be maintained by jurisdictions with such requirements, and jurisdictions without such requirements should consider the potential traffic safety benefits of vision testing for driver license renewal."
        },
        {
            "title": "Self-Reported Hearing/Visual Loss and Mortality in Middle-Aged and Older Adults: Findings From the Komo-Ise Cohort, Japan.",
            "abstract": "Background:\n        \n      \n      The association of sensory loss with mortality remains unclear. We aimed to explore the associations of hearing loss (HL), visual loss (VL), and dual sensory loss (DSL) with survival.\n    \n\n\n          Methods:\n        \n      \n      Data came from the Komo-Ise study cohort in Gunma Prefecture, Japan, where the community-dwelling residents aged 40-69 years were followed up from 1993 to 2010. We analyzed 9,522 individuals who answered the follow-up questionnaires in 2000 (average age 64 [range, 47 to 77] years in 2000). The primary exposures were \"HL only,\" \"VL only,\" or \"DSL\", with \"no HL/VL\" as the reference. These sensory loss statuses were assessed by asking the difficulty in hearing conversation or reading newspaper even with aids in the follow-up questionnaires in 2000. All-cause and cause-specific mortality were ascertained from linkage to death certificate data. Cox proportional hazards models adjusting for confounders, including demographic factors, socioeconomic status, and health status, were used. Potential mediators (depression, walking disability, and social participation) were additionally adjusted for.\n    \n\n\n          Results:\n        \n      \n      There were 1,105 deaths over the 10-year follow-up. After adjustment for the potential confounders, HL and DSL were associated with increased all-cause mortality (hazard ratios of 1.74 [95% CI, 1.18-2.57] and 1.63 [95% CI, 1.09-2.42], respectively). Potential mediators explained a modest portion of the association. As for cause-specific mortality, HL was associated with increased cancer mortality, while VL and DSL were associated with increased cardiovascular disease mortality.\n    \n\n\n          Conclusions:\n        \n      \n      Self-reported HL and DSL may be risk factors of mortality among middle-aged or elderly Japanese populations."
        },
        {
            "title": "Cardiac myosin-binding protein C is a novel marker of myocardial injury and fibrosis in aortic stenosis.",
            "abstract": "Objective:\n        \n      \n      Cardiac myosin-binding protein C (cMyC) is an abundant sarcomeric protein and novel highly specific marker of myocardial injury. Myocyte death characterises the transition from hypertrophy to replacement myocardial fibrosis in advanced aortic stenosis. We hypothesised that serum cMyC concentrations would be associated with cardiac structure and outcomes in patients with aortic stenosis.\n    \n\n\n          Methods:\n        \n      \n      cMyC was measured in two cohorts in which serum had previously been prospectively collected: a mechanism cohort of patients with aortic stenosis (n=161) and healthy controls (n=46) who underwent cardiac MRI, and an outcome cohort with aortic stenosis (n=104) followed for a median of 11.3 years.\n    \n\n\n          Results:\n        \n      \n      In the mechanism cohort, cMyC concentration correlated with left ventricular mass (adjusted Î²=11.0 g/m2 per log unit increase in cMyC, P<0.001), fibrosis volume (adjusted Î²=8.0 g, P<0.001) and extracellular volume (adjusted Î²=1.3%, P=0.01) in patients with aortic stenosis but not in controls. In those with late gadolinium enhancement (LGE) indicative of myocardial fibrosis, cMyC concentrations were higher (32 (21-56) ng/L vs 17 (12-24) ng/L without LGE, P<0.001). cMyC was unrelated to coronary calcium scores. Unadjusted Cox proportional hazards analysis in the outcome cohort showed greater all-cause mortality (HR 1.49 per unit increase in log cMyC, 95% CI 1.11 to 2.01, P=0.009).\n    \n\n\n          Conclusions:\n        \n      \n      Serum cMyC concentration is associated with myocardial hypertrophy, fibrosis and an increased risk of mortality in aortic stenosis. The quantification of serum sarcomeric protein concentrations provides objective measures of disease severity and their clinical utility to monitor the progression of aortic stenosis merits further study.\n    \n\n\n          Clinical trial registration:\n        \nNCT1755936; Post-results."
        },
        {
            "title": "Treatment of acute visceral aortic pathology with fenestrated/branched endovascular repair in high-surgical-risk patients.",
            "abstract": "Objective:\n        \n      \n      The safety and feasibility of fenestrated/branched endovascular repair of acute visceral aortic disease in high-risk patients is unknown. The purpose of this report is to describe our experience with surgeon-modified endovascular aneurysm repair (sm-EVAR) for the urgent or emergent treatment of pathology involving the branched segment of the aorta in patients deemed to have prohibitively high medical and/or anatomic risk for open repair.\n    \n\n\n          Methods:\n        \n      \n      A retrospective review was performed on all patients treated with sm-EVAR for acute indications. Planning was based on three-dimensional computed tomographic angiogram reconstructions and graft configurations included various combinations of branch, fenestration, or scallop modifications.\n    \n\n\n          Results:\n        \n      \n      Sixteen patients (mean age [± standard deviation], 68 ± 10 years; 88% male) deemed high risk for open repair underwent urgent or emergent repair using sm-EVAR. Indications included degenerative suprarenal or thoracoabdominal aneurysm (six), presumed or known mycotic aneurysm (four), anastomotic pseudoaneurysm (three), false lumen rupture of type B dissection (two), and penetrating aortic ulceration (one). Nine (56%) had previous aortic surgery and all patients were either American Society of Anesthesiologists class IV (n = 9) or IV-E (n = 7). A total of 40 visceral vessels (celiac, 10; superior mesenteric artery, 10; right renal artery, 10; left renal artery, 10) were revascularized with a combination of fenestrations (33), directional graft branches (six), and graft scallops (one). Technical success was 94% (n = 15/16), with one open conversion. Median contrast use was 126 mL (range, 41-245) and fluoroscopy time was 70 minutes (range, 18-200). Endoleaks were identified intraoperatively in four patients (type II, n = 3; type IV, n = 1), but none have required remediation. Mean length of stay was 12 ± 15 days (median, 5.5; range, 3-59). Single complications occurred in five (31%) patients as follows: brachial sheath hematoma (one), stroke (one), ileus (one), respiratory failure (one), and renal failure (one). An additional patient experienced multiple complications including spinal cord ischemia (one) and multiorgan failure resulting in death (n = 1; in-hospital mortality, 6.3%). The majority of patients were discharged to home (63%; n = 10) or short-term rehabilitation units (25%; n = 4), while one patient required admission to a long-term acute care setting. There were no reinterventions at a median follow-up of 6.2 (range, 1-16.1) months. Postoperative computed tomographic angiogram was available for all patients and demonstrated 100% branch vessel patency, with one type III endoleak pending intervention. There were two late deaths at 1.4 and 13.4 months due to nonaortic-related pathology.\n    \n\n\n          Conclusions:\n        \n      \n      Urgent or emergent treatment of acute pathology involving the visceral aortic segment with fenestrated/branched endograft repair is feasible and safe in selected high-risk patients; however, the durability of these repairs is yet to be determined."
        },
        {
            "title": "Indicators of \"healthy aging\" in older women (65-69 years of age). A data-mining approach based on prediction of long-term survival.",
            "abstract": "Background:\n        \n      \n      Prediction of long-term survival in healthy adults requires recognition of features that serve as early indicators of successful aging. The aims of this study were to identify predictors of long-term survival in older women and to develop a multivariable model based upon longitudinal data from the Study of Osteoporotic Fractures (SOF).\n    \n\n\n          Methods:\n        \n      \n      We considered only the youngest subjects (n = 4,097) enrolled in the SOF cohort (65 to 69 years of age) and excluded older SOF subjects more likely to exhibit a \"frail\" phenotype. A total of 377 phenotypic measures were screened to determine which were of most value for prediction of long-term (19-year) survival. Prognostic capacity of individual predictors, and combinations of predictors, was evaluated using a cross-validation criterion with prediction accuracy assessed according to time-specific AUC statistics.\n    \n\n\n          Results:\n        \n      \n      Visual contrast sensitivity score was among the top 5 individual predictors relative to all 377 variables evaluated (mean AUC = 0.570). A 13-variable model with strong predictive performance was generated using a forward search strategy (mean AUC = 0.673). Variables within this model included a measure of physical function, smoking and diabetes status, self-reported health, contrast sensitivity, and functional status indices reflecting cumulative number of daily living impairments (HR >or= 0.879 or RH <or= 1.131; P < 0.001). We evaluated this model and show that it predicts long-term survival among subjects assigned differing causes of death (e.g., cancer, cardiovascular disease; P < 0.01). For an average follow-up time of 20 years, output from the model was associated with multiple outcomes among survivors, such as tests of cognitive function, geriatric depression, number of daily living impairments and grip strength (P < 0.03).\n    \n\n\n          Conclusions:\n        \n      \n      The multivariate model we developed characterizes a \"healthy aging\" phenotype based upon an integration of measures that together reflect multiple dimensions of an aging adult (65-69 years of age). Age-sensitive components of this model may be of value as biomarkers in human studies that evaluate anti-aging interventions. Our methodology could be applied to data from other longitudinal cohorts to generalize these findings, identify additional predictors of long-term survival, and to further develop the \"healthy aging\" concept."
        },
        {
            "title": "Detection and management of falls and instability in vulnerable elders by community physicians.",
            "abstract": "Objectives:\n        \n      \n      To investigate quality of care for falls and instability provided to vulnerable elders.\n    \n\n\n          Design:\n        \n      \n      Six process of care quality indicators (QIs) for falls and instability were developed and applied to community-living persons aged 65 and older who were at increased risk of death or decline. QIs were implemented using medical records and patient interviews.\n    \n\n\n          Setting:\n        \n      \n      Northeastern and southwestern United States.\n    \n\n\n          Participants:\n        \n      \n      Three hundred seventy-two vulnerable elders enrolled in two senior managed care plans.\n    \n\n\n          Measurements:\n        \n      \n      Percentage of QIs satisfied concerning falls or mobility disorders.\n    \n\n\n          Results:\n        \n      \n      Of the 372 consenting vulnerable elders with complete medical records, 57 had documentation of 69 episodes of two or more falls or fall with injury during the 13-month study period (14% of patients fell per year, 18% incidence). Double this frequency was reported at interview. An additional 22 patients had documented mobility problems. Clinical history of fall circumstances, comorbidity, medications, and mobility was documented from 47% of fallers and two or more of these four elements from 85%. Documented physical examination was less complete, with only 6% of fallers examined for orthostatic blood pressure, 7% for gait or balance, 25% for vision, and 28% for neurological findings. The evaluation led to specific recommendations in only 26% of cases, but when present they usually led to appropriate treatment modalities. Mobility problems without falls were evaluated with gait or balance examination in 23% of cases and neurological examination in 55%.\n    \n\n\n          Conclusion:\n        \n      \n      Community physicians appear to underdetect falls and gait disorders. Detected falls often receive inadequate evaluation, leading to a paucity of recommendations and treatments. Adhering to guidelines may improve outcomes in community-dwelling older adults."
        },
        {
            "title": "Right Atrial Phasic Function in Heart Failure With Preserved and Reduced Ejection Fraction.",
            "abstract": "Objectives:\n        \n      \n      This study researched right atrial (RA) deformation indexes and their association with all-cause mortality among subjects with or without heart failure (HF).\n    \n\n\n          Background:\n        \n      \n      Although left atrial dysfunction is well described in HF, patterns of RA dysfunction and their prognostic implications are unclear. Cardiac magnetic resonance (CMR) imaging can provide excellent visualization of the RA. We used CMR to characterize RA phasic function in HF and to assess its prognostic implications.\n    \n\n\n          Methods:\n        \n      \n      This study prospectively examined 608 adults without HF (n = 407), as well as adults with HF with a reduced ejection fraction (HFrEF) (n = 105) or with HF with a preserved ejection fraction (HFpEF) (n = 96). Phasic RA function was measured via volume measurements and feature-tracking methods to derive longitudinal strain. All-cause death was ascertained over a median follow-up of 38.9 months. Standardized hazard ratios (HRs) were computed via Cox regression.\n    \n\n\n          Results:\n        \n      \n      Measures of RA phasic function were more prominently impaired in subjects with HFrEF than those in subjects with HFpEF. In analyses that adjusted for demographic factors, HF status, left ventricular ejection fraction, right ventricular end-diastolic volume index, and right ventricular ejection fraction, RA reservoir strain (HR: 0.66; 95% confidence interval [CI]: 0.47 to 0.92; p = 0.0154), RA expansion index (HR: 0.53; 95% CI: 0.31 to 0.91; p = 0.0116), RA conduit strain (HR: 0.58; 95% CI: 0.40 to 0.84; p = 0.0039), and RA conduit strain rate (HR: 1.51; 95% CI: 1.02 to 2.220; p = 0.0373) independently predicted all-cause mortality. In contrast, RA booster pump function and RA volume index did not independently predict the risk of death.\n    \n\n\n          Conclusions:\n        \n      \n      Phasic RA function is predictive of the risk of all-cause death in a diverse group of subjects with and without HF. RA conduit and reservoir function are independent predictors of mortality."
        },
        {
            "title": "Ten-year follow-up of fellow eyes of patients enrolled in Collaborative Ocular Melanoma Study randomized trials: COMS report no. 22.",
            "abstract": "Purpose:\n        \n      \n      To report findings observed in fellow eyes during prospective follow-up of patients with unilateral choroidal melanoma after treatment with standard enucleation or 1 of 2 radiotherapy methods, either iodine 125 (I(125)) brachytherapy or pre-enucleation external radiation, in order to document long-term outcomes and to identify any adverse effect of radiotherapy on the contralateral eye.\n    \n\n\n          Design:\n        \n      \n      Two multicenter randomized trials conducted by the Collaborative Ocular Melanoma Study (COMS) Group.\n    \n\n\n          Participants:\n        \n      \n      Eligible patients assigned randomly to standard enucleation or to the radiotherapy protocol adopted for tumors of the specified size and location and treated as assigned: 994 patients of 1003 enrolled in the COMS trial of pre-enucleation radiation and 1296 patients of 1317 enrolled in the COMS trial of I(125) brachytherapy.\n    \n\n\n          Outcomes:\n        \n      \n      Changes in best-corrected visual acuity (VA), intraocular pressure, and other findings in fellow eyes from baseline to examinations conducted at 6 and 12 months after enrollment and annually thereafter.\n    \n\n\n          Results:\n        \n      \n      Five years after enrollment, 1307 of 2290 fellow eyes were examined; 358 fellow eyes were examined 10 years after enrollment. Mean change in VA of fellow eyes from baseline to each examination was one letter (0.2 lines) or less. Cumulative 5-year incidence rates of cataract surgery and visually significant cataract in initially phakic eyes with good VA and no lenticular opacity were 8% in both trials; 10-year rates were 18% in the trial of pre-enucleation and 15% in the trial of I(125) brachytherapy. Intraocular pressures changed by less than 1 mmHg from baseline to each examination. Apart from lower rates of incident cataracts among fellow eyes of patients treated with pre-enucleation radiation, findings within each trial were similar in the 2 treatment arms.\n    \n\n\n          Conclusions:\n        \n      \n      Almost all surviving patients retained good VA in fellow eyes throughout 5 years of follow-up after treatment for choroidal melanoma. These findings persisted through 10 years of follow-up among patients eligible for examinations beyond 5 years. There was no evidence that fellow eyes of patients whose affected eye was treated with pre-enucleation radiation or with I(125) brachytherapy were at greater risk of loss of VA or new ophthalmic diagnoses than eyes of patients treated with enucleation alone."
        },
        {
            "title": "Noninvasive detection of fibrosis applying contrast-enhanced cardiac magnetic resonance in different forms of left ventricular hypertrophy relation to remodeling.",
            "abstract": "Objectives:\n        \n      \n      We aimed to evaluate the incidence and patterns of late gadolinium enhancement (LGE) in different forms of left ventricular hypertrophy (LVH) and to determine their relation to severity of left ventricular (LV) remodeling.\n    \n\n\n          Background:\n        \n      \n      Left ventricular hypertrophy is an independent predictor of cardiac mortality. The relationship between LVH and myocardial fibrosis as defined by LGE cardiovascular magnetic resonance (CMR) is not well understood.\n    \n\n\n          Methods:\n        \n      \n      A total of 440 patients with aortic stenosis (AS), arterial hypertension (AH), or hypertrophic cardiomyopathy (HCM) fulfilling echo criteria of LVH underwent CMR with assessment of LV size, weight, function, and LGE. Patients with increased left ventricular mass index (LVMI) resulting in global LVH in CMR were included in the study.\n    \n\n\n          Results:\n        \n      \n      Criteria were fulfilled by 83 patients (56 men, age 57 +/- 14 years; AS, n = 21; AH, n = 26; HCM, n = 36). Late gadolinium enhancement was present in all forms of LVH (AS: 62%, AH: 50%; HCM: 72%, p = NS) and was correlated with LVMI (r = 0.237, p = 0.045). There was no significant relationship between morphological obstruction and LGE. The AS subjects with LGE showed higher LV end-diastolic volumes than those without (1.0 +/- 0.2 ml/cm vs. 0.8 +/- 0.2 ml/cm, p < 0.015). Typical patterns of LGE were observed in HCM but not in AS and AH.\n    \n\n\n          Conclusions:\n        \n      \n      Fibrosis as detected by CMR is a frequent feature of LVH, regardless of its cause, and depends on the severity of LV remodeling. As LGE emerges as a useful tool for risk stratification also in nonischemic heart diseases, our findings have the potential to individualize treatment strategies."
        },
        {
            "title": "Vascular risk factors and rhegmatogenous retinal detachment: a follow-up of a national cohort of Swedish men.",
            "abstract": "Background:\n        \n      \n      We aimed to investigate the role of vascular risk factors in the genesis of rhegmatogenous retinal detachment (RRD) using data from a large cohort of Swedish conscripts.\n    \n\n\n          Methods:\n        \n      \n      We used data from a nationwide cohort of 49 321 Swedish men born during 1949-1951, conscripted for compulsory military service in 1969-1970 with nearly complete follow-up to 2009. Information on surgically treated RRD between 1973 and 2009 was collected from the National Patient Register. We fitted Cox regression models stratified on myopia degree and including blood pressure levels, body mass index and cigarette smoking. Population attributable fractions of RRD were estimated through maximum likelihood methods.\n    \n\n\n          Results:\n        \n      \n      We observed 262 cases of RRD in 1 725 770 person-years. At multivariate analysis, the number of cigarettes per day showed a reverse association with the risk of RRD (p for trend 0.01). Conscripts with obesity presented a higher risk compared with normal subjects (adjusted HR 2.51, 95% CI 1.02 to 6.13). We found weak evidence of an association between blood pressure and RRD (HR for men with hypertension compared with normotension 1.41, 95% CI 0.93 to 2.13). All the observed associations were stronger when the analysis was restricted to non-myopic conscripts. In particular, the HR for hypertension was 2.33 (95% CI 1.30 to 4.19) compared with normotension. If this association is causal, we estimated that 42.0% of RRD cases (95% CI 11.5% to 62.0%) occurring among non-myopics are attributable to elevated blood pressure.\n    \n\n\n          Conclusions:\n        \n      \n      Vascular risk factors may be important determinants of RRD, particularly among non-myopics. Further investigations on the role of hypertension and obesity are needed."
        },
        {
            "title": "Are patients attending the smoking cessation clinic aware of the association between eye disease and smoking?",
            "abstract": "It is well established that smoking is related to cardiovascular, respiratory, ophthalmological, and other diseases. Over the years, anti-smoking campaigns have concentrated on heart and lung disease and overall mortality to motivate smokers to stop smoking. The aim of our questionnaire study is to assess the motivation for attending a smoking cessation clinic in a local district hospital in the Highlands, and the level of awareness of the association between smoking and eye disease."
        },
        {
            "title": "How community and healthcare provider perceptions, practices and experiences influence reporting, disclosure and data collection on stillbirth: Findings of a qualitative study in Afghanistan.",
            "abstract": "Quality concerns exist with stillbirth data from low- and middle-income countries including under-reporting and misclassification which affect the reliability of burden estimates. This is particularly problematic for household survey data. Disclosure and reporting of stillbirths are affected by the socio-cultural context in which they occur and societal perceptions around pregnancy loss. In this qualitative study, we aimed to understand how community and healthcare providers' perceptions and practices around stillbirth influence stillbirth data quality in Afghanistan. We collected data through 55 in-depth interviews with women and men that recently experienced a stillbirth, female elders, community health workers, healthcare providers, and government officials in Kabul province, Afghanistan between October-November 2017. The results showed that at the community-level, there was variation in local terminology and interpretation of stillbirth which did not align with the biomedical categories of stillbirth and miscarriage and could lead to misclassification. Specific birth attendant practices such as avoiding showing mothers their stillborn baby had implications for women's ability to recall skin appearance and determine stillbirth timing; however, parents who did see their baby, had a detailed recollection of these characteristics. Birth attendants also unintentionally misclassified birth outcomes. We found several practices that could potentially reduce under-reporting and misclassification of stillbirth; these included the cultural significance of ascertaining signs of life after birth (which meant families distinguished between stillbirths and early neonatal deaths); the perceived value and social recognition of a stillborn; and openness of families to disclose and discuss stillbirths. At the facility-level, we identified that healthcare provider's practices driven by institutional culture and demands, family pressure, and socio-cultural influences, could contribute to under-reporting or misclassification of stillbirths. Data collection methodologies need to take into consideration the socio-cultural context and investigate thoroughly how perceptions and practices might facilitate or impede stillbirth reporting in order to make progress on data quality improvements for stillbirth."
        },
        {
            "title": "Screening for colorectal cancer.",
            "abstract": "Considerable indirect evidence, based on the natural history of colorectal cancer and the ability of tests to detect adenomas and invasive cancers, suggests that screening for colorectal cancer reduces mortality. Without screening, a 50-year-old person at average risk has approximately a 530-in-10,000 chance of developing invasive colorectal cancer in the rest of his or her life and approximately a 250-in-10,000 chance of dying from it. Analysis of indirect evidence with a mathematic model indicates that screening persons for 25 years, from the age of 50 to the age of 75 years should reduce the chance of developing or dying from colorectal cancer by 10% to 75%, depending on which screening tests are used and how often screening is done. Screening for colorectal cancer is optional. A possible recommendation is that annual fecal occult blood tests and 65-cm flexible sigmoidoscopy every 3 to 5 years be done for average-risk men and women who are between 50 and 75 years of age. In addition to having annual fecal occult blood tests, persons with first-degree relatives with colorectal cancer can be offered barium enemas instead of sigmoidoscopies every 3 to 5 years."
        },
        {
            "title": "How many individuals must be screened to reduce oral cancer mortality rate in the Western context? A challenge.",
            "abstract": "Objective:\n        \n      \n      Controlling oral cancer (OC) through screening is appealing. Advantages of this are as follows: OC is often preceded by visible premalignant lesions, early-stage survival is threefold greater than late-stage survival, and visual screening is inexpensive. Disadvantages of this are as follows: high frequency of false positives, undemonstrated cost-effectiveness, and irregular screening attendance by high-risk individuals. Screening effectiveness in Western countries has not been proven, because of low OC prevalence, which disproportionally increases the number of individuals needed to screen (NNS) to decrease mortality. This study estimated the NNS to obtain an evident decrease in OC mortality rate in the UK.\n    \n\n\n          Methods:\n        \n      \n      Data gathered from reliable databanks were used. NNS to detect one case (NNScase ) was estimated using a Bayesian approach. NNS to prevent one death (NNSdeath ) was assessed multiplying NNScase by the number of cases that must be screen-detected to prevent one death. NNS to decrease mortality rate by 1% (NNSmortality ) was assessed multiplying NNSdeath by 1% of annual OC deaths.\n    \n\n\n          Results:\n        \n      \n      NNSmortality was overall 1 125 000 (95% confidence interval - 95CI, 690 000-1 870 000), males 551 000 (95CI, 337 000-916 000), and females 571 000 (95CI, 347 000-942 000).\n    \n\n\n          Conclusions:\n        \n      \n      An OC visual screening campaign capable of producing an evident decrease in mortality rate in the UK requires a large number of adults to be annually and regularly screened."
        },
        {
            "title": "Association between lens opacities and mortality in the Priverno Eye Study.",
            "abstract": "Background:\n        \n      \n      Lens opacities are associated with a higher risk of death, although there are some discrepancies regarding the specific types of cataract representing risk. The purpose of the present study was to further investigate the relationships between different types of lens opacity and patient survival.\n    \n\n\n          Methods:\n        \n      \n      In 1987, 860 residents of Priverno, Italy, aged 45-69 years underwent an ophthalmologic examination. Based on patient histories and the findings of the slit-lamp examination, each of the 860 patients was classified according to the type of opacity (pure cortical, pure nuclear, pure posterior subcapsular, mixed, and surgical aphakia). The survivors of the original cohort were re-examined in 1994. Death and survival rates were computed by the Kaplan-Meier method. Associations between mortality and significant factors were included in a stepwise Cox proportional-hazards regression model.\n    \n\n\n          Results:\n        \n      \n      Forty-four members of the original cohort had died during the 7-year follow-up. Age-adjusted survival curves based on Kaplan-Meier estimates showed significantly lower survival in those whose baseline examinations had revealed pure nuclear opacity (log rank test: P=0.020) and aphakia (log rank test: P<0.001). When adjusted for other mortality risk factors (age, sex, diabetes, cardiovascular diseases), the hazard ratio was 4.32 for pure nuclear opacity (95% CI 1.13-16.4) and 18.3 for aphakia (95% CI 3.21-104.0).\n    \n\n\n          Conclusions:\n        \n      \n      The analysis of the Priverno data seems to confirm an association between lower survival and cataracts, particularly those confined to the lens nucleus and those that had already prompted surgery."
        },
        {
            "title": "Increased risk of a cancer diagnosis after herpes zoster ophthalmicus: a nationwide population-based study.",
            "abstract": "Purpose:\n        \n      \n      Herpes zoster has been associated with immune suppression, as has an increased risk of cancer. This population-based follow-up study aimed to investigate the risk of a subsequent cancer diagnosis after herpes zoster ophthalmicus (HZO).\n    \n\n\n          Design:\n        \n      \n      A retrospective cohort study.\n    \n\n\n          Participants and controls:\n        \n      \n      Retrospective claims data from the Taiwan National Health Insurance Research Database were analyzed. The study cohort comprised all patients with a diagnosis of HZO (International Classification of Diseases, 9th Revision, Clinical Modification code 053.2) in 2003 and 2004 (n=658). The comparison cohort consisted of randomly selected ambulatory care patients, 8 for every patient with HZO (n=5264) matched with the study group on age, gender, monthly income, and urbanization level of the patient's residence.\n    \n\n\n          Methods:\n        \n      \n      The Kaplan-Meier method was used to compute 1-year cancer-free survival rate. Stratified Cox proportional hazard regressions were carried out to compute the adjusted 1-year cancer-free survival rate after adjusting for potential confounding factors.\n    \n\n\n          Main outcome measures:\n        \n      \n      Subsequent claims for all study and comparison patients were captured over a 1-year follow-up period from their index ambulatory care visit to identify whether the patient received a cancer diagnosis during the follow-up period.\n    \n\n\n          Results:\n        \n      \n      During 1-year follow-up, cancer was diagnosed in 4.86% of patients with HZO and 0.53% of patients in the comparison cohort. Patients with HZO had significantly lower 1-year cancer-free survival rates than the comparison cohort. After adjusting for patient age, gender, monthly income, and urbanization level, patients with HZO were found to have a 9.25-fold (95% confidence interval, 5.51-15.55) risk of a subsequent cancer diagnosis than the matched comparison cohort. No significant differences in cancer type were observed between the 2 cohorts.\n    \n\n\n          Conclusions:\n        \n      \n      Herpes zoster ophthalmicus may be a marker of increased risk of being diagnosed with cancer in the following year.\n    \n\n\n          Financial disclosure(s):\n        \n      \n      The author(s) have no proprietary or commercial interest in any materials discussed in this article."
        },
        {
            "title": "Early perfusion changes within 1 week of systemic treatment measured by dynamic contrast-enhanced MRI may predict survival in patients with advanced hepatocellular carcinoma.",
            "abstract": "Objectives:\n        \n      \n      To correlate early changes in the parameters of dynamic contrast-enhanced magnetic resonance imaging (DCE-MRI) within 1 week of systemic therapy with overall survival (OS) in patients with advanced hepatocellular carcinoma (HCC).\n    \n\n\n          Methods:\n        \n      \n      Eighty-nine patients with advanced HCC underwent DCE-MRI before and within 1 week following systemic therapy. The relative changes of six DCE-MRI parameters (Peak, Slope, AUC, Ktrans, Kep and Ve) of the tumours were correlated with OS using the Kaplan-Meier model and the double-sided log-rank test.\n    \n\n\n          Results:\n        \n      \n      All patients died and the median survival was 174 days. Among the six DCE-MRI parameters, reductions in Peak, AUC, and Ktrans, were significantly correlated with one another. In addition, patients with a high Peak reduction following treatment had longer OS (P = 0.023) compared with those with a low Peak reduction. In multivariate analysis, a high Peak reduction was an independent favourable prognostic factor in all patients [hazard ratio (HR), 0.622; P = 0.038] after controlling for age, sex, treatment methods, tumour size and stage, and Eastern Cooperative Oncology Group performance status.\n    \n\n\n          Conclusions:\n        \n      \n      Early perfusion changes within 1 week following systemic therapy measured by DCE-MRI may aid in the prediction of the clinical outcome in patients with advanced HCC.\n    \n\n\n          Key points:\n        \n      \n      • DCE-MRI is helpful to evaluate perfusion changes of HCC after systemic treatment. • Early perfusion changes within 1 week after treatment may predict overall survival. • High Peak reduction was an independent favourable prognostic factor after systemic treatment."
        },
        {
            "title": "Prospective, randomized outcome study of endoscopy versus modified barium swallow in patients with dysphagia.",
            "abstract": "Objective:\n        \n      \n      Aspiration pneumonia is a significant cause of morbidity and mortality in both acute and long-term care settings While there are many reasons for patients to develop aspiration pneumonia, there exists a strong association between difficulty swallowing, or dysphagia, and the development of aspiration pneumonia The modified barium swallow test (MBS) and endoscopic evaluations of swallowing are considered to be the most comprehensive tests used to evaluate and manage patients with dysphagia in an effort to reduce the incidence of pneumonia. The purpose of this study was to provide an initial investigation of whether flexible endoscopic evaluation of swallowing with sensory testing (FEESST) or MBS is superior as the diagnostic test for evaluating and guiding the behavioral and dietary management of outpatients with dysphagia. FEESST combines the standard endoscopic evaluation of swallowing with a technique that determines laryngopharyngeal sensory discrimination thresholds by endoscopically delivering air pulse stimuli to the mucosa innervated by the superior laryngeal nerve.\n    \n\n\n          Study design:\n        \n      \n      Randomized, prospective cohort outcome study in a hospital-based outpatient setting.\n    \n\n\n          Methods:\n        \n      \n      One hundred twenty-six outpatients with dysphagia were randomly assigned to either FEESST or MBS as the diagnostic test used to guide dietary and behavioral management (postural changes, small bites and sips, throat clearing). The outcome variables were pneumonia incidence and pneumonia-free interval. The patients were enrolled for 1 year and followed for 1 year.\n    \n\n\n          Results:\n        \n      \n      Seventy-eight MBS examinations were performed in 76 patients with 14 patients (18.41%) developing pneumonia; 61 FEESST examinations were performed in 50 patients with 6 patients (12.0%) developing pneumonia These differences were not statistically significant (chi2 = 0.93, P = .33). In the MBS group the median pneumonia-free interval was 47 days; in the FEESST group the median pneumonia-free interval was 39 days Based on Wilcoxon's signed-rank test, this difference was not statistically significant (z = 0.04, P = .96).\n    \n\n\n          Conclusion:\n        \n      \n      Whether dysphagic outpatients have their dietary and behavioral management guided by the results of MBS or of FEESST, their outcomes with respect to pneumonia incidence and pneumonia-free interval are essentially the same."
        },
        {
            "title": "Ruthenium brachytherapy for uveal melanoma, 1979-2003: survival and functional outcomes in the Swedish population.",
            "abstract": "Purpose:\n        \n      \n      To evaluate observed and relative survival rates, enucleation rates, and visual outcome after ruthenium 106 brachytherapy for uveal melanoma.\n    \n\n\n          Design:\n        \n      \n      Retrospective cases series from the Swedish national referral center.\n    \n\n\n          Participants:\n        \n      \n      Five hundred seventy-nine patients (579 eyes) with choroidal or ciliary body melanomas, including 55 tumors more than 7 mm in height, treated with ruthenium episcleral plaques from January, 1979, through April, 2003.\n    \n\n\n          Methods:\n        \n      \n      Clinical and radiotherapy data were extracted from a dedicated database, and survival status was determined through population registries. Tumor size was classified according to the Collaborative Ocular Melanoma Study criteria. The 5- and 10-year relative survival rates were estimated, and univariate and multivariate regression models were constructed for predictive factors on observed survival, enucleation, and visual deterioration.\n    \n\n\n          Main outcome measures:\n        \n      \n      Observed and relative survival rate, proportion of secondary enucleation, deterioration of visual acuity to less than 0.5, respectively, to 0.1 or worse.\n    \n\n\n          Results:\n        \n      \n      Tumors were classified as small in 10.5%, medium in 78.4%, and large in 9.2% of patients. The 5- and 10-year observed overall survival rates were 83.3% and 71.5%, respectively, and the corresponding relative rates were 95.5% and 94%, respectively. Factors predicting survival were tumor diameter, patient age, and secondary enucleation. One hundred six patients (18%) underwent enucleation up to 14 years after plaque treatment. The only predictive factor for enucleation was tumor size. At 5 years, 31% of the patients retained 0.5 visual acuity or better, and 49% retained better than 0.1 visual acuity. Predictive factors for visual deterioration were visual acuity and distance from posterior tumor border to the foveola.\n    \n\n\n          Conclusions:\n        \n      \n      After ruthenium brachytherapy for uveal melanoma, the survival rates and visual outcomes in this population-based investigation were similar to previously published results. The eye was retained in 81.7% of patients. Careful patient selection (presently we only treat melanomas 7 mm or smaller in height) and life-long monitoring for recurrences is warranted."
        },
        {
            "title": "Lovastatin 5-year safety and efficacy study. Lovastatin Study Groups I through IV.",
            "abstract": "Background:\n        \n      \n      Inhibitors of hydroxymethylglutaryl co-enzyme A reductase are widely used to treat hypercholesterolemia. They have a good short- to medium-term safety profile, but long-term safety data are limited.\n    \n\n\n          Methods:\n        \n      \n      Seven hundred forty-five patients with severe hypercholesterolemia (mean baseline plasma cholesterol level on diet, 9.3 mmol/L [360 mg/dL]) were treated with lovastatin for a median duration of 5.2 years. Their mean age at baseline was 50 years, 68% were male, 60% had familial hypercholesterolemia, and 42% had a history of coronary heart disease. Seventy-seven percent of patients had titrations of lovastatin to 80 mg/d, and 58% took other lipid-lowering agents, usually bile acid sequestrants, concomitantly.\n    \n\n\n          Results:\n        \n      \n      The mean changes at 5 years in total, low-density lipoprotein, and high-density lipoprotein cholesterol were -35%, -44%, and +14%, respectively. Eighty percent of patients completed the study, 13% were unavailable for follow-up, 4% were discontinued due to adverse events unlikely to be related to lovastatin, and 3% (21) were discontinued because of drug-attributable adverse events: marked but asymptomatic increase in aminotransferase values (10 patients), gastrointestinal disturbance (three patients), rash (two patients), myalgia (one patient), myopathy (two patients), arthralgia (one patient), insomnia (one patient), and weight gain (one patient). Sixteen patients died during the study, all of coronary disease. Of these, 14 had coronary heart disease at baseline. There were no deaths attributable to trauma, suicide, or homicide, and there were only 14 cases of cancer (vs 21 expected). There was no evidence for an adverse effect on the lens.\n    \n\n\n          Conclusions:\n        \n      \n      Lovastatin is a generally well-tolerated and effective drug during long-term use."
        },
        {
            "title": "Impact of preprocedural high-sensitivity C-reactive protein on contrast-induced nephropathy in patients undergoing primary percutaneous coronary intervention.",
            "abstract": "We assessed the impact of preprocedural high-sensitivity C-reactive protein (hsCRP) on the incidence of contrast-induced nephropathy (CIN) in patients with ST-segment elevation myocardial infarction (STEMI) undergoing primary percutaneous coronary intervention (p-PCI). We retrospectively studied 1452 patients with STEMI undergoing p-PCI. Baseline clinical characteristics, CIN incidence, and other inhospital clinical outcomes were compared among hsCRP quartiles; 212 (14.6%) patients developed CIN. The overall inhospital mortality rate was 4.5% (65 patients). Univariate analysis revealed CIN incidence was significantly associated with hsCRP, with 7.44% for quartile Q1 (<3.00 mg/L), 12.6% for Q2 (3.00-5.90 mg/L), 16.9% for Q3 (5.91-11.4 mg/L), and 21.49% for Q4 (>11.4 mg/L; P < .001). Patients with a higher hsCRP experienced a higher rate of inhospital complications. After adjusting for potential confounders, hsCRP >6.50 mg/L was significantly associated with the occurrence of CIN. Preprocedural hsCRP levels are significantly related to the incidence of CIN in patients with STEMI undergoing p-PCI."
        },
        {
            "title": "[Patients' perceptions of and attitudes towards epilepsy surgery: mistaken concepts in Colombia].",
            "abstract": "Introduction:\n        \n      \n      Selected patients with drug-resistant focal epilepsy benefit from epilepsy surgery, however significant delays remain. The aim of this study was to assess knowledge and attitudes toward epilepsy surgery among patients with epilepsy and identify barriers that might delay the treatment.\n    \n\n\n          Patients and methods:\n        \n      \n      A 10-minute questionnaire was administered to patients with epilepsy in Colombia. Survey assessed the following: knowledge of surgical options, perceptions about the risks of surgery vs. ongoing seizures, disease disability, treatment goals, and demographic and socioeconomic variables.\n    \n\n\n          Results:\n        \n      \n      We recruited 88 patients with focal epilepsy. More than half of patients (56%) were not aware that surgery might be an option. Apprehension about epilepsy surgery was evident, 60% of patients perceived epilepsy surgery to be very or moderately dangerous. A large proportion of patients believe death (41%), stroke (47%), vision loss (56%), personality change (56%), paralysis (62%), difficulties in speaking (69%), and memory loss (60%) were frequent side effects. The majority of patients (62%) consider the surgical procedure as the last option of treatment.\n    \n\n\n          Conclusions:\n        \n      \n      There is a negative attitude toward epilepsy surgery based on the patients' misperceptions of suffering neurological deficits during the surgery, reflecting lack of knowledge toward this type of treatment. These perceptions can contribute to delays in surgical care."
        },
        {
            "title": "Cardiac magnetic resonance imaging as a prognostic tool in patients with nonischemic cardiomyopathy.",
            "abstract": "Technical advancements have enabled cardiac magnetic resonance (CMR) imaging to provide a noninvasive assessment of cardiomyopathy. Cardiac magnetic resonance imaging acts as the reference standard for quantifying left and right ventricular function. It also assesses the etiology of cardiomyopathy by demonstrating the presence and size of myocardial scar and by detecting myocardial inflammation and interstitial infiltration. Cardiomyopathy can result in early mortality and arrhythmic risk, and CMR imaging aids in risk stratification among this group of patients. Left ventricular ejection fraction predicts which patients will benefit most from implantable cardioverter-defibrillators (ICDs), but this is not a perfect marker of arrhythmic risk. The etiology of cardiomyopathy, as assessed with CMR imaging, adds further prognostic information with infiltrative cardiomyopathies, resulting in higher mortality than idiopathic cardiomyopathies. Among patients with nonischemic cardiomyopathy (NICM), the degree of fibrosis as determined by the CMR imaging sequence of late gadolinium enhancement (LGE) imaging offers further prognostic information. Late gadolinium enhancement imaging in patients with NICM portends an approximately 3- to 8-fold greater risk of death or hospitalization than NICM without LGE imaging. Further research is needed to determine if the presence of LGE will be helpful in predicting which patients may benefit from ICD implantation."
        },
        {
            "title": "Peroperative Intravascular Ultrasound for Endovascular Aneurysm Repair versus Peroperative Angiography: A Pilot Study in Fit Patients with Favorable Anatomy.",
            "abstract": "Background:\n        \n      \n      The aim of this study was to compare intravascular ultrasound (IVUS) assistance for endovascular aortic aneurysm repair (EVAR) to standard assistance by angiography.\n    \n\n\n          Methods:\n        \n      \n      From June 2015 to June 2017, 173 consecutive patients underwent EVAR. In this group, 69 procedures were IVUS-assisted with X-ray exposure limited to completion angiography for safety purposes because an IVUS probe does not yet incorporate a duplex probe (group A), and 104 were angiography-assisted procedures (group B). All IVUS-assisted procedures were performed by vascular surgeons with basic duplex ultrasound (DUS) training. The primary study endpoints were mean radiation dose, duration of fluoroscopy, amount of contrast media administered, procedure-related outcomes, and renal clearance expressed as the glomerular filtration rate (GFR) before and after the procedure. Secondary endpoints were operative mortality, morbidity, and arterial access complications.\n    \n\n\n          Results:\n        \n      \n      Mean duration of fluoroscopy time was significantly lower for IVUS-assisted procedures (24 ± 15 min vs. 40 ± 30 min for angiography-assisted procedures, P < 0.01). Moreover, mean radiation dose (Air KERMA) was significantly lower in IVUS-assisted procedures (76m Gy [44-102] vs. 131 mGy [58-494]), P < 0.01. IVUS-assisted procedures required fewer contrast media than standard angiography-assisted procedures (60 ± 20 mL vs. 120 ± 40 mL, P < 0.01). The mean duration of the procedure was comparable in the two groups (120 ± 30 min vs. 140 ± 30 min, P = 0.07). No difference in renal clearance before and after the procedure was observed in either of the two groups (99.0 ± 4/97.8 ± 2 mL/min in group A and 98.0 ± 3/97.6 ± 5 mL/min in group B) (P = 0.28). The mean length of follow-up was nine months (6-30 months). No postoperative mortality, morbidity, or arterial access complications occurred. No type 1 endoleak was observed. Early type II endoleaks were observed in 21 patients (11%), 12 in the angiography-assisted group (11%) and nine in the IVUS-assisted group (12%). They were not associated with sac enlargement ≥5 mm diameter and therefore did not require any additional treatment.\n    \n\n\n          Conclusions:\n        \n      \n      Compared with standard angiography-assisted EVAR, IVUS significantly reduces renal load with contrast media, fluoroscopy time, and radiation dose while preserving endograft deployment efficiency. Confirmation from a large prospective study with improved IVUS probes will be required before IVUS-assisted EVAR alone can become standard practice."
        },
        {
            "title": "The utility of the macro-aggregated albumin lung perfusion scan in the diagnosis and prognosis of hepatopulmonary syndrome in cirrhotic patients candidates for liver transplantation.",
            "abstract": "Background:\n        \n      \n      The macro-aggregated albumin lung perfusion scan (99mTc-MAA) is a diagnostic method for hepatopulmonary syndrome (HPS).\n    \n\n\n          Goal:\n        \n      \n      To determine the sensitivity of 99mTc-MAA in diagnosing HPS, to establish the utility of 99mTc-MAA in determining the influence of HPS on hypoxemia in patients with concomitant pulmonary disease and to determine the correlation between 99mTc-MAA values and other respiratory parameters.\n    \n\n\n          Methods:\n        \n      \n      Data from 115 cirrhotic patients who were eligible for liver transplantation (LT) were prospectively analyzed. A transthoracic contrast echocardiography and 99mTc-MAA were performed in 85 patients, and 74 patients were diagnosed with HPS.\n    \n\n\n          Results:\n        \n      \n      The overall sensitivity of 99mTc-MAA for the diagnosis of HPS was 18.9% (14/74) in all of the HPS cases and 66.7% (4/6) in the severe to very severe cases. In HPS patients who did not have lung disease, the degree of brain uptake of 99mTc-MAA was correlated with the alveolar-arterial oxygen gradient (A-a PO2) (r = 0.32, p < 0.05) and estimated oxygen shunt (r = 0.41, p < 0.05) and inversely correlated with partial pressure of arterial oxygen (PaO2) while breathing 100% O2 (r = -0.43, p < 0.05). The 99mTc-MAA was positive in 20.6% (7/36) of the patients with HPS and lung disease. The brain uptake of 99mTc-MAA was not associated with mortality and normalized in all cases six months after LT.\n    \n\n\n          Conclusions:\n        \n      \n      The 99mTc-MAA is a low sensitivity test for the diagnosis of HPS that can be useful in patients who have concomitant lung disease and in severe to very severe cases of HPS. It was not related to mortality, and brain uptake normalized after LT."
        },
        {
            "title": "The SECURE study: long-term safety of ranibizumab 0.5 mg in neovascular age-related macular degeneration.",
            "abstract": "Objective:\n        \n      \n      To evaluate long-term safety of intravitreal ranibizumab 0.5-mg injections in neovascular age-related macular degeneration (nAMD).\n    \n\n\n          Design:\n        \n      \n      Twenty-four-month, open-label, multicenter, phase IV extension study.\n    \n\n\n          Participants:\n        \n      \n      Two hundred thirty-four patients previously treated with ranibizumab for 12 months in the EXCITE/SUSTAIN study.\n    \n\n\n          Methods:\n        \n      \n      Ranibizumab 0.5 mg administered at the investigator's discretion as per the European summary of product characteristics 2007 (SmPC, i.e., ranibizumab was administered if a patient experienced a best-corrected visual acuity [BCVA] loss of >5 Early Treatment Diabetic Retinopathy Study letters measured against the highest visual acuity [VA] value obtained in SECURE or previous studies [EXCITE and SUSTAIN], attributable to the presence or progression of active nAMD in the investigator's opinion).\n    \n\n\n          Main outcome measures:\n        \n      \n      Incidence of ocular or nonocular adverse events (AEs) and serious AEs, mean change in BCVA from baseline over time, and the number of injections.\n    \n\n\n          Results:\n        \n      \n      Of 234 enrolled patients, 210 (89.7%) completed the study. Patients received 6.1 (mean) ranibizumab injections over 24 months. Approximately 42% of patients had 7 or more visits at which ranibizumab was not administered, although they had experienced a VA loss of more than 5 letters, indicating either an undertreatment or that factors other than VA loss were considered for retreatment decision by the investigator. The most frequent ocular AEs (study eye) were retinal hemorrhage (12.8%; 1 event related to study drug), cataract (11.5%; 1 event related to treatment procedure), and increased intraocular pressure (6.4%; 1 event related to study drug). Cataract reported as serious due to hospitalization for cataract surgery occurred in 2.6% of patients; none was suspected to be related to study drug or procedure. Main nonocular AEs were hypertension and nasopharyngitis (9.0% each). Arterial thromboembolic events were reported in 5.6% of the patients. Five (2.1%) deaths occurred during the study, none related to the study drug or procedure. At month 24, mean BCVA declined by 4.3 letters from the SECURE baseline.\n    \n\n\n          Conclusions:\n        \n      \n      The SECURE study showed that ranibizumab administered as per a VA-guided flexible dosing regimen recommended in the European ranibizumab SmPC at the investigator's discretion was well tolerated over 2 years. No new safety signals were identified in patients who received ranibizumab for a total of 3 years. On average, patients lost BCVA from the SECURE study baseline, which may be the result of disease progression or possible undertreatment.\n    \n\n\n          Financial disclosure(s):\n        \n      \n      Proprietary or commercial disclosure may be found after the references."
        },
        {
            "title": "Influence of mammographic parenchymal pattern in screening-detected and interval invasive breast cancers on pathologic features, mammographic features, and patient survival.",
            "abstract": "Objective:\n        \n      \n      The aim of our study was to assess the effect of mammographic parenchymal pattern on patient survival, mammographic features, and pathologic features of breast cancer in a screened population.\n    \n\n\n          Materials and methods:\n        \n      \n      We classified the parenchymal pattern (according to BI-RADS) of 759 screened women who presented with a screening-detected (n = 455) or interval (n = 304) invasive breast cancer. Pathologic details (tumor size, histologic grade, lymph node stage, vascular invasion, and histologic type) and mammographic appearances were recorded. Breast cancer-specific survival was ascertained, with a median follow-up of 9.0 years.\n    \n\n\n          Results:\n        \n      \n      An excess of interval cancers was seen in women with dense breasts (p < 0.0001). Screening-detected (but not interval) tumors were significantly smaller in fatty breasts (p = 0.014). Tumor grade, lymph node stage, vascular invasion, and histologic type did not vary significantly with mammographic parenchymal pattern in screening-detected or interval cancers. Screening-detected cancers in fatty breasts were more likely to appear as indistinct (p = 0.003) or spiculated (p = 0.002) masses in contrast to cancers in dense breasts, which more commonly appeared as architectural distortions (p < 0.0001). No significant breast cancer-specific survival difference was seen by mammographic parenchymal pattern for screening-detected cancers (p = 0.75), interval cancers (p = 0.82), or both groups combined (p = 0.12).\n    \n\n\n          Conclusion:\n        \n      \n      The prognosis of screened women presenting with breast cancer is unrelated to dense mammographic parenchymal pattern despite an excess of interval cancers and larger screening-detected tumors in this group. These data support the mammographic screening of women with dense parenchymal patterns."
        },
        {
            "title": "Why patients are afraid of opioid analgesics: a study on opioid perception in patients with chronic pain.",
            "abstract": "INTRODUCTION Opiophobia is deemed one of the key barriers in effective pain management. OBJECTIVES The study aimed to assess the overall perception of opioids in cancer patients treated for chronic pain, as well as to determine the nature of their most common related fears. PATIENTS AND METHODS The study included 100 palliative care patients who suffered from chronic cancer or noncancer pain. Initially, they had to complete a survey exploring their knowledge on analgesics and potential fear of using opioids. The second phase was based on in‑depth interviews with 10 palliative care patients suffering from cancer pain who were reluctant to use opioids. RESULTS Of the 100 patients, 43 expressed concerns over commencing the treatment with opioids. Fear was reported more often in patients already on strong opioids, who either overtly expressed it (group C) or not (group B), as compared with patients treated with weak opioids (group A) (50%, 48%, and 19% of groups C, B, and A, respectively). The main concerns were drug addiction, fear of death or dying, and undesirable side effects. A qualitative study revealed similar types of fear among patients expressing concerns prior to being put on strong opioids. CONCLUSIONS Opiophobia seems to be common among palliative care patients (up to 50%) treated with strong opioids. They mainly fear drug addiction, undesirable effects, and death or dying. Better awareness of patients' preconceptions about opioids may become instrumental to alleviating their suffering through enhanced pain management."
        },
        {
            "title": "The health burden of diabetes for the elderly in four communities.",
            "abstract": "Although diabetes is a common health problem of the elderly, the impact of diabetes on health and functioning in older persons is not well established. The purpose of this analysis was to identify health conditions accompanying diabetes in four samples of community dwelling elderly people. The study samples consisted of 13,601 persons ages 65 or older who participated in the Established Populations for Epidemiologic Studies in the Elderly (EPESE). Extensive interviews were conducted in respondents' homes to obtain information on diabetes and other health conditions, health behaviors, use of health services, and demographic characteristics. A lifetime history of diabetes was reported by 14 percent of respondents. The prevalence of the disease was higher in blacks than whites, especially among women. Persons with diabetes were more likely to report myocardial infarction, stroke, vision problems, physical disability, incontinence, and nursing home stays than persons without diabetes, but the diabetics were less likely to consume alcohol or tobacco. Those with diabetes were only slightly heavier than those without diabetes at the time of the interview. However, body mass at age 50 was substantially greater among persons with diabetes. Associations between diabetes and other health conditions and behaviors were similar for whites and blacks. These results show that aged persons with diabetes experience substantial comorbidity, which has important ramifications for functioning and survival."
        },
        {
            "title": "[Impact of morbidity on the health of the Basque Country population 2002-2007: a comprehensive approach through health expectancies].",
            "abstract": "Background:\n        \n      \n      The estimation of the impact of morbidity on health is essential to health planning. The objective was to estimate this impact using disability free life expectancy, and to analyze whether the hypothetical elimination of various diseases would have led to a compression or expansion of morbidity.\n    \n\n\n          Methods:\n        \n      \n      Cross-sectional study on the population of the Basque Country. Data on mortality (2002-2006), health survey data (2007) and population based data were used. The impact of different groups of diseases on mortality rates, years of life and potential years of life lost (PYLL) and disability (absolute number and rates) were calculated. An integrated analysis was also done, using disability free life expectancy (DFLE), using the Sullivan method.\n    \n\n\n          Results:\n        \n      \n      The diseases causing the greatest impact on mortality were tumours among men (35,2% and 39,3% of deaths and PYLL respectively), and circulatory diseases (34,5% of deaths) and tumours (43,6% of PYLL) among women. Osteomuscular diseases had a major impact on disability, causing the 26,6% and the 45,2% of the total cases in men and women). Circulatory diseases had the highest impact as a whole (4.2 years of DFLE in men and 3.8 in women). However, osteomuscular diseases had the highest influence on years of life with disability.\n    \n\n\n          Conclusions:\n        \n      \n      The diseases which caused the overall greatest impact on mortality and disability were circulatory system related ones, tumors, and osteomuscular diseases. The elimination of this last group of diseases would have led to a morbidity compression, meaning the greatest reduction in life years with disability among all the causes."
        },
        {
            "title": "Community Health Centers: A Model for Integrating Eye Care Services with the Practice of Primary Care Medicine.",
            "abstract": "Optometry is desperately needed to combat the increasing rate of avoidable visual impairment that goes undiagnosed largely owing to the lack of integration of eye care services with primary care medicine. Government leaders are actively discussing substantive changes to health care legislation that will impact optometrists and their patients. The importance of a regular eye examination for disease prevention has long been undervalued in the setting of primary care. Consequently, many serious and potentially treatable ocular and systemic diseases go undiagnosed. Despite clear indicators that vision impairment increases the risk of morbidity and mortality from chronic systemic disease and decreases quality of life, vision health remains among the greatest unmet health care needs in the United States. To improve vision care services for all Americans, we must focus our attention on two central themes. First, we must educate the public, health care professionals, and policymakers on the importance of routine eye care as a preventive measure in the setting of primary care. Next, we need to recognize that optometrists, through their geographic distribution and advanced training, are in a strategic position to deliver integrated, comprehensive, cost-effective eye care services for individuals most in need. In this perspective, we discuss a model for integrating optometric services with the practice of primary care medicine to facilitate early detection of both eye and systemic disease while reducing serious and preventable health-related consequences."
        },
        {
            "title": "Ethmoid and upper nasal cavity carcinoma: treatment, results and complications.",
            "abstract": "From 1970 to 1985, 45 patients with carcinoma of the upper nasal cavity and ethmoid sinuses were radically treated. The tumor parameters, treatment strategy, radiotherapy technique and the results of treatment of these patients are retrospectively analysed in this study with particular reference to complete blindness as a major complication. In most cases tumor was removed by meticulous surgical dissection, and thereafter quality-controlled radiation therapy (external therapy: mean dose 65 Gy in about 7 weeks, or external therapy with brachytherapy boost: mean dose 82 Gy in about 10 weeks) was used for this group of patients. The results show 68% recurrence-free survival at 5 years, adjusted for intercurrent deaths. Complications of treatment were seen in seven patients (16%) who developed eye damage with some loss of visual acuity, none however leading per se to complete blindness. It may be concluded that blindness may be avoided to a major extent while treating carcinomas in the nose-ethmoid areas without compromising loco-regional control."
        },
        {
            "title": "CT staging and monitoring of fibrotic interstitial lung diseases in clinical practice and treatment trials: a position paper from the Fleischner Society.",
            "abstract": "CT is increasingly being used to stage and quantify the extent of diffuse lung diseases both in clinical practice and in treatment trials. The role of CT in the assessment of patients entering treatment trials has greatly expanded as clinical researchers and pharmaceutical companies have focused their efforts on developing safe and effective drugs for interstitial lung diseases, particularly for idiopathic pulmonary fibrosis. These efforts have culminated in the simultaneous approval by the US Food and Drug Administration of two new drugs for the treatment of idiopathic pulmonary fibrosis. CT features are a key part of the inclusion criteria in many drug trials and CT is now being used to refine the type of patients enrolled. Interest in the potential use of serial CT as an effectiveness endpoint is increasing. For chronic progressive diseases, mortality may not be a feasible endpoint and many surrogate markers have been explored, ranging from pulmonary function decline to biomarkers. However, these surrogate markers are not entirely reliable and combinations of endpoints, including change in disease extent on CT, are being investigated. Methods to assess disease severity with CT range from simple visual estimates to sophisticated quantification by use of software. In this Position Paper, which cannot be regarded as a comprehensive set of guidelines in view of present knowledge, we examine the uses of serial CT in clinical practice and in drug trials and draw attention to uncertainties and challenges for future research."
        },
        {
            "title": "Psychiatric disorders in 36 families with Wolfram syndrome.",
            "abstract": "Objective:\n        \n      \n      The purpose of this study was to test the hypothesis that heterozygous carriers of the gene for the Wolfram syndrome, who constitute about 1% of the population, are predisposed to significant psychiatric illness. The Wolfram syndrome is an autosomal recessive neurodegenerative syndrome in which 25% of the individuals who are homozygous for the condition have severe psychiatric symptoms that lead to suicide attempts or psychiatric hospitalizations.\n    \n\n\n          Method:\n        \n      \n      The authors collected questionnaires, death certificates, and hospital records for blood relatives and their spouses in 36 families of individuals with the Wolfram syndrome and compared the proportion of blood relatives who had had psychiatric hospitalizations, had committed suicide, or had self-reported mental illness to the proportion of spouses with the same manifestations.\n    \n\n\n          Results:\n        \n      \n      The proportion of blood relatives who had had psychiatric hospitalizations, had committed suicide, or had self-reported mental illness significantly exceeded the proportion of spouses with the same manifestations.\n    \n\n\n          Conclusions:\n        \n      \n      Since heterozygous carriers of the gene for the Wolfram syndrome are 50-fold more common among the blood relatives than among the spouses, the larger proportion among blood relatives is evidence that heterozygous carriers of the gene for the Wolfram syndrome are predisposed to significant psychiatric illness."
        },
        {
            "title": "Cardiovascular magnetic resonance predictors of clinical outcome in patients with suspected acute myocarditis.",
            "abstract": "Background:\n        \n      \n      The natural history of acute myocarditis (AM) remains highly variable and predictors of outcome are largely unknown. The objectives were to determine the potential value of various cardiovascular magnetic resonance (CMR) parameters for the prediction of adverse long-term outcome in patients presenting with suspected AM.\n    \n\n\n          Methods:\n        \n      \n      In a single-centre longitudinal prospective study, 203 routine consecutive patients with an initial CMR-based diagnosis of AM (typical Late Gadolinium Enhancement, LGE) were followed over a mean period of 18.9 ± 8.2 months. Various CMR parameters were evaluated as potential predictors of outcome. The primary endpoint was defined as the occurrence of at least one of the combined Major Adverse Clinical Events (MACE) (cardiac death or aborted sudden cardiac death, cardiac transplantation, sustained documented ventricular tachycardia, heart failure, recurrence of acute myocarditis, and the need for hospitalization for cardiac causes).\n    \n\n\n          Results:\n        \n      \n      The vast majority of patients (N = 143,70 %) presented with chest pain, mild to moderate troponin elevation and ST-segment or T wave abnormalities. Various CMR parameters were evaluated on initial CMR performed 3 ± 2 days after acute clinical presentation (LV functional parameters, presence/extent of edema on T2 CMR, and extent of late gadolinium enhancement lesions). Out of the 203 patients, 22 experienced at least one major cardiovascular event (10.8 %) during follow-up for a total of 31 major cardiovascular events. Among all CMR parameters, the only independent CMR predictor of adverse clinical outcome by multivariate analysis was an initial alteration of LVEF (p = 0.04).\n    \n\n\n          Conclusions:\n        \n      \n      In routine consecutive patients without severe hemodynamic compromise and a CMR-based diagnosis of AM, various CMR parameters such as the presence and extent of myocardial edema and the extent of late gadolinium-enhanced LV myocardial lesions were not predictive of outcome. The only independent CMR predictor of adverse clinical outcome was an initial alteration of LVEF."
        },
        {
            "title": "Gout and the risk of age-related macular degeneration in the elderly.",
            "abstract": "Objective:\n        \n      \n      To assess whether gout is associated with incident age-related macular degeneration (AMD).\n    \n\n\n          Methods:\n        \n      \n      We used the 5% Medicare claims data from 2006-12 for all beneficiaries who were enrolled in Medicare fee-for-service (Parts A, B) and not enrolled in a Medicare Advantage Plan, and resided in the U.S. People were censored at the occurrence of new diagnosis of AMD, death or the end of study (12/31/2012), whichever occurred first. We used multivariable-adjusted Cox regression analyses to assess the association of gout with incident AMD, adjusted for demographics, comorbidity, and use of medications for cardiovascular disease and gout. Hazard ratios and 95% confidence intervals were calculated.\n    \n\n\n          Results:\n        \n      \n      In this observational cohort study, of the 1,684,314 eligible people, 116,097 developed incident AMD (6.9%). Incidence rates of AMD per 1,000 person-years were 20.1 for people with gout and 11.7 for people without gout. In multivariable-adjusted analyses, a diagnosis of gout was significantly associated with a higher risk of incident AMD with a hazard ratio of 1.39 (95% CI, 1.35, 1.43). This association was confirmed in sensitivity analyses that substituted Charlson-Romano comorbidity index continuous score with either a categorical Charlson-Romano comorbidity index score or individual Charlson-Romano index comorbidities plus hypertension, hyperlipidemia and coronary artery disease. Other covariates significantly associated with higher hazards of incident AMD were older age, female gender, White race/ethnicity, and higher Charlson-Romano comorbidity index score.\n    \n\n\n          Conclusions:\n        \n      \n      We noted a novel association of gout with AMD in the elderly. Future studies should investigate the pathways that mediate this association."
        },
        {
            "title": "Admission resistin levels predict peripancreatic necrosis and clinical severity in acute pancreatitis.",
            "abstract": "Objectives:\n        \n      \n      Peripancreatic necrosis determines clinical severity in acute pancreatitis. Early markers predicting peripancreatic necrosis and clinical severity are lacking. Because adipocytes of peripancreatic adipose tissue secret highly active adipocytokines, the aim of the study was to investigate whether adipocytokines are able to serve as early markers predicting peripancreatic necrosis and clinical severity.\n    \n\n\n          Methods:\n        \n      \n      A total of 50 patients (20 women, 30 men) with acute pancreatitis were included in this noninterventional, prospective, and monocentric cohort study on diagnostic accuracy. Clinical severity was classified by the Ranson score and the APACHE (Acute Physiology And Chronic Health Evaluation) II score. Pancreatic and peripancreatic necrosis were quantified by using the computed tomography-based Balthazar score, the Schroeder score, and the pancreatic necrosis score. Adiponectin, leptin, and resistin were measured at admission and daily for at least 10 days by enzyme-linked immunosorbent assay.\n    \n\n\n          Results:\n        \n      \n      In contrast to admission C-reactive protein values, admission resistin values were significantly correlated with clinical severity and even with clinical end points such as death and need for interventions. Admission resistin levels were significantly elevated in patients with higher pancreatic and extrapancreatic necrosis scores. It was shown by receiver-operator characteristics that admission resistin concentration provides a positive predictive value of 89% in predicting the extent of peripancreatic necrosis (area under the curve, 0.8; P=0.002; sensitivity, 80%; specificity, 70%) by using a cutoff value of 11.9 ng/ml.\n    \n\n\n          Conclusions:\n        \n      \n      Admission resistin concentration serves as an early predictive marker of peripancreatic necrosis and clinical severity in acute pancreatitis. Resistin may have potential for clinical use as a new and diagnostic serum marker."
        },
        {
            "title": "[Translation and cross-cultural adaptation of the Multiple Sclerosis Walking Scale (MSWS-12) into Brazilian Portuguese].",
            "abstract": "Poor walking performance is predictive of heart disease and osteoporosis and increases the risk of death in the elderly. Gait and vision have been identified as the most valuable physical functions according to multiple sclerosis patients' perceptions. The objective of this study was to perform a translation and cross-cultural adaptation of the Multiple Sclerosis Walking Scale (MSWS-12) into Brazilian Portuguese. A study of cross-cultural adaptation was conducted in ten steps. Participation in the study included four translators, two back-translators, twelve medical experts, twelve patients, twelve healthy subjects, and a Portuguese language expert. Only the question \"Did standing make it more difficult to do things?\" posed difficulty in the translation process. Maximum time for completion was less than three minutes (171 seconds). Internal consistency analyses showed high reliability (Cronbach's alpha = 0.94). The content validation and internal consistency stages were completed satisfactorily."
        },
        {
            "title": "Psychometric performance of the PAncreatic CAncer disease impact (PACADI) score.",
            "abstract": "Background/objective:\n        \n      \n      Pancreatic Cancer Disease Impact (PACADI) score measures the impact of pancreatic cancer (PC) on important health dimensions, selected by patients. The aim of this single center study was to test the psychometric performance of the Pancreatic Cancer Disease Impact (PACADI) score.\n    \n\n\n          Methods:\n        \n      \n      Patients with suspected pancreatic cancer (PC) completed PACADI, the EuroQol-5D (EQ-5D index) and Edmonton Symptom Assessment System (ESAS) in this longitudinal observational study. Measures were compared across patients with PC (n = 210), other malignant lesions (OML) (n = 109) and non-malignant lesions (NML) (n = 41). Associations, test-retest and internal consistency reliability, longitudinal changes, sensitivity to change and prediction of mortality during the first year were examined in patients with PC.\n    \n\n\n          Results:\n        \n      \n      The three measures discriminated between PC and OML. The PACADI score correlated strongly at baseline (n = 199)/after three months (n = 85) with the EQ-5D index and ESAS \"sense of well-being\" (0.64 and 0.66/0.73 and 0.69, p < 0.001, respectively), showed high test-retest reliability (ICC 0.84) and very good internal consistency reliability (Cronbach's alpha 0.81-0.85) across all visits. Scores improved over time at 3, 6, 9 and 12 months for survivors, and standardized response mean (SRM) for improvement between 2 and 3 months (n = 44) was 0.80 (PACADI), -0.59 (EQ-5D index) and 0.69 (ESAS \"sense of well-being\"). The PACADI score significantly predicted mortality within the first year (p = 0.02) in contrast to the EQ-5D index and ESAS \"sense of well-being\".\n    \n\n\n          Conclusion:\n        \n      \n      This study showed satisfactory psychometric performance of the PACADI score. The results support its use in clinical practice and intervention trials."
        },
        {
            "title": "Safety and complications of intravitreal injections performed in an Asian population in Singapore.",
            "abstract": "There has been a rapid rise in the use of intravitreal injections, such as anti-vascular endothelial growth factor (anti-VEGF) agents, performed over the past few years for the treatment of ocular neovascular diseases. This study aims to review the systemic and ocular adverse events among patients treated at a tertiary eye center over a period of 8 years. A retrospective review of all intravitreal injections of anti-VEGF performed over an 8-year period at a tertiary eye care center in Singapore was done. We report the frequency of systemic and ocular adverse events and compared it among the various anti-VEGF agents. A total of 14 001 intravitreal injections were performed on 2225 patients from January 1, 2007 to December 31, 2014, and this included 9992 bevacizumab (71.4 %), 3306 ranibizumab (23.6 %) and 703 aflibercept (5.0 %) injections. Systemic complications related to treatment were 26 (1.17 %) deaths (from any cause), of which 11 (0.49 %) were from fatal thromboembolic events, 7 (0.31 %) non-fatal thromboembolic events and two (0.09 %) serious non-ocular hemorrhage. Ocular complications included one (0.007 %) endophthalmitis, three (0.021 %) traumatic cataracts, and one (0.007 %) retinal detachment. Rates of death and thromboembolic events were similar among ranibizumab (lucentis), bevacizumab (avastin) and aflibercept (Eylea). The systemic and ocular complications associated with intravitreal injections among Asian patients at a tertiary eye center are relatively low and reflect the safety of the treatments."
        },
        {
            "title": "Influences of preclinical dementia and impending death on the magnitude of age-related cognitive deficits.",
            "abstract": "The authors examined the influence of preclinical dementia and impending death on the cross-sectional relationship between age and performance in tasks assessing episodic memory, visuospatial skill, and verbal fluency. Increasing age was associated with a general decrease in cognitive performance. In addition, those who were to be diagnosed with dementia or had died by a 3-year follow-up, were older, and performed at a lower level than the remaining sample across all cognitive tasks at baseline. Nevertheless, removal of the preclinical dementia and impending death groups from the original sample affected the cross-sectional age-cognition relations relatively little. This pattern of findings suggests that the biological aging process exerts negative influences on cognitive functioning beyond those resulting from disease and mortality."
        },
        {
            "title": "The multidisciplinary management of gastrointestinal cancer. Colorectal cancer screening.",
            "abstract": "Colorectal cancer is a worldwide problem having global increases in the number of cases and deaths because of the expanding and aging of the population in both developing and developed countries. Screening methods are available which can reduce the incidence by removal of adenomas and can reduce deaths in diagnosed cancer cases by earlier stage detection. Faecal occult blood testing has the strongest proof of effectiveness based on randomised control trials; sigmoidoscopy has lesser proof based on case control studies, and barium enema the weakest proof of effectiveness. Screening colonoscopy has not been subjected to a randomised trial but there is now considerable evidence of its performance and safety and it has the ability to screen, diagnose, and treat (polypectomy) in one test and it is becoming increasingly offered. Many guidelines are now in place, all with positive a position on the effectiveness of screening. However, screening rates are low and many barriers are present that need to be overcome in order to make a major global impact on colorectal cancer incidence and mortality."
        },
        {
            "title": "Loss and recovery of vision with suprasellar meningiomas.",
            "abstract": "Central visual acuity losses were documented in a group of 23 patients with surgically and histologically verified suprasellar meningiomas. The pattern demonstrated was that of acute, gradual or fluctuating loss in one eye, followed by later loss of central acuity in the other eye. Both optic nerves and chiasm were invariably involved either by stretching or compression. Neither preoperative field abnormalities nor central acuity deficits could be correlated with the anatomical location of the tumor, nor could postoperative changes in vision be correlated with tumor size. Lengthy duration of acuity loss and severe visual deficit did not preclude postoperative recovery of vision. Improvement in sight most frequently occurred within the first several weeks after operation, and further return of vision was not noted after 1 year."
        },
        {
            "title": "Severity of demographic and clinical characteristics, revascularization feasibility, major amputation, and mortality rate in diabetic patients admitted to a tertiary diabetic foot center for critical limb ischemia: comparison of 2 cohorts recruited at a 10-year distance.",
            "abstract": "Background:\n        \n      \n      To compare demographic and clinical characteristics, revascularization, major amputation, and mortality among patients admitted to a diabetic foot center because of critical limb ischemia (CLI) during 1999-2003 (cohort 1) and 2009 (cohort 2).\n    \n\n\n          Methods:\n        \n      \n      During 1999-2003, 564 diabetic patients with CLI (cohort 1) were admitted to our center, and 344 patients (360 affected limbs) were admitted during 2009 (cohort 2). Data on demographic and clinical characteristics, revascularization by peripheral angioplasty (PTA) or bypass graft (BPG), major amputation, and mortality were recorded.\n    \n\n\n          Results:\n        \n      \n      Patients belonging to cohort 2 were older than patients of cohort 1 (P = 0.001). In cohort 2, there were more subjects requiring insulin (P = 0.008) and duration of diabetes was longer (P = 0.001); moreover, there were more patients requiring dialysis (P = 0.001), patients with history of stroke (P = 0.004), or foot ulcer (P = 0.001). No significant difference between the 2 groups was found concerning gender, metabolic control, hypertension, lipid values, neuropathy, and retinopathy. Occlusion was more frequent than stenosis in the posterior tibial (P < 0.001) and peroneal (P = 0.016) arteries. However, the revascularization rate did not differ (P = 0.318) between the 2 groups. Restenosis after PTA was not significantly different (P = 0.627), whereas BPG failure was significantly more frequent (P = 0.010) in cohort 2 (2009). Major amputation (P = 0.222) and mortality rate (P = 0.727) did not differ between the 2 groups.\n    \n\n\n          Conclusions:\n        \n      \n      The severity of either foot lesions or patients comorbidities should be concomitantly assessed and taken into proper consideration when evaluating changes in the amputation rate among different studies or in different temporal settings."
        },
        {
            "title": "Iodine brachytherapy as an alternative to enucleation for large uveal melanomas.",
            "abstract": "Purpose:\n        \n      \n      To evaluate the safety and efficacy of iodine 125 plaque brachytherapy (IBT) for large uveal melanomas.\n    \n\n\n          Design:\n        \n      \n      Retrospective, nonrandomized comparative trial (historical control).\n    \n\n\n          Participants:\n        \n      \n      One hundred twenty-one consecutive patients with a large uveal melanoma according to the Collaborative Ocular Melanoma Study (COMS) criteria who attended a national ocular oncology service.\n    \n\n\n          Methods:\n        \n      \n      Ninety-seven patients (80%) underwent primary IBT (mean dose to tumor apex, 87 Gy) with noncollimated 20- to 25-mm plaques. Assessment of metastatic disease at death and visual outcome followed COMS guidelines. Time to low vision (20/70 or worse) and blindness (loss of 20/400 vision) in the study eye were modeled by Cox proportional hazards regression, based on both single- and repeated-failure data sets. Person-years of retained vision were calculated.\n    \n\n\n          Main outcome measures:\n        \n      \n      All-cause and melanoma-specific survival, local and distant recurrence, and preservation of vision and cosmesis.\n    \n\n\n          Results:\n        \n      \n      Median tumor height was 10.7 mm (range, 4.5-16.8 mm), and largest basal tumor diameter was 16.1 mm (range, 7.3-25.0 mm). The Kaplan-Meier estimate for all-cause and melanoma-specific survival was 62% (95% confidence interval [CI], 49%-72%) and 65% (95% CI, 52%-75%) at 5 years. The corresponding estimate for local tumor recurrence was 6% (95% CI, 2%-14%) and for major cosmetic abnormality was 38% (95% CI, 26%-52%). The median visual acuity in the study eye was 20/100 at baseline and 20/1600 at 2 years after treatment. The Kaplan-Meier estimate for avoiding low vision and blindness was 11% (95% CI, 4%-24%) and 26% (95% CI, 16%-37%) at 2 years, respectively. Tumor height and location entirely posterior to the ora serrata were the most robust predictors of visual loss. In this series, 49 person-years without low vision (median, 0.6 years; range, 0.04-8.2 years) and 111 person-years without blindness (median, 1.0 years; range, 0.03-8.6 years) in the treated eye were conserved.\n    \n\n\n          Conclusions:\n        \n      \n      Iodine 125 plaque brachytherapy seems to be a safe and effective alternative to enucleation with regard to survival and local tumor control. It provides a fair chance of preserving the eye with acceptable cosmesis and a reasonable chance of conserving useful vision for 1 to 2 years."
        },
        {
            "title": "Correction of visual impairment by cataract surgery and improved survival in older persons: the Blue Mountains Eye Study cohort.",
            "abstract": "Objective:\n        \n      \n      We assessed whether correction of visual impairment (VI) by cataract surgery was associated with improved long-term survival in an older Australian population.\n    \n\n\n          Design:\n        \n      \n      Population-based cohort study.\n    \n\n\n          Participants:\n        \n      \n      In the Blue Mountains Eye Study, 354 participants, aged ≥ 49 years, had both cataract and VI or had undergone cataract surgery before baseline examinations. They were subsequently examined after 5- and 10-year follow-ups.\n    \n\n\n          Methods:\n        \n      \n      Associations between the mortality risk and the surgical correction of VI (visual acuity [VA] <20/40, attributable to cataract) were assessed in Cox proportional hazard regression models, after multivariate adjustment, using time-dependent variables for the study factor.\n    \n\n\n          Main outcome measures:\n        \n      \n      All-cause mortality.\n    \n\n\n          Results:\n        \n      \n      The 15-year crude mortality of participants who had undergone cataract surgery at baseline with no subsequent VI (71.8%) was relatively similar to that in participants with cataract-related VI who had not yet undergone surgery (79.4%). However, after adjusting for age and sex, participants who underwent cataract surgery before baseline or during follow-up and no longer had VI had significantly lower long-term mortality risk (hazard ratio [HR], 0.60; 95% confidence interval [CI], 0.46-0.77) than participants with VI due to cataract who had not undergone cataract surgery. This lower mortality risk in the group with surgically corrected VI (HR, 0.54; 95% CI, 0.41-0.73) persisted after further adjustment for smoking, body mass index, home ownership, qualifications, poor self-rated health, the presence of poor mobility, hypertension, diabetes, self-reported history of angina, myocardial infarction, stroke, cancer, asthma, and arthritis. This finding remained significant (HR, 0.55; 95% CI, 0.41-0.73) after additional adjustment for the number of medications taken (continuous variable) and the number (≥ 5 vs. <5) of comorbid conditions (poor mobility, hypertension, diabetes, angina, myocardial infarction, stroke, cancer, asthma, or arthritis) as indicators of frailty.\n    \n\n\n          Conclusions:\n        \n      \n      Surgical correction of VI due to cataract was associated with significantly better long-term survival of older persons after accounting for known cataract and mortality risk factors, and indicators of general health. Whether some uncontrolled factors (frailty or general health) could have influenced decisions not to perform cataract surgery in some participants is unknown. However, this finding strongly supports many previous reports linking VI with poor survival.\n    \n\n\n          Financial disclosure(s):\n        \n      \n      Proprietary or commercial disclosure may be found after the references."
        },
        {
            "title": "Reported visual impairment and risk of suicide: the 1986-1996 national health interview surveys.",
            "abstract": "Objective:\n        \n      \n      To examine the relationship between reported visual impairment and suicide mortality.\n    \n\n\n          Methods:\n        \n      \n      From 1986 through 1996, annual cross-sectional multistage area probability surveys of the US civilian noninstitutionalized population living at addressed dwellings were conducted by the National Center for Health Statistics. We performed mortality linkage through 2002 with the National Death Index of 137,479 adults 18 years and older. The relationships between reported visual impairment and suicide were examined using structural equation modeling.\n    \n\n\n          Results:\n        \n      \n      The mean duration of follow-up was 11.0 years, and 200 suicide deaths were identified. After controlling for survey design, age, sex, race, marital status, number of nonocular health conditions, and self-rated health, the direct effect of visual impairment on death from suicide was elevated but not significant (hazard ratio, 1.50; 95% confidence interval, 0.90-2.49). The approximate indirect effect of visual impairment on death from suicide via poorer self-rated health (1.05; 1.02-1.08) or number of nonocular health conditions (1.12; 1.01-1.24) was significant. The total effect of visual impairment on death from suicide was elevated but not significant (1.64; 0.99-2.72).\n    \n\n\n          Conclusions:\n        \n      \n      Visual impairment may be associated with an increased risk of suicide through its effect on poor health. This suggests that improved treatment of visual impairment and factors causing poor health may potentially reduce suicide risk."
        },
        {
            "title": "Incidence and risk factors for perioperative visual loss after spinal fusion.",
            "abstract": "Background context:\n        \n      \n      Perioperative visual loss (POVL) is a rare but devastating complication that may follow spinal surgeries. The incidence of POVL after spinal fusion is not well characterized during the past decade.\n    \n\n\n          Purpose:\n        \n      \n      A population-based database was analyzed to characterize the incidence and risk factors for POVL associated with spinal fusion surgery on a national level.\n    \n\n\n          Study design:\n        \n      \n      This study consisted of a retrospective database analysis.\n    \n\n\n          Patient sample:\n        \n      \n      A total of 541,485 patients from the Nationwide Inpatient Sample (NIS) database were included in the study.\n    \n\n\n          Outcome measures:\n        \n      \n      Study outcome measures included incidence of POVL, length of stay (LOS), in-hospital costs, mortality, and POVL risk factors.\n    \n\n\n          Methods:\n        \n      \n      Data from the NIS were obtained from 2002 to 2009. Patients undergoing spinal fusion for degenerative pathologies were identified. Patient demographics, comorbidities, LOS, costs, and mortality were assessed. Statistical analyses were conducted using an independent t test for discrete variables and the chi-square test for categorical data. Binomial logistic regression was used to identify independent predictors of POVL. A p value of less than or equal to .001 was used to denote statistical significance. No funds were received by any of the authors for production of this study.\n    \n\n\n          Results:\n        \n      \n      A total of 541,485 spinal fusions were identified in the United States from 2002 to 2009. The overall incidence of POVL was 1.9 events per 10,000 cases. Of patients who had POVL, 56.2% underwent surgery for a diagnosis of spinal deformity. Patients with POVL were significantly younger on average compared with unaffected patients (37.6 years vs. 52.4 years; p<.001). Length of stay and hospital costs doubled for patients with POVL (p<.001). Logistic regression analysis demonstrated that independent predictors of visual loss were deformity surgery (odds ratio [OR]=6.1), diabetes mellitus with end organ damage (OR=13.1), and paralysis (OR=6.0, p<.001).\n    \n\n\n          Conclusions:\n        \n      \n      Our findings demonstrated an overall POVL incidence of 1.9 events per 10,000 spinal fusions. Patients undergoing thoracic fusion for deformity correction accounted for the majority of cases of POVL. Despite being a rare complication after spinal fusion, POVL is an adverse event that may not be entirely preventable. Patients undergoing long-segment fusions for deformity and those with certain risk factors should be counseled regarding the risks of POVL."
        },
        {
            "title": "Do U.S. states' socioeconomic and policy contexts shape adult disability?",
            "abstract": "Growing disparities in adult mortality across U.S. states point to the importance of assessing disparities in other domains of health. Here, we estimate state-level differences in disability, and draw on the WHO socio-ecological framework to assess the role of ecological factors in explaining these differences. Our study is based on data from 5.5 million adults aged 25-94 years in the 2010-2014 waves of the American Community Survey. Disability is defined as difficulty with mobility, independent living, self-care, vision, hearing, or cognition. We first provide estimates of age-standardized and age-specific disability prevalence by state. We then estimate multilevel models to assess how states' socioeconomic and policy contexts shape the probability of having a disability. Age-standardized disability prevalence differs markedly by state, from 12.9% in North Dakota and Minnesota to 23.5% in West Virginia. Disability was lower in states with stronger economic output, more income equality, longer histories of tax credits for low-income workers, and higher cigarette taxes (for middle-age women), net of individuals' socio-demographic characteristics. States' socioeconomic and policy contexts appear particularly important for older adults. Findings underscore the importance of socio-ecological influences on disability."
        },
        {
            "title": "Workplace Safety Interventions for Commercial Fishermen of the Gulf.",
            "abstract": "Commercial fishing continues to have one of the highest rates of occupational fatalities compared with other work sectors in the United States. Attitudes/beliefs among Vietnamese shrimp fishermen of the Gulf of Mexico may influence behaviors that are risk factors for fatal and nonfatal injuries. The study employs a community trial with quasi-experimental pretest/posttest intervention design. An advisory group made up of key stakeholders including representatives from the US Coast Guard was assembled. A survey was designed using the Theory of Planned Behavior as the theoretical framework. Three community groups at port sites along the Texas/Louisiana Gulf Coasts were identified. Focus groups were convened at each site to select priority areas for risk intervention using training and awareness measures. Initial and follow-up surveys were administered pre-/post-interventions for each of the three community groups (2008, n = 217 completed surveys; 2012, n = 206 completed surveys). The follow-up survey was condensed and \"intent to act\" questions were added for the priority concerns identified (noise-induced hearing loss, machinery/winches, and fatigue). Statistically significant changes (P ranging from .000 to .042) were observed in selective attitude/belief responses for hearing/noise and fatigue. Intent to action or to adopt the intervention was high among all three groups of shrimp fishermen (hearing conservation, 82.4%; machinery/winch safety, 94.6%; fatigue awareness, 95.3%). Simple, yet culturally appropriate training and awareness measures in the form of visual and written safety messages favorably influence attitudes, beliefs, and behavioral intent related to priority risk factors identified by Vietnamese commercial shrimp fishermen along the Texas and Louisiana Gulf Coasts."
        },
        {
            "title": "Measuring the impact of dysphonia on quality of life using health state preferences.",
            "abstract": "Objectives:\n        \n      \n      Formal evaluation of health states related to dysphonia have not been rigorously evaluated in affected patients. The objective of this project was to evaluate the health states of mild, moderate, and severe dysphonia using formal health state preference evaluation, and to compare these outcomes with the degree of voice handicap.\n    \n\n\n          Design:\n        \n      \n      Prospective health state preference assessment.\n    \n\n\n          Methods:\n        \n      \n      A convenience sample of patients presenting with voice complaints were enrolled from an academic voice center. Demographic and voice handicap index (VHI-10) data were obtained, and an assessment of preference for five health states (monocular blindness, binocular blindness, mild dysphonia, moderate dysphonia, and severe dysphonia) was performed. Utility scores were calculated on a scale from 0 (death) to 1 (perfect health). Analysis was performed with ANOVA testing with post-hoc comparisons and correlation statistics.\n    \n\n\n          Results:\n        \n      \n      Of 209 assessments, 149 (75.6%) met quality criteria. Relative to monocular blindness (score 0.61 [CI 0.57-0.64]), moderate dysphonia (0.58 [0.54-0.62]) was rated equivalently, with severe dysphonia (0.33 [0.29-0.37]) ranking significantly worse and mild dysphonia (0.96 [0.95-0.98]) significantly better. Binocular blindness (0.18 [0.15-0.21]) was the worst-ranked health state. There was a weak inverse correlation of VHI-10 with dysphonia-related preference scores; with worsening reported voice handicap, scores decreased.\n    \n\n\n          Conclusion:\n        \n      \n      This study demonstrated that dysphonia had a significant impact of quality of life, with moderate dysphonia ranking equivalently with monocular blindness. These numerical estimates may be used for ongoing research into the value and cost-effectiveness of medical, therapeutic, and surgical interventions for voice disorders.\n    \n\n\n          Level of evidence:\n        \n      \n      2c (outcomes research) Laryngoscope, 130:E177-E182, 2020."
        },
        {
            "title": "Self-reported short sleep duration among US adults by disability status and functional disability type: Results from the 2016 Behavioral Risk Factor Surveillance System.",
            "abstract": "Background:\n        \n      \n      Short sleep duration is associated with an increased risk of chronic disease and all-cause death. A better understanding of sleep disparities between people with and without disabilities can help inform interventions designed to improve sleep duration among people with disabilities.\n    \n\n\n          Objective:\n        \n      \n      To examine population-based prevalence estimates of short sleep duration by disability status and disability type among noninstitutionalized adults aged ≥18 years.\n    \n\n\n          Methods:\n        \n      \n      Data from the 2016 Behavioral Risk Factor Surveillance System were used to assess prevalence of short sleep duration among adults without and with disabilities (serious difficulty with cognition, hearing, mobility, or vision; any difficulty with self-care or independent living). Short sleep duration was defined as <7 h per 24-h period. We used log-binomial regression to estimate prevalence ratios (PRs) and 95% confidence intervals (CIs) while adjusting for socioeconomic and health-related characteristics.\n    \n\n\n          Results:\n        \n      \n      Adults with any disability had a higher prevalence of short sleep duration than those without disability (43.8% vs. 31.6%; p < .001). After controlling for selected covariates, short sleep was most prevalent among adults with multiple disabilities (PR 1.40, 95% CI: 1.36-1.43), followed by adults with a single disability type (range: PR 1.13, 95% CI: 1.03-1.24 [for independent living disability] to PR 1.25, 95% CI: 1.21-1.30 [for mobility disability]) compared to adults without disability.\n    \n\n\n          Conclusions:\n        \n      \n      People with disabilities had a higher likelihood of reporting short sleep duration than those without disabilities. Assessment of sleep duration may be an important component in the provision of medical care to people with disabilities."
        },
        {
            "title": "Superior interhemispheric approach for midline meningioma from the anterior cranial base.",
            "abstract": "Background:\n        \n      \n      For suprasellar meningioma, the fronto-basal exposure is considered the standard approach. The superior interhemispheric (IH) approach is less described in the literature.\n    \n\n\n          Objective:\n        \n      \n      To assess the surgical complications, functional outcome (visual, olfaction), morbidity and mortality rates and late recurrence, after resection by superior IH approach of midline skull base meningioma.\n    \n\n\n          Methods:\n        \n      \n      Between 1998 and 2008, 52 consecutive patients with midline meningioma on the anterior portion of the skull base (mean age: 63.8 ± 13.1; sex ratio F/M: 3.7) were operated on via the superior IH approach. After a mean follow-up of 56.9 ± 32.9 months, an independent neurosurgeon proposed a prospective examination of functional outcome to each patient, as well as a visual and olfactory function assessment.\n    \n\n\n          Results:\n        \n      \n      Fifty-two patients were divided into a group with olfactory groove meningioma (n=34) and another with tuberculum sellae meningioma (n=18). The outcome was characterized by postoperative complications in 13 patients (25%), mortality rate in two (3.8%) and long-term morbidity at in 17 (37%) of 50 surviving patients. Based on multivariate analysis, no prognosis factor was significant as regards the favorable outcome. The mean postoperative KPS score (86.6 ± 9.4) was significantly improved. However, dysexecutive syndrome was observed in four patients (8%), hyposmia-anosmia in 34 (68%) and visual acuity deteriorated in one (2%).\n    \n\n\n          Conclusion:\n        \n      \n      The superior IH approach could be considered a safe anteriorly orientated midline approach for removal OGM and TSM meningioma."
        },
        {
            "title": "Efficacy of myocardial contrast echocardiography in the diagnosis and risk stratification of acute coronary syndrome.",
            "abstract": "We examined the hypothesis that myocardial contrast echocardiography (MCE) is superior to conventional electrocardiographic, echocardiographic, and troponin I criteria for the diagnosis of acute coronary syndrome. We prospectively enrolled 114 consecutive patients (60+/-10 years of age, 73 men) who presented to the emergency room with chest pain on exertion and at rest. Exclusion criteria included an age<40 years, presence of Q wave or ST-segment elevation, and a poor echocardiographic window. Echocardiography and MCE were performed to assess regional wall motion abnormalities (RWMAs) and myocardial perfusion defects by using continuous infusion of perfluorocarbon-exposed sonicated dextrose albumin. Acute coronary syndrome was confirmed in 87 patients. There were no deaths; 46 patients had acute myocardial infarction, and 41 patients required urgent revascularization. On multiple logistic regression analysis, myocardial perfusion defect (odd ratio 87, p<0.001) was the only independent variable for diagnosing acute coronary syndrome. Myocardial perfusion defect (odd ratio 21, p=0.001) and troponin I levels (odd ratio 3, p=0.009) were independent predictors for acute myocardial infarction. The sensitivity of myocardial perfusion defect for diagnosing acute coronary syndrome was 77%, which is significantly higher than the sensitivities of ST change, troponin I increase, and RWMA (28%, 34%, and 49%, respectively), with similar specificities of 85% to 96%. In conclusion, MCE is more sensitive than the currently used electrocardiographic and troponin I criteria, and evaluation of myocardial perfusion defect by MCE complements RWMA analysis by conventional echocardiography for accurate diagnosis of acute coronary syndrome."
        },
        {
            "title": "Discriminant ability of the Eating Assessment Tool-10 to detect aspiration in individuals with amyotrophic lateral sclerosis.",
            "abstract": "Background:\n        \n      \n      Oropharyngeal dysphagia is prevalent in individuals with amyotrophic lateral sclerosis (ALS) leading to malnutrition, aspiration pneumonia, and death. These factors necessitate early detection of at-risk patients to prolong maintenance of safe oral intake and pulmonary function. This study aimed to evaluate the discriminant ability of the Eating Assessment Tool (EAT-10) to identify ALS patients with unsafe airway protection during swallowing.\n    \n\n\n          Methods:\n        \n      \n      Seventy ALS patients completed the EAT-10 survey and underwent a standardized videofluoroscopic evaluation of swallowing. Two blinded raters determined airway safety using the Penetration Aspiration Scale (PAS). A between groups anova (safe vs penetrators vs aspirators) was conducted and sensitivity, specificity, area under the curve (AUC), and likelihood ratios calculated.\n    \n\n\n          Key results:\n        \n      \n      Mean EAT-10 scores for safe swallowers, penetrators, and aspirators (SEM) were: 4.28 (0.79) vs 7.10 (1.79) vs 20.50 (3.19), respectively, with significant differences noted for aspirators vs safe swallowers and aspirators vs penetrators (p < 0.001). The EAT-10 demonstrated good discriminant ability to accurately identify ALS penetrator/aspirators (PAS ≥3) with a cut off score of 3 (AUC: 0.77, sensitivity: 88%, specificity: 57%). The EAT-10 demonstrated excellent accuracy at identifying aspirators (PAS ≥6) utilizing a cut off score of 8 (AUC: 0.88, sensitivity: 86%, specificity: 72%, likelihood ratio: 3.1, negative predictive value: 95.5%).\n    \n\n\n          Conclusions & inferences:\n        \n      \n      The EAT-10 differentiated safe vs unsafe swallowing in ALS patients. This patient self-report scale could represent a quick and meaningful aide to dysphagia screening in busy ALS clinics for the identification and referral of dysphagic patients for further instrumental evaluation."
        },
        {
            "title": "Patients With Diabetic Foot Disease Fear Major Lower-Extremity Amputation More Than Death.",
            "abstract": "Background:\n        \n      \n      The aim of this study was to identify the most-feared complications of diabetes mellitus (DM), comparing those with diabetic foot pathology with those without diabetic foot pathology.\n    \n\n\n          Methods:\n        \n      \n      We determined the frequency of patients ranking major lower-extremity amputation (LEA) as their greatest fear in comparison to blindness, death, diabetic foot infection (DFI), or end-stage renal disease (ESRD) requiring dialysis. We further categorized the study group patients (N = 207) by their pathology such as diabetic foot ulcer (DFU), Charcot neuroarthropathy, foot infection, or acute neuropathic fractures and dislocations. The control group (N = 254) was comprised of patients with diabetes who presented with common non-diabetes-related foot pathology.\n    \n\n\n          Results:\n        \n      \n      A total of 461 patients were enrolled in this study and included 254 patients without diabetic foot complications and 207 patients with diabetic foot problems. When comparing patients with and without diabetic disease, no significant differences were observed with regard to their fear of blindness, DFI, or ESRD requiring dialysis. Patients with diabetic foot disease (61 of 207, 31.9%) were 136% more likely (odds ratio [OR] = 2.36; 95% CI = 1.51-3.70; P = .002] to rank major LEA as their greatest fear when compared with diabetic patients without foot disease (42 of 254, 16.5%) and were 49% less likely (OR = 0.51; 95% CI = 0.34-0.79; P = .002) to rank death as their greatest fear compared with patients without diabetic foot disease.\n    \n\n\n          Conclusion:\n        \n      \n      Patients with diabetic foot pathology fear major LEA more than death, foot infection, or ESRD. Variables that were associated with ranking LEA as the greatest fear were the presence of a diabetic-related foot complication, duration of DM ≥10 years, insulin use, and the presence of peripheral neuropathy.\n    \n\n\n          Levels of evidence:\n        \n      \n      Level II: Prospective, Case controlled study."
        },
        {
            "title": "COPD severity score as a predictor of failure in exacerbations of COPD. The ESFERA study.",
            "abstract": "Background:\n        \n      \n      Exacerbations are a frequent cause of morbidity and mortality in COPD. It is crucial to identify risk factors for failure after treatment of exacerbations of COPD. This study evaluates the COPD severity score (COPDSS) as a predictor of clinical failure, together with other severity, activity and quality of life measurements, in patients with exacerbated COPD.\n    \n\n\n          Method:\n        \n      \n      Multicenter, prospective, observational study in ambulatory patients with exacerbation of COPD. The patients completed the COPDSS, the London Chest Activities of Daily Living (LCADL) and the EuroQol 5D (EQ-5D). A follow-up visit was scheduled one month after presentation with the exacerbation to assess the clinical evolution.\n    \n\n\n          Results:\n        \n      \n      A total of 346 patients were included (mean age 68.5 years (SD=9.5 years and 90.7% male) and mean FEV(1)(% predicted) 46.9% (SD=17)). After one month, 28.2% of episodes were classified as failures, with half of them requiring hospital admission. Patients who failed were more frequently active smokers, with more severe dyspnoea at presentation and worse lung function. They had significantly worse scores of COPDSS, LCADL, EQ-5D index and EQ-5D visual analogue score (VAS) and shorter mean time walking per day. ROC analysis of relationship between COPDSS and failure gave AUC 0.72, which improved only to 0.77 when the other significant variables in univariate analysis were considered.\n    \n\n\n          Conclusions:\n        \n      \n      Clinical failure after ambulatory treatment of exacerbation of COPD is frequent. Usual markers of severity (impaired lung function, active smoking and severe dyspnoea) are associated with failure; however, a short severity questionnaire (COPDSS) provides better predictive value than the usual variables."
        },
        {
            "title": "Stevens-Johnson syndrome and toxic epidermal necrolysis: ear, nose, and throat description at acute stage and after remission.",
            "abstract": "Importance:\n        \n      \n      Ear, nose, and throat (ENT) lesions are frequently involved in Stevens-Johnson syndrome and toxic epidermal necrolysis (SJS/TEN), although a detailed description is lacking in the literature.\n    \n\n\n          Objectives:\n        \n      \n      To describe ENT lesions at the acute stage and follow-up in a large series of patients with SJS/TEN and identify factors associated with the severe ENT form.\n    \n\n\n          Design, setting, and participants:\n        \n      \n      Retrospective study of 49 patients with SJS/TEN hospitalized in a referral care center from 2005 to 2010. Patients who underwent a full ENT workup including examination and a nasal fiberoptic endoscopy by an otorhinolaryngologist in the acute phase and during follow-up at 2 and 12 months were included in the study.\n    \n\n\n          Main outcomes and measures:\n        \n      \n      Recorded variables included maximal body surface area (BSA) detachment, SCORTEN (Score of Toxic Epidermal Necrosis [a severity of illness score]), sites and type of ENT mucosal lesions, intensive care unit transfer, pulmonary infection, and mortality. \"Severe ENT form\" was defined by the occurrence of laryngeal lesions with the risk of airways obstruction. Clinical characteristics associated with severe ENT form were analyzed in univariate and multivariable analysis.\n    \n\n\n          Results:\n        \n      \n      Of the 49 patients who underwent a full ENT workup (female to male ratio, 1.1:1), ENT symptoms (eg, odynophagia, dysphagia, dysphonia, dyspnea, earache, nasal obstruction) occurred in 48 (98%). Dyspnea or dysphonia were significantly associated with severe ENT form (21% [P = .03] and 50% [P < .001], respectively). Topographic frequencies of lesions were as followed: lips and oral cavity (n = 46 [93%]) and pharynx and vestibule of the nose (n = 26 [53%]). Fourteen patients (29%) had severe ENT form. Findings for other recorded variables for those with vs without ENT examination are as follows: maximal BSA detachment (20% [0%-95%] vs 5.5% [0%-95%]; P = .004), SCORTEN (1 [0-5] vs 1 [0-5]; P = .54), intensive care unit transfer (10 [20%] vs 9 [19%]; P = .80), pulmonary infection (9 [18%] vs 6 [13%]; P = .10), and mortality (3 [6%] vs 5 [10%]; P = .70). In multivariable analysis, pulmonary infection was significantly associated with severe ENT form (odds ratio, 5.9 [95% CI, 1.1-32.8] [P = .04]). After remission of SJS/TEN, a complete ENT mucosal healing occurred in 36 patients (74%) at 2 months and in nearly all patients (n = 48 [98%]) at 1 year of follow-up.\n    \n\n\n          Conclusions and relevance:\n        \n      \n      Severe ENT form is associated with pulmonary infection and is easily detected by nasal fiberoptic endoscopy. ENT evaluation should be suggested when dysphonia or dyspnea is observed at the acute stage of SJS/TEN."
        },
        {
            "title": "Therapeutic decisions involving disparate clinical outcomes: patient preference survey for treatment of central retinal artery occlusion.",
            "abstract": "Background:\n        \n      \n      Major therapeutic decisions are made by patients with information and guidance provided by their physicians. The values patients place on different outcomes and the risks they are willing to accept are important factors in making these decisions. New beneficial therapies associated with potentially serious complications are now available for some blinding diseases. The authors aim to determine the maximum amount of risk of stroke and death persons would accept to recover vision.\n    \n\n\n          Methods:\n        \n      \n      Standardized survey of adults with normal vision.\n    \n\n\n          Results:\n        \n      \n      Thirty-nine percent and 37% of surveyed adults would accept some risk of stroke and death, respectively, to triple the chances of recovering 20/100 visual acuity in one eye when binocular. More than 80% of persons would accept these risks if they were monocular. Maximum risk scores were significantly higher in the monocular case scenarios than in binocular case scenarios. Medical students and eye physicians were more likely to accept risk than persons with high school or university educational backgrounds.\n    \n\n\n          Conclusions:\n        \n      \n      The value persons place on vision when weighed against the risk of stroke or death varies considerably. More persons are willing to accept life-threatening risks if they are monocular. The reasons physicians and medical students are more likely to accept serious risks to improve vision than nonphysicians is unclear. Further studies are needed to determine how physicians' values effect the patient decision-making processes."
        },
        {
            "title": "Effects of age and illumination on night driving: a road test.",
            "abstract": "Objective:\n        \n      \n      This study investigated the effects of drivers' age and low light on speed, lane keeping, and visual recognition of typical roadway stimuli.\n    \n\n\n          Background:\n        \n      \n      Poor visibility, which is exacerbated by age-related changes in vision, is a leading contributor to fatal nighttime crashes. There is little evidence, however, concerning the extent to which drivers recognize and compensate for their visual limitations at night.\n    \n\n\n          Method:\n        \n      \n      Young, middle-aged, and elder participants drove on a closed road course in day and night conditions at a \"comfortable\" speed without speedometer information. During night tests, headlight intensity was varied over a range of 1.5 log units using neutral density filters.\n    \n\n\n          Results:\n        \n      \n      Average speed and recognition of road signs decreased significantly as functions of increased age and reduced illumination. Recognition of pedestrians at night was significantly enhanced by retroreflective markings of limb joints as compared with markings of the torso, and this benefit was greater for middle-aged and elder drivers. Lane keeping showed nonlinear effects of lighting, which interacted with task conditions and drivers' lateral bias, indicating that older drivers drove more cautiously in low light.\n    \n\n\n          Conclusion:\n        \n      \n      Consistent with the hypothesis that drivers misjudge their visual abilities at night, participants of all age groups failed to compensate fully for diminished visual recognition abilities in low light, although older drivers behaved more cautiously than the younger groups.\n    \n\n\n          Application:\n        \n      \n      These findings highlight the importance of educating all road users about the limitations of night vision and provide new evidence that retroreflective markings of the limbs can be of great benefit to pedestrians' safety at night."
        },
        {
            "title": "Age-related macular degeneration and mortality: the Beijing eye study.",
            "abstract": "Purpose:\n        \n      \n      To assess the association between age-related macular degeneration (AMD) and mortality in a population-based setting.\n    \n\n\n          Procedures:\n        \n      \n      At baseline in 2001, the Beijing Eye Study examined 4,378 subjects for AMD with a detected frequency of 110/4,378 (2.5%) subjects for early AMD and of 12/4,378 (0.3%) subjects for late AMD. In 2006, all study participants were re-invited for a follow-up examination.\n    \n\n\n          Results:\n        \n      \n      Out of the 4,378 subjects, 3,218 (73.5%) returned for a follow-up examination while 138 (3.2%) were dead and 1,022 (23.3%) did not agree to be re-examined or had moved away. Early AMD and late AMD were not significantly associated with mortality (p = 0.40 and 0.33, respectively), neither in univariate analysis nor in multivariate analysis.\n    \n\n\n          Conclusions:\n        \n      \n      AMD may not be associated with an increased mortality in adult Chinese."
        },
        {
            "title": "Stroke prevalence in a poor neighbourhood of São Paulo, Brazil: applying a stroke symptom questionnaire.",
            "abstract": "Background:\n        \n      \n      Brazil has one of the highest stroke mortality rates in the world, these rates are most endemic in the poor. We verified the prevalence of stroke in a deprived neighbourhood in the city of São Paulo, Brazil and compared it with other surveys worldwide.\n    \n\n\n          Methods:\n        \n      \n      A questionnaire with six questions concerning limb and facial weakness, articulation, sensory disturbances, impaired vision, and past diagnosis of stroke was completed door-to-door in a well-defined area of 15,000 people. Questionnaires were considered positive when a participant answered two or more questions about stroke symptoms or the presence of stroke being confirmed by a physician, or at least three questions in the positive, even if not confirmed by a doctor.\n    \n\n\n          Results:\n        \n      \n      Of the 4496 individuals over 35-years old living in the area, 243 initially screened positive for stroke. The age-adjusted prevalence rate for men was 4·6% (95% confidence interval 3·5-5·7). For women, the prevalence rate was 6·5% (95% confidence interval 5·5-7·5); when considering only one question, the rate was 4·8% (95% confidence interval 3·9-5·7). The most commonly reported symptoms were limb weakness and sensory disturbances. Hypertension and heart disease were the conditions most commonly associated with previous stroke.\n    \n\n\n          Conclusion:\n        \n      \n      Stroke prevalence rates were higher in this poor neighbourhood compared with other surveys."
        },
        {
            "title": "Utility assessment to measure the impact of dry eye disease.",
            "abstract": "Utility assessment is a formal method for quantifying and understanding the relative impact of a given health state or disease on patients. In this article, methodology of utility assessment is explained and illustrated, and results of an original study are reported. The study was conducted to determine utility values (patient preferences) associated with dry eye disease and compare them to other disease utilities, as well as to compare patient and physician assessments of disease. Forty-four patients in the United Kingdom with moderate to severe dry eye were surveyed via interactive utility assessment software. Utility values were measured by the Time Trade-Off (TTO) and Standard Gamble (SG) methods and adjusted to scores from 1.0=perfect health to 0.0=death. Patients reported utilities for: self-reported current dry eye status, self-reported current comorbidities, various dry eye severities, and binocular and monocular painful blindness. Patient's dry eye severity was independently classified by patient and physician assessments. Correlation analyses (Pearson) were performed between patients' current dry eye utilities and the physician-assessed severity. Agreement between self-reported and physician-reported patient severity was analyzed (Kappa). Patients reported higher utilities for their current dry eye condition than for monocular and binocular blindness (SG:0.84>0.60>0.51; TTO:0.67>0.43>0.38). Using TTO, the mean score for asymptomatic dry eye (0.68) was similar to that for \"some physical and role limitations with occasional pain\" and severe dry eye requiring surgery scored (0.56) similarly to hospital dialysis (0.56-0.59) and severe angina (0.5). Utilities described for scenarios of dry eye severity levels were slightly higher for patients self-reported as mild-to-moderate versus those self-reported as severe. For current dry eye condition, mean utilities for these groups were 0.72 for self-reported mild-to-moderate and 0.61 for self-reported severe. Utilities for dry eye were in the range of conditions accepted as lowering health utilities. Severe dry eye utilities were similar to those reported for dialysis and severe angina, highlighting the impact of dry eye disease on patients."
        },
        {
            "title": "The burden of pancreatic cancer in Australia attributable to smoking.",
            "abstract": "Objective:\n        \n      \n      To estimate the burden of pancreatic cancer in Australia attributable to modifiable exposures, particularly smoking.\n    \n\n\n          Design:\n        \n      \n      Prospective pooled cohort study.\n    \n\n\n          Setting, participants:\n        \n      \n      Seven prospective Australian study cohorts (total sample size, 365 084 adults); participant data linked to national registries to identify cases of pancreatic cancer and deaths.\n    \n\n\n          Main outcome measures:\n        \n      \n      Associations between exposures and incidence of pancreatic cancer, estimated in a proportional hazards model, adjusted for age, sex, study, and other exposures; future burden of pancreatic cancer avoidable by changes in exposure estimated as population attributable fractions (PAFs) for whole population and for specific population subgroups with a method accounting for competing risk of death.\n    \n\n\n          Results:\n        \n      \n      There were 604 incident cases of pancreatic cancer during the first 10 years of follow-up. Current and recent smoking explained 21.7% (95% CI, 13.8-28.9%) and current smoking alone explained 15.3% (95% CI, 8.6-22.6%) of future pancreatic cancer burden. This proportion of the burden would be avoidable over 25 years were current smokers to quit and there were no new smokers. The burden attributable to current smoking is greater for men (23.9%; 95% CI, 13.3-33.3%) than for women (7.2%; 95% CI, -0.4% to 14.2%; P = 0.007) and for those under 65 (19.0%; 95% CI, 8.1-28.6%) than for older people (6.6%; 95% CI, 1.9-11.1%; P = 0.030). There were no independent relationships between body mass index or alcohol consumption and pancreatic cancer.\n    \n\n\n          Conclusions:\n        \n      \n      Strategies that reduce the uptake of smoking and encourage current smokers to quit could substantially reduce the future incidence of pancreatic cancer in Australia, particularly among men."
        },
        {
            "title": "Mortality in diabetic patients participating in an ophthalmological control and screening programme.",
            "abstract": "The aim of this follow-up study has been to assess retinopathy and change of treatment to insulin therapy as risk factors for mortality in diabetic patients participating in a control and screening programme for retinopathy. A total of 3220 diabetic patients, 483 with an age at diagnosis <30 years, and 2737 with an age at diagnosis > or = 30 years, were included. Retinopathy was graded on fundus photographs using the Wisconsin Scale, and the visual acuity was assessed. The average HbA1c value was calculated for each patient for the previous 8 years to estimate long-term glycaemic control. Mortality data were obtained from death certificates. Two hundred and sixty-three diabetic patients (8.2%) died during the mean follow-up time of 3.4 years, 13 (2.7%) of those with younger-onset (<30 years) and 250 (9.1%) of those with older-onset (> or = 30 years) diabetes. Of them, 148 (56.3%) died from cardiovascular and 23 (8.7%) from cerebrovascular disorders. After adjusting for differences in age and sex, more severe retinopathy and the use of antihypertensive drugs were associated with a decreased overall survival rate as well as an increased mortality from cardiovascular and cerebrovascular diseases. A statistically significant association between HbA1c values in the highest quartile, i.e. > or =8.4%, and cardiovascular and all cause mortality did not remain when retinopathy was entered into the multivariate analyses. Duration of diabetes, but not change of treatment to insulin therapy, was associated with higher cardiovascular mortality in patients whose diabetes was diagnosed after the age of 30 years. We conclude that severe retinopathy, use of antihypertensive drugs, and poor glycaemic control predicted death from cardiovascular disease in diabetic patients participating in an ophthalmological screening programme."
        },
        {
            "title": "[Rational radiologic diagnosis of breast carcinoma].",
            "abstract": "For a favourable prognosis breast cancer must be diagnosed as early as possible. Among available imaging modalities (mammography, sonography, DAS, thermography, CT, MR) only mammography has been shown to produce a significant reduction in mortality, but unfortunately only for women above 50 years of age. The technical requirements, and the standard techniques and the appearance of benign and malignant breast tissues are described. The routine use of ultrasound in the evaluation of palpable masses remains controversial. Ultrasound technology is incapable of detecting early-stage cancers reliably. By \"using dynamic MRM\", i.e., repetitive imaging of the same slice before and at short time intervals after the injection of a contrast medium, high sensitivity and specificity is achieved in detecting breast cancer of different histologies. MR-mammography should be used in all cases where there is a discrepancy among radiographic, sonographic or clinical findings."
        },
        {
            "title": "Left ventricular scar burden specifies the potential for ventricular arrhythmogenesis: an LGE-CMR study.",
            "abstract": "Introduction:\n        \n      \n      The extent of left ventricular (LV) scar, characterized by late gadolinium enhancement cardiac MRI (LGE-CMR), has been shown to predict the occurrence of ventricular arrhythmias in implantable cardioverter defibrillator (ICD) recipients. However, the specificity of LGE-CMR for sudden cardiac death (SCD) versus non-SCD is unclear. The aim of this retrospective, observational study was to evaluate this relationship in a cohort of ICD recipients.\n    \n\n\n          Methods and results:\n        \n      \n      We included consecutive patients who had undergone LGE-CMR before ICD implantation over a 4-year period (2006-2009). Scar (defined as myocardium with a signal intensity ≥50% of the maximum in scar tissue) was characterized in terms of percent scar and number of transmural LV scar segments in a 17-segment model. The endpoints were appropriate ICD therapy and all-cause mortality. Sixty-four patients (average age 66 ± 11 years, 51 male, median LVEF 30%) were included. During 42 ± 13 months follow-up, appropriate ICD therapy occurred in 28 patients (44%), and 14 patients (22%) died. Number of transmural scar segments (P = 0.005) and percentage LV scar (P = 0.03) were both significantly associated with appropriate ICD therapy. However, neither number of transmural scar segments (P = 0.32) or percent LV scar (P = 0.59) was significantly associated with all-cause mortality.\n    \n\n\n          Conclusion:\n        \n      \n      In this observational study, in medium-term follow-up, the extent of LV scar characterized by LGE-CMR was strongly associated with the occurrence of spontaneous ventricular arrhythmias but not all-cause mortality. We hypothesize that scar quantification by LGE-CMR may be more specific for SCD than non-SCD, and may prove a valuable tool for the selection of patients for ICD therapy."
        },
        {
            "title": "Can we use CT pulmonary angiography as an alternative to echocardiography in determining right ventricular dysfunction and its severity in patients with acute pulmonary thromboembolism?",
            "abstract": "Purpose:\n        \n      \n      Our aim was to investigate the role of computed tomography pulmonary angiography (CTPA) in the diagnosis of right ventricular dysfunction (RVD) and massive pulmonary thromboembolism (PTE).\n    \n\n\n          Materials and methods:\n        \n      \n      We retrospectively involved a total of 61 patients. In CTPAs, pulmonary arterial obstruction index (PAOI), right ventricular/left ventricular diameter ratio (RV/LV), and superior vena cava (SVC) diameters were calculated, followed by echocardiography (ECHO), and clinical results were evaluated based on the reports available.\n    \n\n\n          Results:\n        \n      \n      CTPA findings that included PAOI, RV/LV ratio, and SVC diameter were, respectively, 54.9 ± 22.7 %, 1.58 ± 0.51, and 20.3 ± 0.2 mm in patients with RVD on ECHO, whereas corresponding values were, respectively, 37.8 ± 24.2 %, 1.32 ± 0.47, and 18.4 ± 3.3 mm in those without RVD (respectively, p = 0.006, p = 0.038, and p = 0.026). PAOI was 63.3 ± 22.0 % in patients among whom massive PTE was detected and 43.1 ± 23.9 % in the group without massive PTE (p = 0.01). As for mortality; given an RV/LV ratio >1.0, this ratio had 100 % sensitivity and 35.6 % specificity, whereas given a PAOI of ≥50 %, sensitivity and specificity were 83.3 % and 57.8 %, respectively.\n    \n\n\n          Conclusion:\n        \n      \n      We concluded that in the patients with PTE, PAOI ≥50 % and RV/LV >1.0 in CTPA could be helpful to demonstrate RVD."
        },
        {
            "title": "The cost utility of strabismus surgery in adults.",
            "abstract": "Purpose:\n        \n      \n      Cost-utility analysis evaluates the cost of medical care in relation to the gain in quality-adjusted life years (QALYs). Our purpose was to develop a cost model for surgical care for adult strabismus, to estimate the mean cost per case, to determine the associated gain in QALYs, and to perform cost-utility analysis.\n    \n\n\n          Methods:\n        \n      \n      A cost model incorporated surgery, pre- and postoperative care, and a mean of 1.5 procedures per patient. The gain in QALYs was based on the improvement of utility on a scale from 0 (death) to 1 (perfect health). Utility was measured through physician-conducted interviews employing a time tradeoff question (seeking to estimate the portion of life expectancy a patient would be willing to trade for being rid of disease and associated effects). The interviews were conducted before and 5 to 8 weeks after surgery in 35 strabismic patients (age 19-75 years).\n    \n\n\n          Results:\n        \n      \n      The cost model resulted in an estimated total cost of 4,254 dollars per case. A significant improvement of utility was found: 0.96 +/- 0.11 postoperatively versus 0.85 +/- 0.20 preoperatively (p = 0.00008). Based on the mean life expectancy (36.0 years) of these patients, and discounting outcomes and costs by 3% annually, this resulted in a mean value gain of 2.61 QALYs after surgery and a cost-utility for strabismus surgery of 1,632 dollas/QALY.\n    \n\n\n          Conclusions:\n        \n      \n      In the United States, treatments <50,000 dollars/QALY are generally considered \"very cost-effective.\" Strabismus surgery in adults falls well within this range."
        },
        {
            "title": "Stress myocardial perfusion imaging by CMR provides strong prognostic value to cardiac events regardless of patient's sex.",
            "abstract": "Objectives:\n        \n      \n      The major aim of this study is to test the hypothesis that stress cardiac magnetic resonance (CMR) imaging can provide robust prognostic value in women presenting with suspected ischemia, to the same extent as in men.\n    \n\n\n          Background:\n        \n      \n      Compelling evidence indicates that women with coronary artery disease (CAD) experience worse outcomes than men owing to a lack of early diagnosis and management. Numerous clinical studies have shown that stress CMR detects evidence of myocardial ischemia and infarction at high accuracy. Compared to nuclear scintigraphy, CMR is free of ionizing radiation, has high spatial resolution for imaging small hearts, and overcomes breast attenuation artifacts, which are substantial advantages when imaging women for CAD.\n    \n\n\n          Methods:\n        \n      \n      We performed stress CMR in 405 patients (168 women, mean age 58 ± 14 years) referred for ischemia assessment. CMR techniques included cine cardiac function, perfusion imaging during vasodilating stress, and late gadolinium enhancement imaging. All patients were followed for major adverse cardiac events (MACE).\n    \n\n\n          Results:\n        \n      \n      At a median follow-up of 30 months, MACE occurred in 36 patients (9%) including 21 cardiac deaths and 15 acute myocardial infarctions. In women, CMR evidence of ischemia (ISCHEMIA) demonstrated strong association with MACE (unadjusted hazard ratio: 49.9, p < 0.0001). While women with ISCHEMIA(+) had an annual MACE rate of 15%, women with ISCHEMIA(-) had very low annual MACE rate (0.3%), which was not statistically different from the low annual MACE rate in men with ISCHEMIA(-) (1.1%). CMR myocardial ischemia score was the strongest multivariable predictor of MACE in this cohort, for both women and men, indicating robust cardiac prognostication regardless of sex.\n    \n\n\n          Conclusions:\n        \n      \n      In addition to avoiding exposure to ionizing radiation, stress CMR myocardial perfusion imaging is an effective and robust risk-stratifying tool for patients of either sex presenting with possible ischemia."
        },
        {
            "title": "Age, microbiology and prognostic scores help to differentiate between secondary and tertiary peritonitis.",
            "abstract": "Background and aims:\n        \n      \n      Tertiary peritonitis is a severe persisting intra-abdominal infection and associated with high mortality. The aim was to find significant risk factors for mortality and tertiary peritonitis including the Mannheim Peritonitis Index (MPI), the Acute Physiology and Chronic Health Evaluation (APACHE) II score, and a sumscore of both.\n    \n\n\n          Materials and methods:\n        \n      \n      In this retrospective single-center cohort study, 122 patients were treated at the Surgical Department of a University Hospital.\n    \n\n\n          Results:\n        \n      \n      Sixty-nine patients (56.6%) developed tertiary peritonitis. Nineteen patients (27.5%), who suffered from tertiary peritonitis, died in contrast to eight patients (15.1%) with secondary peritonitis (P = 0.101). Patients with tertiary peritonitis had significantly higher APACHE II (P < 0.001), MPI (P = 0.035), and combined APACHE II and MPI scores (P < 0.001) than patients with secondary peritonitis. Age (P = 0.035), fungal infections (P = 0.025), and infections with more than one microbial organism (P = 0.047) were predictive for tertiary peritonitis. Combined APACHE II and MPI scores detected tertiary peritonitis better than the MPI (P = 0.014). Detection of mortality was comparable in all evaluated prognostic scores.\n    \n\n\n          Conclusion:\n        \n      \n      Prognostic scores besides age and fungal infections are risk factors for mortality and help to differentiate between secondary and tertiary peritonitis. The combination of prognostic scores is comparable to the APACHE II and superior compared to the MPI in regard to detection of tertiary peritonitis."
        },
        {
            "title": "Open Transcranial Resection of Small (<35 mm) Meningiomas of the Anterior Midline Skull Base in Current Microsurgical Practice.",
            "abstract": "Objective:\n        \n      \n      Despite technical surgical advance, the ultimate management of midline anterior skull base meningiomas remains to be defined. Open transcranial surgery is usually the first treatment option for large meningiomas, while less invasive techniques such as endoscopic surgery or radiosurgery might represent an alternative to open microsurgery for smaller lesions. The aim of our study is to investigate the outcome of open transcranial microsurgery in the resection of small (<35 mm) meningiomas of the midline anterior cranial base.\n    \n\n\n          Methods:\n        \n      \n      Clinical and surgical data from 43 patients affected by small midline anterior skull base meningiomas operated via an open transcranial approach were retrospectively reviewed.\n    \n\n\n          Results:\n        \n      \n      The tumor diameter on its major axis ranged from 12 to 35 mm, with a mean diameter of 28 mm. Gross total resection (Simpson grades I-II) was achieved in 100% of cases through a pterional approach. Postoperative overall morbidity was 9%. It was 3% among patients <70 years. No mortality was reported. Postoperative visual outcome was significantly associated with preoperative visual performance (P = 0.02), but not with preoperative optic nerve compression as detected by magnetic resonance imaging (P = 0.116). Age >70 years was associated with postoperative visual impairment, although not significantly (P = 0.06). Visual function was preserved or improved in 95% of cases, in 100% of patients <70 years, and in 71% of patients with preoperative visual impairment.\n    \n\n\n          Conclusions:\n        \n      \n      In our experience, open transcranial surgery proved safe and effective for midline anterior skull base meningiomas smaller than 35 mm in all patients <70 years and in patients >70 years without preoperative visual deficit. Our data are consistent with the literature. Conversely, the standard of treatment for the subgroup of patients >70 years with preoperative visual deficit has not yet been defined. This specific subgroup of patients offers a topic for further investigation."
        },
        {
            "title": "Patient-reported outcomes helped predict survival in multiple myeloma using partial least squares analysis.",
            "abstract": "Objective:\n        \n      \n      The prognostic value of Patient-Reported Outcomes (PRO) in predicting mortality during treatment of multiple myeloma (MM) patients was assessed using partial least square (PLS) regression, a statistical method that is well-adapted for highly correlated data.\n    \n\n\n          Study design and setting:\n        \n      \n      Four PRO measures, The European Organisation for Research and Treatment of Cancer (EORTC) QLQ-C30, the EORTC QLQ-MY24, the FACIT-Fatigue scale, and the FACT/GOG-Ntx scale, were administered during a trial designed to evaluate the efficacy and safety of bortezomib (VELCADE 1.3mg/m(2)) in MM patients (N=202). Clinical and PRO data were analyzed for predictive value by univariate and multivariate logistic regression methods and then by PLS regression.\n    \n\n\n          Results:\n        \n      \n      Fifteen baseline PRO parameters were significant in predicting mortality during treatment when univariate logistic regression was used. In contrast, only two variables were retained in the multivariate analysis, as correlated variables were excluded from the model. Using PLS regression, 14 of the 21 PRO predictors were significant in predicting mortality. Clinical and PRO data used together increased the predictive power of all models compared to clinical data alone.\n    \n\n\n          Conclusion:\n        \n      \n      The prognostic value of PRO was established and was more informative using PLS regression. PLS regression may therefore be a valuable method for analyzing PRO data."
        },
        {
            "title": "The long-term mortality and morbidity of Behcet syndrome: a 2-decade outcome survey of 387 patients followed at a dedicated center.",
            "abstract": "We surveyed the 20-year outcome of a cohort of patients with Behçet syndrome with emphasis on both mortality and morbidity. During 1999 and 2000, we collected outcome information on 387/428 (90.4%) of a cohort of patients (262 male, 125 female) who had registered in a dedicated outpatient clinic between July 1977 and December 1983. In 245/345 (71.0%) patients, outcome information was based on a formal hospital reevaluation, and in the remaining patients, on detailed telephone interviews. Forty-two patients (9.8%) (39 male, 3 female) had died, mainly due to major vessel disease and neurologic involvement. Mortality, as measured by standardized mortality ratios (SMR), was specifically increased among young males, among whom morbidity was also the highest. However, the SMR tended to decrease significantly with the passage of time. The same was also true for all mucocutaneous and articular manifestations. Both the onset of eye disease and its greatest damage were also usually within the first few years of disease onset. These suggest that the \"disease burden\" of Behçet syndrome is usually confined to the early years of its course, and in many patients the syndrome \"burns out.\" However, central nervous system involvement and major vessel disease are exceptions. They can have their onset late (5-10 yr) during the disease course. As reflected in the mortality figures, the disease was less severe among the females for almost each disease manifestation. There were no female patients with arterial aneurysms. Severely impaired vision did not always mean an eventual loss of useful vision, and those patients with a late onset of eye disease had a better visual prognosis."
        },
        {
            "title": "Long-term follow-up after uveal melanoma charged particle therapy.",
            "abstract": "Purpose:\n        \n      \n      To examine the results of helium ion irradiation in 218 uveal melanoma patients treated more than 10 years ago.\n    \n\n\n          Methods:\n        \n      \n      Retrospective review of 218 eyes treated with helium ion radiation for uveal melanoma between 1978 and 1984. Several parametric and non-parametric statistical analysis techniques were used.\n    \n\n\n          Results:\n        \n      \n      In 218 eyes treated with helium ion radiation for uveal melanoma, the mean dimension for largest basal diameter was 11.9 mm (range 5 mm to 24 mm). The mean tumor thickness was 6.7 mm (range 1.3 mm to 14.2 mm). Following helium ion radiation 208 (95.4%) of 218 eyes had local tumor control. At 10 years after radiation 46 (22.4%) of 218 eyes were enucleated; the majority (37 of 46) of enucleations were due to anterior ocular segment complications. At 10 years after radiation 102 (46.8%) of the 218 patients were dead; half had non-tumor related deaths and 51 died from metastatic melanoma. Best corrected visual acuity after radiation was > or = 20/40 in 21 of 93 eyes of patients that were alive and retained their eyes 10 or more years after treatment. In patients with tumors that were less than 6 mm in height and more than 3 mm away from the nerve or the fovea, 13 of 18 (72%) retained > or = 20/40. In contrast, only 11% of the patients with either thicker tumors or those close to the nerve or fovea retained that level of acuity. The actuarial enucleation rate at 5 years was 17.2% (2.7% S.E.) and at 10 years this was 22.4% (3.1% S.E). The recurrence tumor control rate at both 5 and 10 years was 5.3% (S.E 1.7%).\n    \n\n\n          Conclusions:\n        \n      \n      Helium ion radiation of uveal melanoma is associated with good local tumor control and reasonable retention of the treated eye 10 years after treatment. In tumors that are less than 6 mm in thickness and greater than 3 mm from the optic nerve and fovea, many retain excellent vision. Approximately one-half of the deaths 10 years after treatment were due to non-tumor-related causes."
        },
        {
            "title": "Exploring Flow, Factors, and Outcomes of Temporal Event Sequences with the Outflow Visualization.",
            "abstract": "Event sequence data is common in many domains, ranging from electronic medical records (EMRs) to sports events. Moreover, such sequences often result in measurable outcomes (e.g., life or death, win or loss). Collections of event sequences can be aggregated together to form event progression pathways. These pathways can then be connected with outcomes to model how alternative chains of events may lead to different results. This paper describes the Outflow visualization technique, designed to (1) aggregate multiple event sequences, (2) display the aggregate pathways through different event states with timing and cardinality, (3) summarize the pathways' corresponding outcomes, and (4) allow users to explore external factors that correlate with specific pathway state transitions. Results from a user study with twelve participants show that users were able to learn how to use Outflow easily with limited training and perform a range of tasks both accurately and rapidly."
        },
        {
            "title": "The severity classification system for acquired immunodeficiency syndrome hospitalizations. Association with survival after discharge and inpatient resource use.",
            "abstract": "The Severity Classification for AIDS Hospitalizations (SCAH) was applied to a longitudinal person-based data set of Maryland adult residents diagnosed with acquired immunodeficiency syndrome (AIDS) between 1983 and 1989 to predict long-term survival. In contrast to other AIDS severity measures, SCAH can be applied to administrative data bases for analyses of large populations. Although SCAH was created to predict inpatient mortality using cross-sectional hospital discharge data, the models used in this study show SCAH stage at first AIDS hospitalization to predict long-term survival in persons with AIDS, even after adjusting for sociodemographic and treatment variables. Additional models in the study show SCAH stage at first hospitalization has a strong association with inpatient length of stay and associated charges, making it useful for health care resource planning."
        },
        {
            "title": "Survival from uveal melanoma in England and Wales 1986 to 2001.",
            "abstract": "Purpose:\n        \n      \n      To analyse survival from uveal melanoma diagnosed in England and Wales between 1986-1999 and followed up to 2001.\n    \n\n\n          Methods:\n        \n      \n      Data from the National Cancer Registry at the Office for National Statistics were analysed. The data were compiled from population-based cancer registries covering all of England and Wales for all adults (aged 15-99) diagnosed with primary ocular malignancy, excluding eyelid tumours. Level of poverty was based on the national classification of area of residence at time of diagnosis. Regression models explored the influence of sex, age, and level of poverty on relative survival for patients diagnosed with uveal melanoma during successive calendar periods.\n    \n\n\n          Results:\n        \n      \n      Of 5,519 adults identified with primary ocular malignancy, 4,717 had melanoma, of which 4,308 (91%) were eligible for analysis. Two-thirds (67%) of the ocular melanomas were uveal, 5% conjunctival, and 2% orbital; the subsite was unspecified in 26%. Relative survival from uveal melanoma was 95% at 1 year and 72% at 5 years. There was no statistically significant variation in 1-year or 5-year survival by sex or poverty level and no significant trend over time. Older patients had significantly worse survival (p < 0.001).\n    \n\n\n          Conclusions:\n        \n      \n      This study provides national population-based survival estimates for England and Wales for uveal melanoma, the most common primary intraocular malignancy in adults. Five-year relative survival, an important indicator of the quality of cancer care, has not improved since the 1980s. Greater age, but not gender or level of poverty, is associated with a poorer prognosis. A standardised classification of uveal melanoma is required to improve reporting to cancer registries. Further research is required to explore reasons for lower relative survival in older persons."
        },
        {
            "title": "Effects of alcohol intoxication goggles (fatal vision goggles) with a concurrent cognitive task on simulated driving performance.",
            "abstract": "Objective: Fatal vision goggles (FVGs) are image-distorting equipment used to simulate alcohol impairment in driver education programs. Unlike alcohol, which disrupts cognitive processes, FVG only induces visual impairment. Performing concurrent cognitive tasks while wearing FVG may reduce the wearer's attentional resources and provide a better simulation of alcohol intoxication. This study examined the impact of wearing FVG with/without administration of a concurrent cognitive task on simulated driving.Methods: Twenty-one males (23 ± 3 y, mean ± SD) participated in this randomized, repeated-measures study involving two experimental trials. In each trial, participants completed a baseline drive then an experimental drive under one of two conditions: (1) FVG and (2) FVG with additional cognitive demand (FVG + CD). The driving test included 3 separate scenarios (Task 1, 2, 3) lasting ∼5min each. Lateral (standard deviation of lane position [SDLP]; number of lane crossings [LCs]) and longitudinal control parameters (average speed; standard deviation of speed [SDSP]; distance headway; minimum distance headway) were monitored in Tasks 1 and 2. Latency to two different stimuli (choice reaction time [CRT]) was examined in Task 3.Results: In Task 1, SDLP and LC were unaffected by either condition. However, SDSP increased significantly from baseline with FVG, irrespective of cognitive demand. In Task 2, distance headway decreased significantly from baseline with FVG, but increased significantly with FVG + CD. Minimum distance headway was significantly decreased, while SDLP increased significantly and LC increased (although not statistically significant) in both conditions relative to baseline. In Task 3, a significant increase in CRT occurred with FVG + CD, but not with FVG alone.Conclusions: Wearing FVG negatively impacted simulated driving performance. However, effects were isolated to specific performance outcomes and were dependent on complexity of the driving task. Addition of a secondary cognitive task exacerbates the effects of FVG on select driving outcomes (i.e. lane position, SDSP), influences the effect direction on other measures (i.e. distance headway), and has a detrimental effect on reaction time to stimuli embedded in the scenario, that is not observed with FVG alone. Future studies using FVG as a surrogate means to alcohol intoxication should consider these results, informing methodological decisions to reduce potential for confounding effects."
        },
        {
            "title": "Utility values associated with vitreous floaters.",
            "abstract": "Purpose:\n        \n      \n      To ascertain the health-related quality of life associated with symptomatic degenerative vitreous floaters.\n    \n\n\n          Design:\n        \n      \n      Cross-sectional questionnaire survey.\n    \n\n\n          Methods:\n        \n      \n      In this institution-based study, 311 outpatients aged 21 years and older who presented with symptoms of floaters were enrolled. Data from 266 patients (85.5%) who completed the questionnaire were analyzed. Utility values were assessed using a standardized utility value questionnaire. The time trade-off (TTO) and standard gamble (SG) for death and blindness techniques were used to calculate the utility values. Descriptive, univariate, and multivariate analyses were performed using Stata Release 6.0.\n    \n\n\n          Results:\n        \n      \n      The mean age of the study population was 52.9 ± 12.02 years (range, 21-97). The mean utility values were 0.89, 0.89, and 0.93 for TTO, SG (death), and SG (blindness), respectively. Patients aged ≤55 years reported significantly lower SG (blindness) utility values when compared with patients above 55 years of age (age ≤55 = 0.92, age >55 = 0.94, P = .007). Utility measurements did not demonstrate significant relationship with any of the other socio-demographic variables examined in this study. The utility values did not demonstrate any significant relationship with other ocular characteristics such as duration of symptoms, presence of a posterior vitreous detachment, and presence or severity of myopia.\n    \n\n\n          Conclusions:\n        \n      \n      Symptomatic degenerative vitreous floaters have a negative impact on health-related quality of life. Younger symptomatic patients are more likely to take a risk of blindness to get rid of the floaters than older patients."
        },
        {
            "title": "Adherence to diabetes guidelines for screening, physical activity and medication and onset of complications and death.",
            "abstract": "Aims:\n        \n      \n      Analyze relationships between adherence to guidelines for diabetes care - regular screening; physical activity; and medication - and diabetes complications and mortality.\n    \n\n\n          Methods:\n        \n      \n      Outcomes were onset of congestive heart failure (CHF), stroke, renal failure, moderate complications of lower extremities, lower-limb amputation, proliferative diabetic retinopathy (PDR), and mortality during follow-up. Participants were persons aged 65+ in the Health and Retirement Study (HRS) 2003 Diabetes Study and had Medicare claims in follow-up period (2004-8).\n    \n\n\n          Results:\n        \n      \n      Adherence to screening recommendations decreased risks of developing CHF (odds ratio (OR)=0.83; 95% confidence interval (CI): 0.72-0.96), stroke (OR=0.80; 95% CI: 0.68-0.94); renal failure (OR=0. 82; 95% CI: 0.71-0.95); and death (OR=0.86; 95% CI: 0.74-0.99). Adherence to physical activity recommendation reduced risks of stroke (OR=0.64; 95% CI: 0.45-0.90), renal failure (OR=0.71; 95% CI: 0.52-0.97), moderate lower-extremity complications (OR=0.71; 95% CI: 0.51-0.99), having a lower limb amputation (OR=0.31, 95% CI: 0.11-0.85), and death (OR=0.56, 95% CI: 0.41-0.77). Medication adherence was associated with lower risks of PDR (OR=0.35, 95% CI: 0.13-0.93).\n    \n\n\n          Conclusions:\n        \n      \n      Adherence to screening, physical activity and medication guidelines was associated with lower risks of diabetes complications and death. Relative importance of adherence differed among outcome measures."
        },
        {
            "title": "Endovascular aortic aneurysm repair with carbon dioxide-guided angiography in patients with renal insufficiency.",
            "abstract": "Objective:\n        \n      \n      Renal dysfunction following endovascular abdominal aortic aneurysm repair (EVAR) remains a significant source of morbidity and mortality. We studied the use of carbon dioxide (CO(2)) as a non-nephrotoxic contrast agent for EVAR.\n    \n\n\n          Methods:\n        \n      \n      Recorded data from 114 consecutive patients who underwent EVAR with CO(2) as the contrast agent over 44 months were retrospectively analyzed. CO(2) was used exclusively in 72 patients and in an additional 42 patients iodinated contrast (IC) was given (mean, 37 mL). Renal and hypogastric artery localization and completion angiography were done with CO(2) in all patients, including additional arterial embolization in 16 cases. Preoperative National Kidney Foundation glomerular filtration rate (GFR) classification was normal in 16 patients, mildly decreased in 52, moderate to severely decreased in 44, and two patients were on dialysis.\n    \n\n\n          Results:\n        \n      \n      All graft deployments were successful with no surgical conversions. CO(2) angiography identified 20 endoleaks (two type 1, 16 type 2, and two type 4) and three unintentionally covered arteries. Additional use of IC in 42 patients did not modify the procedure in any case. When compared with a cohort of patients who underwent EVAR using exclusively IC, the operative time was shorter with CO(2) (177 vs 194 minutes; P = .01); fluoroscopy time was less (21 vs 28 minutes; P = .002), and volume of IC was lower (37 vs 106 mL; P < .001). Postoperatively, there were two deaths, two instances of renal failure requiring dialysis, and no complications related to CO(2) use. Among patients with moderate to severely decreased GFR, those undergoing EVAR with IC had a 12.7% greater decrease in GFR compared with the CO(2) EVAR group (P = .004). At 1, 6, and 12-month follow-up, computed tomography angiography showed well-positioned endografts with the expected patent renal and hypogastric arteries in all patients and no difference in endoleak detection compared with the IC EVAR group. During follow-up, eight transluminal interventions and one open conversion were required, and no aneurysm-related deaths occurred.\n    \n\n\n          Conclusions:\n        \n      \n      CO(2)-guided EVAR is technically feasible and safe; it eliminates or reduces the need for IC use, may expedite the procedure, and avoids deterioration in renal function in patients with pre-existing renal insufficiency. A prospective trial comparing CO(2) with IC during EVAR is warranted."
        },
        {
            "title": "Baseline self-reported cataract and subsequent mortality in Physicians' Health Study I.",
            "abstract": "Purpose:\n        \n      \n      To examine whether a reported history of cataract, a possible marker of aging, is associated with future mortality.\n    \n\n\n          Methods:\n        \n      \n      Participants were 18,669 of the 22,071 U.S. male physicians enrolled in the Physicians' Health Study I who had complete information at study entry, including self-report of presence or absence of baseline cataract. Participants were without a previous history of myocardial infarction, stroke, transient cerebral ischemia, or cancer (except non-melanoma skin cancer). Reported deaths were confirmed by an End Points Committee of physicians.\n    \n\n\n          Results:\n        \n      \n      A total of 581 participants reported a personal history of cataract at baseline. During an average of 12.4 years of follow-up, there were 1,514 deaths including 496 due to cardiovascular (CV) and 1,018 due to non-CV causes. After adjustment for differences in age, men who reported cataract at baseline had a non-significant 9% increased risk of death from any cause compared to men who did not report cataract (RR, 1.09; 95% CI, 0.91-1.30). The RRs were 1.03 (95% CI, 0.75-1.41) for CV death and 1.12 (95% CI, 0.90-1.40) for non-CV death. Adjustment for other risk factors had little effect on these estimates. Similar results were obtained in analyses conducted separately among those with and without self-reported diabetes at baseline.\n    \n\n\n          Conclusions:\n        \n      \n      These results from a population of generally healthy physicians indicate that a report of a history of cataract is not associated with any material increase in mortality after adjustment for differences in age between men with and without cataract. Additional investigation of this cohort is in progress to determine whether incident age-related cataracts as well as their subtypes, confirmed by medical record review, are associated with increased mortality."
        },
        {
            "title": "Impact of Diabetes Mellitus on Head and Neck Cancer Patients Undergoing Surgery.",
            "abstract": "Objective:\n        \n      \n      The impact of diabetes mellitus (DM) on surgical outcomes and cost of care for patients undergoing surgery for head and neck cancer (HNCA) is not well established. We used the Nationwide Inpatient Sample to analyze the postoperative impact of DM on HNCA patients.\n    \n\n\n          Study design:\n        \n      \n      Population-based inpatient registry analysis.\n    \n\n\n          Setting:\n        \n      \n      Academic medical center.\n    \n\n\n          Subjects and methods:\n        \n      \n      Discharge data from the Nationwide Inpatient Sample were analyzed for patients undergoing HNCA surgery from 2002 to 2010. Patient demographics, comorbidities, length of stay, hospital charges, and postoperative complications were compared between HNCA patients with and without DM.\n    \n\n\n          Results:\n        \n      \n      Of 31,075 patients, 4029 patients (13.0%) had a DM diagnosis. DM patients were older (65.7 ± 10.8 vs 61.1 ± 14.1 years old; P < .001), had more preexisting comorbidities, had longer hospitalizations, and incurred greater hospital charges. Compared with the non-DM cohort, DM patients experienced significantly higher rates of postoperative infections (2.6% vs 2.1%, P = .025), cardiac events (9.0% vs 4.3%, P < .001), pulmonary edema/failure (6.6% vs 5.7%, P = .023), acute renal failure (3.3% vs 1.5%, P < .001), and urinary tract infections (2.8 % vs 2.1%, P = .005). No differences in surgical wound healing rates were observed (0.1 vs 0.1, P = .794). On multivariate logistic regression corrected for age and race, DM patients had greater odds of postoperative infections (1.382, P = .007), cardiac events (1.893, P < .001), and acute renal failure (2.023, P < .001).\n    \n\n\n          Conclusions:\n        \n      \n      DM is associated with greater length of stay and hospital charges among HNCA patients. DM patients have significantly greater rates of postoperative complications, including postoperative infections, cardiac events, and acute renal failure."
        },
        {
            "title": "Predictive Value of Computed Tomography in Acute Pulmonary Embolism: Systematic Review and Meta-analysis.",
            "abstract": "Background:\n        \n      \n      Many computed tomography (CT) parameters have been proposed as potential predictors of outcome in acute pulmonary embolism. We sought to summarize available evidence on the predictive value of CT severity parameters for short-term clinical outcome in pulmonary embolism.\n    \n\n\n          Methods:\n        \n      \n      We searched PubMed and EMBASE through February 2014 for studies that reported on the association between CT parameters of acute pulmonary embolism severity and short-term (≤6 months) clinical outcome. Risk estimates for quantitative parameters of right ventricular (RV) dysfunction (abnormally increased RV/left ventricular [LV] diameter ratio on transverse sections and 4-chamber views), qualitative parameters of RV dysfunction (abnormal septal morphology and contrast reflux), thrombus load, and central thrombus location were derived using random effect regression analysis. Meta-regression analysis was performed to quantify and explain study heterogeneity.\n    \n\n\n          Results:\n        \n      \n      A total of 49 studies with 13,162 patients with acute pulmonary embolism (median age of 61 years, 55.1% were women) who underwent diagnostic CT imaging were included in the analysis. An abnormally increased RV/LV diameter ratio measured on transverse sections was associated with an approximately 2.5-fold risk for all-cause mortality (pooled odds ratio [OR], 2.5; 95% confidence interval [CI], 1.8-3.5) and adverse outcome (OR, 2.3; 95% CI, 1.6-3.4) and a 5-fold risk for pulmonary embolism-related mortality (OR, 5.0; 95% CI, 2.7-9.2). Thrombus load (OR, 1.6, 95% CI, 0.7-3.9; P = .2896) and central location (OR, 1.7; 95% CI, 0.7-4.2; P = .2609) were not predictive for all-cause mortality, although both were associated with adverse clinical outcome.\n    \n\n\n          Conclusions:\n        \n      \n      Across all end points, the RV/LV diameter ratio on transverse CT sections has the strongest predictive value and most robust evidence base for adverse clinical outcomes in patients with acute pulmonary embolism."
        },
        {
            "title": "Mammographic screening: case-control studies.",
            "abstract": "Background:\n        \n      \n      The case-control design can be used to evaluate the benefit of cancer screening programmes.\n    \n\n\n          Materials and methods:\n        \n      \n      This paper outlines the main methodological features of the case-control design in this context, and indicates some potential biases. It also reviews the existing case-control literature on mammographic screening.\n    \n\n\n          Results:\n        \n      \n      Case-control studies consistently indicate a reduction of approximately 50% in breast cancer mortality associated with mammography. This result indicates greater benefit than shown in randomised trials; however, one should recognise that trials indicate effectiveness whereas case-control studies indicate efficacy. The two types of evidence are broadly compatible when one allows for screening non-compliance and contamination in the randomised trials.\n    \n\n\n          Conclusions:\n        \n      \n      The case-control evidence supports and is consistent with the findings of randomised trials of mammography. Effectiveness estimates from trials indicate the benefit of screening to the population as a whole, and are pertinent to the public policy debate as to the value of offering screening. In contrast, case-control studies indicate benefit to actual screening participants. As such, case-control estimates of efficacy are appropriate for individual decision-making by women about their use of mammography when it is potentially available to them."
        },
        {
            "title": "Profile and evolution of the Global Burden of Morbidity in the Maghreb (Tunisia,Morocco, Algeria). The Triple burden of morbidity.",
            "abstract": "Background:\n        \n      \n      The Global Burden of Disease (GBD) is an objective method of measurement of disease disability, allowing the quantification of a population's health status, the identification of its health needs, and the determination of its public health priorities.\n    \n\n\n          Objectives:\n        \n      \n      To document the epidemiological transition in Maghreb countries (Tunisia, Morocco, Algeria) over the past three decades and to identify their priority health problems, which are responsible for a considerable burden of disability.\n    \n\n\n          Methods:\n        \n      \n      This is a data synthesis work of the Institute for Health Metrics and Evaluation (IHME) global burden of disease, through its project \"GBD Compare Data Visualization\". Data covering the period from 1990 to 2016, examined the three major categories of health problems \"communicable, maternal, neonatal and nutritional diseases\", \"noncommunicable diseases\" and \"injuries\", as well as the three types of risk: metabolic, environmental / professional and behavioral.\n    \n\n\n          Results:\n        \n      \n      Since 1990, cardiovascular diseases have consistently been the leading cause of death in the three Maghreb countries. During the period 1990-2016, and at varying speeds, the positions of communicable and neonatal diseases declined, while noncommunicable diseases (particularly cardiovascular diseases, cancers, mental disorders, diabetes and neurological disorders) increased significantly, to be at the top of the list of components of the global burden of disease.In 2016, road accidents have been ranked eighth in the ranking of the main components of the overall burden of morbidity in Tunisia and Morocco and ninth in Algeria. During the same period, the environmental and behavioral risk factors registered an overall decrease in the three Maghreb countries, in contrast to the metabolic risk factors that experienced a gradual and homogeneous increase in the Greater Maghreb.\n    \n\n\n          Conclusion:\n        \n      \n      This GBD analysis confirmed the rather old and fairly advanced epidemiological transition in Maghreb countries, leading to a real \"triple burden\" threatening the stability and sustainability of national health systems. Hence the urgency of supporting the following five projects: the curriculum reform of the faculties of health sciences, the development of the second line of care, the participative management of health services, universal health coverage and the implementation of a comprehensive and integrated strategy for prevention and health promotion."
        },
        {
            "title": "Terson's syndrome in subarachnoid hemorrhage and severe brain injury accompanied by acutely raised intracranial pressure.",
            "abstract": "Object:\n        \n      \n      The syndrome of retinal or vitreous hemorrhage in association with subarachnoid hemorrhage (SAH) is known as Terson's syndrome. The authors' purpose was to determine whether intraocular hemorrhage occurs with similar incidence when caused by severe brain injury accompanied by acutely raised intracranial pressure (ICP).\n    \n\n\n          Methods:\n        \n      \n      Prospective ophthalmological examination was performed in 22 consecutive patients with SAH or severe brain injury and elevated ICP. Thirteen patients were admitted for SAH (World Federation of Neurological Surgeons Grades II-IV) and nine for severe brain injury (Glasgow Coma Scale scores 3-10). Monitoring of ICP was performed at the time of admission via a ventricular catheter. Initial ICP exceeded 20 mm Hg in all patients. Indirect ophthalmoscopy without induced mydriasis was performed within the 1st week after the acute event. Retinal or vitreous hemorrhage was seen in six (46%) of 13 patients with SAH and in four (44%) of nine patients with severe brain injury. Ocular bleeding was found bilaterally in three patients with SAH and in one patient with severe brain injury (18%). Six of the 10 patients with Terson's syndrome died as a result of their acute event.\n    \n\n\n          Conclusions:\n        \n      \n      The present results indicate that Terson's syndrome may be related to acute elevation of ICP, independent of its causes, and may occur with similar incidence in patients with severe brain injury and those with SAH. Because recognition and treatment of Terson's syndrome may prevent visual impairment and associated secondary damage to the eye, increased awareness of this entity in all patients with acute raised intracranial hypertension is recommended."
        },
        {
            "title": "Prognostic value of Morise clinical score, calcium score and computed tomography coronary angiography in patients with suspected or known coronary artery disease.",
            "abstract": "Purpose:\n        \n      \n      Our aim was to determine the prognostic value of computed tomography coronary angiography (CTCA), coronary artery calcium scoring (CACS) and Morise clinical score in patients with known or suspected coronary artery disease (CAD).\n    \n\n\n          Materials and methods:\n        \n      \n      A total of 722 patients (480 men; 62.7±10.9 years) who were referred for further cardiac evaluation underwent CACS and contrast-enhanced CTCA to evaluate the presence and severity of CAD. Of these, 511 (71%) patients were without previous history of CAD. Patients were stratified according to the Morise clinical score (low, intermediate, high), to CACS (0-10, 11-100, 101-400, 401-1,000, >1,000) and to CTCA (absence of CAD, nonsignificant CAD, obstructive CAD). Patients were followed up for the occurrence of major events: cardiac death, nonfatal myocardial infarction, unstable angina and revascularisation.\n    \n\n\n          Results:\n        \n      \n      Significant CAD (>50% luminal narrowing) was detected in 260 (36%) patients; nonsignificant CAD (<50% luminal narrowing) in 250 (35%) and absence of CAD in 212 (29%). During a mean follow-up of 20±4 months, 116 events (21 hard) occurred. In patients with normal coronary arteries on CTCA, the major event rate was 0% vs. 1.7% in patients with nonsignificant CAD and 7.3% in patients with significant CAD (p<0.0001). Three hard events (14%) occurred in patients with CACS≤100 and two (9.5%) in patients with intermediate Morise score; one revascularisation was observed in a patient with low Morise score. At multivariate analysis, diabetes, obstructive CAD and CACS >1,000 were significant predictors of events (p<0.05).\n    \n\n\n          Conclusions:\n        \n      \n      An excellent prognosis was noted in patients with a normal CTCA (0% event rate). CACS ≤100 and low-intermediate Morise score did not exclude the possibility of events at follow-up."
        },
        {
            "title": "Glaucoma and survival: the National Health Interview Survey 1986-1994.",
            "abstract": "Objective:\n        \n      \n      Associations between glaucoma and survival have not been studied extensively, in part, because of the relatively low prevalence of this condition. This study examines associations between self-reported glaucoma and mortality in a nationally representative sample of U.S. adults.\n    \n\n\n          Design:\n        \n      \n      Annual cross-sectional multistage area probability survey of the U.S. civilian noninstitutionalized population living at addressed dwellings.\n    \n\n\n          Participants:\n        \n      \n      Mortality linkage with >96% of participants from the 1986 to 1994 National Health Interview Survey was performed by the National Center for Health Statistics through 1997. Complete data were available on 116796 adults >or=018 years old.\n    \n\n\n          Methods:\n        \n      \n      Adults within randomly selected households were administered a chronic conditions list that included questions about glaucoma and visual impairment. Proxy information on these conditions was obtained when household members were unavailable for interview. Statistical methods included Cox regression models with adjustments for covariates, as well as for the complex sample survey design.\n    \n\n\n          Main outcome measure:\n        \n      \n      All-cause mortality and cardiovascular and cancer mortality.\n    \n\n\n          Results:\n        \n      \n      A total of 1559 (1.3%) glaucoma cases were reported. Nearly 19% of participants with reported glaucoma also had reported visual impairment (n = 303). Mortality linkage identified 8949 deaths; the average follow-up was 7.0 years. After controlling for survey design, gender, age, race, marital status, education level, and self-rated health, participants with reported glaucoma but without reported visual impairment were at significantly increased risk of death relative to participants without reported glaucoma, irrespective of visual impairment status (hazard ratio [HR], 1.35; 95% confidence interval [CI], 1.19-1.53); similar associations were found for participants with reported glaucoma and visual impairment vs. participants with no reported glaucoma (HR, 1.39; 95% CI, 1.14-1.71). An increased risk of cardiovascular disease mortality was found for participants with reported glaucoma both without (HR, 1.31; 95% CI, 1.11-1.55) and with (HR, 1.53; 95% CI, 1.15-2.05) reported visual impairment. Risk of mortality due to cancer was increased only in participants with reported glaucoma but without reported visual impairment (HR, 1.57; 95% CI, 1.25-1.98); this association was stronger when the mortality analysis was restricted to cancers amenable to early screening, including breast, cervical, colon, and prostate cancer (HR, 1.99; 95% CI, 1.41-2.81).\n    \n\n\n          Conclusions:\n        \n      \n      Among adults residing in the United States, reported glaucoma is associated with an increased risk of all-cause and cardiovascular disease mortality. Associations between glaucoma and cancer were inconsistent and may reflect, in part, a detection bias, in which glaucoma is more likely to be diagnosed in adults receiving health care because of other medical conditions."
        },
        {
            "title": "Results of an ophthalmologic screening programme for identification of cases with Anderson-Fabry disease.",
            "abstract": "Purpose:\n        \n      \n      Anderson-Fabry disease is an inherited lysosomal storage disease with a broad and unspecific range of symptoms, a painful course of disease and early death. The recent development of new enzyme therapy emphasises the need for early diagnosis and treatment of undiagnosed patients. One of the affected organs of Anderson-Fabry disease is the eye. Cornea verticillata--corneal opacities and corneal dystrophy--as well as tortuositas vasorum can occur in an early stage of the disease affecting almost all hemizygous men and more than 70% of heterozygous women. In order to identify unknown cases with Anderson-Fabry disease, we carried out a screening programme contacting Austrian ophthalmologists.\n    \n\n\n          Methods:\n        \n      \n      All 658 Austrian ophthalmologists were asked to record patients with cornea verticillata as well as tortuositas vasorum--twice at an interval of 3 months.\n    \n\n\n          Results:\n        \n      \n      33% of the contacted ophthalmologists replied, identifying 5 patients suspected of having Anderson-Fabry disease. After additional examinations including tests for enzyme activities Anderson-Fabry disease was confirmed in 1 man.\n    \n\n\n          Conclusion:\n        \n      \n      We have identified 1 case with Anderson-Fabry disease through our ophthalmology screening programme among a population of approximately of 8 million. Ophthalmologic screening programmes for ocular manifestations typical of Anderson-Fabry disease are limited because of the moderate visual affection in these patients. Nevertheless, considering the limited options to detect such cases otherwise, ophthalmologists have a major responsibility to identify patients with Anderson-Fabry disease on routine examinations."
        },
        {
            "title": "Quality-of-Life Impairments Persist Six Months After Treatment of Graves' Hyperthyroidism and Toxic Nodular Goiter: A Prospective Cohort Study.",
            "abstract": "Background:\n        \n      \n      The treatment of hyperthyroidism is aimed at improving health-related quality of life (HRQoL) and reducing morbidity and mortality. However, few studies have used validated questionnaires to assess HRQoL prospectively in such patients. The purpose of this study was to assess the impact of hyperthyroidism and its treatment on HRQoL using validated disease-specific and generic questionnaires.\n    \n\n\n          Methods:\n        \n      \n      This prospective cohort study enrolled 88 patients with Graves' hyperthyroidism and 68 with toxic nodular goiter from endocrine outpatient clinics at two Danish university hospitals. The patients were treated with antithyroid drugs, radioactive iodine, or surgery. Disease-specific and generic HRQoL were assessed using the thyroid-related patient-reported outcome (ThyPRO) and the Medical Outcomes Study 36-item Short Form (SF-36), respectively, evaluated at baseline and six-month follow-up. The scores were compared with those from two general population samples who completed ThyPRO (n = 739) and SF-36 (n = 6638).\n    \n\n\n          Results:\n        \n      \n      Baseline scores for patients with Graves' hyperthyroidism and toxic nodular goiter were significantly worse than those for the general population scores on all comparable ThyPRO scales and all SF-36 scales and component summaries. ThyPRO scores improved significantly with treatment on all scales in Graves' hyperthyroidism and four scales in toxic nodular goiter, while SF-36 scores improved on five scales and both component summaries in Graves' hyperthyroidism and only one scale in toxic nodular goiter. In Graves' hyperthyroidism, large treatment effects were observed on three ThyPRO scales (Hyperthyroid Symptoms, Tiredness, Overall HRQoL) and moderate effects on three scales (Anxiety, Emotional Susceptibility, Impaired Daily Life), while moderate effects were seen in two ThyPRO scales in toxic nodular goiter (Anxiety, Overall HRQoL). However, significant disease-specific and generic HRQoL deficits persisted on multiple domains across both patient groups.\n    \n\n\n          Conclusions:\n        \n      \n      Graves' hyperthyroidism and toxic nodular goiter cause severe disease-specific and generic HRQoL impairments, and HRQoL deficits persist in both patient groups six months after treatment. These data have the potential to improve communication between physicians and patients by offering realistic estimates of expected HRQoL impairments and treatment effects. Future studies should identify risk factors for persistent HRQoL deficits, compare HRQoL effects of the various therapies, and thereby aid in determining the optimal treatment strategies."
        },
        {
            "title": "Severe hypertension with lone bilateral papilloedema: a variant of malignant hypertension.",
            "abstract": "Patients with severe hypertension with retinoscopic bilateral papilloedema only are not classically regarded as having malignant hypertension (MHT). We have encountered 23 such patients between 1965-1993, whilst over a similar period we have seen 315 patients who fulfilled the conventional criteria for MHT with bilateral retinal haemorrhages, exudates with or without papilloedema. We hypothesised that patients with \"lone\" papilloedema and severe hypertension were suffering from a disease which was identical in aetiology and outcome to conventional MHT. There were no significant differences in age, mean blood pressure, proteinuria or renal function at presentation, ethnic composition, smoking status and followup blood pressure control between the papilloedema group and those presenting with conventional MHT. Clinical features at presentation in the papilloedema only group included strokes in 4, visual disturbance in 2, headaches in 3 and heart failure in 1 patient. Many patients however had no complications at presentation. After a mean followup of 59.8 months, of the \"lone\" papilloedema group, 7 patients (30.4%) were still alive, 1 patient was on renal dialysis therapy, 13 were dead (56.5%) and 2 (8.7%) were lost to followup. The commonest causes of death were stroke in 4 patients, renal failure in 4 and heart disease in 2. This was a similar pattern of mortality to those patients with \"conventional\" MHT. Lifetable analyses showed a median survival of 35.9 months for the papilloedema group which was significantly worse than the 108.7 months for the conventional MHT group (Lee-Desu statistic 4.04, p = 0.045). We suggest that patients with high blood pressure and lone bilateral papilloedema may comprise a hitherto unrecognised subgroup of patients with MHT. Once intracerebral pathology has been excluded, these patients need to be treated as aggressively as those with MHT."
        },
        {
            "title": "Relationship of glycemic control, exogenous insulin, and C-peptide levels to ischemic heart disease mortality over a 16-year period in people with older-onset diabetes: the Wisconsin Epidemiologic Study of Diabetic Retinopathy (WESDR).",
            "abstract": "Objective:\n        \n      \n      The purpose of this study was to examine the relationship of glycemic control and exogenous and endogenous insulin levels with all-cause and cause-specific mortality (ischemic heart disease and stroke) in an older-onset diabetic population.\n    \n\n\n          Research design and methods:\n        \n      \n      The Wisconsin Epidemiologic Study of Diabetic Retinopathy (WESDR) is an ongoing, prospective, population-based cohort study of individuals with diabetes first examined in 1980-1982. A stratified sample of all individuals with diabetes diagnosed at 30 years of age or older was labeled \"older-onset\" (n = 1,370). Those participating in the 1984-1986 examination phase (n = 1,007) were included in the analysis. Endogenous insulin was determined by measurements of plasma C-peptide (in nanomoles per liter), and exogenous insulin was calculated in units per kilogram per day. Glycemic control was determined by levels of glycosylated hemoglobin (HbA(1)).\n    \n\n\n          Results:\n        \n      \n      After 16 years of follow-up, 824 individuals died (all-cause mortality); 358 deaths involved ischemic heart disease and 137 involved stroke. C-peptide and HbA(1) were significantly associated with all-cause and ischemic heart disease mortality in our study. The hazard ratio (95% CI) values for all-cause mortality were 1.12 (1.07-1.17) per 1% increase in HbA(1), 1.20 (0.85-1.69) per 1 unit x kg(-1) x day(-1) increase in exogenous insulin, and 1.15 (1.04-1.29) per 1 nmol/l increase in C-peptide and for ischemic heart disease mortality were 1.14 (1.06-1.22), 1.50 (0.92-2.46), and 1.19 (1.02-1.39) for HbA(1), exogenous insulin, and C-peptide, respectively, after adjusting for relevant confounders. C-peptide was associated with stroke mortality only among men (1.65 [1.07-2.53]).\n    \n\n\n          Conclusions:\n        \n      \n      Our results show that individuals with higher endogenous insulin levels are at higher risk of all-cause, ischemic heart disease, and stroke mortality."
        },
        {
            "title": "The cost-effectiveness of grid laser photocoagulation for the treatment of diabetic macular edema: results of a patient-based cost-utility analysis.",
            "abstract": "Grid laser therapy has been demonstrated to be of benefit for the treatment of diabetic macular edema. The purpose of the present study was to determine the cost-effectiveness of grid laser therapy for the treatment of diabetic macular edema. The analysis was performed from the perspective of a third-party insurer. Decision analyses and cost-effectiveness analyses were performed by incorporating the data from the Early Treatment Diabetic Retinopathy Study, expected longevity data, and patient-based utilities. Various sensitivity analyses were performed to determine the robustness of the models. Laser treatment conferred an overall improvement in quality-of-life adjusted years of approximately 3 months over the duration of disease for a hypothetical patient. The unadjusted cost per quality-of-life adjusted year (QALY) was US$3,101. Net present value analysis demonstrated that the cost per QALY could increase to $3,655, assuming a 5% discount rate. Overall, grid laser photocoagulation for diabetic macular edema is a very cost-effective treatment based on the results of this cost-utility analysis."
        },
        {
            "title": "Impact of near-death experiences on dialysis patients: a multicenter collaborative study.",
            "abstract": "Background:\n        \n      \n      People who have come close to death may report an unusual experience known as a near-death experience (NDE). This study aims to investigate NDEs and their aftereffects in dialysis patients.\n    \n\n\n          Study design:\n        \n      \n      Cross-sectional study.\n    \n\n\n          Setting & participants:\n        \n      \n      710 dialysis patients at 7 centers in Taipei, Taiwan.\n    \n\n\n          Predictor:\n        \n      \n      Demographic characteristics, life-threatening experience, depression, and religiosity.\n    \n\n\n          Outcomes:\n        \n      \n      NDE and self-perceived changes in attitudes or behaviors.\n    \n\n\n          Measurements:\n        \n      \n      Greyson's NDE scale, Royal Free Questionnaire, 10-Question Survey, Ring's Weighted Core Experience Index, and Beck Depression Inventory.\n    \n\n\n          Results:\n        \n      \n      45 patients had 51 NDEs. Mean NDE score was 11.9 (95% confidence interval, 11.0 to 12.9). Out-of-body experience was found in 51.0% of NDEs. Purported precognitive visions, awareness of being dead, and \"tunnel experience\" were uncommon (<10%). Compared with the no-NDE group, subjects in the NDE group were more likely to be women and younger at life-threatening events. Both frequency of participation in religious ceremonies and pious religious activity correlated significantly with NDE score in patients with NDEs (P < 0.01 and P = 0.01, respectively). The NDE group reported being kinder to others (P = 0.04) and more motivated (P = 0.02) after their life-threatening events than the no-NDE group.\n    \n\n\n          Limitations:\n        \n      \n      Determining the incidence of NDEs is dependent on self-reporting. Many NDEs occurred before the patient began long-term dialysis therapy. Causality between NDE and aftereffects cannot be inferred.\n    \n\n\n          Conclusions:\n        \n      \n      NDE is not uncommon in the dialysis population and is associated with positive aftereffects. Nephrology care providers should be aware of the occurrence and aftereffects of NDEs. The high occurrence of life-threatening events, availability of medical records, and accessibility and cooperativeness of patients make the dialysis population very suitable for NDE research."
        },
        {
            "title": "Brain activity in near-death experiencers during a meditative state.",
            "abstract": "Aim:\n        \n      \n      To measure brain activity in near-death experiencers during a meditative state.\n    \n\n\n          Methods:\n        \n      \n      In two separate experiments, brain activity was measured with functional magnetic resonance imaging (fMRI) and electroencephalography (EEG) during a Meditation condition and a Control condition. In the Meditation condition, participants were asked to mentally visualize and emotionally connect with the \"being of light\" allegedly encountered during their \"near-death experience\". In the Control condition, participants were instructed to mentally visualize the light emitted by a lamp.\n    \n\n\n          Results:\n        \n      \n      In the fMRI experiment, significant loci of activation were found during the Meditation condition (compared to the Control condition) in the right brainstem, right lateral orbitofrontal cortex, right medial prefrontal cortex, right superior parietal lobule, left superior occipital gyrus, left anterior temporal pole, left inferior temporal gyrus, left anterior insula, left parahippocampal gyrus and left substantia nigra. In the EEG experiment, electrode sites showed greater theta power in the Meditation condition relative to the Control condition at FP1, F7, F3, T5, P3, O1, FP2, F4, F8, P4, Fz, Cz and Pz. In addition, higher alpha power was detected at FP1, F7, T3 and FP2, whereas higher gamma power was found at FP2, F7, T4 and T5.\n    \n\n\n          Conclusions:\n        \n      \n      The results indicate that the meditative state was associated with marked hemodynamic and neuroelectric changes in brain regions known to be involved either in positive emotions, visual mental imagery, attention or spiritual experiences."
        },
        {
            "title": "Feasibility of contrast-enhanced MRI derived textural features to predict overall survival in locally advanced breast cancer.",
            "abstract": "Background:\n        \n      \n      The prognosis for women with locally advanced breast cancer (LABC) is poor and there is a need for better treatment stratification. Gray-level co-occurrence matrix (GLCM) texture analysis of magnetic resonance (MR) images has been shown to predict pathological response and could become useful in stratifying patients to more targeted treatments.\n    \n\n\n          Purpose:\n        \n      \n      To evaluate the ability of GLCM textural features obtained before neoadjuvant chemotherapy to predict overall survival (OS) seven years after diagnosis of patients with LABC.\n    \n\n\n          Material and methods:\n        \n      \n      This retrospective study includes data from 55 patients with LABC. GLCM textural features were extracted from segmented tumors in pre-treatment dynamic contrast-enhanced 3-T MR images. Prediction of OS by GLCM textural features was assessed and compared to predictions using traditional clinical variables.\n    \n\n\n          Results:\n        \n      \n      Linear mixed-effect models showed significant differences in five GLCM features (f1, f2, f5, f10, f11) between survivors and non-survivors. Using discriminant analysis for prediction of survival, GLCM features from 2 min post-contrast images achieved a classification accuracy of 73% (P < 0.001), whereas traditional prognostic factors resulted in a classification accuracy of 67% (P = 0.005). Using a combination of both yielded the highest classification accuracy (78%, P < 0.001). Median values for features f1, f2, f10, and f11 provided significantly different survival curves in Kaplan-Meier analysis.\n    \n\n\n          Conclusion:\n        \n      \n      This study shows a clear association between textural features from post-contrast images obtained before neoadjuvant chemotherapy and OS seven years after diagnosis. Further studies in larger cohorts should be undertaken to investigate how this prognostic information can be used to benefit treatment stratification."
        },
        {
            "title": "Conjunctival malignant melanoma in Sweden 1969-91.",
            "abstract": "Clinical information, follow-up and histopathological parameters of the primary lesions were assessed for all (45) individuals with conjunctival malignant melanomas in Sweden presenting during a 22.5 year period (1969 to mid 1991). The annual incidence of conjunctival malignant melanoma in Sweden was 0.0240 per 100,000. On average, two new cases were diagnosed each year (population 8.6 million in 1991). Sixty-two per cent of the lesions recurred, but re-growth in itself was not correlated to reduced survival. The actuarial 10-year survival proportion using life-table analysis was 70%. A significantly reduced survival due to tumour-related death was noted in patients with tumours with high mitotic indices, many epithelioid cells and in lesions exceeding 10 mm in diameter. Other factors that may influence survival are presented in the context of previous reports. The present policy in Sweden for treating patients with malignant melanoma of the conjunctiva is outlined and discussed."
        },
        {
            "title": "Trends in survival of patients diagnosed with cancers of the brain and nervous system, thyroid, eye, bone, and soft tissues in the Nordic countries 1964-2003 followed up until the end of 2006.",
            "abstract": "Background:\n        \n      \n      Diagnoses of cancer of the brain, thyroid, eye, bone, and soft tissues are categorised by heterogeneity in disease frequency, survival, aetiology and prospects for curative therapy. In this paper, temporal trends in patient survival in the Nordic countries are considered.\n    \n\n\n          Material and methods:\n        \n      \n      Age-standardised incidence and mortality rates, 5-year relative survival, and excess mortality rates for varying follow-up periods are presented, as are age-specific 5-year relative survival by country, sex and 5-year diagnostic period.\n    \n\n\n          Results:\n        \n      \n      Brain cancer incidence rates have been rising but mortality has been relatively stable, with 5-year survival uniformly increasing from the early-1970s, particularly in younger patients. Five-year survival from brain cancer among men varies between 45% and 50% for men and 60% to 70% in women, with excess deaths decreasing with time in each of the Nordic populations. Age-standardised incidence rates of thyroid cancer have been mainly increasing during the 1960s and 1970s, although trends thereafter diverge, with 5-year relative survival increasing 20-30 percentage points over the last 40 years to around 80-90%. Thyroid cancer survival is consistently lower in Denmark, particularly in patients diagnosed aged over 60, while there is less geographic variation in excess deaths three months beyond initial diagnosis. Relative survival from eye cancer increased with time from approximately 60% in 1964-1968 to 80% 1999-2003, while for bone sarcoma, incidence rates remained stable, mortality rates declined, and 5-year survival increased slightly to around 55-65%. Soft tissue sarcoma incidence and survival have been slowly increasing since the 1960s, with little variation in survival (around 65%) for the most recent period.\n    \n\n\n          Conclusions:\n        \n      \n      There have been some notable changes in survival that can be linked to epidemiological and clinical factors in different countries over time. Time-varying proportions of the major histological subtypes might however have affected the survival estimates for a number of the cancer forms reviewed here."
        },
        {
            "title": "Associations of mortality with ocular disorders and an intervention of high-dose antioxidants and zinc in the Age-Related Eye Disease Study: AREDS Report No. 13.",
            "abstract": "## OBJECTIVE\nTo assess the association of ocular disorders and high doses of antioxidants or zinc with mortality in the Age-Related Eye Disease Study (AREDS).\n## METHODS\nBaseline fundus and lens photographs were used to grade the macular and lens status of AREDS participants. Participants were randomly assigned to receive oral supplements of high-dose antioxidants, zinc, antioxidants plus zinc, or placebo. Risk of all-cause and cause-specific mortality was assessed using adjusted Cox proportional hazards models.\n## RESULTS\nDuring median follow-up of 6.5 years, 534 (11%) of 4753 AREDS participants died. In fully adjusted models, participants with advanced age-related macular degeneration (AMD) compared with participants with few, if any, drusen had increased mortality (relative risk [RR], 1.41; 95% confidence interval [CI], 1.08-1.86). Advanced AMD was associated with cardiovascular deaths. Compared with participants having good acuity in both eyes, those with visual acuity worse than 20/40 in 1 eye had increased mortality (RR, 1.36; 95% CI, 1.12-1.65). Nuclear opacity (RR, 1.40; 95% CI, 1.12-1.75) and cataract surgery (RR, 1.55; 95% CI, 1.18-2.05) were associated with increased all-cause mortality and with cancer deaths. Participants randomly assigned to receive zinc had lower mortality than those not taking zinc (RR, 0.73; 95% CI, 0.61-0.89).\n## CONCLUSIONS\nThe decreased survival of AREDS participants with AMD and cataract suggests that these conditions may reflect systemic rather than only local processes. The improved survival in individuals randomly assigned to receive zinc requires further study.\n"
        },
        {
            "title": "Predictive Indices for Functional Improvement and Deterioration, Institutionalization, and Death Among Elderly Medicare Beneficiaries.",
            "abstract": "Background:\n        \n      \n      Prediction models can help clinicians provide the best and most appropriate care to their patients and can help policy makers design services for groups at highest risk for poor outcomes.\n    \n\n\n          Objective:\n        \n      \n      To develop prediction models identifying both risk factors and protective factors for functional deterioration, institutionalization, and death.\n    \n\n\n          Design:\n        \n      \n      Cohort study using data from the Medicare Current Beneficiary Survey (MCBS).\n    \n\n\n          Setting:\n        \n      \n      Community survey.\n    \n\n\n          Participants:\n        \n      \n      This study included 21,264 Medicare beneficiaries 65 years of age and older who participated in the MCBS from the 2001-2008 entry panels and were followed up for 2 years.\n    \n\n\n          Methods:\n        \n      \n      The index was derived in 60% and validated in the remaining 40%. β Coefficients from a multinomial logistic regression model were used to derive points, which were added together to create scores associated with the outcome.\n    \n\n\n          Main outcome measure:\n        \n      \n      The outcome was activity of daily living (ADL) stage transitions over 2 years following entry into the MCBS. Beneficiaries were categorized into 1 of 4 outcome categories: stable or improved function, functional deterioration, institutionalization, or death.\n    \n\n\n          Results:\n        \n      \n      Our model identified 16 factors for functional deterioration (age, gender, education, living arrangement, dual eligibility, proxy use, Alzheimer disease/dementia, angina pectoris/coronary heart disease, diabetes, emphysema/asthma/chronic obstructive pulmonary disease, mental/psychiatric disorder, Parkinson disease, stroke/brain hemorrhage, hearing impairment, vision impairment, and baseline ADL stage) after backward selection (P < .05). Compared to stable or improved function, the risk of functional deterioration ranged from ≤1 to ≥6, ≤4 to ≥22 for the risk of institutionalization, and ≤3 to ≥16 for the risk of death.\n    \n\n\n          Conclusion:\n        \n      \n      Predictive indices, or point and scoring systems used to predict outcomes, can identify elderly Medicare beneficiaries at risk for functional deterioration, institutionalization, and death and can aid policy makers, clinicians, and family members in improving care for older adults and supporting successful aging in the community.\n    \n\n\n          Level of evidence:\n        \n      \n      III."
        },
        {
            "title": "Patient and technique survival for blind and sighted diabetics on continuous ambulatory peritoneal dialysis: a ten-year analysis.",
            "abstract": "A retrospective analysis of patient and technique survival over 10 years in a group of 66 diabetics (40 being blind) and 71 non-diabetics was undertaken. Patient survival profiles showed that the blind diabetics lived longer than the sighted, but for a shorter time than the nondiabetics. In technique success, the sighted diabetics out did the blind and the non-diabetics, long term. Short term, the blind performed better than sighted diabetics. The key to success and longer survival on CAPD depended on motivation on the part of the patient, patient's acceptance of given disability, family (social) support, and willingness on the part of renal care personnel to train the disabled diabetic to perform CAPD. With adequate education and support, blind diabetics did CAPD as well as sighted patients. There was no increased frequency of peritonitis in blind diabetics compared to sighted diabetics. Both blind diabetics and non-diabetics had fewer episodes than sighted diabetics. Intraperitoneal route of insulin administration achieved good glycemic control in diabetic population. Refractory congestive cardiac failure and/or fatal arrhythmias was the most common cardiac cause of death in diabetics on CAPD."
        },
        {
            "title": "Twenty-four-Month Outcomes of the Ranibizumab for Edema of the Macula in Diabetes - Protocol 3 with High Dose (READ-3) Study.",
            "abstract": "Purpose:\n        \n      \n      To compare 2.0 mg ranibizumab (RBZ) injections with 0.5 mg RBZ for eyes with center-involved diabetic macular edema (DME).\n    \n\n\n          Design:\n        \n      \n      Randomized, controlled, double-masked (to the dose), interventional, multicenter clinical trial.\n    \n\n\n          Participants:\n        \n      \n      A total of 152 patients (152 eyes) with DME.\n    \n\n\n          Methods:\n        \n      \n      Eligible eyes were randomized in a 1:1 ratio to 0.5 mg (n = 77) or 2.0 mg (n = 75) RBZ. Study eyes received 6 monthly mandatory injections followed by as-needed injections until month 24.\n    \n\n\n          Main outcome measures:\n        \n      \n      The primary efficacy end point of the study was mean change in best-corrected visual acuity (BCVA) and central foveal thickness (CFT) at month 6. Secondary outcomes included the mean change in BCVA and CFT at month 24, and incidence and severity of systemic and ocular adverse events through month 24.\n    \n\n\n          Results:\n        \n      \n      A total of 152 eyes were randomized in the study. At month 24, the mean improvement from baseline BCVA was +11.06 letters in the 0.5 mg RBZ group (n = 59) and +6.78 letters in the 2.0 mg RBZ group (n = 54) (P = 0.02). The mean numbers of RBZ injections through month 24 were 18.4 and 17.3 in the 0.5 mg and 2.0 mg RBZ groups, respectively (P = 0.08). The mean change in CFT was -192.53 μm in the 0.5 mg RBZ group and -170.64 μm in the 2.0 mg RBZ group (P = 0.41). By month 24, 3 deaths had occurred in the 0.5 mg RBZ group and 3 deaths had occurred in the 2.0 mg RBZ group; 5 of these 6 deaths occurred secondary to cardiovascular causes, and 1 death occurred as the result of severe pneumonia. All 5 patients with a cardiovascular cause of death had a history of coronary heart disease.\n    \n\n\n          Conclusions:\n        \n      \n      At month 24, there were significant visual and anatomic improvements in both groups, with subjects in the 0.5 mg RBZ group gaining more vision. Visual and anatomic gains achieved at month 6 were largely maintained through month 24. No new safety events were identified. In this study population, 2.0 mg RBZ does not appear to provide additional benefit over 0.5 mg RBZ."
        },
        {
            "title": "Sonography for deep venous thrombosis: current and future applications.",
            "abstract": "Deep venous thrombosis (DVT) is a one of the most common problems facing the clinician in medicine today. It is often asymptomatic and goes undiagnosed with potentially fatal consequences. Ultrasound has become the \"gold standard\" in the diagnosis of deep venous thrombosis and with proper attention to technique sensitivity of this test is approximately 97%. An understanding of anatomy, pathophysiology, and risk factors is important. Thrombus formation usually begins beneath a valve leaflet below the knee. Approximately 40% will resolve spontaneously, 40% will become organized, and 20% will propagate. Whether or not a calf vein thrombus is identified, a repeat examination in 7 to 10 days is recommended in patients with risk factors or when deep venous thrombosis is suspected. The three main risk factors for thrombus formation are age greater than 75 years, previous history of deep venous thrombosis, and underlying malignancy. Other diagnostic studies include the contrast venogram, CT or MRI venogram, Tc99m Apcitide study, and the laboratory test D-Dimer. The D-Dimer study is being used more frequently as a screening test with 99% sensitivity in detecting thrombus, whether deep venous thrombosis or pulmonary embolism. However, specificity is only approximately 50% with many conditions leading to false-positive exams. Therefore, a negative examination is useful in avoiding other diagnostic studies, but a positive one may be misleading. Conditions that can lead to a false-positive examination include, but are not limited to diabetes, pregnancy, liver disease, heart conditions, recent surgery, and some gastrointestinal diseases. Like the sonogram, two negative D-Dimer studies a week apart exclude the diagnosis of deep venous thrombosis. Compression sonography with color Doppler remains the best overall test for deep venous thrombosis. It is easy to perform, less expensive than most \"high tech\" studies, can be performed as a portable examination, and is highly reliable when done properly."
        },
        {
            "title": "Criteria for the diagnosis and severity stratification of acute pancreatitis.",
            "abstract": "Recent diagnostic and therapeutic progress for severe acute pancreatitis (SAP) remarkably decreased the case-mortality rate. To further decrease the mortality rate of SAP, it is important to precisely evaluate the severity at an early stage, and initiate appropriate treatment as early as possible. Research Committee of Intractable Diseases of the Pancreas in Japan developed simpler criteria combining routinely available data with clinical signs. Severity can be evaluated by laboratory examinations or by clinical signs, reducing the defect values of the severity factors. Moreover, the severity criteria considered laboratory/clinical severity scores and contrast-enhanced computed tomography (CE-CT) findings as independent risk factors. Thus, CE-CT scans are not necessarily required to evaluate the severity of acute pancreatitis. There was no fatal case in mild AP diagnosed by the CE-CT severity score, whereas case-mortality rate in those with SAP was 14.8%. Case-mortality of SAP that fulfilled both the laboratory/clinical and the CE-CT severity criteria was 30.8%. It is recommended, therefore, to perform CE-CT examination to clarify the prognosis in those patients who were diagnosed as SAP by laboratory/clinical severity criteria. Because the mortality rate of these patients with SAP is high, such patients should be transferred to advanced medical units."
        },
        {
            "title": "Continuous venovenous hemodiafiltration versus hemodialysis as renal replacement therapy in patients with acute renal failure in the intensive care unit.",
            "abstract": "Objective:\n        \n      \n      Hemodialysis (HD) and continuous venovenous hemodiafiltration (CVVHDF) have been adopted as forms of renal replacement therapy (RRT) in patients with acute renal failure (ARF). Although CVVHDF has many advantages, previous studies reported no definite improvement in survival rate compared to HD.\n    \n\n\n          Material and methods:\n        \n      \n      In this retrospective study, 148 intensive care unit patients underwent HD (70 males, 25 females; mean age 45 +/- 17 years) or CVVHDF (42 males, 11 females; mean age 52 +/- 18 years). The severity of illness was estimated at the initiation of RRT and on the third day of RRT and presented using the APACHE III scoring system. The number of organ failures was checked at the initiation of RRT.\n    \n\n\n          Results:\n        \n      \n      The survival rate was 46% in the HD group and 21% in the CVVHDF group (p = 0.002). CVVHDF was applied to the more severely ill patients, who had longer periods using a ventilator (p = 0.002) and/or vasopressor (p < 0.001), higher numbers of organ failures (p < 0.001) and higher initial APACHE III scores (p < 0.001). Among patients with APACHE III scores > 103, the survival rate was 13% in the CVVHDF group and 0% in the HD group. In patients with kidney failure and failure of two other organs, the survival rate was 9% in the HD group and 36% in the CVVHDF group (p = 0.035).\n    \n\n\n          Conclusion:\n        \n      \n      The mortality rate in the CVVHDF group was higher than that in the HD group, which may have been because CVVHDF was applied to the more severely ill patients. In contrast, CVVHDF may give a chance of survival to patients with APACHE III scores > 103 and may be more useful than HD in patients with failure of three or more organs."
        },
        {
            "title": "Long-term uveal melanoma survivors: measuring their quality of life.",
            "abstract": "Purpose:\n        \n      \n      Patients with uveal melanoma (UM) undergo lifelong follow-up as metastases can occur more than 20 years after diagnosis. Little is known about the quality of life (QoL) of UM survivors over such an extended period. To investigate their QoL, we used various estimating factors.\n    \n\n\n          Methods:\n        \n      \n      A cohort of patients diagnosed and treated for UM with regularly scheduled follow-up visits was asked to fill in a European Organization for Research and Treatment of Cancer (EORTC) questionnaire comprised of the EORTC QLQ-C30 and EORTC QLQ-OPT30 modules. An additional open question examined other changes in the patients' lifestyle since diagnosis. Independent demographic and medical data were collected from patient records.\n    \n\n\n          Results:\n        \n      \n      Two hundred and thirty-two of 294 patients agreed to complete the questionnaire. General QoL correlated highly with the eye-related QoL. Statistically significant higher QoL was associated with tumours not involving the ciliary body, and with better best-corrected visual acuity (BCVA). A subgroup of 39% of the patients reported severe disability affecting eye-related tasks. Thirty-three per cent were highly concerned about various aspects of their future health. Patients who underwent enucleation reported lower eye-related QoL and described problems related to body image in response to the open question.\n    \n\n\n          Conclusion:\n        \n      \n      General QoL of UM patients is only slightly affected by their malignancy. However, body image and psychosocial adjustment are major issues involved in evaluating QoL. Continuous long-term psychosocial treatment is needed from the time of diagnosis in a subgroup of patients suffering from eye-related disabilities."
        },
        {
            "title": "Effect of acute physiologic derangements on outcome after subarachnoid hemorrhage.",
            "abstract": "Objective:\n        \n      \n      To determine the effect that acute physiologic derangements have on outcome after subarachnoid hemorrhage (SAH) and to design a composite score summarizing these abnormalities.\n    \n\n\n          Design:\n        \n      \n      Prospective observational study.\n    \n\n\n          Setting:\n        \n      \n      Neuroscience intensive care unit in a tertiary care academic center.\n    \n\n\n          Patients:\n        \n      \n      Consecutive cohort of 413 patients with SAH admitted within 3 days of SAH onset with 3-month modified Rankin Scale scores.\n    \n\n\n          Interventions:\n        \n      \n      None.\n    \n\n\n          Results:\n        \n      \n      Among 20 physiologic variables assessed within 24 hrs of admission, four were independently associated with death or severe disability (modified Rankin Scale score, 4-6) at 3 months in a multivariate analysis: arterio-alveolar gradient of >125 mm Hg (odds ratio [OR], 4.5; 95% confidence interval [CI], 2.7-7.6), serum bicarbonate of <20 mmol/L (OR, 2.9; 95% CI, 1.6-5.6), serum glucose of >180 mg/dL (OR, 2.8; 95% CI, 1.6-4.8), and mean arterial pressure of <70 or >130 mm Hg (OR, 1.7; 95% CI, 1.0-2.9). Based on their proportional contribution to outcome, we constructed the SAH Physiologic Derangement Score (SAH-PDS; range, 0-8) by assigning the following weights for abnormal findings: arterio-alveolar gradient, 3 points; bicarbonate, 2 points; glucose, 2 points; and mean arterial pressure, 1 point. After controlling for known predictors of death or severe disability (age, admission neurologic status, loss of consciousness, aneurysm size, intraventricular hemorrhage, and rebleeding), the SAH Physiologic Derangement Score was independently associated with poor outcome (OR, 1.3 for each point increase; 95% CI, 1.1-1.6). By contrast, the systemic inflammatory response syndrome score and the Acute Physiology and Chronic Health Evaluation II physiologic subscore did not add predictive value to the model.\n    \n\n\n          Conclusion:\n        \n      \n      Acute interventions specifically targeting hypoxemia, metabolic acidosis, hyperglycemia, and cardiovascular instability may improve the outcome of SAH patients. The SAH Physiologic Derangement Score may prove useful for rapidly quantifying the severity of important physiologic derangements in acute SAH."
        },
        {
            "title": "Mortality incidence and the severity of coronary atherosclerosis assessed by computed tomography angiography.",
            "abstract": "Objectives:\n        \n      \n      This study investigated whether cardiac computed tomography angiography (CTA) can predict all-cause mortality in symptomatic patients.\n    \n\n\n          Background:\n        \n      \n      Noninvasive coronary angiography is being increasingly performed by CTA to assess for obstructive coronary artery disease (CAD), and minimal outcome data exist for coronary CTA. We have utilized a cohort of symptomatic patients who underwent electron beam tomography to allow for longer follow-up (up to 12 years) than currently available with newer 64-slice multidetector-row computed tomography studies.\n    \n\n\n          Methods:\n        \n      \n      In all, 2,538 consecutive patients who underwent CTA by electron beam tomography (age 59 +/- 14 years, 70% males) without known CAD were studied. Computed tomographic angiography results were categorized as significant CAD (> or =50% luminal narrowing), mild CAD (<50% stenosis), and normal coronary arteries. Multivariable Cox proportional hazards models were developed to predict all-cause mortality. Risk-adjusted models incorporated traditional risk factors for coronary disease and coronary artery calcification (CAC).\n    \n\n\n          Results:\n        \n      \n      During a mean follow-up of 78 +/- 12 months, the death rate was 3.4% (86 deaths). The CTA-diagnosed CAD was an independent predictor of mortality in a multivariable model adjusted for age, gender, cardiac risk factors, and CAC (p < 0.0001). The addition of CAC to CTA-diagnosed CAD increased the concordance index significantly (0.69 for risk factors, 0.83 for the CTA-diagnosed CAD, and 0.89 for the addition of CAC to CAD, p < 0.0001). Risk-adjusted hazard ratios for CTA-diagnosed CAD were 1.7-, 1.8-, 2.3-, and 2.6-fold for 3-vessel nonobstructive, 1-vessel obstructive, 2-vessel obstructive, and 3-vessel obstructive CAD, respectively (p < 0.0001), when compared with the group who did not have CAD.\n    \n\n\n          Conclusions:\n        \n      \n      The primary results of our study reveal that the burden of angiographic disease detected by CTA provides both independent and incremental value in predicting all-cause mortality in symptomatic patients independent of age, gender, conventional risk factors, and CAC."
        },
        {
            "title": "Observed versus indirect estimates of incidence of open-angle glaucoma.",
            "abstract": "Incidence data on open-angle glaucoma (OAG) are limited and difficult to obtain. To date, few studies have reported incidence directly measured from population-based cohorts. Other reported estimates have been derived indirectly from age-specific prevalence by using several assumptions, and their validity is unknown. To the authors' knowledge, this report presents the first comparison of observed versus indirect estimates of OAG incidence based on data from the population-based Barbados Incidence Study of Eye Diseases (1992-1997) (n = 3,427; 85% participation). The observed 4-year incidence of OAG was 1.2% (95% confidence interval (CI): 0.6, 2.1%) at ages 40-49 years, 1.5% (95% CI: 0.8, 2.5%) at ages 50-59 years, 3.2% (95% CI: 2.0, 4.8%) at ages 60-69 years, and 4.2% (95% CI: 2.6, 6.3%) in persons at ages 70 or more years. When incidence was calculated from the prevalence data, power function fitting achieved a closer approximation to observed incidence than did logistic curve fitting. Calculated incidence rates for each group were similar when assuming mortality that was equal (incidence rate = 0.7, 1.3, 2.3, and 4.8%) or differential (incidence rate = 0.7, 1.2, 2.4, and 4.8%). Other nonlogistic approaches also increased the resemblance of observed and calculated estimates. In the absence of longitudinal data, reasonably valid incidence estimates of OAG were obtained from available prevalence data. These estimation techniques can be useful when OAG incidence estimates are required for research or public health purposes."
        },
        {
            "title": "Relation of contrast-induced nephropathy to long-term mortality after percutaneous coronary intervention.",
            "abstract": "There is little information on the effect of contrast-induced nephropathy (CIN) on long-term mortality after percutaneous coronary intervention in patients with or without chronic kidney disease (CKD). Of 4,371 patients who had paired serum creatinine (SCr) measurements before and after percutaneous coronary intervention and were discharged alive in the Coronary REvascularization Demonstrating Outcome Study in Kyoto registry, the incidence of CIN (an increase in SCr of ≥0.5 mg/dl from the baseline) was 5% in our study cohort. The rate of CIN in patients with CKD was 11%, although it was 2% without CKD (p <0.0001). During a median follow-up of 42.3 months after discharge, 374 patients (8.6%) died. After adjustment for prespecified confounders, CIN was significantly correlated with long-term mortality in the entire cohort (hazard ratio [HR] 2.26, 95% confidence interval [CI] 1.62 to 2.29, p <0.0001) and in patients with CKD (HR 2.62, 95% CI 1.91 to 3.57, p <0.0001) but not in patients without CKD (HR 1.23, 95% CI 0.47 to 2.62, p = 0.6). Sensitivity analyses confirmed these results using the criteria defined as elevations of the SCr by ≥25% and 0.3 mg/dl from the baseline, respectively. In conclusion, CIN was significantly correlated with long-term mortality in patients with CKD but not in those without CKD."
        },
        {
            "title": "Risk of early recurrent stroke in symptomatic carotid stenosis.",
            "abstract": "Objectives:\n        \n      \n      The risk of recurrent stroke in patients with symptomatic carotid artery stenosis is highest in the first weeks after a transient ischemic attack (TIA) or minor stroke and can be reduced with carotid endarterectomy (CEA). The optimal timing of CEA remains a controversial issue since very urgent CEA is associated with an increased procedural risk. The aim of this study was to determine the risk of very early recurrent stroke in a population with symptomatic high grade carotid stenosis.\n    \n\n\n          Methods:\n        \n      \n      Data were analyzed on all patients with ocular TIA, TIA, or minor stroke with >70% carotid stenosis as assessed by carotid ultrasound at Sahlgrenska University Hospital during the periods 2004-2006 and 2010-2012. The two time periods were chosen to minimize selection bias and to analyze changes over time. The risk of recurrent stroke within 30 days of the referring event was assessed.\n    \n\n\n          Results:\n        \n      \n      397 patients with symptomatic carotid stenosis were identified. The risk of recurrent stroke in the total cohort was 2.0% (CI 95% 0.6-3.4) by day 2, 4.0% (CI 95% 2.0-5.9) by day 7, and 7.5% (CI 95% 4.4-10.6) by day 30. There was no significant difference between the two time periods. Patients with minor stroke had a significantly higher risk of recurrent stroke than patients with TIA or ocular TIA as the referring event.\n    \n\n\n          Conclusions:\n        \n      \n      The data suggest that the early risk of recurrent stroke in symptomatic significant carotid stenosis is not as high as some earlier studies have shown. The risk is similar to several studies in which a modern medical treatment regime could be assumed."
        },
        {
            "title": "Characterizing the risk profiles of intensive care units.",
            "abstract": "Objective:\n        \n      \n      To develop a new method to evaluate the performance of individual ICUs through the calculation and visualisation of risk profiles.\n    \n\n\n          Methods:\n        \n      \n      The study included 102,561 patients consecutively admitted to 77 ICUs in Austria. We customized the function which predicts hospital mortality (using SAPS II) for each ICU. We then compared the risks of hospital mortality resulting from this function with the risks which would be obtained using the original function. The derived risk ratio was then plotted together with point-wise confidence intervals in order to visualise the individual risk profile of each ICU over the whole spectrum of expected hospital mortality.\n    \n\n\n          Main measurements and results:\n        \n      \n      We calculated risk profiles for all ICUs in the ASDI data set according to the proposed method. We show examples how the clinical performance of ICUs may depend on the severity of illness of their patients. Both the distribution of the Hosmer-Lemeshow goodness-of-fit test statistics and the histogram of the corresponding P values demonstrated a good fit of the individual risk models.\n    \n\n\n          Conclusions:\n        \n      \n      Our risk profile model makes it possible to evaluate ICUs on the basis of the specific risk for patients to die compared to a reference sample over the whole spectrum of hospital mortality. Thus, ICUs at different levels of severity of illness can be directly compared, giving a clear advantage over the use of the conventional single point estimate of the overall observed-to-expected mortality ratio."
        },
        {
            "title": "Six-minute walk test distance and resting oxygen saturations but not functional class predict outcome in adult patients with Eisenmenger syndrome.",
            "abstract": "Background:\n        \n      \n      Eisenmenger syndrome (ES) represents the extreme manifestation of pulmonary arterial hypertension in patients with congenital heart disease, associated with significant exercise intolerance and mortality. Even though of six-minute-walk-test (6MWT) is routinely used in these patients, little is known about its prognostic value in comparison to functional class.\n    \n\n\n          Methods and results:\n        \n      \n      We included 210 adult patients with ES who underwent a total of 822 6MWTs. Median walking distance (6MWD) was 330 m [IQR 260-395], oxygen saturation (SO2) at baseline 86% [IQR 82-91%] and SO2 at peak-exercise 69% [IQR 60-80%]. In patients commenced on advanced therapy for pulmonary hypertension, but not in the reminder, there was a significant improvement in walking distance (297±97 m vs. 325±87 m,P=0.0019), SO2 at rest (84.9±7.1 vs. 86.8±5.9%,P=0.003), SO2 at peak exercise (69.1±12.7 vs. 72.3±12.2%,P=0.04) and NYHA functional class (P=0.0047). During a follow up of 3.3 years, 29 patients died. On time-dependent Cox analysis, 6MWD (HR 0.94 per 10 m, 95%CI: 0.91-0.97,P<0.001) and baseline SO2 (HR 0.90, 95%CI:0.86-0.94,P<0.0001) were predictors of death. In contrast, age, functional class, peak-exercise SO2 and SO2 change were not related to mortality. A three-fold increased risk of death was identified in patients not reaching a 6MWD of 350 m or with baseline SO2 below 85%.\n    \n\n\n          Conclusions:\n        \n      \n      The 6MWD and resting SO2, but not functional class were predictive of outcome in this contemporary cohort of Eisenmenger patients and should be incorporated in both risk stratification and management algorithms for these patients."
        },
        {
            "title": "Clinical and pathologic characteristics of biopsy-proven iris melanoma: a multicenter international study.",
            "abstract": "Objective:\n        \n      \n      To collaborate with multiple centers to identify representative epidemiological, clinical, and pathologic characteristics of melanoma of the iris. This international, multicenter, Internet-assisted study in ophthalmic oncology demonstrates the collaboration among eye cancer specialists to stage and describe the clinical and pathologic characteristics of biopsy-proven melanoma of the iris.\n    \n\n\n          Methods:\n        \n      \n      A computer program was created to allow for Internet-assisted multicenter, privacy-protected, online data entry. Eight eye cancer centers in 6 countries performed retrospective chart reviews. Statistical analysis included patient and tumor characteristics, ocular and angle abnormalities, management, histopathology, and outcomes.\n    \n\n\n          Results:\n        \n      \n      A total of 131 patients with iris melanoma (mean age, 64 years [range, 20-100 years]) were found to have blue-gray (62.2%), green-hazel (29.1%), or brown (8.7%) irides. Iris melanoma color was brown (65.6%), amelanotic (9.9%), and multicolored (6.9%). A mean of 2.5 clock hours of iris was visibly involved with melanoma, typically centered at the 6-o'clock meridian. Presentations included iritis, glaucoma, hyphema, and sector cataract. High-frequency ultrasonography revealed a largest mean tumor diameter of 4.9 mm, a mean maximum tumor thickness of 1.9 mm, angle blunting (52%), iris root disinsertion (9%), and posterior iris pigment epithelium displacement (9%). Using the American Joint Commission on Cancer-International Union Against Cancer classification, we identified 56% of tumors as T1, 34% of tumors as T2, 2% of tumors as T3, and 1% of tumors as T4. Histopathologic grades were G1-spindle (54%), G2-mixed (28%), G3-epithelioid (5%), and undetermined (13%) cell types. Primary treatment involved radiation (26%) and surgery (64%). Kaplan-Meier analysis found a 10.7% risk of metastatic melanoma at 5 years.\n    \n\n\n          Conclusions:\n        \n      \n      Iris melanomas were most likely to be brown and found in the inferior quadrants of patients with light irides. Typically small and unifocal, melanomas are commonly associated with angle blunting and spindle cell histopathology. This multicenter, Internet-based, international study successfully pooled data and extracted information on biopsy-proven melanoma of the iris."
        },
        {
            "title": "Serial MR imaging of intracranial metastases after radiosurgery.",
            "abstract": "Purpose:\n        \n      \n      To evaluate the spatiotemporal evolution of radiosurgical induced changes both in metastases and in normal brain tissue adjacent to the lesions by serial magnetic resonance (MR) imaging.\n    \n\n\n          Methods and materials:\n        \n      \n      Thirty-five intracranial metastases of different primaries were treated in 25 patients by single high-dose radiosurgery. MR images acquired before radiosurgery were available in all patients. Sixty-three follow-up MR studies were performed in these patients including T2- and contrast-enhanced T1-weighted MR images. The average follow-up time was 9 +/- 5 months (mean +/- standard deviation [SD]). Based on contrast-enhanced T1-weighted MR images, tumor response was radiologically classified in the following four groups: stable disease was assumed if the average tumor diameter after treatment did not show a tumor shrinkage of more than 50% and an increase of more than 25%, partial remission as a shrinkage of tumor size of more than 50%, a disappearance of contrast-enhancing tumor as a complete remission, and an increase of tumor diameter of more than 25% as tumor progress. Moreover, we analysed signal changes on T2-weighted images in brain parenchyma adjacent to the enhancing metastases.\n    \n\n\n          Results:\n        \n      \n      The overall mean survival time was 10.5 +/- 7 months, with a 1-year actuarial survival rate of 40%. Stable disease, partial or complete remission of the metastatic tumor was observed in 22 patients (88%). Central or homogeneous loss of contrast enhancement appeared to be a good prognostic sign for stable disease or partial remission. This association was statistically significant (p < 0.05). Three patients (12%) suffered from tumor progression. In eight patients (32%) with stable disease or partial remission, signal changes on T2-weighted images were observed in tissue adjacent to the contrast enhancing lesions. A progression of the high signal on T2-weighted images was seen in seven of the eight patients between 3 and 6 months after therapy, followed by a signal regression 6-18 months after irradiation.\n    \n\n\n          Conclusion:\n        \n      \n      MR imaging is a sensitive imaging tool to evaluate tumor response as well as the presence or absence of adjacent parenchymal changes following radiosurgery. Loss of homogeneous or central contrast enhancement on Gd-enhanced MR images appeared to be a good prognostic sign for tumor response. Tumor shrinkage seems not to be dependent on time. In addition, most cases of radiation induced changes in normal brain parenchyma observed on T2-weighted images seem to be self limited."
        },
        {
            "title": "Estimates of ocular and visual retention following treatment of extra-large uveal melanomas by proton beam radiotherapy.",
            "abstract": "Objective:\n        \n      \n      To assess outcomes of proton beam radiotherapy for the treatment of extra-large uveal melanomas in patients specifically referred to the University of California, San Francisco, for ocular conservation therapy. Series patients uniformly refused enucleation both at an outside institution and again as a treatment option after extensive discussion at the University of California, San Francisco.\n    \n\n\n          Design:\n        \n      \n      In a retrospective, nonrandomized cohort study, 21 patients with extra-large choroidal or ciliochoroidal melanomas measuring at least 10 mm in maximum thickness or 20 mm in maximum basal diameter or tumors located within 3 mm of the optic nerve measuring at least 8 mm in maximum thickness or 16 mm in maximum basal diameter met inclusion criteria. Main outcome measures were frequency of (1) anterior segment complications (lash loss, keratopathy, cataract, and neovascular glaucoma), (2) posterior segment complications (vitreous hemorrhage, radiation retinopathy, and radiation papillopathy), (3) treatment failure (tumor growth, enucleation, or metastases), and (4) final visual acuity.\n    \n\n\n          Results:\n        \n      \n      Median follow-up was 28 months. Mean age at treatment was 58.3 years. The frequencies of hypertension and diabetes mellitus were 14.3% and 9.5%, respectively. Mean tumor thickness and mean basal diameter were 8.6 mm and 18.7 mm, respectively. Lash loss occurred in 52.4%; dry eye, in 23.8%; cataract, in 28.6%; neovascular glaucoma, in 38.1% (100% in patients with diabetes mellitus); radiation retinopathy, in 9.5%; and radiation papillopathy, in 9.5%. No patient developed radiation-associated scleral necrosis or vitreous hemorrhage. The 2-year Kaplan-Meier estimate of local tumor growth after treatment was 33%, and the rate of distant metastasis was 10%. Visual acuity of 20/200 or better was preserved in 25% of patients, including 4 patients (19%) who experienced an average of 4 lines of Snellen visual acuity improvement. Development of neovascular glaucoma was associated with tumors in close proximity to the optic nerve (P = .04), while cataract (P = .03) and lash loss (P = .02) occurred with more anteriorly located tumors. Proton beam radiotherapy provided a 67% probability of local control and 90% probability of clinically discernible metastases-free survival at 24 months after treatment.\n    \n\n\n          Conclusion:\n        \n      \n      Proton beam radiotherapy is an ocular-conserving option that may be considered for the treatment of extra-large uveal melanoma in carefully selected patients."
        },
        {
            "title": "Evaluation of chemotherapy with magnetic resonance imaging in patients with regionally metastatic or unresectable bladder cancer.",
            "abstract": "Objectives:\n        \n      \n      To determine whether the failure of chemotherapy in patients with regionally metastatic or unresectable transitional cell carcinoma (TCC) of the bladder can be predicted early in the course of chemotherapy with magnetic resonance (MR) imaging.\n    \n\n\n          Methods:\n        \n      \n      In this prospective study, 36 patients with regionally metastatic or unresectable TCC of the urinary bladder underwent MR imaging before and after two, four, and six cycles of chemotherapy with Methotrexate, Vinblastine, Adriamycin (doxorubicin) and Cisplatin (MVAC). The response after two cycles of MVAC was evaluated by using conventional tumour size parameters with unenhanced MR imaging and with changes in the time to the start of tumour or lymph node enhanced at fast dynamic contrast-enhanced MR imaging. The results obtained with these techniques were compared with the findings at histopathology in cystectomy or transurethral resection specimens that were obtained after chemotherapy. Duration of survival was defined as the time from the start of chemotherapy until disease-specific death. Kaplan-Meier curves were drawn to determine the difference in prognosis between responders and nonresponders.\n    \n\n\n          Results:\n        \n      \n      After two cycles of chemotherapy, the accuracy, sensitivity, and specificity in distinguishing responders from nonresponders with conventional MR imaging were 69%, 81%, and 50%, respectively. With the fast dynamic contrast-enhanced technique, accuracy, sensitivity, and specificity were 92%, 91%, and 93% respectively. The median bladder cancer specific survival was 28 months for all patients studied. Responders to chemotherapy at fast dynamic contrast-enhanced MR had better median disease-specific survival than nonresponders (42 months vs. 12 months [p<0.0001]).\n    \n\n\n          Conclusion:\n        \n      \n      We can predict whether a patient will respond to chemotherapy after two cycles of chemotherapy with fast dynamic contrast-enhanced MR imaging."
        },
        {
            "title": "Radiological studies after laparoscopic Roux-en-Y gastric bypass: routine or selective?",
            "abstract": "Early detection of complications after laparoscopic Roux-en-Y gastric bypass (LRYGB) can be difficult because of the subtle clinical findings in obese patients. Consequently, routine postoperative upper gastrointestinal contrast studies (UGI) have been advocated for detection of leak from the gastrojejunostomy. The medical records of 368 consecutive patients undergoing LRYGB were analyzed to determine the efficacy of selective use of radiological studies after LRYGB. Forty-one patients (11%) developed signs suggestive of complications. Of the 41 symptomatic patients, two were explored urgently, 39 (10%) had radiological studies, and 16 of them (41%) were diagnosed with postoperative complications. Overall morbidity of the series was 4.8 per cent. Four patients (1.1%) developed a leak from the gastrojejunostomy and were correctly diagnosed by computerized tomography (CT). The sensitivity and specificity of CT in determining leak was 100 per cent, with positive and negative predictive value of 100 per cent. The mortality of the series was 0 per cent. No radiologic studies were performed in asymptomatic patients, and no complications developed in these patients. Our results show that selective radiological evaluation in patients with suspected complications after LRYGB is safe. High sensitivity makes CT the test of choice in patients with suspected complication after LRYGB. Routine radiological studies are not warranted."
        },
        {
            "title": "Added value of CT criteria compared to the clinical SAP score in patients with acute pancreatitis.",
            "abstract": "Background:\n        \n      \n      To assess the added value of established computed tomography (CT) scores versus the Simplified Acute Physiology (SAP) score in predicting outcome in patients with acute pancreatitis.\n    \n\n\n          Methods:\n        \n      \n      Contrast-enhanced CT was performed in 45 patients with acute pancreatitis. The Balthazar score, CT severity index (CTSI), and Schröder score were assessed, and the SAP score was calculated. The predictive values of CT score and SAP score for mortality, need for one or more interventions, and length of hospital stay were compared. The added value of the SAP score to the CT scores was assessed by using ROC (receiver operating curve) analysis.\n    \n\n\n          Results:\n        \n      \n      The positive predictive values of the higher Balthazar, CTSI, Schröder, and SAP scores, reflecting severe disease, were 50%, 41%, 41%, and 48%, respectively, for mortality, 85%, 84%, 84%, and 83%, respectively, for need for one or more interventions, and 55%, 66%, 66%, and 65%, respectively, for longer hospital stay. The negative predictive values of the lower Balthazar, CTSI, Schröder and SAP scores were 84%, 92%, 92%, and 42%, respectively, for mortality, 44%, 69%, 69%, and 45%, respectively, for need for one or more interventions, and 44%, 69%, 69%, and 55%, respectively, for longer hospital stay. When CT scores were added to the SAP score, there was no improvement in discriminating power for mortality.\n    \n\n\n          Conclusion:\n        \n      \n      To identify patients with severe outcome, there is no clear benefit using established CT scores as opposed to the SAP score. However, the Balthazar score and CTSI are better than the SAP score in predicting a favorable outcome."
        },
        {
            "title": "Sublingual capnometry: a new noninvasive measurement for diagnosis and quantitation of severity of circulatory shock.",
            "abstract": "Objective:\n        \n      \n      To investigate the feasibility and predictive value of sublingual Pco2 (P(SL)CO2) measurements as a noninvasive and early indicator of systemic perfusion failure.\n    \n\n\n          Design:\n        \n      \n      A prospective, criterion study.\n    \n\n\n          Setting:\n        \n      \n      Emergency department and medical and surgical intensive care units of an urban community medical center.\n    \n\n\n          Participants and patients:\n        \n      \n      Five normal human volunteers and 46 patients with acutely life-threatening illness or injuries.\n    \n\n\n          Interventions:\n        \n      \n      Intra-arterial or automated cuff blood pressure and arterial blood lactate (LAC) were measured concurrently with P(SL)CO2.\n    \n\n\n          Results:\n        \n      \n      P(SL)CO2 in five healthy volunteers was 45.2 +/- 0.7 mm Hg (mean +/- sD). Twenty-six patients with physical signs of circulatory shock and LAC >2.5 mmol/L had a P(SL)CO2 of 81 +/- 24 mm Hg. This contrasted with patients admitted without clinical signs of shock and LAC of <2.5 mmol/L who had a P(SL)CO2 of 53 +/- 8 mm Hg (p < .001). The initial P(SL)CO2 of 12 patients who died before recovery from shock was 93 +/- 27 mm Hg, and this contrasted with 58 +/- 11 mm Hg (p < .001) in hospital survivors. Increases in P(SL)CO2 were correlated with increases in LAC (r2 = .84; p < .001). When P(SL)CO2 exceeded a threshold of 70 mm Hg, its positive predictive value for the presence of physical signs of circulatory shock was 1.00. When it was <70 mm Hg, it predicted survival with a predictive value of 0.93.\n    \n\n\n          Conclusion:\n        \n      \n      P(SL)CO2 may serve as a technically simple and noninvasive clinical measurement for the diagnosis and estimation of the severity of circulatory shock states."
        },
        {
            "title": "Diabetes mellitus: long-term prognostic value of whole-body MR imaging for the occurrence of cardiac and cerebrovascular events.",
            "abstract": "Purpose:\n        \n      \n      To study the predictive value of whole-body magnetic resonance (MR) imaging for the occurrence of cardiac and cerebrovascular events in a cohort of patients with diabetes mellitus (DM).\n    \n\n\n          Materials and methods:\n        \n      \n      This HIPAA-compliant study was approved by the institutional review board. Informed consent was obtained from all patients before enrollment into the study. The authors followed up 65 patients with DM (types 1 and 2) who underwent a comprehensive, contrast material-enhanced whole-body MR imaging protocol, including brain, cardiac, and vascular sequences at baseline. Follow-up was performed by phone interview. The primary endpoint was a major adverse cardiac and cerebrovascular event (MACCE), which was defined as composite cardiac-cerebrovascular death, myocardial infarction, cerebrovascular event, or revascularization. MR images were assessed for the presence of systemic atherosclerotic vessel changes, white matter lesions, and myocardial changes. Kaplan-Meier survival and Cox regression analyses were performed to determine associations.\n    \n\n\n          Results:\n        \n      \n      Follow-up was completed in 61 patients (94%; median age, 67.5 years; 30 women [49%]; median follow-up, 70 months); 14 of the 61 patients (23%) experienced MACCE. Although normal whole-body MR imaging excluded MACCE during the follow-up period (0%; 95% confidence interval [CI]: 0%, 17%), any detectable ischemic and/or atherosclerotic changes at whole-body MR imaging (prevalence, 66%) conferred a cumulative event rate of 20% at 3 years and 35% at 6 years. Whole-body MR imaging summary estimate of disease was strongly predictive for MACCE (one increment of vessel score and each territory with atherosclerotic changes: hazard ratio, 13.2 [95% CI: 4.5, 40.1] and 3.9 [95% CI: 2.2, 7.5], respectively), also beyond clinical characteristics as well as individual cardiac or cerebrovascular MR findings.\n    \n\n\n          Conclusion:\n        \n      \n      These initial data indicate that disease burden as assessed with whole-body MR imaging confers strong prognostic information in patients with DM. Online supplemental material is available for this article."
        },
        {
            "title": "Excess mortality associated with blindness in the onchocerciasis focus of the Mbam Valley, Cameroon.",
            "abstract": "The association between blindness, mortality and nutritional status was investigated in a retrospective cohort study in villages of central Cameroon where onchocerciasis is hyper-endemic. Overall, 101 blind subjects and 101 non-blind controls matched with the blind for age, sex and (generally) village of residence were followed for 10 years. Blindness gave rise to a significant increase in mortality (relative risk = 2.3; P = 0.012), the life expectancy of the blind adults being reduced by 4 years compared with that of their controls. For a given age, excess mortality was found to be associated with a late onset of blindness. The causes of death were similar for the blind and the controls but blind subjects had relatively low body mass indices, which may lead to relatively early fatal disease outcomes. These results are similar to those obtained in other parts of Africa and emphasise, once more, the demographic impact of blindness in developing countries."
        },
        {
            "title": "Contrast extravasation on CT angiography predicts hematoma expansion and mortality in acute traumatic subdural hemorrhage.",
            "abstract": "Background and purpose:\n        \n      \n      The presence of active contrast extravasation at CTA predicts hematoma expansion and in-hospital mortality in patients with nontraumatic intracerebral hemorrhage. This study aims to determine the frequency and predictive value of the contrast extravasation in patients with aSDH.\n    \n\n\n          Materials and methods:\n        \n      \n      We retrospectively reviewed 157 consecutive patients who presented to our emergency department over a 9-year period with aSDH and underwent CTA at admission and a follow-up NCCT within 48 hours. Two experienced readers, blinded to clinical data, reviewed the CTAs to assess for the presence of contrast extravasation. Medical records were reviewed for baseline clinical characteristics and in-hospital mortality. aSDH maximum width in the axial plane was measured on both baseline and follow-up NCCTs, with hematoma expansion defined as >20% increase from baseline.\n    \n\n\n          Results:\n        \n      \n      Active contrast extravasation was identified in 30 of 199 discrete aSDHs (15.1%), with excellent interobserver agreement (κ = 0.80; 95% CI, 0.7-0.9). The presence of contrast extravasation indicated a significantly increased risk of hematoma expansion (odds ratio, 4.5; 95% CI, 2.0-10.1; P = .0001) and in-hospital mortality (odds ratio, 7.6; 95% CI, 2.6-22.3; P = 0.0004). In a multivariate analysis controlled for standard risk factors, the presence of contrast extravasation was an independent predictor of aSDH expansion (P = .001) and in-hospital mortality (P = .0003).\n    \n\n\n          Conclusions:\n        \n      \n      Contrast extravasation stratifies patients with aSDH into those at high risk and those at low risk of hematoma expansion and in-hospital mortality. This distinction could affect patient treatment, clinical trial selection, and possible surgical intervention."
        },
        {
            "title": "Intravitreal bevacizumab and ranibizumab for age-related macular degeneration a multicenter, retrospective study.",
            "abstract": "Objective:\n        \n      \n      To compare visual acuity (VA) outcomes after bevacizumab or ranibizumab treatment for AMD.\n    \n\n\n          Design:\n        \n      \n      Comparative, retrospective case series.\n    \n\n\n          Participants:\n        \n      \n      We followed 452 patients in a retrospective study of exudative AMD treated with anti-vascular endothelial growth factor drugs; 324 patients were treated with bevacizumab and 128 patients with ranibizumab.\n    \n\n\n          Methods:\n        \n      \n      All treatment-naïve patients who received either bevacizumab or ranibizumab were followed for 1 year. Baseline characteristics and VA were recorded using standard descriptive statistics.\n    \n\n\n          Main outcome measures:\n        \n      \n      Visual acuity.\n    \n\n\n          Results:\n        \n      \n      At 12 months, the distribution of VA improved in both groups with 22.9% of bevacizumab and 25.0% of ranibizumab attaining >or=20/40. Improvement in vision was observed in 27.3% of the bevacizumab group and 20.2% of the ranibizumab group. The mean number of injections at 12 months was 4.4 for bevacizumab and 6.2 for ranibizumab. There were 8 (2%) deaths in the bevacizumab group and 4 (3%) in the ranibizumab group. Two patients developed endophthalmitis in the bevacizumab group and the ranibizumab group. The bevacizumab group had slightly worse acuity at baseline, but both groups showed improvement and stability of vision over time.\n    \n\n\n          Conclusions:\n        \n      \n      Both treatments seem to be effective in stabilizing VA loss. There was no difference in VA outcome between the 2 treatment groups. Because the study is a nonrandomized comparison, selection bias could mask a true treatment difference. Results from the Comparison of the Age-related Macular Degeneration Treatment Trials will provide more definitive information about the comparative effectiveness of these drugs."
        },
        {
            "title": "Carotid endarterectomy: operative risks, recurrent stenosis, and long-term stroke rates in a modern series.",
            "abstract": "To determine whether carotid endarterectomy (CEA) safely and effectively maintained a durable reduction in stroke complications over an extended period, we reviewed our data on 478 consecutive patients who underwent 544 CEA's since 1976. Follow-up was complete in 83% of patients (mean 44 months). There were 7 early deaths (1.3%), only 1 stroke related (0.2%). Perioperative stroke rates (overall 2.9%) varied according to operative indications: asymptomatic, 1.4%; transient ischemic attacks (TIA)/amaurosis fugax (AF), 1.3%; nonhemispheric symptoms (NH), 4.9%; and prior stroke (CVA), 7.1%. Five and 10-year stroke-free rates were 96% and 92% in the asymptomatic group, 93% and 87% in the TIA/AF group, 92% and 92% in the NH group, and 80% and 73% in the CVA group. Late ipsilateral strokes occurred infrequently (8 patients, 1.7%). Late deaths were primarily cardiac related (51.3%). Stroke-free rates were significantly (p less than 0.0001) greater than stroke-free survival rates, confirming a non-stroke related cause for late death. Restenoses greater than 50% according to duplex scanning developed in 13%, most (67%) within 2 years after CEA. Most of these (77%) were asymptomatic, and only 0.3% (1 patient) presented with a permanent neurologic deficit. The results of carotid endarterectomy are superior to those of optimal medical management in symptomatic and asymptomatic patients in terms of long-term stroke prevention. When low perioperative stroke mortality/morbidity rates are achieved, carotid endarterectomy is justified for treatment of patients with carotid bifurcation disease."
        },
        {
            "title": "Evaluation of the ECG based Selvester scoring method to estimate myocardial scar burden and predict clinical outcome in patients with left bundle branch block, with comparison to late gadolinium enhancement CMR imaging.",
            "abstract": "Background:\n        \n      \n      Myocardial scar burden quantification is an emerging clinical parameter for risk stratification of sudden cardiac death and prediction of ventricular arrhythmias in patients with left ventricular dysfunction. We investigated the relationships among semiautomated Selvester score burden and late gadolinium enhancement-cardiovascular magnetic resonance (LGE-CMR) assessed scar burden and clinical outcome in patients with underlying heart failure, left bundle branch block (LBBB) and implantable cardioverter-defibrillator (ICD) treatment.\n    \n\n\n          Methods:\n        \n      \n      Selvester QRS scoring was performed on all subjects with ischemic and nonischemic dilated cardiomyopathy at Skåne University Hospital Lund (2002-2013) who had undergone LGE-CMR and 12-lead ECG with strict LBBB pre-ICD implantation.\n    \n\n\n          Results:\n        \n      \n      Sixty patients were included; 57% nonischemic dilated cardiomyopathy and 43% ischemic cardiomyopathy with mean left ventricular ejection fraction of 27.6% ± 11.7. All patients had scar by Selvester scoring. Sixty-two percent had scar by LGE-CMR (n = 37). The Spearman correlation coefficient for LGE-CMR and Selvester score derived scar was r = .35 (p = .007). In scar negative LGE-CMR, there was evidence of scar by Selvester scoring in all patients (range 3%-33%, median 15%). Fourteen patients (23%) had an event during the follow-up period; 11 (18%) deaths and six adequate therapies (10%). There was a moderate trend indicating that presence of scar increased the risk of clinical endpoints in the LGE-CMR analysis (p = .045).\n    \n\n\n          Conclusion:\n        \n      \n      There is a modest correlation between LGE-CMR and Selvester scoring verified myocardial scar. CMR based scar burden is correlated to clinical outcome, but Selvester scoring is not. The Selvester scoring algorithm needs to be further refined in order to be clinically relevant and reliable for detailed scar evaluation in patients with LBBB."
        },
        {
            "title": "The use of the NHS Central Register when estimating patient survival.",
            "abstract": "National and international comparisons of survival of patients are hampered by the bias introduced when different sources of data are used to confirm a patient's continuing survival. Three methods are proposed for processing survival data which, when used in conjunction, will provide limits for the extent of this bias. Using data on heart valve replacement patients it is shown that the use of the United Kingdom National Health Service Central Register, supplemented by data from routine hospital clinic visits, will lead to a maximum overestimate of survival of 1.7% over 10 years, and in practice the overestimate will be much less. It is proposed that for publications two survival curves should be calculated using the most favorable and unfavorable assumptions about patients \"lost to sight\". This will provide upper and lower bounds to the survival, the difference between these curves being dependent on the quantity of missing data."
        },
        {
            "title": "Planning for a possible national colorectal cancer screening programme.",
            "abstract": "This report presents the planning, projected costs, and manpower requirements for a possible national colorectal cancer screening programme. Screening would be offered to all those aged 50-69, who comprise 20% of the United Kingdom population. The initial screening test would be faecal occult blood testing every two years. A local programme, administered by a screening centre serving a population of one million, would be responsible for inviting 100,000 subjects a year. The response rate in Nottingham, the UK trial centre, was below 60%. Good informed compliance would require the active support of primary care. The invitation and test kit would be sent by post, and completed tests returned to the screening centre, for reading and reporting. Those with a positive initial screen (about 2%) would be recalled for assessment. This would result in 60,000 investigations each year across England and Wales, given a screening uptake rate of 60%. Clearly any deviation from this predicted rate would have a major effect on resources. Assessment and any subsequent treatment would be by a multidisciplinary team working at the cancer unit, as recommended in recent NHS executive guidance. The best method for investigation is colonoscopy. When completed successfully this allows visualisation of the whole bowel. However, performance varies widely across the UK, and there is insufficient skilled manpower to undertake this additional workload. Most significantly the technique has a mortality rate of 0.02%, so the programme might expect 12 deaths a year, which would not be acceptable. Alternatively, assessment of screen positive cases could be by a combination of double contrast barium enema and flexible sigmoidoscopy, with a comparable sensitivity. Both procedures have much lower morbidity and mortality rates. Colonoscopy would then only be required for a smaller number of patients, with cancer or suspicious lesions, or after unsatisfactory investigations. Quality assurance should be an integral part of the programme, as in the other NHS cancer screening programmes, involving all professional groups and coordinated by a regional quality assurance reference centre. Cost estimates are over 40 million Pound a year, together with any allowance for general practitioners, with additional capital and training costs at the start of the programme. Given a 60% overall uptake rate, a test sensitivity of 60%, and a recall rate of 2%, about 35% of the cases of colorectal cancer in the eligible population--that is, about 5400 cases, could be detected each year. As this would also depend on maintaining good compliance, a continuing value of 4000 cases is more realistic. Appreciable savings on costs of treatment are unlikely as aggressive curative treatments would be expensive."
        },
        {
            "title": "Glycemic index, retinal vascular caliber, and stroke mortality.",
            "abstract": "Background and purpose:\n        \n      \n      It is unclear whether diets with high glycemic index (GI) and low cereal fiber (CF) are associated with greater risk of stroke. We aimed to assess the relationship between dietary GI and CF content, retinal microvasculature changes, and stroke-related mortality.\n    \n\n\n          Methods:\n        \n      \n      The study consisted of a population-based cohort, 49+ years, examined at baseline (1992 to 1994). At baseline, participants completed validated food frequency questionnaires. Mean GI was calculated using an Australian database. Retinal arteriolar and venular diameters were measured from photographs. Mortality data were derived using the Australian National Death Index.\n    \n\n\n          Results:\n        \n      \n      Over 13 years, 95 of 2897 participants (3.5%) died from stroke. Increasing GI (hazard ratio, 1.91; 95% CI, 1.01 to 3.47, highest versus lowest tertile) and decreasing CF (hazard ratio, 2.13; 95% CI, 1.19 to 3.80, lowest versus highest tertile) predicted greater risk of stroke death adjusting for multiple stroke risk factors. Persons consuming food in the highest GI tertile and lowest CF tertile had a 5-fold increased risk of stroke death (hazard ratio, 5.06; 95% CI, 1.67 to 15.22). Increasing GI and decreasing CF were also associated with retinal venular caliber widening (P(trend)<0.01). Adjustment for retinal venular caliber attenuated stroke death risk associated with high GI by 50% but did not affect the risk associated with low CF consumption.\n    \n\n\n          Conclusions:\n        \n      \n      High-GI and low-CF diets predict greater stroke mortality and wider retinal venular caliber. The association between a high-GI diet and stroke death was partly explained by GI effects on retinal venular caliber, suggesting that a high-GI diet may produce deleterious anatomic changes in the microvasculature."
        },
        {
            "title": "Clinical and prognostic significance of parotid scintigraphy in 405 patients with primary Sjogren's syndrome.",
            "abstract": "Objective:\n        \n      \n      To evaluate the association between the degree of involvement shown by parotid scintigraphy at diagnosis and the disease expression, outcomes, and prognosis of primary Sjögren's syndrome (SS).\n    \n\n\n          Methods:\n        \n      \n      All patients consecutively diagnosed with primary SS in our department between 1984 and 2008 were evaluated. The scintigraphic stages were classified into class 4 (severe involvement), class 2-3 (mild to moderate involvement), and class 1 (normal results).\n    \n\n\n          Results:\n        \n      \n      A total of 405 patients with primary SS underwent parotid scintigraphy at diagnosis (47 had class 1 involvement, 314 had class 2-3, and 44 had class 4). Patients with class 4 had a higher frequency of parotid enlargement (p < 0.001), systemic involvement (p = 0.007), high titers of antinuclear antibody (p = 0.016), positive rheumatoid factor (p = 0.002), anti-Ro/SSA (p = 0.001), anti-La/SSB (p = 0.001), low C4 levels (p = 0.001), and low CH50 (p = 0.001) in comparison with the other 2 groups. A higher rate of lymphoma development was observed in patients with class 4 involvement. Adjusted multivariate Cox regression analysis showed a hazard ratio (HR) of 10.51 (p = 0.002) and Kaplan-Meier analysis a log-rank of 0.0005. Mortality was 5-fold higher in patients with class 4 involvement. Adjusted multivariate Cox regression analysis showed an HR of 5.33 (p = 0.001) and Kaplan-Meier analysis a log-rank of 0.033.\n    \n\n\n          Conclusion:\n        \n      \n      Patients with SS presenting with severe scintigraphic involvement at diagnosis had a more pronounced autoimmune expression, a higher risk of developing systemic features and lymphoma, and a lower survival rate. Study of the degree of salivary gland dysfunction at diagnosis by parotid scintigraphy offers valuable clinical information on the prognosis and outcome of primary SS."
        },
        {
            "title": "Risk factors for five-year incident age-related macular degeneration: the Reykjavik Eye Study.",
            "abstract": "Purpose:\n        \n      \n      To establish risk factors for five-year incidence of age-related macular degeneration (AMD).\n    \n\n\n          Design:\n        \n      \n      Population-based, prospective cohort study, and risk analysis.\n    \n\n\n          Methods:\n        \n      \n      A random sample from the Reykjavik Population Census for individuals 50 years and older was selected. We took fundus stereo color photographs and used standard grading system to study the five-year incidence of drusen, pigmentary abnormalities, and AMD and to examine possible risk factors. A questionnaire including information on disease, medication, diet, and lifestyle from the Reykjavik Eye Study database provided additional information.\n    \n\n\n          Results:\n        \n      \n      Current alcohol consumption decreased the risk for drusen. Being married rather than divorced or widowed decreased the risk for soft drusen; being single decreased the risk of hypopigmentation as compared with being divorced or married. Both consuming dietary fiber-rich vegetables and meat and meat products once a week or less frequently was a risk factor for developing soft drusen and decreased the risk of pigmentary abnormalities. Those who had smoked 20 pack-years or more as compared with nonsmokers had decreased survival rate over the five years (odds ratio (OR) 0.46, 95% confidence interval (CI) 0.27 to 0.80; P = .006).\n    \n\n\n          Conclusions:\n        \n      \n      Risk factors for drusen appear to differ from risk factors for pigmentary abnormalities. The effect of smoking on developing AMD is partly masked by selective mortality."
        },
        {
            "title": "Effects of changes in self-reported vision on cognitive, affective, and functional status and living arrangements among the elderly.",
            "abstract": "Purpose:\n        \n      \n      To study effects of changes in self-reported vision on functional status, cognition, depressive symptoms, and living arrangements.\n    \n\n\n          Design:\n        \n      \n      Longitudinal analysis of household survey data.\n    \n\n\n          Methods:\n        \n      \n      A total of 6234 sample persons observed in the study of Assets and Health Dynamics Among the Oldest Old (AHEAD) 1995 were followed in 1998, 2000, and 2002 or until death or sample attrition. Effects of changes in self-reported vision and other factors were assessed by means of ordinary least-squares and logistic regression with panel data methods. Main outcome measures were limitations of instrumental activities of daily living (IADLs), activities of daily living (ADLs), and other, cognition, depressive symptoms, and living arrangements.\n    \n\n\n          Results:\n        \n      \n      A decline from excellent/good vision to fair/poor near and distance vision had statistically significant effects on several IADL limitations, and some ADL and other limitations. Largest effects were for driving (OR for no limitation: 0.55, P = .003), managing money (OR: 0.61, P < .001), and preparing hot meals (OR: 0.61, P < .001). Onset of fair-poor near vision increased the likelihood of onset of at least one IADL (OR for no limitation: 0.71, P < .01) and ADL (OR: 0.74, P = .003) limitation. Onset of legal blindness resulted in a 78% increase in the likelihood of an IADL limitation (OR for no limitation: 0.22, P < .001). Effects of vision declines on cognition and depressive symptoms were statistically significant but small. Decline in vision increased the probability of nursing home residence.\n    \n\n\n          Conclusions:\n        \n      \n      Visual impairment has major impacts on functional status. Preventing vision loss is likely to appreciably improve the functioning of elderly persons."
        },
        {
            "title": "Mortality after endophthalmitis following contemporary phacoemulsification cataract surgery.",
            "abstract": "Importance:\n        \n      \n      To determine if endophthalmitis following cataract surgery is linked to increased mortality.\n    \n\n\n          Background:\n        \n      \n      Increased mortality has been linked to patients with cataract and cataract surgery. We tested the hypothesis that post-cataract endophthalmitis has a greater risk of death than pseudophakes who do not develop this complication.\n    \n\n\n          Design:\n        \n      \n      Case-control study conducted in a tertiary public hospital.\n    \n\n\n          Participants:\n        \n      \n      The study group comprised 50 consecutive patients with post-cataract endophthalmitis, and these were matched with selected controls.\n    \n\n\n          Methods:\n        \n      \n      Patients with endophthalmitis following cataract surgery were identified from a prospective electronic surgical database. Subsequently, it was determined if the patient was deceased at the time of sequestration (September 2015), and the date of death was recorded. A previously described population who had undergone cataract surgery in the same facility was selected as a control group, and the population was case-matched in terms age, gender, presence or absence of diabetes and/or hypertension.\n    \n\n\n          Main outcome measures:\n        \n      \n      The median survival rates were determined for the control group and the patients with post-cataract endophthalmitis.\n    \n\n\n          Results:\n        \n      \n      Fifty patients were identified as undergoing endophthalmitis post-cataract surgery, and 48 (n = 48) met inclusion criteria (mean age 72 years ±12 SD with 30:18 F:M); 17% were diabetic, and 50% had systemic hypertension. No statistically significant difference in median survival between the study and control cases was identified (100 months (95% confidence interval 86-114) vs. 106 months (95% confidence interval 66-146), respectively, P = 0.756).\n    \n\n\n          Conclusions and relevance:\n        \n      \n      Post-cataract endophthalmitis was not associated with an increased rate of mortality in this study."
        },
        {
            "title": "Impairments in hearing and vision impact on mortality in older people: the AGES-Reykjavik Study.",
            "abstract": "Objective:\n        \n      \n      to examine the relationships between impairments in hearing and vision and mortality from all-causes and cardiovascular disease (CVD) among older people.\n    \n\n\n          Design:\n        \n      \n      population-based cohort study.\n    \n\n\n          Participants:\n        \n      \n      the study population included 4,926 Icelandic individuals, aged ≥67 years, 43.4% male, who completed vision and hearing examinations between 2002 and 2006 in the Age, Gene/Environment Susceptibility-Reykjavik Study (AGES-RS) and were followed prospectively for mortality through 2009.\n    \n\n\n          Methods:\n        \n      \n      participants were classified as having 'moderate or greater' degree of impairment for vision only (VI), hearing only (HI), and both vision and hearing (dual sensory impairment, DSI). Cox proportional hazard regression, with age as the time scale, was used to calculate hazard ratios (HR) associated with impairment and mortality due to all-causes and specifically CVD after a median follow-up of 5.3 years.\n    \n\n\n          Results:\n        \n      \n      the prevalence of HI, VI and DSI were 25.4, 9.2 and 7.0%, respectively. After adjusting for age, significantly (P < 0.01) increased mortality from all causes, and CVD was observed for HI and DSI, especially among men. After further adjustment for established mortality risk factors, people with HI remained at higher risk for CVD mortality [HR: 1.70 (1.27-2.27)], whereas people with DSI remained at higher risk of all-cause mortality [HR: 1.43 (1.11-1.85)] and CVD mortality [HR: 1.78 (1.18-2.69)]. Mortality rates were significantly higher in men with HI and DSI and were elevated, although not significantly, among women with HI.\n    \n\n\n          Conclusions:\n        \n      \n      older men with HI or DSI had a greater risk of dying from any cause and particularly cardiovascular causes within a median 5-year follow-up. Women with hearing impairment had a non-significantly elevated risk. Vision impairment alone was not associated with increased mortality."
        },
        {
            "title": "The role of magnetic resonance imaging in screening women at high risk of breast cancer.",
            "abstract": "Most women at very high risk of breast cancer because of a mutation in the genes BRCA1 or BRCA2, or a very strong family history of breast cancer, opt for intensive breast screening rather than bilateral prophylactic mastectomy. Annual screening mammography has low sensitivity in this population in part because of the greater breast density and faster tumor growth of younger women, resulting in cancers being detected at a suboptimal stage. In 11 prospective comparative studies, the addition of annual contrast-enhanced magnetic resonance imaging (MRI) of the breast to mammography demonstrated more than 90% sensitivity, more than twice that of mammography alone. False-positive rates were higher with the addition of MRI, but specificity improved on successive rounds of screening. Although survival data are not yet available, the stage distribution of these tumors predicts a significant reduction in breast cancer mortality rate compared with that of screening without MRI. Accordingly, annual MRI plus mammography is now the standard of care for screening women aged 30 years or older who are known or likely to have inherited a strong predisposition to breast cancer (based on the above evidence) and for women who received radiation therapy to the chest before the age of 30 years (based on expert opinion). Further research is necessary to define the optimal screening schedule for different subgroups. Formal studies of other high-risk populations (eg, biopsy showing lobular neoplasia or atypical ductal hyperplasia, dense breasts, and personal history of breast cancer at a young age) should be done before MRI screening is routinely adopted for these women."
        },
        {
            "title": "Age-related eye disease, visual impairment, and survival: the Beaver Dam Eye Study.",
            "abstract": "## OBJECTIVE\nTo investigate the relationship of age-related maculopathy, cataract, glaucoma, visual impairment, and diabetic retinopathy to survival during a 14-year period.\n## METHODS\nPersons ranging in age from 43 to 84 years in the period from September 15, 1987, to May 4, 1988, participated in the baseline examination of the population-based Beaver Dam Eye Study (n = 4926). Standardized protocols, including photography, were used to determine the presence of ocular disease. Survival was followed using standardized protocols.\n## RESULTS\nAs of December 31, 2002, 32% of the baseline population had died (median follow-up, 13.2 years). After adjusting for age, sex, and systemic and lifestyle factors, poorer survival was associated with cortical cataract (hazard ratio [HR], 1.21; 95% confidence interval [CI], 1.06-1.37), any cataract (HR, 1.16; 95% CI, 1.03-1.32), diabetic retinopathy (HR per 1-step increase in 4-level severity, 1.36; 95% CI, 1.14-1.63), and visual impairment (HR, 1.24; 95% CI, 1.04-1.48) and marginally associated with increasing severity of nuclear sclerosis (HR, 1.07; 95% CI, 0.99-1.16). Age-related maculopathy and glaucoma were not associated with poorer survival. Associations tended to be slightly stronger in men than women.\n## CONCLUSIONS\nCataract, diabetic retinopathy, and visual impairment were associated with poorer survival and not explained by traditional risk factors for mortality. These ocular conditions may serve as markers for mortality in the general population.\n"
        },
        {
            "title": "Long-term survival in patients undergoing percutaneous interventions with or without intracoronary pressure wire guidance or intracoronary ultrasonographic imaging: a large cohort study.",
            "abstract": "Importance:\n        \n      \n      Intracoronary pressure wire-derived measurements of fractional flow reserve (FFR) and intravascular ultrasonography (IVUS) provide functional and anatomical information that can be used to guide coronary stent implantation. Although these devices are widely used and recommended by guidelines, limited data exist about their effect on clinical end points.\n    \n\n\n          Objective:\n        \n      \n      To determine the effect on long-term survival of using FFR and IVUS during percutaneous coronary intervention (PCI).\n    \n\n\n          Design and setting:\n        \n      \n      Cohort study based on the pan-London (United Kingdom) PCI registry. In total, 64,232 patients are included in this registry covering the London, England, area.\n    \n\n\n          Participants:\n        \n      \n      All patients (n = 41,688) who underwent elective or urgent PCI in National Health Service hospitals in London between January 1, 2004, and July 31, 2011, were included. Patients with ST-segment elevation myocardial infarction (n = 11,370) were excluded.\n    \n\n\n          Interventions:\n        \n      \n      Patients underwent PCI guided by angiography (visual lesion assessment) alone, PCI guided by FFR, or IVUS-guided PCI.\n    \n\n\n          Main outcomes and measures:\n        \n      \n      The primary end point was all-cause mortality at a median of 3.3 years.\n    \n\n\n          Results:\n        \n      \n      Fractional flow reserve was used in 2767 patients (6.6%) and IVUS was used in 1831 patients (4.4%). No difference in mortality was observed between patients who underwent angiography-guided PCI compared with patients who underwent FFR-guided PCI (hazard ratio, 0.88; 95% CI, 0.67-1.16; P = .37). Patients who underwent IVUS had a slightly higher adjusted mortality (hazard ratio, 1.39; 95% CI, 1.09-1.78; P = .009) compared with patients who underwent angiography-guided PCI. However, this difference was no longer statistically significant in a propensity score-based analysis (hazard ratio, 1.33; 95% CI, 0.85-2.09; P = .25). The mean (SD) number of implanted stents was lower in the FFR group (1.1 [1.2] stents) compared with the IVUS group (1.6 [1.3]) and the angiography-guided group (1.7 [1.1]) (P < .001).\n    \n\n\n          Conclusions and relevance:\n        \n      \n      In this large observational study, FFR-guided PCI and IVUS-guided PCI were not associated with improved long-term survival compared with standard angiography-guided PCI. The use of FFR was associated with the implantation of fewer stents."
        }
    ]
}