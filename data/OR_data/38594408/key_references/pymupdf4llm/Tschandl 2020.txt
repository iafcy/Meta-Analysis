Letters
https://doi.org/10.1038/s41591-020-0942-0
Human–computer collaboration for skin cancer
recognition
Philipp Tschandl 1,17, Christoph Rinner 2,17, Zoe Apalla3, Giuseppe Argenziano 4, Noel Codella5,
Allan Halpern6, Monika Janda7, Aimilios Lallas3, Caterina Longo8,9, Josep Malvehy10,11, John Paoli12,13,
Susana Puig10,11, Cliff Rosendahl14, H. Peter Soyer 15, Iris Zalaudek16 and Harald Kittler 1 ✉
The rapid increase in telemedicine coupled with recent competitive view of AI is evolving based on studies suggesting that
advances in diagnostic artificial intelligence (AI) create the a more promising approach is human–AI cooperation10–15. The
imperative to consider the opportunities and risks of insert- role of human–computer collaboration in health-care delivery, the
ing AI-based support into new paradigms of care. Here we appropriate settings in which it can be applied and its impact on the
build on recent achievements in the accuracy of image-based quality of care have yet to be evaluated16. To this end, we studied
AI for skin cancer diagnosis to address the effects of varied the use case of skin cancer diagnosis to address the effects of varied
representations of AI-based support across different levels of representations of AI-based support across different levels of clini-
clinical expertise and multiple clinical workflows. We find that cal expertise and multiple clinical workflows.
good quality AI-based support of clinical decision-making To explore the impact of different representations of current
improves diagnostic accuracy over that of either AI or physi- state-of-the-art AI on diagnostic accuracy of clinicians in different
cians alone, and that the least experienced clinicians gain the scenarios, we first trained a 34-layer residual network (ResNet34), a
most from AI-based support. We further find that AI-based particular type of convolutional neural network (CNN), on the train-
multiclass probabilities outperformed content-based image ing dataset of a publicly available image benchmark of pigmented
retrieval (CBIR) representations of AI in the mobile technol- lesions containing seven diagnostic categories, including malignant
ogy environment, and AI-based support had utility in simula- (melanomas (MELs), basal cell carcinomas (BCCs) and actinic
tions of second opinions and of telemedicine triage. In addition keratoses and intraepithelial carcinomas (AKIECs)) and benign
to demonstrating the potential benefits associated with good (melanocytic nevi (NVs), benign keratinocytic lesions (BKLs), der-
quality AI in the hands of non-expert clinicians, we find that matofibromas (DFs) and vascular lesions (VASCs)) proliferations17.
faulty AI can mislead the entire spectrum of clinicians, includ- When tested on the corresponding publicly available benchmark
ing experts. Lastly, we show that insights derived from AI test set, the mean recall of our CNN across all disease categories
class-activation maps can inform improvements in human was 77.7% (95% confidence interval (CI) 70.3% to 85.1%), and the
diagnosis. Together, our approach and findings offer a frame- accuracy was 80.3%. When compared with the results of a recently
work for future studies across the spectrum of image-based published reader study, this CNN outperforms most human rat-
diagnostics to improve human–computer collaboration in clin- ers and ranks in the top quartile of machine-learning algorithms
ical practice. that were developed and tested with the same image dataset18. To
Image-based AI has the potential to improve visual diagnostic examine whether human–computer collaboration is influenced by
accuracy. Limited physical access to health-care providers dur- the way that the output from the CNN is presented to humans, we
ing the recent COVID-19 pandemic is prompting changes in developed a web-based user interface for comparing three forms of
health-care delivery and accelerating the adoption of telemedicine1. output from the CNN as decision support to human raters (Fig. 1).
AI-based triage and decision support could assist readers in man- The representations of AI that we selected derive from the litera-
aging workloads and expanding their performance. Most research ture and differ in key characteristics, including simplicity, granular-
to date has been predicated on head-to-head comparisons of the ity and concreteness. Because our task was a multiclass classification
diagnostic accuracy of AI-based systems with that of humans2–4. problem, one obvious approach was to provide AI-based multiclass
Similarly, recent studies in dermatology demonstrate that AI for probabilities. The second approach was motivated by solutions
selected lesions is equivalent or even superior to human experts already implemented in currently available AI-based support for
in image-based diagnosis under experimental conditions5–9. This skin cancer diagnosis6; we dichotomized the disease categories into
1ViDIR Group, Department of Dermatology, Medical University of Vienna, Vienna, Austria. 2Center for Medical Statistics, Informatics and Intelligent
Systems (CeMSIIS), Medical University of Vienna, Vienna, Austria. 3Department of Dermatology, Aristotle University of Thessaloniki, Thessaloniki,
Greece. 4Dermatology Unit, University of Campania, Naples, Italy. 5IBM T. J. Watson Research Center, New York, NY, USA. 6Dermatology Service,
Memorial Sloan Kettering Cancer Center, New York, NY, USA. 7Centre for Health Services Research, Faculty of Medicine, The University of Queensland,
Brisbane, Queensland, Australia. 8Dermatology Unit, University of Modena and Reggio Emilia, Modena, Italy. 9Centro Oncologico ad Alta Tecnologia
Diagnostica-Dermatologia, Azienda Unità Sanitaria Locale—IRCCS di Reggio Emilia, Reggio Emilia, Italy. 10Dermatology Department, Melanoma Unit,
Hospital Clínic de Barcelona, IDIBAPS, Universitat de Barcelona, Barcelona, Spain. 11Centro de Investigación Biomédica en Red de Enfermedades Raras
(CIBER ER), Instituto de Salud Carlos III, Barcelona, Spain. 12Department of Dermatology and Venereology, Institute of Clinical Sciences, Sahlgrenska
Academy, University of Gothenburg, Gothenburg, Sweden. 13Department of Dermatology and Venereology, Region Västra Götaland, Sahlgrenska University
Hospital, Gothenburg, Sweden. 14Faculty of Medicine, The University of Queensland, Brisbane, Queensland, Australia. 15Dermatology Research Centre, The
University of Queensland Diamantina Institute, The University of Queensland, Brisbane, Queensland, Australia. 16Department of Dermatology, Medical
University of Trieste, Trieste, Italy. 17These authors contributed equally: Philipp Tschandl, Christoph Rinner. ✉e-mail: harald.kittler@meduniwien.ac.at
NATuRe MedICINe | www.nature.com/naturemedicine
Letters NATuRE MEDICINE
AKIECBCC BKL DF MEL NV VASC
84%
Malignancy
80
AKIECBCC BKL DF MEL NV VASC
60
40
AKIECBCC BKL DF MEL NV VASC
20
0
AKIECBCC BKL DF MEL NV VASC 1 2 3 4 5 6 7 8 91011121314151617181920212223242526272829303132333435
Number of interactions
a benign and a malignant class and displayed the AI-predicted prob- to perform a biopsy or not, but not for a multiclass diagnostic
ability of malignancy. For the third and fundamentally different problem. The studied form of AI-based CBIR is neither simple nor
approach, we used the same CNN to implement a form of AI-based concrete; it needs more extensive cognitive engagement in terms of
CBIR that supports physicians in the interpretation of images by time and decision-making, because the rater needs to extrapolate
searching databases to retrieve similar images with known diag- the diagnosis from similarities between the test image and images
noses11,19,20. As an alternative to AI-based decision support, we also with known diagnoses. The raters needed significantly more time to
provided previously collected9 rating frequencies of 511 human rat- interact with AI-based CBIR decision support than with other types
ers for each disease category (crowd-based multiclass probabilities). of support (Fig. 1b). Over time, human raters also tended to ignore
Next, we invited human raters to participate in a reader study. the AI-based CBIR decision support (Fig. 1c). However, given that
A total of 302 raters from 41 countries participated, including 169 a large spectrum of CBIR approaches are described in the literature,
(56.0%) board-certified dermatologists, 77 (25.5%) dermatology another form of CBIR may still provide benefit. It has been shown
residents and 38 (12.6%) general practitioners. The raters’ task was that human-centered refinement tools improve the end user experi-
to diagnose batches of images, first without and then with one type ence of CBIR in pathology and increase trust and utility21. Future
of decision support. We recorded the time needed to reach a diag- work should, therefore, study a broader variety of layouts and com-
nosis, normalized this time over all individual ratings for each user binations of collaborations between AI and humans.
and interaction modality, and used this as a surrogate marker for After we established that multiclass probabilities were the best
confidence. form of CNN output for the given task, we focused on this form to
We collected 512 tests and 13,428 ratings. Our results show that explore the impact of AI-based support on human performance in
decision support with AI-based multiclass probabilities improves more detail. We show an inverse relationship between the net gain
the accuracy of human raters from 63.6% to 77.0% (increase of from AI-based support and rater experience (Pearson’s r = −0.18,
13.3%, 95% CI 11.5% to 15.2%; P = 4.9 × 10−35, two-sided paired 95% CI −0.28 to −0.07, P = 1.5 × 10−2; n = 302 raters). Raters in the
t-test, t = 14.5, d.f. = 301; n = 302 raters), but no improvement was least experienced group changed their initial diagnosis more often
observed for decision support with AI-based prediction of malig- than experts (mean 26.0%, 95% CI 21.3% to 30.7% versus mean
nancy or with our representation of AI-based CBIR (Fig. 2a–d and 14.7%, 95% CI 9.9% to 19.6%). Expert raters benefited only margin-
Supplementary Tables 1 and 2). ally (net gain 13.4%, 95% CI 6.3% to 20.6%) and only if they were not
This suggests that the form of decision support should be in confident with their initial diagnosis, but not if they were confident
accordance with the given task. The probability of malignancy may (−0.7%, 95% CI −6.8% to 5.4%; Fig. 2e,f). If experts were confident,
be useful for simple binary management decisions, such as whether they were usually correct and did not need support. This finding
deweiv
slianbmuht
fo
rebmuN
No interaction
Multiclass probabilities (AI)
Malignancy probability (AI)
CBIR (AI)
Multiclass probabilities (crowd)
0 40 80 120
Time to answer (s)
Proportion
of users (%)
100
80
60
40
20
0
ytiladom
noitcaretnI
a b
I
II
III
c
IV
Fig. 1 | Human interactions with four different types of support. a, Schematic overview of the interaction modalities offered: (I) AI-based multiclass
probabilities, (II) AI-based probability of malignancy, (III) AI-based CBIR and (IV) crowd-based multiclass probabilities. b, Raters needed significantly
more time to engage with CBIR support (n = 302 ratings; mean 16.5 s, 95% CI 14.5 to 18.6 s) than with multiclass probabilities (n = 302 ratings; mean
4.6 s, 95% CI 4.3 to 4.9 s; P = 2.6 × 10−24), malignancy probability (n = 301 ratings; mean 5.2 s, 95% CI 4.6 to 5.7 s; P = 1.0 × 10−22), crowd-based multiclass
probabilities (n = 301 ratings, mean 4.5 s, 95% CI 4.1 to 4.9 s; P = 2.0 × 10−25) or without support (n = 302 ratings; mean 5.6 s, 95% CI 5.4 to 5.8 s;
P = 4.5 × 10−22). All P values were derived from two-sided paired t-tests with Holm–Bonferroni correction for multiple comparisons. In the CBIR group,
one outlier of >200 s is not shown on the plot. The bars denote means, and error bars represent 95% CIs. c, The number of interactions with CBIR-based
support, as measured by enlarged thumbnails, is low and decreases further with the number of interactions, indicating that this type of support is not
appreciated over time.
NATuRe MedICINe | www.nature.com/naturemedicine
NATuRE MEDICINE Letters
Regular AI predictions Wrong AI predictions
100
50
0
−50
−100
n = 65n = 30n = 29n = 17n = 14 n = 63n = 29n = 29n = 17n = 14
<1
year
>1
year
>3
years
>5
years
>10
years
<1
year
>1
year
>3
years
>5
years
>10
years
Experience
suggests that, if experts have high confidence in their initial diagno- if their initial diagnosis was not in agreement with the top class
sis, they should ignore AI-based support or not use it at all. This sim- predicted by the CNN, the experts changed their initial diagnosis
ple heuristic corresponds to what we observed in our experiments; less often if they were confident (29.8%, 95% CI 14.1% to 45.4%)
)%(
noitcaretni
fo
tifeneB
AKIECBCCBKLDFMELNVVASC AKIECBCCBKLDFMELNVVASC
Multiclass
probabilities
(AI)
Malignancy
probability
(AI)
CBIR
(AI)
Multiclass
probabilities
(crowd)
100
50
0
−50
−100
100
50
0
−50
−100
100
50
0
−50
−100
100
50
0
−50
−100
0 25 50 75 100
Correct without interaction (%)
)%(
noitcaretni
fo
tifeneB
100
75
50
25
0
n = 685 n = 628
Possible benefit Possible loss Rater convinced Rater not convinced
)%(
)sisongaid
resu
laitini(
ytilibaborp
IA
−
)ssalc
pot
IA(
ytilibaborp
IA
Rater changed decision Rater did not change decision
2,000
1,500
1,000
500
0
1 2 3 4 5 6 7 1 2 3 4 5 6 7
AI-estimated rank of final rater decision
sgnitar
fo
rebmuN
Confident Not confident
100
50
0
−50
−100
n = 49 n = 45 n = 110 n = 83 n = 15 n = 48 n = 45 n = 110 n = 83 n = 15
<1 year >1 year >3 years >5 years >10 years <1 year >1 year >3 years >5 years >10 years
Experience
)%(
noitcaretni
fo
tifeneB
a e f
b
g
c
h
i
d
Fig. 2 | Gain from different types of decision support. a–d, AI-based multiclass probabilities (a), AI-based probability of malignancy (b), AI-based
CBIR (c) and crowd-based multiclass probabilities (d). In a multiclass classification problem, humans show a net gain from support by AI-based and
crowd-based multiclass probabilities but not from other less granular or less explicit types of decision support. e,f, Net gain with respect to the frequency
of correct diagnoses decreases with experience and confidence. Experts who are confident in a given diagnosis do not benefit from AI-based support.
Bars denote means, whiskers represent 95% CIs and dots denote individual raters. g, AI-estimated rank of a diagnosis for final rater decisions, grouped
by whether the rater changed their initial diagnosis. While changes occurred almost exclusively for the top class (class 1; left), a substantial number
of decisions remained unchanged in cases where the AI evaluated them as second or third ranked (right). h, When in disagreement with the top AI
predictions (class 1) before interaction, raters changed their opinion to these predictions if the AI multiclass probabilities were large. Bars denote means,
and error bars represent 95% CIs. i, Raters were susceptible to faulty AI-based support. The significant gain in accuracy (left, n = 155 raters; median 9.5%;
P = 1.2 × 10−12, two-sided paired Wilcoxon signed-rank test) turned into a significant loss (right, n = 155 raters; median −6.3%; P = 6.0 × 10−13, two-sided
paired Wilcoxon signed-rank test) when AI-based multiclass probabilities of the top predictions (class 1) were changed to a random incorrect answer.
Thick central lines denote the medians, lower and upper box limits denote the first and third quartiles and whiskers extend from the box to the outermost
extreme value but no further than 1.5 times the interquartile range (IQR).
NATuRe MedICINe | www.nature.com/naturemedicine
Letters NATuRE MEDICINE
and more often if they were not confident (53.9%, 95% CI 33.2 % recent findings in AI-based breast cancer screening3, our results
to 74.7%). The least experienced raters tended to accept AI-based indicate that AI-based skin cancer screening could triage high-risk
support that contradicted their initial diagnosis even if they were cases and extend the intervals between face-to-face visits in low-risk
confident. In general, raters changed their initial diagnosis less often cases. The optimal operating points to balance the potential benefits
if they were confident than if they were not confident in their deci- of AI-based triage with the risk of filtering out patients with skin
sion (14.7%, 95% CI 12.6% to 16.8% versus 37.5%, 95% CI 34.0% to cancer remain to be determined.
41.0%; P = 1.9 × 10−25, two-sided paired t-test; n = 302 raters). A possible explanation for the reasonably accurate perfor-
Having established a positive impact of good quality AI-based mance of the CNN as a tool for triage in telemedicine, despite the
support on diagnostic accuracy, we tested the impact of ‘faulty’ inclusion of non-pigmented skin lesions, is that pigmented and
AI on diagnostic accuracy. Faulty AI could result from the applica- non-pigmented variants of keratinocyte cancer share common
tion of AI algorithms on examples beyond the domain of images criteria. However, this cannot be guaranteed in other settings; the
on which the AI was trained7,9,22 or the more remote possibility results of the International Skin Imaging Collaboration (ISIC) 2019
of adversarial attacks23–25. To represent faulty AI, we intention- challenge, for example, demonstrated that AI does not work reliably
ally generated misleading AI-based multiclass probabilities. If the on out-of-distribution images28. Furthermore, we show that, within
top class probability of the CNN favored the correct diagnosis, the domain of pigmented skin lesions, AI-based support helps less
we switched the probabilities in such a way that the CNN output experienced raters to improve to the expert level in telemedicine
favored a random incorrect diagnosis. We demonstrate that any (Fig. 3c). Limitations of the telemedicine setting are that the sample
previously observed gains in accuracy with AI-based support turn did not include melanomas and the number of malignant cases was
into a loss when that AI support is faulty. Figure 2i shows that all relatively small.
groups of raters are susceptible to underperforming in this sce- In another scenario, we asked dermatologists to rethink their
nario. Our results suggest that, if raters build up the trust that is face-to-face decisions in suspicious cases after providing them with
necessary to benefit from AI-based support, they are also vulner- AI-based multiclass probabilities, but without making them aware
able to perform below their expected ability if there is a fault with that they had previously managed the patient. As shown in Fig. 3d,
the AI. Whether techniques to facilitate interpretability or explain- with AI-based support, dermatologists switched from ‘excision’ to
ability mitigate the risk of this negative impact remains an open ‘monitor’ in 15.5% (7 of 45) of decisions for benign lesions, without
topic of research21,26. increasing the number of malignant lesions that switched contrari-
Another finding of importance is that the benefit of human– wise. This result illustrates how human–computer collaboration
computer collaboration is asymmetrically distributed across disease could decrease the number of unwarranted interventions and costs.
categories. Our data showed that the net gain was higher for the AI-based support in this setting increased the frequency of correct
class of pigmented actinic keratoses and intraepithelial carcinoma specific diagnoses from 55.6% to 75.0% (P = 0.029, two-sided paired
(increase of 31.5%, 95% CI 22.9% to 40.1%; n = 43 images) than for Wilcoxon signed-rank test; n = 11 raters).
other categories (Supplementary Table 3). This suggests that the Finally, we demonstrate that explanations for AI-based pre-
benefit of AI-based support needs to be adapted to the given task dictions can be translated into a human-understandable visual
and the expected prevalence of target conditions. concept. In a previous study, we showed that misclassification of
We further demonstrate that AI-based multiclass ranking and pigmented actinic keratoses by humans is one reason for the supe-
probabilities have an impact on the raters’ tendency to change their riority of AI over human experts9. By analyzing gradient-weighted
initial diagnosis. Most changes occurred in favor of the AI-predicted class activation mapping (Grad-CAM29), we show that atten-
top category. Raters typically maintained their decisions that tion of the CNN outside the object is higher for the prediction
were in disagreement with the AI prediction only if that decision of actinic keratoses than for other categories (Extended Data
was ranked by AI prediction as at least the second or third option Fig. 1). Background attention30,31 is not necessarily a Clever Hans
(Fig. 2g). Furthermore, raters tended to change their assessments predictor32,33 but can be part of a valid general concept. Chronic
more frequently when the difference in the AI-predicted probabil- ultraviolet light damage causes actinic keratoses and is always
ity between the initially selected category and the AI top category present in the surrounding skin of actinic keratoses but not nec-
was high (Fig. 2h). This suggests that the distribution of class prob- essarily in other disease categories. We hypothesize that, due to
abilities affects the behavior of raters. Big winners and top-ranked visual entrenchment, humans focus on the lesion and not on the
classes are preferred to small winners, and categories with low prob- background and frequently miss this clue. Here we show that
abilities will barely affect the decision of raters. teaching medical students to pay attention to chronic sun damage
Additionally, we demonstrate that aggregated AI-based multi- in the background improved the frequency of correct diagnoses of
class probabilities and crowd wisdom significantly increased the pigmented actinic keratoses from 32.5% (95% CI 30.0% to 35.0%)
number of correct diagnoses in comparison to individual raters or to 47.3% (95% CI 43.9% to 50.8%; P = 3.6 × 10−13, two-sided paired
AI in isolation (Fig. 3a). The disadvantage of crowd wisdom is that t-test; n = 189 raters). The overall frequency of correct diagnoses
it is not readily and instantly available; in contrast to software, raters in all categories combined increased from 55.2% to 59.1% (mean
cannot be cloned. difference of 3.7%, 95% CI 2.4% to 5.3%; P = 3.4 × 10−6, two-sided
Next, we analyzed the impact of AI-based support in clinically paired t-test, t = 5.2, d.f. = 188; n = 189 raters; Fig. 3e).
relevant scenarios. To examine the potential of AI-based sup- This study examines human–computer collaboration from
port in telemedicine, we reused prospectively collected images of multiple angles and under varying conditions. We used the domain
a randomized controlled trial on self-examinations in high-risk of skin cancer recognition for simplicity, but our study could serve
patients27. Ninety-three participants submitted 1,521 self-made as a framework for similar research in image-based diagnostic
photographs of 596 suspicious lesions for telediagnosis. While the medicine. In contrast to the current narrative, our findings sug-
CNN was trained only on curated images of pigmented lesions, gest that the primary focus should shift from human–computer
this sample also included non-pigmented variants of keratinocyte competition to human–computer collaboration. From a regula-
cancer, mucosal lesions and low-quality images. Although the pro- tory perspective, the performance of AI-based systems should be
portion of correct specific diagnoses was significantly lower for tested under real-world conditions in the hands of the intended
these images (53.9% versus 76.2%; P = 8.9 × 10−14, chi-squared test; users and not as stand-alone devices. Only then can we expect to
n = 1,430 images), the CNN was able to recognize 95.2% of patients rationally adopt and improve AI-based decision support and to
with skin cancer at a specificity of 59.2% (Fig. 3b). Similarly to accelerate its evolution.
NATuRe MedICINe | www.nature.com/naturemedicine
NATuRE MEDICINE Letters
80
60
40
20
0
n = 189 n = 189
Before teaching After teaching
)%(
srewsna
cificeps
tcerroC
−20 0 20 40
Change in correct answers after teaching (%)
sesaC
Image level: sensitivity: 58.7%; specificity: 93.3% AI prediction 100
Truth
80 0 400 800 1,200
Number of images
Lesion level: sensitivity: 81.1%; specificity: 88.3% 60
AI prediction
40
Truth
0 200 400
20
Number of lesions
Patient level: sensitivity: 95.2%; specificity: 59.2%
AI prediction 0 n = 64 n = 64 n = 14
Truth
0 25 50 75
Number of patients
Malignant Benign
Benign cases
True
diagnosis
AKIEC
+ BCC
BKL
DF
Malignant cases
MEL
NV
VASC
+
0 10 20 30 40
Management decision Excise Monitor No intervention
)%(
srewsna
cificeps
tcerroC
100
80
60
40
20
0 n = 154 n = 154
)%(
srewsna
cificeps
tcerroC
AI top prediction accuracy
a
| Crowd wisdom
AKIECBCCBKL DF MEL NV VASC Single rater
Collective
AKIECBCCBKL DF MEL NV VASC
+ Collective + AI probabilities
AKIECBCC BKL DF MEL NV VASC
0 25 50 75 100
Mean correct rating of a case (%)
b c
| Telemedicine triage | Telemedicine support
P = 1.2 × 10–1 P = 1.9 × 10–13 P = 2.7 × 10–1
P = 2.3 × 10–4
All participants + <1 year + >10 years +
e
| Explainable AI-guided teaching
d P = 3.4 × 10–6
| Second opinion
Fig. 3 | Human–computer collaboration in different scenarios. a, Single human raters (top) achieve the lowest mean accuracy (64.8%, 95% CI 62.4%
to 67.3%; n = 600 images). Ratings of bootstrapped human collectives (middle) show significantly higher accuracy (73.7%, 95% CI 70.9% to 76.6%;
P = 1.5 × 10−35; n = 600 images), similar to the raw top class predictions of the CNN (blue line; 76.9%). The highest accuracy is achieved by combining
AI-based multiclass probabilities and human collectives (bottom), which is significantly higher than for collectives alone (81.0%, 95% CI 78.2% to 83.9%;
P = 8.6 × 10−9; n = 600 images). Bars denote means, whiskers represent 95% CIs and dots represent the mean correct rating of the corresponding group
of a single image; groups were compared using a two-sided paired Wilcoxon signed-rank test. b, Performance of CNN predictions used as a filter in a
screening setting of high-risk patients who provided self-made dermoscopic photographs of their skin lesions over 3 months. Top bars denote whether
the CNN predicted malignancy on an image, lesion or patient level, and bottom bars denote the corresponding ground truth. While the CNN shows low
sensitivity for single images, it detects the majority of skin cancer cases from multiple images (lesion level) and almost every patient with skin cancer
(patient level). c, Changes of raters’ decisions with AI-based support in a telemedical setting with dermoscopic images of pigmented lesions taken by
patients. P values were derived from two-sided paired t-tests with Holm–Bonferroni correction for multiple comparisons. Colored dots and whiskers
denote means and 95% CIs, and gray dots represent correct answers of raters. d, Switch of management decisions using CNN predictions as a second
opinion. Raters’ decisions before (top bar) and after (bottom bar) seeing CNN predictions are shown, grouped by ground truth. e, Change of correct
answers after explainable AI-guided teaching about chronic sun damage in the background of pigmented actinic keratoses. The overall percentage
of correct answers increased with teaching (left), mostly as a result of improved recognition of actinic keratoses (right). P values were derived from
two-sided paired t-tests with Holm–Bonferroni correction for multiple comparisons. Colored dots and whiskers denote means and 95% CIs, and gray dots
represent correct answers of raters.
NATuRe MedICINe | www.nature.com/naturemedicine
Letters NATuRE MEDICINE
Online content 15. Hekler, A. et al. Superior skin cancer classification by the combination of
Any methods, additional references, Nature Research report- human and artificial intelligence. Eur. J. Cancer 120, 114–121 (2019).
16. Lakhani, P. & Sundaram, B. Deep learning at chest radiography: automated
ing summaries, source data, extended data, supplementary infor-
classification of pulmonary tuberculosis by using convolutional neural
mation, acknowledgements, peer review information; details of networks. Radiology 284, 574–582 (2017).
author contributions and competing interests; and statements of 17. Tschandl, P., Rosendahl, C. & Kittler, H. The HAM10000 dataset, a large
data and code availability are available at https://doi.org/10.1038/ collection of multi-source dermatoscopic images of common pigmented skin
s41591-020-0942-0. lesions. Sci. Data 5, 180161 (2018).
18. Codella, N. et al. Skin lesion analysis toward melanoma detection 2018: a
challenge hosted by the International Skin Imaging Collaboration (ISIC).
Received: 26 September 2019; Accepted: 15 May 2020;
Preprint at https://arxiv.org/abs/1902.03368 (2019).
Published: xx xx xxxx 19. Sadeghi, M., Chilana, P. K. & Atkins, M. S. How users perceive content-based
image retrieval for identifying skin images. In Understanding and Interpreting
Machine Learning in Medical Image Computing Applications (eds., Kenji
References
Suzuki, Mauricio Reyes, Tanveer Syeda-Mahmood, ETH Zurich, Ben Glocker,
1. Webster, P. Virtual health care in the era of COVID-19. Lancet 395, Roland Wiest, Yaniv Gur, Hayit Greenspan, Anant Madabhushi) 141–148
1180–1181 (2020). (Springer International Publishing, 2018).
2. He, J. et al. The practical implementation of artificial intelligence technologies 20. Tschandl, P., Argenziano, G., Razmara, M. & Yap, J. Diagnostic accuracy of
in medicine. Nat. Med. 25, 30–36 (2019). content-based dermatoscopic image retrieval with deep classification features.
3. McKinney, S. M. et al. International evaluation of an AI system for breast Br. J. Dermatol. 181, 155–165 (2019).
cancer screening. Nature 577, 89–94 (2020). 21. Cai, C. J. et al. Human-centered tools for coping with imperfect algorithms
4. Gulshan, V. et al. Development and validation of a deep learning algorithm during medical decision-making. In Proc. 2019 CHI Conference on Human
for detection of diabetic retinopathy in retinal fundus photographs. JAMA Factors in Computing Systems 1–14 (Association for Computing Machinery,
316, 2402–2410 (2016). 2019).
5. Esteva, A. et al. Dermatologist-level classification of skin cancer with deep 22. Wang, M. & Deng, W. Deep visual domain adaptation: a survey.
neural networks. Nature 542, 115–118 (2017). Neurocomputing 312, 135–153 (2018).
6. Haenssle, H. A. et al. Man against machine: diagnostic performance of a deep 23. Finlayson, S.G. et al. Adversarial attacks on medical machine learning. Science
learning convolutional neural network for dermoscopic melanoma 363, 1287–1289 (2019).
recognition in comparison to 58 dermatologists. Ann. Oncol. 29, 24. Navarrete-Dechent, C. et al. Automated dermatological diagnosis: hype or
1836–1842 (2018). reality? J. Invest. Dermatol. 138, 2277–2279 (2018).
7. Han, S. S. et al. Classification of the clinical images for benign and malignant 25. Winkler, J. K. et al. Association between surgical skin markings in
cutaneous tumors using a deep learning algorithm. J. Invest. Dermatol. 138, dermoscopic images and diagnostic performance of a deep learning
1529–1538 (2018). convolutional neural network for melanoma recognition. JAMA Dermatol.
8. Marchetti, M. A. et al. Results of the 2016 International Skin Imaging 155, 1135–1141 (2019).
Collaboration International Symposium on Biomedical Imaging challenge: 26. Cai, C. J., Winter, S., Steiner, D., Wilcox, L. & Terry, M. ‘Hello AI’:
comparison of the accuracy of computer algorithms to dermatologists for the uncovering the onboarding needs of medical practitioners for human–AI
diagnosis of melanoma from dermoscopic images. J. Am. Acad. Dermatol. 78, collaborative decision-making. In Proc. ACM on Human–Computer
270–277 (2018). Interaction (Association for Computing Machinery, 2019).
9. Tschandl, P. et al. Comparison of the accuracy of human readers versus 27. Janda, M. et al. Accuracy of mobile digital teledermoscopy for skin
machine-learning algorithms for pigmented skin lesion classification: an self-examinations in adults at high risk of skin cancer: an open-label,
open, web-based, international, diagnostic study. Lancet Oncol. 20, randomised controlled trial. Lancet Digit. Health 2, e129–e137 (2020).
938–947 (2019). 28. Gessert, N., Nielsen, M., Shaikh, M., Werner, R. & Schlaefer, A. Skin lesion
10. Garg, A. X. et al. Effects of computerized clinical decision support systems on classification using ensembles of multi-resolution EfficientNets with meta
practitioner performance and patient outcomes: a systematic review. JAMA data. MethodsX 7, 100864 (2020).
293, 1223–1238 (2005). 29. Selvaraju, R. R. et al. Grad-CAM: visual explanations from deep networks via
11. Codella, N. C. F. et al. Collaborative human–AI (CHAI): evidence-based gradient-based localization. Int. J. Comput. Vis. 128, 336–359 (2020).
interpretable melanoma classification in dermoscopic images. In 30. Li, X., Wu, J., Chen, E. Z. & Jiang, H. From deep learning towards finding
Understanding and Interpreting Machine Learning in Medical Image skin lesion biomarkers. Conf. Proc. IEEE Eng. Med. Biol. Soc. 2019,
Computing Applications (eds., Kenji Suzuki, Mauricio Reyes, Tanveer 2797–2800 (2019).
Syeda-Mahmood, ETH Zurich, Ben Glocker, Roland Wiest, Yaniv Gur, 31. Bissoto, A., Fornaciali, M., Valle, E. & Avila, S. (De)constructing bias on skin
Hayit Greenspan, Anant Madabhushi) 97–105 (Springer International lesion datasets. Preprint at https://arxiv.org/abs/1904.08818 (2019).
Publishing, 2018). 32. Lapuschkin, S. et al. Unmasking Clever Hans predictors and assessing what
12. Bien, N. et al. Deep-learning-assisted diagnosis for knee magnetic resonance machines really learn. Nat. Commun. 10, 1096 (2019).
imaging: development and retrospective validation of MRNet. PLoS Med. 15, 33. Samek, W., Montavon, G., Vedaldi, A., Hansen, L. K. & Müller, K.-R.
e1002699 (2018). Explainable AI: Interpreting, Explaining and Visualizing Deep Learning
13. Mobiny, A., Singh, A. & Van Nguyen, H. Risk-aware machine learning (Springer Nature, 2019).
classifier for skin lesion diagnosis. J. Clin. Med. 8, 1241 (2019).
14. Han, S. S. et al. Augment intelligence dermatology: deep neural networks
empower medical professionals in diagnosing skin cancer and predicting Publisher’s note Springer Nature remains neutral with regard to jurisdictional claims in
treatment options for 134 skin disorders. J. Invest. Dermatol. https://doi. published maps and institutional affiliations.
org/10.1016/j.jid.2020.01.019 (2020). © The Author(s), under exclusive licence to Springer Nature America, Inc. 2020
NATuRe MedICINe | www.nature.com/naturemedicine
NATuRE MEDICINE Letters
Methods sun damage. Routine pathology evaluation (n = 786), biology (that is, >1.5 years
Network training. We fine-tuned a CNN for classification of seven different of sequential dermoscopic imaging without changes; n = 458), expert consensus in
categories of the HAM10000 dataset17. We performed training on NVIDIA common, straightforward, non-melanocytic cases that were not excised (n = 260)
graphics processing units (GPUs) using the Pytorch34 framework and chose a and in vivo confocal images (n = 7) served as the ground truth. Controversial cases
ResNet34 (ref. 35) architecture, with weights initiated by pretraining on ImageNet36 with ambiguous histopathologic reports were excluded. Due to random sampling,
data. Cross-entropy served as the loss function, with weighting dependent on the only 1,412 of 1,511 images were finally used and evaluated by the raters. The 1,412
frequency of classes within the dataset. The learning rate was initialized at 0.0001, used cases consisted of 43 AKIECs, 93 BCCs, 217 BKLs, 44 DFs, 171 MELs,
with a tenfold reduction in case of no validation loss improvement for more than 809 NVs and 35 VASCs.
three epochs, but a minimum of 1 × 10−9. We used adaptive moment estimation For the telemedicine study, we included 93 of 98 participants (mean age
(Adam37) as the optimizer and performed a maximum of 100 training epochs 41.1 years (s.d. 12.2 years); 71% female) from the intervention arm of a recently
with early stopping. Images were presented in batches of 32, randomly cropped conducted prospective randomized study27 on mobile teledermoscopy for skin
and resized to 224 × 224 pixels without normalization of a mean pixel, randomly self-examinations. All 93 patients permitted reuse of their images. The participants
rotated by 90 degrees and flipped with minor jitter in color, contrast, saturation had at least two skin cancer risk factors (light skin complexion and fair hair; skin
and hue. that never or rarely tans and always or mostly burns; a family history of melanoma
The publicly available HAM10000 dataset, which corresponds to the training or a personal history of skin cancer, or many nevi; and residing in Queensland) as
set of the ISIC 2018 challenge18, was the source of images used for training and self-reported in the eligibility survey. A teledermoscopic evaluation was performed
fivefold cross-validation. We selected the single best performing network on for all lesions. Face-to-face examination by an experienced board-certified
the hold-out validation set for further interaction with raters. For inference, dermatologist (H.P.S.) or the histopathologic report, in cases where the lesion was
images were cropped to 80% and resized to 224 × 224 pixels, with minor test-time removed, served as the ground truth. The set of lesions consisted of 1,521 images
augmentation consisting of horizontal flipping and rotation by 0 or 90 degrees. For of 596 lesions, including 29 AKIECs, 6 BCCs, 102 BKLs, 410 NVs, 2 squamous
the telemedicine dataset, we also applied color normalization via Shades of Gray38 cell carcinomas (SCCs) and 9 VASCs. For calculation of diagnostic values,
with a Minkowski norm of 6. The multiclass probabilities presented to the raters ground-truth data were mapped to classes of the HAM10000 dataset, if possible.
were obtained by applying a softmax function to contain all class probabilities We excluded nonspecific categories (n = 38 lesions) such as ‘other’, ‘no lesion’ or
between 0–100%. To find similar images, we used the same CNN to extract the ‘previously removed’, because they could be mapped to neither the ‘benign’ nor
feature vector of the target image and compared it to feature vectors of images in ‘malignant’ category. The sample also included images that were not represented in
the HAM10000 dataset via cosine similarity20. We stored the four closest images of the training data (non-pigmented variants of keratinocyte cancers, mucosal lesions
each class and presented them in the AI-based CBIR decision support. and low-quality images), which were excluded from the telemedicine support study
but not from the triage study, to better simulate a realistic scenario.
Interaction platform and raters. Online interaction platform. The web-based For the second-opinion study, we searched the database of the Department of
platform DermaChallenge, which was developed at the Medical University of Dermatology at the Medical University of Vienna for dermoscopy images taken
Vienna, served as the interface through which the performance of human raters between April and September 2019. We included images if the lesion was excised
and AI for the diagnostic task was evaluated and quantified. The platform is split and had a definite histopathologic diagnosis and if lesions were examined by a
into a back end and a front end, and both are deployed on a stack of well-known physician who was responsible for the face-to-face diagnosis of at least two other
web technologies (Linux, Apache, MySQL and PHP). Please refer to the Nature cases in this time period. The final sample set (n = 79) included 3 AKIECs, 23
Research Reporting Summary for details of the specific software versions used. The BCCs, 13 BKLs, 2 DFs, 15 MELs, 21 NVs, 1 ‘other’ (scar) and 1 SCC. The mean
back end offers a representational state transfer interface to load and persist data, age of patients was 64.6 years (s.d. 19.8 years), and 34.5% of patients were female.
as well as JavaScript Object Notation web tokens to authenticate participants. The Patients were mainly of European ancestry and had skin type II (41.7%), III (57.1%)
transport layer security and secure sockets layer protocol are used to encrypt all or IV (1.2%). As in the telemedicine scenario, we did not exclude images of
communications. The front end is optimized for mobile devices (mobile phones categories that were not present in the training data or images of low quality.
and tablets) but can also be used on any other platform via a JavaScript-enabled For the knowledge transfer study, the sample cases (n = 25) were randomly
web browser. Before public deployment, five users tested the platform. selected from the ISIC 2018 test set and stratified by diagnosis (6 AKIECs, 3 BCCs,
3 BKLs, 3 DFs, 3 MELs, 4 NVs and 3 VASCs).
Recruitment and characteristics of raters. We used mailing lists and social media
posts of the International Society of Dermoscopy to recruit online raters. To Design of diagnostic studies. To test the interaction of raters with different forms
participate in the study, raters had to register with a username, valid email of AI-based decision support, we generated batches of 28 images. Each batch
address and password. In addition, we asked raters for details on their age (age contained four randomly selected examples of every class. The raters’ task was
groups spanning 10 years), gender, country, profession and years of experience to diagnose the 28 unknown test images, first without and then with one type
in dermatoscopy ((1) less than 1 year, (2) opportunistic use for more than 1 year, of decision support. We created a stratified randomization procedure to ensure
(3) regular use for 1 to 5 years, (4) regular use for more than 5 years or (5) more a balanced distribution of the four types of decision support over all disease
than 10 years of experience). Each rater had to perform multiple screening tests categories. The interaction study was online from 29 May 2019 to 15 January 2020.
to ensure that the self-reported experience matched actual skills. Screening tests We excluded tests if the number of correct answers was lower than expected by
consisted of simple domain-specific tasks, for example, to assign one of the seven chance to avoid noisy random data. We included only the first five tests for each
possible diagnoses to ten cases, to separate melanomas from non-melanomas and rater to avoid biasing the results toward raters with high repetitions.
to separate seborrheic keratoses from other lesions. We recruited 302 raters for The extended interaction study was open for participation between 15
the first interaction study that screened different forms of AI-based support, and January 2020 and 18 February 2020, presenting only multiclass probabilities as
155 raters were recruited for the extended interaction study (inclusion of images decision support. It included one image for every diagnosis from the ISIC 2018
with faulty AI-based support) and the telemedicine study (Supplementary Table 4). test set with unaltered AI-based multiclass probabilities, two images with shuffled
The distribution of raters according to task is presented in Supplementary Table 4. (resulting as incorrect) AI-based multiclass probabilities and eight images from the
Second-opinion raters consisted of eight board-certified dermatologists and three telemedicine study (see ‘Characteristics of images and patients’). The image sources
dermatology residents, who were recruited because they diagnosed and managed were not disclosed to the raters.
more than two suspicious skin lesions on a face-to-face basis between April The second-opinion study was performed on a local web interface. Physicians
and September 2019. For the knowledge transfer study, we invited fourth-year who examined the patient face to face in real life were asked to reconsider their
medical students to participate; of the 650 medical students invited, 200 agreed to diagnosis and decisions with AI-based support. The case presentations included
participate and 189 answered more than 50% of the test questions. metadata (age, gender and localization), overview and close-up images (if
available) and dermoscopic images. Physicians were not made aware that they had
Characteristics of images and patients. The benchmark test set of the ISIC treated the patient before or of their previous decision on the case. Physicians were
2018 challenge served as the sample for the interaction studies9. Of the 1,511 asked to provide their best diagnosis out of the seven predefined disease categories,
dermoscopic images in this set, 928 images were collected in the Department as well as an extra category termed ‘other’, followed by their management decision
of Dermatology at the Medical University of Vienna, 267 images were collected (‘no intervention’, ‘monitor’ or ‘excise’). No time constraints were set for this task.
in the skin cancer practice of Cliff Rosendahl in Queensland and the remaining For the knowledge transfer study, we first examined the gradient-weighted
316 images were collected in other centers in Turkey (n = 117), New Zealand class-activation maps29, which were created for all images of the training set. We
(n = 87), Sweden (n = 92) and Argentina (n = 20), to ensure diversity of skin types. observed that the background attention of the CNN was significantly higher for
The mean age of patients was 50.8 years (s.d. 17.4 years), and 46.2% of patients predictions of the ‘pigmented actinic keratosis’ class than for other classes
were female. The Austrian image set consists of lesions from patients referred (P = 4.6 × 10−12, two-sided unpaired t-test; Extended Data Fig. 2). We interpreted
to a tertiary European center specializing in the early detection of melanoma in this finding as a diagnostic clue that points to the severely sun-damaged skin in
high-risk groups. This group of patients is mainly of European ancestry and have the background of actinic keratoses, which is usually absent or not as severe in
a large number of nevi and skin types I–III. The Australian image set includes other disease categories. To test the hypothesis that teaching this clue to humans
lesions from patients of a primary-care facility in an area with a high incidence of will improve their diagnostic skills, fourth-year medical students without previous
skin cancer. Patients are typified by Celtic complexion, skin type I or II and chronic knowledge of skin cancer detection received a 30-min introductory lecture
NATuRe MedICINe | www.nature.com/naturemedicine
Letters NATuRE MEDICINE
about dermoscopy, and immediately thereafter students had to diagnose 25 test 36. Russakovsky, O. et al. ImageNet large scale visual recognition challenge.
images (single best diagnosis). Answers were collected with a wireless audience Int. J. Comput. Vis. 115, 211–252 (2015).
response and voting system. Next, the lecturer presented an additional clue of 37. Kingma, D. P. & Ba, J. Adam: a method for stochastic optimization.
‘sun-damaged skin in the surrounding skin of actinic keratoses’ and the students In Proc. 3rd International Conference for Learning Representations
repeated the test. (eds., Bengio, Y., LeCun, Y.) (2015).
38. Barata, C., Celebi, M. E. & Marques, J. S. Improving dermoscopy image
Statistics. To simulate collective ratings of realistically small human groups classification using color constancy. IEEE J. Biomed. Health Inform. 19,
(Fig. 3a), we confined the dataset to images with at least three distinct ratings 1146–1152 (2015).
(resulting range of ratings per image: 3–69). For each image, we created 30 39. Rinner, C., Kittler, H., Rosendahl, C. & Tschandl, P. Analysis of collective
bootstraps of three to five randomly selected ratings, whichever was the human intelligence for diagnosis of pigmented skin lesions harnessed by
maximum available without replacement, and determined the most common gamification via a web-based training platform: simulation reader study.
rating as the prediction of the collective (first past the post). Ties were J. Med. Internet Res. 22, e15597 (2020).
broken randomly. Next, we calculated the proportion of correct bootstrapped 40. Holm, S. A simple sequentially rejective multiple test procedure. Scand. J.
predictions to obtain the mean accuracy for each image as published Statist. 6, 65–70 (1979).
previously39. To combine human collectives with CNN-based predictions, we 41. R Core Team. R: A Language and Environment for Statistical Computing
took the arithmetic mean of the sum of the human multiclass probabilities, (R Foundation for Statistical Computing, 2019).
which were derived from the frequencies of bootstrapped human ratings, and the 42. Wickham, H. ggplot2: Elegant Graphics for Data Analysis (Springer-Verlag,
corresponding CNN-based multiclass probabilities. For analyses of diagnoses, 2016).
we averaged the results for each image before comparisons; for analyses of
raters with and without decision support, we calculated the arithmetic mean for Acknowledgements
each user before comparisons. The mean answering time for each user in every
We thank all dermachallenge.com users for their participation. We gratefully
interaction modality served as a surrogate marker for confidence; answers that
acknowledge the support of NVIDIA Corporation with the donation of the Titan Xp
were faster or slower than the individual mean were regarded as ‘confident’ or
GPU used for this research.
‘non-confident’, respectively.
For the filtering procedure in the telemedicine study, we used a predefined
Author contributions
cutoff of ≥0.17 to indicate malignancy, because this cutoff was selected by human
raters in the interaction study (Extended Data Fig. 2). If patients photographed P.T., C.R. and H.K. conceived and designed experiments. P.T. trained the CNN and
a lesion more than once, a single image above the cutoff was sufficient to produced image predictions. C.R. and P.T. developed the web-based reader platforms
label the lesion as ‘probably malignant’ and likewise on the patient level. We together with H.K., and M.J. and H.P.S. provided data for the telemedicine study. H.K.
used a one-sample t-test to distinguish whether continuous data with normal and P.T. conducted statistical analyses. H.K., P.T., C.R., Z.A., G.A., N.C., A.H., M.J., A.L.,
distributions deviated from zero. Comparisons of continuous data between groups C.L., J.M., J.P., S.P., C.R., H.P.S. and I.Z. helped collect rater data and interpreted findings.
were performed with paired or unpaired t-tests or Wilcoxon signed-rank test, as H.K., P.T., C.R., N.C. and A.H. wrote the manuscript with input from all authors.
appropriate. A chi-squared test was used to compare proportions. All reported P
values were corrected for multiple testing (Holm–Bonferroni40), and a two-sided P Competing interests
value < 0.05 was regarded as statistically significant. All analyses were performed The authors declare the following competing interests: P.T. received fees from Silverchair
using R v3.6.2 (ref. 41), and plots were created with ggplot v3.2.1 (ref. 42) and and an unrestricted 1-year postdoc grant from MetaOptima Technology. N.C. is an IBM
ggalluvial v0.11.1. employee and owns diverse investments across technology and health-care companies.
A.H. is a consultant to Canfield Scientific and an advisory board member of Scibase.
Reporting Summary. Further information on research design is available in the M.J. is funded by a National Health and Medical Research Council (NHMRC) TRIP
Nature Research Reporting Summary linked to this article. Fellowship (APP1151021). H.P.S. is a shareholder of MoleMap and e-derm-consult
and undertakes regular teledermatological reporting for both companies, is a medical
data availability consultant for Canfield Scientific and Revenio Research Oy and a medical advisor
The origin of training set images is reported in the dataset publication of for First Derm and MetaOptima Technology, and has a Medical Advisory Board
HAM10000 (https://doi.org/10.1038/sdata.2018.161)17. Training set and test set Appointment with MoleMap. H.P.S. holds an Australian NHMRC Practitioner
images with reader data are available from the Harvard Dataverse (https://doi. Fellowship (APP1137127). All other authors report no conflict of interest in the topic
org/10.7910/DVN/DBW86T), where lesion segmentation data will be available of this manuscript. This project was conducted after ethical committee review at the
upon publication of this work. Test-set images with ground truth are available upon Medical University of Vienna under protocol numbers 1804/2017, 1503/2018 and
request from the ISIC archive (challenge@isic-archive.com). 2308/2019. All participants of the study platform agreed to academic research of usage
data upon registration and have continuous ability to withdraw that consent.
Code availability
Code for the CNN is available upon request from the corresponding author for Additional information
academic use.
Extended data is available for this paper at https://doi.org/10.1038/s41591-020-0942-0.
Supplementary information is available for this paper at https://doi.org/10.1038/
References
s41591-020-0942-0.
34. Paszke, A. et al. PyTorch: an imperative style, high-performance deep
Correspondence and requests for materials should be addressed to H.K.
learning library. In Advances in Neural Information Processing Systems 32
(eds. Wallach, H. et al.) 8026–8037 (Curran Associates, 2019). Peer review information Javier Carmona was the primary editor on this article and
35. He, K., Zhang, X., Ren, S. & Sun, J. Deep residual learning for image managed its editorial process and peer review in collaboration with the rest of the
recognition. In Proc. IEEE Conference on Computer Vision and Pattern editorial team.
Recognition 770–778 (2016). Reprints and permissions information is available at www.nature.com/reprints.
NATuRe MedICINe | www.nature.com/naturemedicine
NATuRE MEDICINE Letters
Extended Data Fig. 1 | The neural network puts more relative attention to the non-lesion background in actinic keratoses. Gradient-weighted Class
Activation Maps (Grad-CAM, right column) for the top-1 prediction class of the CNN were created for all HAM10000 images, a sample for every
ground-truth class is shown in the left column. The mean activation value per pixel of background- and lesion-area were estimated using manual
segmentation masks (middle column). The quotient of background over lesion activation showed higher background activation for the predictions of the
class AKIEC class versus all other classes (mean .48 vs. .32, p = 4.6 × 10−12, two-sided unpaired t-test). Thick central lines denote the median, lower and
upper box limits the first and third quartiles, whiskers extend from the box to the most extreme value not further than 1.5 times the IQR.
NATuRe MedICINe | www.nature.com/naturemedicine
Letters NATuRE MEDICINE
Extended Data Fig. 2 | Raters choose an asymmetric decision cutoff for malignancy. a, When changing answers from benign to malignant (dark blue) or
malignant to benign (light blue) diagnoses, the average cutoff for the AI-provided malignancy-probability was not 50% but <25% (yellow dotted line).
b, On the ROC-curve for detecting malignant cases of the underlying AI (black line), this cutoff chosen inherently by the users (yellow dot), that is without
instructions or prior knowledge about the AI accuracy, had a higher sensitivity and was closer to the ideal cutoff (blue dot), as measured by Youden’s
index, than the ‘symmetric’ 50% cutoff (black dot).
NATuRe MedICINe | www.nature.com/naturemedicine

2
nature
research
|
reporting
summary
April
2020
corresponding author upon reasonable request. The code for the CNN is available upon request from the corresponding author HK for academic use. The ground-
truth data for the test-cases is currently not public as they are used in a continued "live-challenge" for continued analyses of research groups on https://
challenge2018.isic-archive.com/live-leaderboards/ but will be made available upon publication of the study.
Field-specific reporting
Please select the one below that is the best fit for your research. If you are not sure, read the appropriate sections before making your selection.
Life sciences Behavioural & social sciences Ecological, evolutionary & environmental sciences
For a reference copy of the document with all sections, see nature.com/documents/nr-reporting-summary-flat.pdf
Behavioural & social sciences study design
All studies must disclose on these points even when the disclosure is negative.
Study description Quantitative experimental research, quantitative data
Research sample Participants are users of a public open reader platform. The raters varied according to experiment. The main experiment included
302 raters. A detailed description of raters according to experiment is given in supplementary table 4. Raters were recruited from the
International Dermoscopy Society (online experiments); forth-year medical students were recruited from the Medical University of
Vienna, Austria (knowledge transfer experiment), and for the second opinion experiment we recruited staff physicians from the
Department of Dermatology at the Medical University of Vienna, Austria. Origin of training and test set images is reported in the
dataset-publication of HAM10000 in Nature Scientific Data (doi: 10.1038/sdata.2018.161) and The Lancet Oncology (doi: 10.1016/
S1470-2045(19)30333-X) and origin of the samples used for telemedicine studies are reported in The Lancet Digital Health
(doi.org/10.1016/S2589-7500(20)30001-7)
Sampling strategy Data collection for the interaction experiments was performed prospectively, and is continously ongoing. Based on findings in a
previous study (doi: 10.1016/S1470-2045(19)30333-X), a Cohen's D of ~0.255 was measured as an effect size to differentiate
between the accuracy of average raters and experts. Given this effect size and a power of 0.8 at a two-sided p-value <.05, more than
122 participants would need to be available for analysis. We finally recruited 302 raters.
Data collection Data was recorded in a web-based training and study platform hosted by the Medical University of Vienna. The researchers were not
present during interaction experiments, as raters could participate on any device (including smartphones, tablets, laptops and
desktop computers) with internet connection and a JavaScript-enabled browser. Participants were asked to use the different
presented methods of interaction based on an AI-model, but no further information was given on specific study hypotheses. The
second opinion experiment was performed on a separate desk top computer without supervision and time constraints. The
researcher was not present during the experiment. The knowledge transfer was conducted in a lecture room with Digivote Wireless
Audience Response & Voting Systems. The researcher, who was not blinded with regard to study hyopthesis and experimental
conditions, was present during the experiment but did not interact with participants during voting.
Timing Interaction study was open from May 29th 2019 to January 15th, 2020 without any gaps, and the extended interaction study from
January 15th to February, 18th, 2020
Data exclusions We excluded tests if the number of correct answers was lower than expected by chance to avoid noisy random data. We included
only the first 5 tests per rater to avoid biasing results towards raters with high repetitions.
Non-participation No participants dropped out.
Randomization Participants were not allocated into experimental groups. For each experiment, images were randomly selected as described in the
Methods section
Reporting for specific materials, systems and methods
We require information from authors about some types of materials, experimental systems and methods used in many studies. Here, indicate whether each material,
system or method listed is relevant to your study. If you are not sure if a list item applies to your research, read the appropriate section before selecting a response.
3
nature
research
|
reporting
summary
April
2020
Materials & experimental systems Methods
n/a Involved in the study n/a Involved in the study
Antibodies ChIP-seq
Eukaryotic cell lines Flow cytometry
Palaeontology and archaeology MRI-based neuroimaging
Animals and other organisms
Human research participants
Clinical data
Dual use research of concern
Human research participants
Policy information about studies involving human research participants
Population characteristics We recruited 302 participants (61.9% female ) for interaction experiments including board-certified dermatologist (56.0%),
dermatology resident (25.5%) and general practitioner (12.6%). Age groups of participants were 20-30 years,21.5% (n=65), -
31-40 years, 35.8% (n=108), 1-50 years, 26.8% (n=81), 51-60 years, 12.3% (n=37), 61+ years, 3.6% (n=11). Self-declared
years of experience were 16.2% <1 year, 14.9% 1-3 years, 36.4% 3-5 years, 27.5% 5-10 years, and 5% >10 years. A detailed
description of raters according to experiment is given in supplementary table 4.
Recruitment Mailings and social media posts of the International Dermoscopy Society were used to recruit targeted groups. The
recruitment was focused on medical personell interested in the diagnosis of skin cancer. It is possible that recruitment of
raters is influenced by self-selection bias and therefore biased towards the selection of motivated and skilled raters. Skill
level was included as a covariate in the interaction experiments. Each rater had to perform multiple screening tests to
ensure that the self-reported experience matched actual skills. Because of self selection bias, the generalisability of our
results to a less motivated group of readers may be limited.
Ethics oversight Ethics review board of the Medical University of Vienna
Note that full information on the approval of the study protocol must also be provided in the manuscript.
